+ source /opt/miniconda3/bin/activate
++ _CONDA_ROOT=/opt/miniconda3
++ . /opt/miniconda3/etc/profile.d/conda.sh
+++ export CONDA_EXE=/opt/miniconda3/bin/conda
+++ CONDA_EXE=/opt/miniconda3/bin/conda
+++ export _CE_M=
+++ _CE_M=
+++ export _CE_CONDA=
+++ _CE_CONDA=
+++ export CONDA_PYTHON_EXE=/opt/miniconda3/bin/python
+++ CONDA_PYTHON_EXE=/opt/miniconda3/bin/python
+++ '[' -z '' ']'
+++ export CONDA_SHLVL=0
+++ CONDA_SHLVL=0
+++ '[' -n '' ']'
+++++ dirname /opt/miniconda3/bin/conda
++++ dirname /opt/miniconda3/bin
+++ PATH=/opt/miniconda3/condabin:/opt/miniconda3/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin
+++ export PATH
+++ '[' -z '' ']'
+++ PS1=
++ conda activate
++ local cmd=activate
++ case "$cmd" in
++ __conda_activate activate
++ '[' -n '' ']'
++ local ask_conda
+++ PS1=
+++ __conda_exe shell.posix activate
+++ /opt/miniconda3/bin/conda shell.posix activate
++ ask_conda='PS1='\''(base) '\''
export PATH='\''/opt/miniconda3/bin:/opt/miniconda3/condabin:/opt/miniconda3/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin'\''
export CONDA_PREFIX='\''/opt/miniconda3'\''
export CONDA_SHLVL='\''1'\''
export CONDA_DEFAULT_ENV='\''base'\''
export CONDA_PROMPT_MODIFIER='\''(base) '\''
export CONDA_EXE='\''/opt/miniconda3/bin/conda'\''
export _CE_M='\'''\''
export _CE_CONDA='\'''\''
export CONDA_PYTHON_EXE='\''/opt/miniconda3/bin/python'\'''
++ eval 'PS1='\''(base) '\''
export PATH='\''/opt/miniconda3/bin:/opt/miniconda3/condabin:/opt/miniconda3/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin'\''
export CONDA_PREFIX='\''/opt/miniconda3'\''
export CONDA_SHLVL='\''1'\''
export CONDA_DEFAULT_ENV='\''base'\''
export CONDA_PROMPT_MODIFIER='\''(base) '\''
export CONDA_EXE='\''/opt/miniconda3/bin/conda'\''
export _CE_M='\'''\''
export _CE_CONDA='\'''\''
export CONDA_PYTHON_EXE='\''/opt/miniconda3/bin/python'\'''
+++ PS1='(base) '
+++ export PATH=/opt/miniconda3/bin:/opt/miniconda3/condabin:/opt/miniconda3/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin
+++ PATH=/opt/miniconda3/bin:/opt/miniconda3/condabin:/opt/miniconda3/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin
+++ export CONDA_PREFIX=/opt/miniconda3
+++ CONDA_PREFIX=/opt/miniconda3
+++ export CONDA_SHLVL=1
+++ CONDA_SHLVL=1
+++ export CONDA_DEFAULT_ENV=base
+++ CONDA_DEFAULT_ENV=base
+++ export 'CONDA_PROMPT_MODIFIER=(base) '
+++ CONDA_PROMPT_MODIFIER='(base) '
+++ export CONDA_EXE=/opt/miniconda3/bin/conda
+++ CONDA_EXE=/opt/miniconda3/bin/conda
+++ export _CE_M=
+++ _CE_M=
+++ export _CE_CONDA=
+++ _CE_CONDA=
+++ export CONDA_PYTHON_EXE=/opt/miniconda3/bin/python
+++ CONDA_PYTHON_EXE=/opt/miniconda3/bin/python
++ __conda_hashr
++ '[' -n '' ']'
++ '[' -n '' ']'
++ hash -r
+ conda activate testbed
+ local cmd=activate
+ case "$cmd" in
+ __conda_activate activate testbed
+ '[' -n '' ']'
+ local ask_conda
++ PS1='(base) '
++ __conda_exe shell.posix activate testbed
++ /opt/miniconda3/bin/conda shell.posix activate testbed
+ ask_conda='PS1='\''(testbed) '\''
export PATH='\''/opt/miniconda3/envs/testbed/bin:/opt/miniconda3/condabin:/opt/miniconda3/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin'\''
export CONDA_PREFIX='\''/opt/miniconda3/envs/testbed'\''
export CONDA_SHLVL='\''2'\''
export CONDA_DEFAULT_ENV='\''testbed'\''
export CONDA_PROMPT_MODIFIER='\''(testbed) '\''
export CONDA_PREFIX_1='\''/opt/miniconda3'\''
export CONDA_EXE='\''/opt/miniconda3/bin/conda'\''
export _CE_M='\'''\''
export _CE_CONDA='\'''\''
export CONDA_PYTHON_EXE='\''/opt/miniconda3/bin/python'\'''
+ eval 'PS1='\''(testbed) '\''
export PATH='\''/opt/miniconda3/envs/testbed/bin:/opt/miniconda3/condabin:/opt/miniconda3/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin'\''
export CONDA_PREFIX='\''/opt/miniconda3/envs/testbed'\''
export CONDA_SHLVL='\''2'\''
export CONDA_DEFAULT_ENV='\''testbed'\''
export CONDA_PROMPT_MODIFIER='\''(testbed) '\''
export CONDA_PREFIX_1='\''/opt/miniconda3'\''
export CONDA_EXE='\''/opt/miniconda3/bin/conda'\''
export _CE_M='\'''\''
export _CE_CONDA='\'''\''
export CONDA_PYTHON_EXE='\''/opt/miniconda3/bin/python'\'''
++ PS1='(testbed) '
++ export PATH=/opt/miniconda3/envs/testbed/bin:/opt/miniconda3/condabin:/opt/miniconda3/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin
++ PATH=/opt/miniconda3/envs/testbed/bin:/opt/miniconda3/condabin:/opt/miniconda3/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin
++ export CONDA_PREFIX=/opt/miniconda3/envs/testbed
++ CONDA_PREFIX=/opt/miniconda3/envs/testbed
++ export CONDA_SHLVL=2
++ CONDA_SHLVL=2
++ export CONDA_DEFAULT_ENV=testbed
++ CONDA_DEFAULT_ENV=testbed
++ export 'CONDA_PROMPT_MODIFIER=(testbed) '
++ CONDA_PROMPT_MODIFIER='(testbed) '
++ export CONDA_PREFIX_1=/opt/miniconda3
++ CONDA_PREFIX_1=/opt/miniconda3
++ export CONDA_EXE=/opt/miniconda3/bin/conda
++ CONDA_EXE=/opt/miniconda3/bin/conda
++ export _CE_M=
++ _CE_M=
++ export _CE_CONDA=
++ _CE_CONDA=
++ export CONDA_PYTHON_EXE=/opt/miniconda3/bin/python
++ CONDA_PYTHON_EXE=/opt/miniconda3/bin/python
+ __conda_hashr
+ '[' -n '' ']'
+ '[' -n '' ']'
+ hash -r
+ cd /testbed
+ git diff HEAD a5743ed36fbd3fbc8e351bdab16561fbfca7dfa1
+ git config --global --add safe.directory /testbed
+ cd /testbed
+ git status
On branch main
nothing to commit, working tree clean
+ git show
commit a5743ed36fbd3fbc8e351bdab16561fbfca7dfa1
Author: Nicolas Hug <contact@nicolas-hug.com>
Date:   Thu Jun 13 14:52:56 2019 -0400

    TST add test for LAD-loss and quantile loss equivalence (old GBDT code) (#14086)

diff --git a/sklearn/ensemble/tests/test_gradient_boosting_loss_functions.py b/sklearn/ensemble/tests/test_gradient_boosting_loss_functions.py
index 32c65aabfa..a82dbab4e7 100644
--- a/sklearn/ensemble/tests/test_gradient_boosting_loss_functions.py
+++ b/sklearn/ensemble/tests/test_gradient_boosting_loss_functions.py
@@ -6,6 +6,7 @@ import numpy as np
 from numpy.testing import assert_almost_equal
 from numpy.testing import assert_allclose
 from numpy.testing import assert_equal
+import pytest
 
 from sklearn.utils import check_random_state
 from sklearn.utils.stats import _weighted_percentile
@@ -273,3 +274,24 @@ def test_init_raw_predictions_values():
         for k in range(n_classes):
             p = (y == k).mean()
         assert_almost_equal(raw_predictions[:, k], np.log(p))
+
+
+@pytest.mark.parametrize('seed', range(5))
+def test_lad_equals_quantile_50(seed):
+    # Make sure quantile loss with alpha = .5 is equivalent to LAD
+    lad = LeastAbsoluteError(n_classes=1)
+    ql = QuantileLossFunction(n_classes=1, alpha=0.5)
+
+    n_samples = 50
+    rng = np.random.RandomState(seed)
+    raw_predictions = rng.normal(size=(n_samples))
+    y_true = rng.normal(size=(n_samples))
+
+    lad_loss = lad(y_true, raw_predictions)
+    ql_loss = ql(y_true, raw_predictions)
+    assert_almost_equal(lad_loss, 2 * ql_loss)
+
+    weights = np.linspace(0, 1, n_samples) ** 2
+    lad_weighted_loss = lad(y_true, raw_predictions, sample_weight=weights)
+    ql_weighted_loss = ql(y_true, raw_predictions, sample_weight=weights)
+    assert_almost_equal(lad_weighted_loss, 2 * ql_weighted_loss)
+ git diff a5743ed36fbd3fbc8e351bdab16561fbfca7dfa1
+ source /opt/miniconda3/bin/activate
++ _CONDA_ROOT=/opt/miniconda3
++ . /opt/miniconda3/etc/profile.d/conda.sh
+++ export CONDA_EXE=/opt/miniconda3/bin/conda
+++ CONDA_EXE=/opt/miniconda3/bin/conda
+++ export _CE_M=
+++ _CE_M=
+++ export _CE_CONDA=
+++ _CE_CONDA=
+++ export CONDA_PYTHON_EXE=/opt/miniconda3/bin/python
+++ CONDA_PYTHON_EXE=/opt/miniconda3/bin/python
+++ '[' -z x ']'
++ conda activate
++ local cmd=activate
++ case "$cmd" in
++ __conda_activate activate
++ '[' -n '' ']'
++ local ask_conda
+++ PS1='(testbed) '
+++ __conda_exe shell.posix activate
+++ /opt/miniconda3/bin/conda shell.posix activate
++ ask_conda='PS1='\''(base) '\''
export PATH='\''/opt/miniconda3/bin:/opt/miniconda3/condabin:/opt/miniconda3/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin'\''
export CONDA_PREFIX='\''/opt/miniconda3'\''
export CONDA_SHLVL='\''3'\''
export CONDA_DEFAULT_ENV='\''base'\''
export CONDA_PROMPT_MODIFIER='\''(base) '\''
export CONDA_PREFIX_2='\''/opt/miniconda3/envs/testbed'\''
export CONDA_EXE='\''/opt/miniconda3/bin/conda'\''
export _CE_M='\'''\''
export _CE_CONDA='\'''\''
export CONDA_PYTHON_EXE='\''/opt/miniconda3/bin/python'\'''
++ eval 'PS1='\''(base) '\''
export PATH='\''/opt/miniconda3/bin:/opt/miniconda3/condabin:/opt/miniconda3/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin'\''
export CONDA_PREFIX='\''/opt/miniconda3'\''
export CONDA_SHLVL='\''3'\''
export CONDA_DEFAULT_ENV='\''base'\''
export CONDA_PROMPT_MODIFIER='\''(base) '\''
export CONDA_PREFIX_2='\''/opt/miniconda3/envs/testbed'\''
export CONDA_EXE='\''/opt/miniconda3/bin/conda'\''
export _CE_M='\'''\''
export _CE_CONDA='\'''\''
export CONDA_PYTHON_EXE='\''/opt/miniconda3/bin/python'\'''
+++ PS1='(base) '
+++ export PATH=/opt/miniconda3/bin:/opt/miniconda3/condabin:/opt/miniconda3/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin
+++ PATH=/opt/miniconda3/bin:/opt/miniconda3/condabin:/opt/miniconda3/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin
+++ export CONDA_PREFIX=/opt/miniconda3
+++ CONDA_PREFIX=/opt/miniconda3
+++ export CONDA_SHLVL=3
+++ CONDA_SHLVL=3
+++ export CONDA_DEFAULT_ENV=base
+++ CONDA_DEFAULT_ENV=base
+++ export 'CONDA_PROMPT_MODIFIER=(base) '
+++ CONDA_PROMPT_MODIFIER='(base) '
+++ export CONDA_PREFIX_2=/opt/miniconda3/envs/testbed
+++ CONDA_PREFIX_2=/opt/miniconda3/envs/testbed
+++ export CONDA_EXE=/opt/miniconda3/bin/conda
+++ CONDA_EXE=/opt/miniconda3/bin/conda
+++ export _CE_M=
+++ _CE_M=
+++ export _CE_CONDA=
+++ _CE_CONDA=
+++ export CONDA_PYTHON_EXE=/opt/miniconda3/bin/python
+++ CONDA_PYTHON_EXE=/opt/miniconda3/bin/python
++ __conda_hashr
++ '[' -n '' ']'
++ '[' -n '' ']'
++ hash -r
+ conda activate testbed
+ local cmd=activate
+ case "$cmd" in
+ __conda_activate activate testbed
+ '[' -n '' ']'
+ local ask_conda
++ PS1='(base) '
++ __conda_exe shell.posix activate testbed
++ /opt/miniconda3/bin/conda shell.posix activate testbed
+ ask_conda='PS1='\''(testbed) '\''
export PATH='\''/opt/miniconda3/envs/testbed/bin:/opt/miniconda3/condabin:/opt/miniconda3/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin'\''
export CONDA_PREFIX='\''/opt/miniconda3/envs/testbed'\''
export CONDA_SHLVL='\''4'\''
export CONDA_DEFAULT_ENV='\''testbed'\''
export CONDA_PROMPT_MODIFIER='\''(testbed) '\''
export CONDA_PREFIX_3='\''/opt/miniconda3'\''
export CONDA_EXE='\''/opt/miniconda3/bin/conda'\''
export _CE_M='\'''\''
export _CE_CONDA='\'''\''
export CONDA_PYTHON_EXE='\''/opt/miniconda3/bin/python'\'''
+ eval 'PS1='\''(testbed) '\''
export PATH='\''/opt/miniconda3/envs/testbed/bin:/opt/miniconda3/condabin:/opt/miniconda3/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin'\''
export CONDA_PREFIX='\''/opt/miniconda3/envs/testbed'\''
export CONDA_SHLVL='\''4'\''
export CONDA_DEFAULT_ENV='\''testbed'\''
export CONDA_PROMPT_MODIFIER='\''(testbed) '\''
export CONDA_PREFIX_3='\''/opt/miniconda3'\''
export CONDA_EXE='\''/opt/miniconda3/bin/conda'\''
export _CE_M='\'''\''
export _CE_CONDA='\'''\''
export CONDA_PYTHON_EXE='\''/opt/miniconda3/bin/python'\'''
++ PS1='(testbed) '
++ export PATH=/opt/miniconda3/envs/testbed/bin:/opt/miniconda3/condabin:/opt/miniconda3/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin
++ PATH=/opt/miniconda3/envs/testbed/bin:/opt/miniconda3/condabin:/opt/miniconda3/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin
++ export CONDA_PREFIX=/opt/miniconda3/envs/testbed
++ CONDA_PREFIX=/opt/miniconda3/envs/testbed
++ export CONDA_SHLVL=4
++ CONDA_SHLVL=4
++ export CONDA_DEFAULT_ENV=testbed
++ CONDA_DEFAULT_ENV=testbed
++ export 'CONDA_PROMPT_MODIFIER=(testbed) '
++ CONDA_PROMPT_MODIFIER='(testbed) '
++ export CONDA_PREFIX_3=/opt/miniconda3
++ CONDA_PREFIX_3=/opt/miniconda3
++ export CONDA_EXE=/opt/miniconda3/bin/conda
++ CONDA_EXE=/opt/miniconda3/bin/conda
++ export _CE_M=
++ _CE_M=
++ export _CE_CONDA=
++ _CE_CONDA=
++ export CONDA_PYTHON_EXE=/opt/miniconda3/bin/python
++ CONDA_PYTHON_EXE=/opt/miniconda3/bin/python
+ __conda_hashr
+ '[' -n '' ']'
+ '[' -n '' ']'
+ hash -r
+ python -m pip install -v --no-use-pep517 --no-build-isolation -e .
Using pip 21.2.2 from /opt/miniconda3/envs/testbed/lib/python3.6/site-packages/pip (python 3.6)
Obtaining file:///testbed
    Running command python setup.py egg_info
    running egg_info
    creating /tmp/pip-pip-egg-info-kc6n_8jk/scikit_learn.egg-info
    writing /tmp/pip-pip-egg-info-kc6n_8jk/scikit_learn.egg-info/PKG-INFO
    writing dependency_links to /tmp/pip-pip-egg-info-kc6n_8jk/scikit_learn.egg-info/dependency_links.txt
    writing requirements to /tmp/pip-pip-egg-info-kc6n_8jk/scikit_learn.egg-info/requires.txt
    writing top-level names to /tmp/pip-pip-egg-info-kc6n_8jk/scikit_learn.egg-info/top_level.txt
    writing manifest file '/tmp/pip-pip-egg-info-kc6n_8jk/scikit_learn.egg-info/SOURCES.txt'
    reading manifest file '/tmp/pip-pip-egg-info-kc6n_8jk/scikit_learn.egg-info/SOURCES.txt'
    reading manifest template 'MANIFEST.in'
    adding license file 'COPYING'
    writing manifest file '/tmp/pip-pip-egg-info-kc6n_8jk/scikit_learn.egg-info/SOURCES.txt'
    Partial import of sklearn during the build process.
Requirement already satisfied: numpy>=1.11.0 in /opt/miniconda3/envs/testbed/lib/python3.6/site-packages (from scikit-learn==0.22.dev0) (1.19.2)
Requirement already satisfied: scipy>=0.17.0 in /opt/miniconda3/envs/testbed/lib/python3.6/site-packages (from scikit-learn==0.22.dev0) (1.5.2)
Requirement already satisfied: joblib>=0.11 in /opt/miniconda3/envs/testbed/lib/python3.6/site-packages (from scikit-learn==0.22.dev0) (1.1.1)
Installing collected packages: scikit-learn
  Attempting uninstall: scikit-learn
    Found existing installation: scikit-learn 0.22.dev0
    Uninstalling scikit-learn-0.22.dev0:
      Removing file or directory /opt/miniconda3/envs/testbed/lib/python3.6/site-packages/scikit-learn.egg-link
      Removing pth entries from /opt/miniconda3/envs/testbed/lib/python3.6/site-packages/easy-install.pth:
      Removing entry: /testbed
      Successfully uninstalled scikit-learn-0.22.dev0
  Running setup.py develop for scikit-learn
    Running command /opt/miniconda3/envs/testbed/bin/python -c 'import io, os, sys, setuptools, tokenize; sys.argv[0] = '"'"'/testbed/setup.py'"'"'; __file__='"'"'/testbed/setup.py'"'"';f = getattr(tokenize, '"'"'open'"'"', open)(__file__) if os.path.exists(__file__) else io.StringIO('"'"'from setuptools import setup; setup()'"'"');code = f.read().replace('"'"'\r\n'"'"', '"'"'\n'"'"');f.close();exec(compile(code, __file__, '"'"'exec'"'"'))' develop --no-deps
    blas_opt_info:
    blas_mkl_info:
    customize UnixCCompiler
      libraries mkl_rt not found in ['/opt/miniconda3/envs/testbed/lib', '/usr/local/lib', '/usr/lib64', '/usr/lib', '/usr/lib/x86_64-linux-gnu']
      NOT AVAILABLE

    blis_info:
      libraries blis not found in ['/opt/miniconda3/envs/testbed/lib', '/usr/local/lib', '/usr/lib64', '/usr/lib', '/usr/lib/x86_64-linux-gnu']
      NOT AVAILABLE

    openblas_info:
    C compiler: gcc -pthread -B /opt/miniconda3/envs/testbed/compiler_compat -Wl,--sysroot=/ -Wsign-compare -DNDEBUG -g -fwrapv -O3 -Wall -Wstrict-prototypes -fPIC

    creating /tmp/tmpfjbnjbpl/tmp
    creating /tmp/tmpfjbnjbpl/tmp/tmpfjbnjbpl
    compile options: '-c'
    gcc: /tmp/tmpfjbnjbpl/source.c
    gcc -pthread -B /opt/miniconda3/envs/testbed/compiler_compat -Wl,--sysroot=/ /tmp/tmpfjbnjbpl/tmp/tmpfjbnjbpl/source.o -L/opt/miniconda3/envs/testbed/lib -lopenblas -o /tmp/tmpfjbnjbpl/a.out
      FOUND:
        libraries = ['openblas', 'openblas']
        library_dirs = ['/opt/miniconda3/envs/testbed/lib']
        language = c
        define_macros = [('HAVE_CBLAS', None)]

      FOUND:
        libraries = ['openblas', 'openblas']
        library_dirs = ['/opt/miniconda3/envs/testbed/lib']
        language = c
        define_macros = [('HAVE_CBLAS', None)]

    C compiler: gcc -pthread -B /opt/miniconda3/envs/testbed/compiler_compat -Wl,--sysroot=/ -Wsign-compare -DNDEBUG -g -fwrapv -O3 -Wall -Wstrict-prototypes -fPIC

    compile options: '-c'
    extra options: '-fopenmp'
    gcc: test_openmp.c
    gcc -pthread -B /opt/miniconda3/envs/testbed/compiler_compat -Wl,--sysroot=/ objects/test_openmp.o -o test_openmp -fopenmp
    running develop
    running build_scripts
    running egg_info
    running build_src
    build_src
    building library "libsvm-skl" sources
    building extension "sklearn.__check_build._check_build" sources
    building extension "sklearn.preprocessing._csr_polynomial_expansion" sources
    building extension "sklearn.cluster._dbscan_inner" sources
    building extension "sklearn.cluster._hierarchical" sources
    building extension "sklearn.cluster._k_means_elkan" sources
    building extension "sklearn.cluster._k_means" sources
    building extension "sklearn.datasets._svmlight_format" sources
    building extension "sklearn.decomposition._online_lda" sources
    building extension "sklearn.decomposition.cdnmf_fast" sources
    building extension "sklearn.ensemble._gradient_boosting" sources
    building extension "sklearn.ensemble._hist_gradient_boosting._gradient_boosting" sources
    building extension "sklearn.ensemble._hist_gradient_boosting.histogram" sources
    building extension "sklearn.ensemble._hist_gradient_boosting.splitting" sources
    building extension "sklearn.ensemble._hist_gradient_boosting._binning" sources
    building extension "sklearn.ensemble._hist_gradient_boosting._predictor" sources
    building extension "sklearn.ensemble._hist_gradient_boosting._loss" sources
    building extension "sklearn.ensemble._hist_gradient_boosting.types" sources
    building extension "sklearn.ensemble._hist_gradient_boosting.utils" sources
    building extension "sklearn.feature_extraction._hashing" sources
    building extension "sklearn.manifold._utils" sources
    building extension "sklearn.manifold._barnes_hut_tsne" sources
    building extension "sklearn.metrics.cluster.expected_mutual_info_fast" sources
    building extension "sklearn.metrics.pairwise_fast" sources
    building extension "sklearn.neighbors.ball_tree" sources
    building extension "sklearn.neighbors.kd_tree" sources
    building extension "sklearn.neighbors.dist_metrics" sources
    building extension "sklearn.neighbors.typedefs" sources
    building extension "sklearn.neighbors.quad_tree" sources
    building extension "sklearn.tree._tree" sources
    building extension "sklearn.tree._splitter" sources
    building extension "sklearn.tree._criterion" sources
    building extension "sklearn.tree._utils" sources
    building extension "sklearn.utils.sparsefuncs_fast" sources
    building extension "sklearn.utils._cython_blas" sources
    building extension "sklearn.utils.arrayfuncs" sources
    building extension "sklearn.utils.murmurhash" sources
    building extension "sklearn.utils.lgamma" sources
    building extension "sklearn.utils.graph_shortest_path" sources
    building extension "sklearn.utils.fast_dict" sources
    building extension "sklearn.utils.seq_dataset" sources
    building extension "sklearn.utils.weight_vector" sources
    building extension "sklearn.utils._random" sources
    building extension "sklearn.utils._logistic_sigmoid" sources
    building extension "sklearn.svm.libsvm" sources
    building extension "sklearn.svm.liblinear" sources
    building extension "sklearn.svm.libsvm_sparse" sources
    building extension "sklearn.linear_model.cd_fast" sources
    building extension "sklearn.linear_model.sgd_fast" sources
    building extension "sklearn.linear_model.sag_fast" sources
    building extension "sklearn._isotonic" sources
    building data_files sources
    build_src: building npy-pkg config files
    writing scikit_learn.egg-info/PKG-INFO
    writing dependency_links to scikit_learn.egg-info/dependency_links.txt
    writing requirements to scikit_learn.egg-info/requires.txt
    writing top-level names to scikit_learn.egg-info/top_level.txt
    reading manifest file 'scikit_learn.egg-info/SOURCES.txt'
    reading manifest template 'MANIFEST.in'
    adding license file 'COPYING'
    writing manifest file 'scikit_learn.egg-info/SOURCES.txt'
    running build_ext
    customize UnixCCompiler
    customize UnixCCompiler using build_clib
    customize UnixCCompiler
    customize UnixCCompiler using build_ext_subclass
    customize UnixCCompiler
    customize UnixCCompiler using build_ext_subclass
    Creating /opt/miniconda3/envs/testbed/lib/python3.6/site-packages/scikit-learn.egg-link (link to .)
    Adding scikit-learn 0.22.dev0 to easy-install.pth file

    Installed /testbed
    Partial import of sklearn during the build process.
Successfully installed scikit-learn-0.22.dev0
WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv
+ git apply -v -
<stdin>:14: trailing whitespace.
    
<stdin>:20: trailing whitespace.
    
<stdin>:23: trailing whitespace.
    
Checking patch sklearn/tests/test_coverup_scikit-learn__scikit-learn-14087.py...
Applied patch sklearn/tests/test_coverup_scikit-learn__scikit-learn-14087.py cleanly.
warning: 3 lines add whitespace errors.
+ python3 /root/trace.py --timing --trace --count -C coverage.cover --include-pattern '/testbed/(sklearn/linear_model/logistic\.py)' -m pytest --no-header -rA -p no:cacheprovider sklearn/tests/test_coverup_scikit-learn__scikit-learn-14087.py
['--timing', '--trace', '--count', '-C', 'coverage.cover', '--include-pattern', '/testbed/(sklearn/linear_model/logistic\\.py)']
============================= test session starts ==============================
collected 1 item

sklearn/tests/test_coverup_scikit-learn__scikit-learn-14087.py F         [100%]

=================================== FAILURES ===================================
_____________ test_logistic_regression_cv_refit_false_index_error ______________

    def test_logistic_regression_cv_refit_false_index_error():
        # Set random seed for reproducibility
        np.random.seed(29)
    
        # Generate random data
        X = np.random.normal(size=(1000, 3))
        beta = np.random.normal(size=3)
        intercept = np.random.normal(size=None)
        y = np.sign(intercept + X @ beta)
    
        # Initialize LogisticRegressionCV with refit=False
        model = LogisticRegressionCV(cv=5, solver='saga', tol=1e-2, refit=False)
    
        # Expect no error to be raised when the bug is fixed
        try:
>           model.fit(X, y)

sklearn/tests/test_coverup_scikit-learn__scikit-learn-14087.py:20: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = LogisticRegressionCV(Cs=10, class_weight=None, cv=5, dual=False,
                     fit_intercept=True, intercept_sc...   penalty='l2', random_state=None, refit=False, scoring=None,
                     solver='saga', tol=0.01, verbose=0)
X = array([[-0.41748213,  0.7060321 ,  1.9159847 ],
       [-2.1417555 ,  0.71905689,  0.46707262],
       [ 0.76672253,  ...1452074,  0.02198272],
       [-1.10351235,  0.80772658,  0.92438973],
       [-0.37344752,  0.13256818, -1.12946431]])
y = array([1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0,
       0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0,...0, 0, 1,
       0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0,
       0, 0, 0, 0, 1, 1, 0, 0, 1, 0])
sample_weight = None

    def fit(self, X, y, sample_weight=None):
        """Fit the model according to the given training data.
    
        Parameters
        ----------
        X : {array-like, sparse matrix}, shape (n_samples, n_features)
            Training vector, where n_samples is the number of samples and
            n_features is the number of features.
    
        y : array-like, shape (n_samples,)
            Target vector relative to X.
    
        sample_weight : array-like, shape (n_samples,) optional
            Array of weights that are assigned to individual samples.
            If not provided, then each sample is given unit weight.
    
        Returns
        -------
        self : object
        """
        solver = _check_solver(self.solver, self.penalty, self.dual)
    
        if not isinstance(self.max_iter, numbers.Number) or self.max_iter < 0:
            raise ValueError("Maximum number of iteration must be positive;"
                             " got (max_iter=%r)" % self.max_iter)
        if not isinstance(self.tol, numbers.Number) or self.tol < 0:
            raise ValueError("Tolerance for stopping criteria must be "
                             "positive; got (tol=%r)" % self.tol)
        if self.penalty == 'elasticnet':
            if self.l1_ratios is None or len(self.l1_ratios) == 0 or any(
                    (not isinstance(l1_ratio, numbers.Number) or l1_ratio < 0
                     or l1_ratio > 1) for l1_ratio in self.l1_ratios):
                raise ValueError("l1_ratios must be a list of numbers between "
                                 "0 and 1; got (l1_ratios=%r)" %
                                 self.l1_ratios)
            l1_ratios_ = self.l1_ratios
        else:
            if self.l1_ratios is not None:
                warnings.warn("l1_ratios parameter is only used when penalty "
                              "is 'elasticnet'. Got (penalty={})".format(
                                  self.penalty))
    
            l1_ratios_ = [None]
    
        if self.penalty == 'none':
            raise ValueError(
                "penalty='none' is not useful and not supported by "
                "LogisticRegressionCV."
            )
    
        X, y = check_X_y(X, y, accept_sparse='csr', dtype=np.float64,
                         order="C",
                         accept_large_sparse=solver != 'liblinear')
        check_classification_targets(y)
    
        class_weight = self.class_weight
    
        # Encode for string labels
        label_encoder = LabelEncoder().fit(y)
        y = label_encoder.transform(y)
        if isinstance(class_weight, dict):
            class_weight = {label_encoder.transform([cls])[0]: v
                            for cls, v in class_weight.items()}
    
        # The original class labels
        classes = self.classes_ = label_encoder.classes_
        encoded_labels = label_encoder.transform(label_encoder.classes_)
    
        multi_class = _check_multi_class(self.multi_class, solver,
                                         len(classes))
    
        if solver in ['sag', 'saga']:
            max_squared_sum = row_norms(X, squared=True).max()
        else:
            max_squared_sum = None
    
        # init cross-validation generator
        cv = check_cv(self.cv, y, classifier=True)
        folds = list(cv.split(X, y))
    
        # Use the label encoded classes
        n_classes = len(encoded_labels)
    
        if n_classes < 2:
            raise ValueError("This solver needs samples of at least 2 classes"
                             " in the data, but the data contains only one"
                             " class: %r" % classes[0])
    
        if n_classes == 2:
            # OvR in case of binary problems is as good as fitting
            # the higher label
            n_classes = 1
            encoded_labels = encoded_labels[1:]
            classes = classes[1:]
    
        # We need this hack to iterate only once over labels, in the case of
        # multi_class = multinomial, without changing the value of the labels.
        if multi_class == 'multinomial':
            iter_encoded_labels = iter_classes = [None]
        else:
            iter_encoded_labels = encoded_labels
            iter_classes = classes
    
        # compute the class weights for the entire dataset y
        if class_weight == "balanced":
            class_weight = compute_class_weight(class_weight,
                                                np.arange(len(self.classes_)),
                                                y)
            class_weight = dict(enumerate(class_weight))
    
        path_func = delayed(_log_reg_scoring_path)
    
        # The SAG solver releases the GIL so it's more efficient to use
        # threads for this solver.
        if self.solver in ['sag', 'saga']:
            prefer = 'threads'
        else:
            prefer = 'processes'
    
        fold_coefs_ = Parallel(n_jobs=self.n_jobs, verbose=self.verbose,
                               **_joblib_parallel_args(prefer=prefer))(
            path_func(X, y, train, test, pos_class=label, Cs=self.Cs,
                      fit_intercept=self.fit_intercept, penalty=self.penalty,
                      dual=self.dual, solver=solver, tol=self.tol,
                      max_iter=self.max_iter, verbose=self.verbose,
                      class_weight=class_weight, scoring=self.scoring,
                      multi_class=multi_class,
                      intercept_scaling=self.intercept_scaling,
                      random_state=self.random_state,
                      max_squared_sum=max_squared_sum,
                      sample_weight=sample_weight,
                      l1_ratio=l1_ratio
                      )
            for label in iter_encoded_labels
            for train, test in folds
            for l1_ratio in l1_ratios_)
    
        # _log_reg_scoring_path will output different shapes depending on the
        # multi_class param, so we need to reshape the outputs accordingly.
        # Cs is of shape (n_classes . n_folds . n_l1_ratios, n_Cs) and all the
        # rows are equal, so we just take the first one.
        # After reshaping,
        # - scores is of shape (n_classes, n_folds, n_Cs . n_l1_ratios)
        # - coefs_paths is of shape
        #  (n_classes, n_folds, n_Cs . n_l1_ratios, n_features)
        # - n_iter is of shape
        #  (n_classes, n_folds, n_Cs . n_l1_ratios) or
        #  (1, n_folds, n_Cs . n_l1_ratios)
        coefs_paths, Cs, scores, n_iter_ = zip(*fold_coefs_)
        self.Cs_ = Cs[0]
        if multi_class == 'multinomial':
            coefs_paths = np.reshape(
                coefs_paths,
                (len(folds),  len(l1_ratios_) * len(self.Cs_), n_classes, -1)
            )
            # equiv to coefs_paths = np.moveaxis(coefs_paths, (0, 1, 2, 3),
            #                                                 (1, 2, 0, 3))
            coefs_paths = np.swapaxes(coefs_paths, 0, 1)
            coefs_paths = np.swapaxes(coefs_paths, 0, 2)
            self.n_iter_ = np.reshape(
                n_iter_,
                (1, len(folds), len(self.Cs_) * len(l1_ratios_))
            )
            # repeat same scores across all classes
            scores = np.tile(scores, (n_classes, 1, 1))
        else:
            coefs_paths = np.reshape(
                coefs_paths,
                (n_classes, len(folds), len(self.Cs_) * len(l1_ratios_),
                 -1)
            )
            self.n_iter_ = np.reshape(
                n_iter_,
                (n_classes, len(folds), len(self.Cs_) * len(l1_ratios_))
            )
        scores = np.reshape(scores, (n_classes, len(folds), -1))
        self.scores_ = dict(zip(classes, scores))
        self.coefs_paths_ = dict(zip(classes, coefs_paths))
    
        self.C_ = list()
        self.l1_ratio_ = list()
        self.coef_ = np.empty((n_classes, X.shape[1]))
        self.intercept_ = np.zeros(n_classes)
        for index, (cls, encoded_label) in enumerate(
                zip(iter_classes, iter_encoded_labels)):
    
            if multi_class == 'ovr':
                scores = self.scores_[cls]
                coefs_paths = self.coefs_paths_[cls]
            else:
                # For multinomial, all scores are the same across classes
                scores = scores[0]
                # coefs_paths will keep its original shape because
                # logistic_regression_path expects it this way
    
            if self.refit:
                # best_index is between 0 and (n_Cs . n_l1_ratios - 1)
                # for example, with n_cs=2 and n_l1_ratios=3
                # the layout of scores is
                # [c1, c2, c1, c2, c1, c2]
                #   l1_1 ,  l1_2 ,  l1_3
                best_index = scores.sum(axis=0).argmax()
    
                best_index_C = best_index % len(self.Cs_)
                C_ = self.Cs_[best_index_C]
                self.C_.append(C_)
    
                best_index_l1 = best_index // len(self.Cs_)
                l1_ratio_ = l1_ratios_[best_index_l1]
                self.l1_ratio_.append(l1_ratio_)
    
                if multi_class == 'multinomial':
                    coef_init = np.mean(coefs_paths[:, :, best_index, :],
                                        axis=1)
                else:
                    coef_init = np.mean(coefs_paths[:, best_index, :], axis=0)
    
                # Note that y is label encoded and hence pos_class must be
                # the encoded label / None (for 'multinomial')
                w, _, _ = _logistic_regression_path(
                    X, y, pos_class=encoded_label, Cs=[C_], solver=solver,
                    fit_intercept=self.fit_intercept, coef=coef_init,
                    max_iter=self.max_iter, tol=self.tol,
                    penalty=self.penalty,
                    class_weight=class_weight,
                    multi_class=multi_class,
                    verbose=max(0, self.verbose - 1),
                    random_state=self.random_state,
                    check_input=False, max_squared_sum=max_squared_sum,
                    sample_weight=sample_weight,
                    l1_ratio=l1_ratio_)
                w = w[0]
    
            else:
                # Take the best scores across every fold and the average of
                # all coefficients corresponding to the best scores.
                best_indices = np.argmax(scores, axis=1)
                if self.multi_class == 'ovr':
                    w = np.mean([coefs_paths[i, best_indices[i], :]
                                 for i in range(len(folds))], axis=0)
                else:
                    w = np.mean([coefs_paths[:, i, best_indices[i], :]
>                                for i in range(len(folds))], axis=0)

sklearn/linear_model/logistic.py:2178: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

.0 = <range_iterator object at 0x7f4cb3747b40>

    w = np.mean([coefs_paths[:, i, best_indices[i], :]
>                for i in range(len(folds))], axis=0)
E   IndexError: too many indices for array: array is 3-dimensional, but 4 were indexed

sklearn/linear_model/logistic.py:2178: IndexError

During handling of the above exception, another exception occurred:

    def test_logistic_regression_cv_refit_false_index_error():
        # Set random seed for reproducibility
        np.random.seed(29)
    
        # Generate random data
        X = np.random.normal(size=(1000, 3))
        beta = np.random.normal(size=3)
        intercept = np.random.normal(size=None)
        y = np.sign(intercept + X @ beta)
    
        # Initialize LogisticRegressionCV with refit=False
        model = LogisticRegressionCV(cv=5, solver='saga', tol=1e-2, refit=False)
    
        # Expect no error to be raised when the bug is fixed
        try:
            model.fit(X, y)
        except IndexError:
>           pytest.fail("IndexError was raised with refit=False, indicating a bug.")
E           Failed: IndexError was raised with refit=False, indicating a bug.

sklearn/tests/test_coverup_scikit-learn__scikit-learn-14087.py:22: Failed
----------------------------- Captured stdout call -----------------------------
0.99 logistic.py(1918):         self.Cs = Cs
0.99 logistic.py(1919):         self.fit_intercept = fit_intercept
0.99 logistic.py(1920):         self.cv = cv
0.99 logistic.py(1921):         self.dual = dual
0.99 logistic.py(1922):         self.penalty = penalty
0.99 logistic.py(1923):         self.scoring = scoring
0.99 logistic.py(1924):         self.tol = tol
0.99 logistic.py(1925):         self.max_iter = max_iter
0.99 logistic.py(1926):         self.class_weight = class_weight
0.99 logistic.py(1927):         self.n_jobs = n_jobs
0.99 logistic.py(1928):         self.verbose = verbose
0.99 logistic.py(1929):         self.solver = solver
0.99 logistic.py(1930):         self.refit = refit
0.99 logistic.py(1931):         self.intercept_scaling = intercept_scaling
0.99 logistic.py(1932):         self.multi_class = multi_class
0.99 logistic.py(1933):         self.random_state = random_state
0.99 logistic.py(1934):         self.l1_ratios = l1_ratios
0.99 logistic.py(1956):         solver = _check_solver(self.solver, self.penalty, self.dual)
0.99 logistic.py(428):     all_solvers = ['liblinear', 'newton-cg', 'lbfgs', 'sag', 'saga']
0.99 logistic.py(429):     if solver not in all_solvers:
0.99 logistic.py(433):     all_penalties = ['l1', 'l2', 'elasticnet', 'none']
0.99 logistic.py(434):     if penalty not in all_penalties:
0.99 logistic.py(438):     if solver not in ['liblinear', 'saga'] and penalty not in ('l2', 'none'):
0.99 logistic.py(441):     if solver != 'liblinear' and dual:
0.99 logistic.py(445):     if penalty == 'elasticnet' and solver != 'saga':
0.99 logistic.py(449):     if solver == 'liblinear' and penalty == 'none':
0.99 logistic.py(454):     return solver
0.99 logistic.py(1958):         if not isinstance(self.max_iter, numbers.Number) or self.max_iter < 0:
0.99 logistic.py(1961):         if not isinstance(self.tol, numbers.Number) or self.tol < 0:
0.99 logistic.py(1964):         if self.penalty == 'elasticnet':
0.99 logistic.py(1973):             if self.l1_ratios is not None:
0.99 logistic.py(1978):             l1_ratios_ = [None]
0.99 logistic.py(1980):         if self.penalty == 'none':
0.99 logistic.py(1986):         X, y = check_X_y(X, y, accept_sparse='csr', dtype=np.float64,
0.99 logistic.py(1987):                          order="C",
0.99 logistic.py(1988):                          accept_large_sparse=solver != 'liblinear')
0.99 logistic.py(1989):         check_classification_targets(y)
1.00 logistic.py(1991):         class_weight = self.class_weight
1.00 logistic.py(1994):         label_encoder = LabelEncoder().fit(y)
1.00 logistic.py(1995):         y = label_encoder.transform(y)
1.00 logistic.py(1996):         if isinstance(class_weight, dict):
1.00 logistic.py(2001):         classes = self.classes_ = label_encoder.classes_
1.00 logistic.py(2002):         encoded_labels = label_encoder.transform(label_encoder.classes_)
1.00 logistic.py(2004):         multi_class = _check_multi_class(self.multi_class, solver,
1.00 logistic.py(2005):                                          len(classes))
1.00 logistic.py(458):     if multi_class == 'auto':
1.00 logistic.py(459):         if solver == 'liblinear':
1.00 logistic.py(461):         elif n_classes > 2:
1.00 logistic.py(464):             multi_class = 'ovr'
1.00 logistic.py(465):     if multi_class not in ('multinomial', 'ovr'):
1.00 logistic.py(468):     if multi_class == 'multinomial' and solver == 'liblinear':
1.00 logistic.py(471):     return multi_class
1.00 logistic.py(2007):         if solver in ['sag', 'saga']:
1.00 logistic.py(2008):             max_squared_sum = row_norms(X, squared=True).max()
1.00 logistic.py(2013):         cv = check_cv(self.cv, y, classifier=True)
1.00 logistic.py(2014):         folds = list(cv.split(X, y))
1.00 logistic.py(2017):         n_classes = len(encoded_labels)
1.00 logistic.py(2019):         if n_classes < 2:
1.00 logistic.py(2024):         if n_classes == 2:
1.00 logistic.py(2027):             n_classes = 1
1.00 logistic.py(2028):             encoded_labels = encoded_labels[1:]
1.00 logistic.py(2029):             classes = classes[1:]
1.00 logistic.py(2033):         if multi_class == 'multinomial':
1.00 logistic.py(2036):             iter_encoded_labels = encoded_labels
1.00 logistic.py(2037):             iter_classes = classes
1.00 logistic.py(2040):         if class_weight == "balanced":
1.00 logistic.py(2046):         path_func = delayed(_log_reg_scoring_path)
1.00 logistic.py(2050):         if self.solver in ['sag', 'saga']:
1.00 logistic.py(2051):             prefer = 'threads'
1.00 logistic.py(2055):         fold_coefs_ = Parallel(n_jobs=self.n_jobs, verbose=self.verbose,
1.00 logistic.py(2056):                                **_joblib_parallel_args(prefer=prefer))(
1.00 logistic.py(2057):             path_func(X, y, train, test, pos_class=label, Cs=self.Cs,
1.00 logistic.py(2069):             for label in iter_encoded_labels
1.00 logistic.py(2057):             path_func(X, y, train, test, pos_class=label, Cs=self.Cs,
1.00 logistic.py(2069):             for label in iter_encoded_labels
1.00 logistic.py(2070):             for train, test in folds
1.00 logistic.py(2071):             for l1_ratio in l1_ratios_)
1.00 logistic.py(1132):     X_train = X[train]
1.00 logistic.py(1133):     X_test = X[test]
1.00 logistic.py(1134):     y_train = y[train]
1.00 logistic.py(1135):     y_test = y[test]
1.00 logistic.py(1137):     if sample_weight is not None:
1.00 logistic.py(1143):     coefs, Cs, n_iter = _logistic_regression_path(
1.00 logistic.py(1144):         X_train, y_train, Cs=Cs, l1_ratio=l1_ratio,
1.00 logistic.py(1145):         fit_intercept=fit_intercept, solver=solver, max_iter=max_iter,
1.00 logistic.py(1146):         class_weight=class_weight, pos_class=pos_class,
1.00 logistic.py(1147):         multi_class=multi_class, tol=tol, verbose=verbose, dual=dual,
1.00 logistic.py(1148):         penalty=penalty, intercept_scaling=intercept_scaling,
1.00 logistic.py(1149):         random_state=random_state, check_input=False,
1.00 logistic.py(1150):         max_squared_sum=max_squared_sum, sample_weight=sample_weight)
1.00 logistic.py(803):     if isinstance(Cs, numbers.Integral):
1.00 logistic.py(804):         Cs = np.logspace(-4, 4, Cs)
1.00 logistic.py(806):     solver = _check_solver(solver, penalty, dual)
1.00 logistic.py(428):     all_solvers = ['liblinear', 'newton-cg', 'lbfgs', 'sag', 'saga']
1.00 logistic.py(429):     if solver not in all_solvers:
1.00 logistic.py(433):     all_penalties = ['l1', 'l2', 'elasticnet', 'none']
1.00 logistic.py(434):     if penalty not in all_penalties:
1.00 logistic.py(438):     if solver not in ['liblinear', 'saga'] and penalty not in ('l2', 'none'):
1.00 logistic.py(441):     if solver != 'liblinear' and dual:
1.00 logistic.py(445):     if penalty == 'elasticnet' and solver != 'saga':
1.00 logistic.py(449):     if solver == 'liblinear' and penalty == 'none':
1.00 logistic.py(454):     return solver
1.00 logistic.py(809):     if check_input:
1.00 logistic.py(814):     _, n_features = X.shape
1.00 logistic.py(816):     classes = np.unique(y)
1.00 logistic.py(817):     random_state = check_random_state(random_state)
1.00 logistic.py(819):     multi_class = _check_multi_class(multi_class, solver, len(classes))
1.00 logistic.py(458):     if multi_class == 'auto':
1.00 logistic.py(465):     if multi_class not in ('multinomial', 'ovr'):
1.00 logistic.py(468):     if multi_class == 'multinomial' and solver == 'liblinear':
1.00 logistic.py(471):     return multi_class
1.00 logistic.py(820):     if pos_class is None and multi_class != 'multinomial':
1.00 logistic.py(829):     if sample_weight is not None:
1.00 logistic.py(833):         sample_weight = np.ones(X.shape[0], dtype=X.dtype)
1.00 logistic.py(838):     le = LabelEncoder()
1.00 logistic.py(839):     if isinstance(class_weight, dict) or multi_class == 'multinomial':
1.00 logistic.py(845):     if multi_class == 'ovr':
1.00 logistic.py(846):         w0 = np.zeros(n_features + int(fit_intercept), dtype=X.dtype)
1.00 logistic.py(847):         mask_classes = np.array([-1, 1])
1.00 logistic.py(848):         mask = (y == pos_class)
1.00 logistic.py(849):         y_bin = np.ones(y.shape, dtype=X.dtype)
1.00 logistic.py(850):         y_bin[~mask] = -1.
1.00 logistic.py(853):         if class_weight == "balanced":
1.00 logistic.py(872):     if coef is not None:
1.00 logistic.py(901):     if multi_class == 'multinomial':
1.00 logistic.py(914):         target = y_bin
1.00 logistic.py(915):         if solver == 'lbfgs':
1.00 logistic.py(917):         elif solver == 'newton-cg':
1.00 logistic.py(921):         warm_start_sag = {'coef': np.expand_dims(w0, axis=1)}
1.00 logistic.py(923):     coefs = list()
1.00 logistic.py(924):     n_iter = np.zeros(len(Cs), dtype=np.int32)
1.00 logistic.py(925):     for i, C in enumerate(Cs):
1.00 logistic.py(926):         if solver == 'lbfgs':
1.00 logistic.py(939):         elif solver == 'newton-cg':
1.00 logistic.py(943):         elif solver == 'liblinear':
1.00 logistic.py(953):         elif solver in ['sag', 'saga']:
1.00 logistic.py(954):             if multi_class == 'multinomial':
1.00 logistic.py(958):                 loss = 'log'
1.00 logistic.py(960):             if penalty == 'l1':
1.00 logistic.py(963):             elif penalty == 'l2':
1.00 logistic.py(964):                 alpha = 1. / C
1.00 logistic.py(965):                 beta = 0.
1.00 logistic.py(970):             w0, n_iter_i, warm_start_sag = sag_solver(
1.00 logistic.py(971):                 X, target, sample_weight, loss, alpha,
1.00 logistic.py(972):                 beta, max_iter, tol,
1.00 logistic.py(973):                 verbose, random_state, False, max_squared_sum, warm_start_sag,
1.00 logistic.py(974):                 is_saga=(solver == 'saga'))
1.00 logistic.py(980):         if multi_class == 'multinomial':
1.00 logistic.py(987):             coefs.append(w0.copy())
1.00 logistic.py(989):         n_iter[i] = n_iter_i
1.00 logistic.py(925):     for i, C in enumerate(Cs):
1.00 logistic.py(926):         if solver == 'lbfgs':
1.00 logistic.py(939):         elif solver == 'newton-cg':
1.00 logistic.py(943):         elif solver == 'liblinear':
1.00 logistic.py(953):         elif solver in ['sag', 'saga']:
1.00 logistic.py(954):             if multi_class == 'multinomial':
1.00 logistic.py(958):                 loss = 'log'
1.00 logistic.py(960):             if penalty == 'l1':
1.00 logistic.py(963):             elif penalty == 'l2':
1.00 logistic.py(964):                 alpha = 1. / C
1.00 logistic.py(965):                 beta = 0.
1.00 logistic.py(970):             w0, n_iter_i, warm_start_sag = sag_solver(
1.00 logistic.py(971):                 X, target, sample_weight, loss, alpha,
1.00 logistic.py(972):                 beta, max_iter, tol,
1.00 logistic.py(973):                 verbose, random_state, False, max_squared_sum, warm_start_sag,
1.00 logistic.py(974):                 is_saga=(solver == 'saga'))
1.00 logistic.py(980):         if multi_class == 'multinomial':
1.00 logistic.py(987):             coefs.append(w0.copy())
1.00 logistic.py(989):         n_iter[i] = n_iter_i
1.00 logistic.py(925):     for i, C in enumerate(Cs):
1.00 logistic.py(926):         if solver == 'lbfgs':
1.00 logistic.py(939):         elif solver == 'newton-cg':
1.00 logistic.py(943):         elif solver == 'liblinear':
1.00 logistic.py(953):         elif solver in ['sag', 'saga']:
1.00 logistic.py(954):             if multi_class == 'multinomial':
1.00 logistic.py(958):                 loss = 'log'
1.00 logistic.py(960):             if penalty == 'l1':
1.00 logistic.py(963):             elif penalty == 'l2':
1.00 logistic.py(964):                 alpha = 1. / C
1.00 logistic.py(965):                 beta = 0.
1.00 logistic.py(970):             w0, n_iter_i, warm_start_sag = sag_solver(
1.00 logistic.py(971):                 X, target, sample_weight, loss, alpha,
1.00 logistic.py(972):                 beta, max_iter, tol,
1.00 logistic.py(973):                 verbose, random_state, False, max_squared_sum, warm_start_sag,
1.00 logistic.py(974):                 is_saga=(solver == 'saga'))
1.00 logistic.py(980):         if multi_class == 'multinomial':
1.00 logistic.py(987):             coefs.append(w0.copy())
1.00 logistic.py(989):         n_iter[i] = n_iter_i
1.00 logistic.py(925):     for i, C in enumerate(Cs):
1.00 logistic.py(926):         if solver == 'lbfgs':
1.00 logistic.py(939):         elif solver == 'newton-cg':
1.00 logistic.py(943):         elif solver == 'liblinear':
1.00 logistic.py(953):         elif solver in ['sag', 'saga']:
1.00 logistic.py(954):             if multi_class == 'multinomial':
1.00 logistic.py(958):                 loss = 'log'
1.00 logistic.py(960):             if penalty == 'l1':
1.00 logistic.py(963):             elif penalty == 'l2':
1.00 logistic.py(964):                 alpha = 1. / C
1.00 logistic.py(965):                 beta = 0.
1.00 logistic.py(970):             w0, n_iter_i, warm_start_sag = sag_solver(
1.00 logistic.py(971):                 X, target, sample_weight, loss, alpha,
1.00 logistic.py(972):                 beta, max_iter, tol,
1.00 logistic.py(973):                 verbose, random_state, False, max_squared_sum, warm_start_sag,
1.00 logistic.py(974):                 is_saga=(solver == 'saga'))
1.00 logistic.py(980):         if multi_class == 'multinomial':
1.00 logistic.py(987):             coefs.append(w0.copy())
1.00 logistic.py(989):         n_iter[i] = n_iter_i
1.00 logistic.py(925):     for i, C in enumerate(Cs):
1.00 logistic.py(926):         if solver == 'lbfgs':
1.00 logistic.py(939):         elif solver == 'newton-cg':
1.00 logistic.py(943):         elif solver == 'liblinear':
1.00 logistic.py(953):         elif solver in ['sag', 'saga']:
1.00 logistic.py(954):             if multi_class == 'multinomial':
1.00 logistic.py(958):                 loss = 'log'
1.00 logistic.py(960):             if penalty == 'l1':
1.00 logistic.py(963):             elif penalty == 'l2':
1.00 logistic.py(964):                 alpha = 1. / C
1.00 logistic.py(965):                 beta = 0.
1.00 logistic.py(970):             w0, n_iter_i, warm_start_sag = sag_solver(
1.00 logistic.py(971):                 X, target, sample_weight, loss, alpha,
1.00 logistic.py(972):                 beta, max_iter, tol,
1.00 logistic.py(973):                 verbose, random_state, False, max_squared_sum, warm_start_sag,
1.00 logistic.py(974):                 is_saga=(solver == 'saga'))
1.00 logistic.py(980):         if multi_class == 'multinomial':
1.00 logistic.py(987):             coefs.append(w0.copy())
1.00 logistic.py(989):         n_iter[i] = n_iter_i
1.00 logistic.py(925):     for i, C in enumerate(Cs):
1.00 logistic.py(926):         if solver == 'lbfgs':
1.00 logistic.py(939):         elif solver == 'newton-cg':
1.00 logistic.py(943):         elif solver == 'liblinear':
1.00 logistic.py(953):         elif solver in ['sag', 'saga']:
1.00 logistic.py(954):             if multi_class == 'multinomial':
1.00 logistic.py(958):                 loss = 'log'
1.00 logistic.py(960):             if penalty == 'l1':
1.00 logistic.py(963):             elif penalty == 'l2':
1.00 logistic.py(964):                 alpha = 1. / C
1.00 logistic.py(965):                 beta = 0.
1.00 logistic.py(970):             w0, n_iter_i, warm_start_sag = sag_solver(
1.00 logistic.py(971):                 X, target, sample_weight, loss, alpha,
1.00 logistic.py(972):                 beta, max_iter, tol,
1.00 logistic.py(973):                 verbose, random_state, False, max_squared_sum, warm_start_sag,
1.00 logistic.py(974):                 is_saga=(solver == 'saga'))
1.00 logistic.py(980):         if multi_class == 'multinomial':
1.00 logistic.py(987):             coefs.append(w0.copy())
1.00 logistic.py(989):         n_iter[i] = n_iter_i
1.00 logistic.py(925):     for i, C in enumerate(Cs):
1.00 logistic.py(926):         if solver == 'lbfgs':
1.00 logistic.py(939):         elif solver == 'newton-cg':
1.00 logistic.py(943):         elif solver == 'liblinear':
1.00 logistic.py(953):         elif solver in ['sag', 'saga']:
1.00 logistic.py(954):             if multi_class == 'multinomial':
1.00 logistic.py(958):                 loss = 'log'
1.00 logistic.py(960):             if penalty == 'l1':
1.00 logistic.py(963):             elif penalty == 'l2':
1.00 logistic.py(964):                 alpha = 1. / C
1.00 logistic.py(965):                 beta = 0.
1.00 logistic.py(970):             w0, n_iter_i, warm_start_sag = sag_solver(
1.00 logistic.py(971):                 X, target, sample_weight, loss, alpha,
1.00 logistic.py(972):                 beta, max_iter, tol,
1.00 logistic.py(973):                 verbose, random_state, False, max_squared_sum, warm_start_sag,
1.00 logistic.py(974):                 is_saga=(solver == 'saga'))
1.00 logistic.py(980):         if multi_class == 'multinomial':
1.00 logistic.py(987):             coefs.append(w0.copy())
1.00 logistic.py(989):         n_iter[i] = n_iter_i
1.00 logistic.py(925):     for i, C in enumerate(Cs):
1.00 logistic.py(926):         if solver == 'lbfgs':
1.00 logistic.py(939):         elif solver == 'newton-cg':
1.00 logistic.py(943):         elif solver == 'liblinear':
1.00 logistic.py(953):         elif solver in ['sag', 'saga']:
1.00 logistic.py(954):             if multi_class == 'multinomial':
1.00 logistic.py(958):                 loss = 'log'
1.00 logistic.py(960):             if penalty == 'l1':
1.00 logistic.py(963):             elif penalty == 'l2':
1.00 logistic.py(964):                 alpha = 1. / C
1.00 logistic.py(965):                 beta = 0.
1.00 logistic.py(970):             w0, n_iter_i, warm_start_sag = sag_solver(
1.00 logistic.py(971):                 X, target, sample_weight, loss, alpha,
1.00 logistic.py(972):                 beta, max_iter, tol,
1.00 logistic.py(973):                 verbose, random_state, False, max_squared_sum, warm_start_sag,
1.00 logistic.py(974):                 is_saga=(solver == 'saga'))
1.00 logistic.py(980):         if multi_class == 'multinomial':
1.00 logistic.py(987):             coefs.append(w0.copy())
1.00 logistic.py(989):         n_iter[i] = n_iter_i
1.00 logistic.py(925):     for i, C in enumerate(Cs):
1.00 logistic.py(926):         if solver == 'lbfgs':
1.00 logistic.py(939):         elif solver == 'newton-cg':
1.00 logistic.py(943):         elif solver == 'liblinear':
1.00 logistic.py(953):         elif solver in ['sag', 'saga']:
1.00 logistic.py(954):             if multi_class == 'multinomial':
1.00 logistic.py(958):                 loss = 'log'
1.00 logistic.py(960):             if penalty == 'l1':
1.00 logistic.py(963):             elif penalty == 'l2':
1.00 logistic.py(964):                 alpha = 1. / C
1.00 logistic.py(965):                 beta = 0.
1.00 logistic.py(970):             w0, n_iter_i, warm_start_sag = sag_solver(
1.00 logistic.py(971):                 X, target, sample_weight, loss, alpha,
1.00 logistic.py(972):                 beta, max_iter, tol,
1.00 logistic.py(973):                 verbose, random_state, False, max_squared_sum, warm_start_sag,
1.00 logistic.py(974):                 is_saga=(solver == 'saga'))
1.00 logistic.py(980):         if multi_class == 'multinomial':
1.00 logistic.py(987):             coefs.append(w0.copy())
1.00 logistic.py(989):         n_iter[i] = n_iter_i
1.00 logistic.py(925):     for i, C in enumerate(Cs):
1.00 logistic.py(926):         if solver == 'lbfgs':
1.00 logistic.py(939):         elif solver == 'newton-cg':
1.00 logistic.py(943):         elif solver == 'liblinear':
1.00 logistic.py(953):         elif solver in ['sag', 'saga']:
1.00 logistic.py(954):             if multi_class == 'multinomial':
1.00 logistic.py(958):                 loss = 'log'
1.00 logistic.py(960):             if penalty == 'l1':
1.00 logistic.py(963):             elif penalty == 'l2':
1.00 logistic.py(964):                 alpha = 1. / C
1.00 logistic.py(965):                 beta = 0.
1.00 logistic.py(970):             w0, n_iter_i, warm_start_sag = sag_solver(
1.00 logistic.py(971):                 X, target, sample_weight, loss, alpha,
1.00 logistic.py(972):                 beta, max_iter, tol,
1.00 logistic.py(973):                 verbose, random_state, False, max_squared_sum, warm_start_sag,
1.00 logistic.py(974):                 is_saga=(solver == 'saga'))
1.01 logistic.py(980):         if multi_class == 'multinomial':
1.01 logistic.py(987):             coefs.append(w0.copy())
1.01 logistic.py(989):         n_iter[i] = n_iter_i
1.01 logistic.py(925):     for i, C in enumerate(Cs):
1.01 logistic.py(991):     return np.array(coefs), np.array(Cs), n_iter
1.01 logistic.py(1152):     log_reg = LogisticRegression(solver=solver, multi_class=multi_class)
1.01 logistic.py(1437):         self.penalty = penalty
1.01 logistic.py(1438):         self.dual = dual
1.01 logistic.py(1439):         self.tol = tol
1.01 logistic.py(1440):         self.C = C
1.01 logistic.py(1441):         self.fit_intercept = fit_intercept
1.01 logistic.py(1442):         self.intercept_scaling = intercept_scaling
1.01 logistic.py(1443):         self.class_weight = class_weight
1.01 logistic.py(1444):         self.random_state = random_state
1.01 logistic.py(1445):         self.solver = solver
1.01 logistic.py(1446):         self.max_iter = max_iter
1.01 logistic.py(1447):         self.multi_class = multi_class
1.01 logistic.py(1448):         self.verbose = verbose
1.01 logistic.py(1449):         self.warm_start = warm_start
1.01 logistic.py(1450):         self.n_jobs = n_jobs
1.01 logistic.py(1451):         self.l1_ratio = l1_ratio
1.01 logistic.py(1155):     if multi_class == 'ovr':
1.01 logistic.py(1156):         log_reg.classes_ = np.array([-1, 1])
1.01 logistic.py(1163):     if pos_class is not None:
1.01 logistic.py(1164):         mask = (y_test == pos_class)
1.01 logistic.py(1165):         y_test = np.ones(y_test.shape, dtype=np.float64)
1.01 logistic.py(1166):         y_test[~mask] = -1.
1.01 logistic.py(1168):     scores = list()
1.01 logistic.py(1170):     if isinstance(scoring, str):
1.01 logistic.py(1172):     for w in coefs:
1.01 logistic.py(1173):         if multi_class == 'ovr':
1.01 logistic.py(1174):             w = w[np.newaxis, :]
1.01 logistic.py(1175):         if fit_intercept:
1.01 logistic.py(1176):             log_reg.coef_ = w[:, :-1]
1.01 logistic.py(1177):             log_reg.intercept_ = w[:, -1]
1.01 logistic.py(1182):         if scoring is None:
1.01 logistic.py(1183):             scores.append(log_reg.score(X_test, y_test))
1.01 logistic.py(1172):     for w in coefs:
1.01 logistic.py(1173):         if multi_class == 'ovr':
1.01 logistic.py(1174):             w = w[np.newaxis, :]
1.01 logistic.py(1175):         if fit_intercept:
1.01 logistic.py(1176):             log_reg.coef_ = w[:, :-1]
1.01 logistic.py(1177):             log_reg.intercept_ = w[:, -1]
1.01 logistic.py(1182):         if scoring is None:
1.01 logistic.py(1183):             scores.append(log_reg.score(X_test, y_test))
1.01 logistic.py(1172):     for w in coefs:
1.01 logistic.py(1173):         if multi_class == 'ovr':
1.01 logistic.py(1174):             w = w[np.newaxis, :]
1.01 logistic.py(1175):         if fit_intercept:
1.01 logistic.py(1176):             log_reg.coef_ = w[:, :-1]
1.01 logistic.py(1177):             log_reg.intercept_ = w[:, -1]
1.01 logistic.py(1182):         if scoring is None:
1.01 logistic.py(1183):             scores.append(log_reg.score(X_test, y_test))
1.01 logistic.py(1172):     for w in coefs:
1.01 logistic.py(1173):         if multi_class == 'ovr':
1.01 logistic.py(1174):             w = w[np.newaxis, :]
1.01 logistic.py(1175):         if fit_intercept:
1.01 logistic.py(1176):             log_reg.coef_ = w[:, :-1]
1.01 logistic.py(1177):             log_reg.intercept_ = w[:, -1]
1.01 logistic.py(1182):         if scoring is None:
1.01 logistic.py(1183):             scores.append(log_reg.score(X_test, y_test))
1.01 logistic.py(1172):     for w in coefs:
1.01 logistic.py(1173):         if multi_class == 'ovr':
1.01 logistic.py(1174):             w = w[np.newaxis, :]
1.01 logistic.py(1175):         if fit_intercept:
1.01 logistic.py(1176):             log_reg.coef_ = w[:, :-1]
1.01 logistic.py(1177):             log_reg.intercept_ = w[:, -1]
1.01 logistic.py(1182):         if scoring is None:
1.01 logistic.py(1183):             scores.append(log_reg.score(X_test, y_test))
1.01 logistic.py(1172):     for w in coefs:
1.01 logistic.py(1173):         if multi_class == 'ovr':
1.01 logistic.py(1174):             w = w[np.newaxis, :]
1.01 logistic.py(1175):         if fit_intercept:
1.01 logistic.py(1176):             log_reg.coef_ = w[:, :-1]
1.01 logistic.py(1177):             log_reg.intercept_ = w[:, -1]
1.01 logistic.py(1182):         if scoring is None:
1.01 logistic.py(1183):             scores.append(log_reg.score(X_test, y_test))
1.01 logistic.py(1172):     for w in coefs:
1.01 logistic.py(1173):         if multi_class == 'ovr':
1.01 logistic.py(1174):             w = w[np.newaxis, :]
1.01 logistic.py(1175):         if fit_intercept:
1.01 logistic.py(1176):             log_reg.coef_ = w[:, :-1]
1.01 logistic.py(1177):             log_reg.intercept_ = w[:, -1]
1.01 logistic.py(1182):         if scoring is None:
1.01 logistic.py(1183):             scores.append(log_reg.score(X_test, y_test))
1.01 logistic.py(1172):     for w in coefs:
1.01 logistic.py(1173):         if multi_class == 'ovr':
1.01 logistic.py(1174):             w = w[np.newaxis, :]
1.01 logistic.py(1175):         if fit_intercept:
1.01 logistic.py(1176):             log_reg.coef_ = w[:, :-1]
1.01 logistic.py(1177):             log_reg.intercept_ = w[:, -1]
1.01 logistic.py(1182):         if scoring is None:
1.01 logistic.py(1183):             scores.append(log_reg.score(X_test, y_test))
1.01 logistic.py(1172):     for w in coefs:
1.01 logistic.py(1173):         if multi_class == 'ovr':
1.01 logistic.py(1174):             w = w[np.newaxis, :]
1.01 logistic.py(1175):         if fit_intercept:
1.01 logistic.py(1176):             log_reg.coef_ = w[:, :-1]
1.01 logistic.py(1177):             log_reg.intercept_ = w[:, -1]
1.01 logistic.py(1182):         if scoring is None:
1.01 logistic.py(1183):             scores.append(log_reg.score(X_test, y_test))
1.01 logistic.py(1172):     for w in coefs:
1.01 logistic.py(1173):         if multi_class == 'ovr':
1.01 logistic.py(1174):             w = w[np.newaxis, :]
1.01 logistic.py(1175):         if fit_intercept:
1.01 logistic.py(1176):             log_reg.coef_ = w[:, :-1]
1.01 logistic.py(1177):             log_reg.intercept_ = w[:, -1]
1.01 logistic.py(1182):         if scoring is None:
1.01 logistic.py(1183):             scores.append(log_reg.score(X_test, y_test))
1.01 logistic.py(1172):     for w in coefs:
1.01 logistic.py(1187):     return coefs, Cs, np.array(scores), n_iter
1.01 logistic.py(2071):             for l1_ratio in l1_ratios_)
1.01 logistic.py(2070):             for train, test in folds
1.01 logistic.py(2071):             for l1_ratio in l1_ratios_)
1.01 logistic.py(1132):     X_train = X[train]
1.01 logistic.py(1133):     X_test = X[test]
1.01 logistic.py(1134):     y_train = y[train]
1.01 logistic.py(1135):     y_test = y[test]
1.01 logistic.py(1137):     if sample_weight is not None:
1.01 logistic.py(1143):     coefs, Cs, n_iter = _logistic_regression_path(
1.01 logistic.py(1144):         X_train, y_train, Cs=Cs, l1_ratio=l1_ratio,
1.01 logistic.py(1145):         fit_intercept=fit_intercept, solver=solver, max_iter=max_iter,
1.01 logistic.py(1146):         class_weight=class_weight, pos_class=pos_class,
1.01 logistic.py(1147):         multi_class=multi_class, tol=tol, verbose=verbose, dual=dual,
1.01 logistic.py(1148):         penalty=penalty, intercept_scaling=intercept_scaling,
1.01 logistic.py(1149):         random_state=random_state, check_input=False,
1.01 logistic.py(1150):         max_squared_sum=max_squared_sum, sample_weight=sample_weight)
1.01 logistic.py(803):     if isinstance(Cs, numbers.Integral):
1.01 logistic.py(804):         Cs = np.logspace(-4, 4, Cs)
1.01 logistic.py(806):     solver = _check_solver(solver, penalty, dual)
1.01 logistic.py(428):     all_solvers = ['liblinear', 'newton-cg', 'lbfgs', 'sag', 'saga']
1.01 logistic.py(429):     if solver not in all_solvers:
1.01 logistic.py(433):     all_penalties = ['l1', 'l2', 'elasticnet', 'none']
1.01 logistic.py(434):     if penalty not in all_penalties:
1.01 logistic.py(438):     if solver not in ['liblinear', 'saga'] and penalty not in ('l2', 'none'):
1.01 logistic.py(441):     if solver != 'liblinear' and dual:
1.01 logistic.py(445):     if penalty == 'elasticnet' and solver != 'saga':
1.01 logistic.py(449):     if solver == 'liblinear' and penalty == 'none':
1.01 logistic.py(454):     return solver
1.01 logistic.py(809):     if check_input:
1.01 logistic.py(814):     _, n_features = X.shape
1.01 logistic.py(816):     classes = np.unique(y)
1.01 logistic.py(817):     random_state = check_random_state(random_state)
1.01 logistic.py(819):     multi_class = _check_multi_class(multi_class, solver, len(classes))
1.01 logistic.py(458):     if multi_class == 'auto':
1.01 logistic.py(465):     if multi_class not in ('multinomial', 'ovr'):
1.01 logistic.py(468):     if multi_class == 'multinomial' and solver == 'liblinear':
1.01 logistic.py(471):     return multi_class
1.01 logistic.py(820):     if pos_class is None and multi_class != 'multinomial':
1.01 logistic.py(829):     if sample_weight is not None:
1.01 logistic.py(833):         sample_weight = np.ones(X.shape[0], dtype=X.dtype)
1.01 logistic.py(838):     le = LabelEncoder()
1.01 logistic.py(839):     if isinstance(class_weight, dict) or multi_class == 'multinomial':
1.01 logistic.py(845):     if multi_class == 'ovr':
1.01 logistic.py(846):         w0 = np.zeros(n_features + int(fit_intercept), dtype=X.dtype)
1.01 logistic.py(847):         mask_classes = np.array([-1, 1])
1.01 logistic.py(848):         mask = (y == pos_class)
1.01 logistic.py(849):         y_bin = np.ones(y.shape, dtype=X.dtype)
1.01 logistic.py(850):         y_bin[~mask] = -1.
1.01 logistic.py(853):         if class_weight == "balanced":
1.01 logistic.py(872):     if coef is not None:
1.01 logistic.py(901):     if multi_class == 'multinomial':
1.01 logistic.py(914):         target = y_bin
1.01 logistic.py(915):         if solver == 'lbfgs':
1.01 logistic.py(917):         elif solver == 'newton-cg':
1.01 logistic.py(921):         warm_start_sag = {'coef': np.expand_dims(w0, axis=1)}
1.01 logistic.py(923):     coefs = list()
1.01 logistic.py(924):     n_iter = np.zeros(len(Cs), dtype=np.int32)
1.01 logistic.py(925):     for i, C in enumerate(Cs):
1.01 logistic.py(926):         if solver == 'lbfgs':
1.01 logistic.py(939):         elif solver == 'newton-cg':
1.01 logistic.py(943):         elif solver == 'liblinear':
1.01 logistic.py(953):         elif solver in ['sag', 'saga']:
1.01 logistic.py(954):             if multi_class == 'multinomial':
1.01 logistic.py(958):                 loss = 'log'
1.01 logistic.py(960):             if penalty == 'l1':
1.01 logistic.py(963):             elif penalty == 'l2':
1.01 logistic.py(964):                 alpha = 1. / C
1.01 logistic.py(965):                 beta = 0.
1.01 logistic.py(970):             w0, n_iter_i, warm_start_sag = sag_solver(
1.01 logistic.py(971):                 X, target, sample_weight, loss, alpha,
1.01 logistic.py(972):                 beta, max_iter, tol,
1.01 logistic.py(973):                 verbose, random_state, False, max_squared_sum, warm_start_sag,
1.01 logistic.py(974):                 is_saga=(solver == 'saga'))
1.01 logistic.py(980):         if multi_class == 'multinomial':
1.01 logistic.py(987):             coefs.append(w0.copy())
1.01 logistic.py(989):         n_iter[i] = n_iter_i
1.01 logistic.py(925):     for i, C in enumerate(Cs):
1.01 logistic.py(926):         if solver == 'lbfgs':
1.01 logistic.py(939):         elif solver == 'newton-cg':
1.01 logistic.py(943):         elif solver == 'liblinear':
1.01 logistic.py(953):         elif solver in ['sag', 'saga']:
1.01 logistic.py(954):             if multi_class == 'multinomial':
1.01 logistic.py(958):                 loss = 'log'
1.01 logistic.py(960):             if penalty == 'l1':
1.01 logistic.py(963):             elif penalty == 'l2':
1.01 logistic.py(964):                 alpha = 1. / C
1.01 logistic.py(965):                 beta = 0.
1.01 logistic.py(970):             w0, n_iter_i, warm_start_sag = sag_solver(
1.01 logistic.py(971):                 X, target, sample_weight, loss, alpha,
1.01 logistic.py(972):                 beta, max_iter, tol,
1.01 logistic.py(973):                 verbose, random_state, False, max_squared_sum, warm_start_sag,
1.01 logistic.py(974):                 is_saga=(solver == 'saga'))
1.01 logistic.py(980):         if multi_class == 'multinomial':
1.01 logistic.py(987):             coefs.append(w0.copy())
1.01 logistic.py(989):         n_iter[i] = n_iter_i
1.01 logistic.py(925):     for i, C in enumerate(Cs):
1.01 logistic.py(926):         if solver == 'lbfgs':
1.01 logistic.py(939):         elif solver == 'newton-cg':
1.01 logistic.py(943):         elif solver == 'liblinear':
1.01 logistic.py(953):         elif solver in ['sag', 'saga']:
1.01 logistic.py(954):             if multi_class == 'multinomial':
1.01 logistic.py(958):                 loss = 'log'
1.01 logistic.py(960):             if penalty == 'l1':
1.01 logistic.py(963):             elif penalty == 'l2':
1.01 logistic.py(964):                 alpha = 1. / C
1.01 logistic.py(965):                 beta = 0.
1.01 logistic.py(970):             w0, n_iter_i, warm_start_sag = sag_solver(
1.01 logistic.py(971):                 X, target, sample_weight, loss, alpha,
1.01 logistic.py(972):                 beta, max_iter, tol,
1.01 logistic.py(973):                 verbose, random_state, False, max_squared_sum, warm_start_sag,
1.01 logistic.py(974):                 is_saga=(solver == 'saga'))
1.01 logistic.py(980):         if multi_class == 'multinomial':
1.01 logistic.py(987):             coefs.append(w0.copy())
1.01 logistic.py(989):         n_iter[i] = n_iter_i
1.01 logistic.py(925):     for i, C in enumerate(Cs):
1.01 logistic.py(926):         if solver == 'lbfgs':
1.01 logistic.py(939):         elif solver == 'newton-cg':
1.01 logistic.py(943):         elif solver == 'liblinear':
1.01 logistic.py(953):         elif solver in ['sag', 'saga']:
1.01 logistic.py(954):             if multi_class == 'multinomial':
1.01 logistic.py(958):                 loss = 'log'
1.01 logistic.py(960):             if penalty == 'l1':
1.01 logistic.py(963):             elif penalty == 'l2':
1.01 logistic.py(964):                 alpha = 1. / C
1.01 logistic.py(965):                 beta = 0.
1.01 logistic.py(970):             w0, n_iter_i, warm_start_sag = sag_solver(
1.01 logistic.py(971):                 X, target, sample_weight, loss, alpha,
1.01 logistic.py(972):                 beta, max_iter, tol,
1.01 logistic.py(973):                 verbose, random_state, False, max_squared_sum, warm_start_sag,
1.01 logistic.py(974):                 is_saga=(solver == 'saga'))
1.01 logistic.py(980):         if multi_class == 'multinomial':
1.01 logistic.py(987):             coefs.append(w0.copy())
1.01 logistic.py(989):         n_iter[i] = n_iter_i
1.01 logistic.py(925):     for i, C in enumerate(Cs):
1.01 logistic.py(926):         if solver == 'lbfgs':
1.01 logistic.py(939):         elif solver == 'newton-cg':
1.01 logistic.py(943):         elif solver == 'liblinear':
1.01 logistic.py(953):         elif solver in ['sag', 'saga']:
1.01 logistic.py(954):             if multi_class == 'multinomial':
1.01 logistic.py(958):                 loss = 'log'
1.01 logistic.py(960):             if penalty == 'l1':
1.01 logistic.py(963):             elif penalty == 'l2':
1.01 logistic.py(964):                 alpha = 1. / C
1.01 logistic.py(965):                 beta = 0.
1.01 logistic.py(970):             w0, n_iter_i, warm_start_sag = sag_solver(
1.01 logistic.py(971):                 X, target, sample_weight, loss, alpha,
1.01 logistic.py(972):                 beta, max_iter, tol,
1.01 logistic.py(973):                 verbose, random_state, False, max_squared_sum, warm_start_sag,
1.01 logistic.py(974):                 is_saga=(solver == 'saga'))
1.01 logistic.py(980):         if multi_class == 'multinomial':
1.01 logistic.py(987):             coefs.append(w0.copy())
1.01 logistic.py(989):         n_iter[i] = n_iter_i
1.01 logistic.py(925):     for i, C in enumerate(Cs):
1.01 logistic.py(926):         if solver == 'lbfgs':
1.01 logistic.py(939):         elif solver == 'newton-cg':
1.01 logistic.py(943):         elif solver == 'liblinear':
1.01 logistic.py(953):         elif solver in ['sag', 'saga']:
1.01 logistic.py(954):             if multi_class == 'multinomial':
1.01 logistic.py(958):                 loss = 'log'
1.01 logistic.py(960):             if penalty == 'l1':
1.01 logistic.py(963):             elif penalty == 'l2':
1.01 logistic.py(964):                 alpha = 1. / C
1.01 logistic.py(965):                 beta = 0.
1.01 logistic.py(970):             w0, n_iter_i, warm_start_sag = sag_solver(
1.01 logistic.py(971):                 X, target, sample_weight, loss, alpha,
1.01 logistic.py(972):                 beta, max_iter, tol,
1.01 logistic.py(973):                 verbose, random_state, False, max_squared_sum, warm_start_sag,
1.01 logistic.py(974):                 is_saga=(solver == 'saga'))
1.01 logistic.py(980):         if multi_class == 'multinomial':
1.01 logistic.py(987):             coefs.append(w0.copy())
1.01 logistic.py(989):         n_iter[i] = n_iter_i
1.01 logistic.py(925):     for i, C in enumerate(Cs):
1.01 logistic.py(926):         if solver == 'lbfgs':
1.01 logistic.py(939):         elif solver == 'newton-cg':
1.01 logistic.py(943):         elif solver == 'liblinear':
1.01 logistic.py(953):         elif solver in ['sag', 'saga']:
1.01 logistic.py(954):             if multi_class == 'multinomial':
1.01 logistic.py(958):                 loss = 'log'
1.01 logistic.py(960):             if penalty == 'l1':
1.01 logistic.py(963):             elif penalty == 'l2':
1.01 logistic.py(964):                 alpha = 1. / C
1.01 logistic.py(965):                 beta = 0.
1.01 logistic.py(970):             w0, n_iter_i, warm_start_sag = sag_solver(
1.01 logistic.py(971):                 X, target, sample_weight, loss, alpha,
1.01 logistic.py(972):                 beta, max_iter, tol,
1.01 logistic.py(973):                 verbose, random_state, False, max_squared_sum, warm_start_sag,
1.01 logistic.py(974):                 is_saga=(solver == 'saga'))
1.02 logistic.py(980):         if multi_class == 'multinomial':
1.02 logistic.py(987):             coefs.append(w0.copy())
1.02 logistic.py(989):         n_iter[i] = n_iter_i
1.02 logistic.py(925):     for i, C in enumerate(Cs):
1.02 logistic.py(926):         if solver == 'lbfgs':
1.02 logistic.py(939):         elif solver == 'newton-cg':
1.02 logistic.py(943):         elif solver == 'liblinear':
1.02 logistic.py(953):         elif solver in ['sag', 'saga']:
1.02 logistic.py(954):             if multi_class == 'multinomial':
1.02 logistic.py(958):                 loss = 'log'
1.02 logistic.py(960):             if penalty == 'l1':
1.02 logistic.py(963):             elif penalty == 'l2':
1.02 logistic.py(964):                 alpha = 1. / C
1.02 logistic.py(965):                 beta = 0.
1.02 logistic.py(970):             w0, n_iter_i, warm_start_sag = sag_solver(
1.02 logistic.py(971):                 X, target, sample_weight, loss, alpha,
1.02 logistic.py(972):                 beta, max_iter, tol,
1.02 logistic.py(973):                 verbose, random_state, False, max_squared_sum, warm_start_sag,
1.02 logistic.py(974):                 is_saga=(solver == 'saga'))
1.02 logistic.py(980):         if multi_class == 'multinomial':
1.02 logistic.py(987):             coefs.append(w0.copy())
1.02 logistic.py(989):         n_iter[i] = n_iter_i
1.02 logistic.py(925):     for i, C in enumerate(Cs):
1.02 logistic.py(926):         if solver == 'lbfgs':
1.02 logistic.py(939):         elif solver == 'newton-cg':
1.02 logistic.py(943):         elif solver == 'liblinear':
1.02 logistic.py(953):         elif solver in ['sag', 'saga']:
1.02 logistic.py(954):             if multi_class == 'multinomial':
1.02 logistic.py(958):                 loss = 'log'
1.02 logistic.py(960):             if penalty == 'l1':
1.02 logistic.py(963):             elif penalty == 'l2':
1.02 logistic.py(964):                 alpha = 1. / C
1.02 logistic.py(965):                 beta = 0.
1.02 logistic.py(970):             w0, n_iter_i, warm_start_sag = sag_solver(
1.02 logistic.py(971):                 X, target, sample_weight, loss, alpha,
1.02 logistic.py(972):                 beta, max_iter, tol,
1.02 logistic.py(973):                 verbose, random_state, False, max_squared_sum, warm_start_sag,
1.02 logistic.py(974):                 is_saga=(solver == 'saga'))
1.02 logistic.py(980):         if multi_class == 'multinomial':
1.02 logistic.py(987):             coefs.append(w0.copy())
1.02 logistic.py(989):         n_iter[i] = n_iter_i
1.02 logistic.py(925):     for i, C in enumerate(Cs):
1.02 logistic.py(926):         if solver == 'lbfgs':
1.02 logistic.py(939):         elif solver == 'newton-cg':
1.02 logistic.py(943):         elif solver == 'liblinear':
1.02 logistic.py(953):         elif solver in ['sag', 'saga']:
1.02 logistic.py(954):             if multi_class == 'multinomial':
1.02 logistic.py(958):                 loss = 'log'
1.02 logistic.py(960):             if penalty == 'l1':
1.02 logistic.py(963):             elif penalty == 'l2':
1.02 logistic.py(964):                 alpha = 1. / C
1.02 logistic.py(965):                 beta = 0.
1.02 logistic.py(970):             w0, n_iter_i, warm_start_sag = sag_solver(
1.02 logistic.py(971):                 X, target, sample_weight, loss, alpha,
1.02 logistic.py(972):                 beta, max_iter, tol,
1.02 logistic.py(973):                 verbose, random_state, False, max_squared_sum, warm_start_sag,
1.02 logistic.py(974):                 is_saga=(solver == 'saga'))
1.02 logistic.py(980):         if multi_class == 'multinomial':
1.02 logistic.py(987):             coefs.append(w0.copy())
1.02 logistic.py(989):         n_iter[i] = n_iter_i
1.02 logistic.py(925):     for i, C in enumerate(Cs):
1.02 logistic.py(991):     return np.array(coefs), np.array(Cs), n_iter
1.02 logistic.py(1152):     log_reg = LogisticRegression(solver=solver, multi_class=multi_class)
1.02 logistic.py(1437):         self.penalty = penalty
1.02 logistic.py(1438):         self.dual = dual
1.02 logistic.py(1439):         self.tol = tol
1.02 logistic.py(1440):         self.C = C
1.02 logistic.py(1441):         self.fit_intercept = fit_intercept
1.02 logistic.py(1442):         self.intercept_scaling = intercept_scaling
1.02 logistic.py(1443):         self.class_weight = class_weight
1.02 logistic.py(1444):         self.random_state = random_state
1.02 logistic.py(1445):         self.solver = solver
1.02 logistic.py(1446):         self.max_iter = max_iter
1.02 logistic.py(1447):         self.multi_class = multi_class
1.02 logistic.py(1448):         self.verbose = verbose
1.02 logistic.py(1449):         self.warm_start = warm_start
1.02 logistic.py(1450):         self.n_jobs = n_jobs
1.02 logistic.py(1451):         self.l1_ratio = l1_ratio
1.02 logistic.py(1155):     if multi_class == 'ovr':
1.02 logistic.py(1156):         log_reg.classes_ = np.array([-1, 1])
1.02 logistic.py(1163):     if pos_class is not None:
1.02 logistic.py(1164):         mask = (y_test == pos_class)
1.02 logistic.py(1165):         y_test = np.ones(y_test.shape, dtype=np.float64)
1.02 logistic.py(1166):         y_test[~mask] = -1.
1.02 logistic.py(1168):     scores = list()
1.02 logistic.py(1170):     if isinstance(scoring, str):
1.02 logistic.py(1172):     for w in coefs:
1.02 logistic.py(1173):         if multi_class == 'ovr':
1.02 logistic.py(1174):             w = w[np.newaxis, :]
1.02 logistic.py(1175):         if fit_intercept:
1.02 logistic.py(1176):             log_reg.coef_ = w[:, :-1]
1.02 logistic.py(1177):             log_reg.intercept_ = w[:, -1]
1.02 logistic.py(1182):         if scoring is None:
1.02 logistic.py(1183):             scores.append(log_reg.score(X_test, y_test))
1.02 logistic.py(1172):     for w in coefs:
1.02 logistic.py(1173):         if multi_class == 'ovr':
1.02 logistic.py(1174):             w = w[np.newaxis, :]
1.02 logistic.py(1175):         if fit_intercept:
1.02 logistic.py(1176):             log_reg.coef_ = w[:, :-1]
1.02 logistic.py(1177):             log_reg.intercept_ = w[:, -1]
1.02 logistic.py(1182):         if scoring is None:
1.02 logistic.py(1183):             scores.append(log_reg.score(X_test, y_test))
1.02 logistic.py(1172):     for w in coefs:
1.02 logistic.py(1173):         if multi_class == 'ovr':
1.02 logistic.py(1174):             w = w[np.newaxis, :]
1.02 logistic.py(1175):         if fit_intercept:
1.02 logistic.py(1176):             log_reg.coef_ = w[:, :-1]
1.02 logistic.py(1177):             log_reg.intercept_ = w[:, -1]
1.02 logistic.py(1182):         if scoring is None:
1.02 logistic.py(1183):             scores.append(log_reg.score(X_test, y_test))
1.02 logistic.py(1172):     for w in coefs:
1.02 logistic.py(1173):         if multi_class == 'ovr':
1.02 logistic.py(1174):             w = w[np.newaxis, :]
1.02 logistic.py(1175):         if fit_intercept:
1.02 logistic.py(1176):             log_reg.coef_ = w[:, :-1]
1.02 logistic.py(1177):             log_reg.intercept_ = w[:, -1]
1.02 logistic.py(1182):         if scoring is None:
1.02 logistic.py(1183):             scores.append(log_reg.score(X_test, y_test))
1.02 logistic.py(1172):     for w in coefs:
1.02 logistic.py(1173):         if multi_class == 'ovr':
1.02 logistic.py(1174):             w = w[np.newaxis, :]
1.02 logistic.py(1175):         if fit_intercept:
1.02 logistic.py(1176):             log_reg.coef_ = w[:, :-1]
1.02 logistic.py(1177):             log_reg.intercept_ = w[:, -1]
1.02 logistic.py(1182):         if scoring is None:
1.02 logistic.py(1183):             scores.append(log_reg.score(X_test, y_test))
1.02 logistic.py(1172):     for w in coefs:
1.02 logistic.py(1173):         if multi_class == 'ovr':
1.02 logistic.py(1174):             w = w[np.newaxis, :]
1.02 logistic.py(1175):         if fit_intercept:
1.02 logistic.py(1176):             log_reg.coef_ = w[:, :-1]
1.02 logistic.py(1177):             log_reg.intercept_ = w[:, -1]
1.02 logistic.py(1182):         if scoring is None:
1.02 logistic.py(1183):             scores.append(log_reg.score(X_test, y_test))
1.02 logistic.py(1172):     for w in coefs:
1.02 logistic.py(1173):         if multi_class == 'ovr':
1.02 logistic.py(1174):             w = w[np.newaxis, :]
1.02 logistic.py(1175):         if fit_intercept:
1.02 logistic.py(1176):             log_reg.coef_ = w[:, :-1]
1.02 logistic.py(1177):             log_reg.intercept_ = w[:, -1]
1.02 logistic.py(1182):         if scoring is None:
1.02 logistic.py(1183):             scores.append(log_reg.score(X_test, y_test))
1.02 logistic.py(1172):     for w in coefs:
1.02 logistic.py(1173):         if multi_class == 'ovr':
1.02 logistic.py(1174):             w = w[np.newaxis, :]
1.02 logistic.py(1175):         if fit_intercept:
1.02 logistic.py(1176):             log_reg.coef_ = w[:, :-1]
1.02 logistic.py(1177):             log_reg.intercept_ = w[:, -1]
1.02 logistic.py(1182):         if scoring is None:
1.02 logistic.py(1183):             scores.append(log_reg.score(X_test, y_test))
1.02 logistic.py(1172):     for w in coefs:
1.02 logistic.py(1173):         if multi_class == 'ovr':
1.02 logistic.py(1174):             w = w[np.newaxis, :]
1.02 logistic.py(1175):         if fit_intercept:
1.02 logistic.py(1176):             log_reg.coef_ = w[:, :-1]
1.02 logistic.py(1177):             log_reg.intercept_ = w[:, -1]
1.02 logistic.py(1182):         if scoring is None:
1.02 logistic.py(1183):             scores.append(log_reg.score(X_test, y_test))
1.02 logistic.py(1172):     for w in coefs:
1.02 logistic.py(1173):         if multi_class == 'ovr':
1.02 logistic.py(1174):             w = w[np.newaxis, :]
1.02 logistic.py(1175):         if fit_intercept:
1.02 logistic.py(1176):             log_reg.coef_ = w[:, :-1]
1.02 logistic.py(1177):             log_reg.intercept_ = w[:, -1]
1.02 logistic.py(1182):         if scoring is None:
1.02 logistic.py(1183):             scores.append(log_reg.score(X_test, y_test))
1.02 logistic.py(1172):     for w in coefs:
1.02 logistic.py(1187):     return coefs, Cs, np.array(scores), n_iter
1.02 logistic.py(2071):             for l1_ratio in l1_ratios_)
1.02 logistic.py(2070):             for train, test in folds
1.02 logistic.py(2071):             for l1_ratio in l1_ratios_)
1.02 logistic.py(1132):     X_train = X[train]
1.02 logistic.py(1133):     X_test = X[test]
1.02 logistic.py(1134):     y_train = y[train]
1.02 logistic.py(1135):     y_test = y[test]
1.02 logistic.py(1137):     if sample_weight is not None:
1.02 logistic.py(1143):     coefs, Cs, n_iter = _logistic_regression_path(
1.02 logistic.py(1144):         X_train, y_train, Cs=Cs, l1_ratio=l1_ratio,
1.02 logistic.py(1145):         fit_intercept=fit_intercept, solver=solver, max_iter=max_iter,
1.02 logistic.py(1146):         class_weight=class_weight, pos_class=pos_class,
1.02 logistic.py(1147):         multi_class=multi_class, tol=tol, verbose=verbose, dual=dual,
1.02 logistic.py(1148):         penalty=penalty, intercept_scaling=intercept_scaling,
1.02 logistic.py(1149):         random_state=random_state, check_input=False,
1.02 logistic.py(1150):         max_squared_sum=max_squared_sum, sample_weight=sample_weight)
1.02 logistic.py(803):     if isinstance(Cs, numbers.Integral):
1.02 logistic.py(804):         Cs = np.logspace(-4, 4, Cs)
1.02 logistic.py(806):     solver = _check_solver(solver, penalty, dual)
1.02 logistic.py(428):     all_solvers = ['liblinear', 'newton-cg', 'lbfgs', 'sag', 'saga']
1.02 logistic.py(429):     if solver not in all_solvers:
1.02 logistic.py(433):     all_penalties = ['l1', 'l2', 'elasticnet', 'none']
1.02 logistic.py(434):     if penalty not in all_penalties:
1.02 logistic.py(438):     if solver not in ['liblinear', 'saga'] and penalty not in ('l2', 'none'):
1.02 logistic.py(441):     if solver != 'liblinear' and dual:
1.02 logistic.py(445):     if penalty == 'elasticnet' and solver != 'saga':
1.02 logistic.py(449):     if solver == 'liblinear' and penalty == 'none':
1.02 logistic.py(454):     return solver
1.02 logistic.py(809):     if check_input:
1.02 logistic.py(814):     _, n_features = X.shape
1.02 logistic.py(816):     classes = np.unique(y)
1.02 logistic.py(817):     random_state = check_random_state(random_state)
1.02 logistic.py(819):     multi_class = _check_multi_class(multi_class, solver, len(classes))
1.02 logistic.py(458):     if multi_class == 'auto':
1.02 logistic.py(465):     if multi_class not in ('multinomial', 'ovr'):
1.02 logistic.py(468):     if multi_class == 'multinomial' and solver == 'liblinear':
1.02 logistic.py(471):     return multi_class
1.02 logistic.py(820):     if pos_class is None and multi_class != 'multinomial':
1.02 logistic.py(829):     if sample_weight is not None:
1.02 logistic.py(833):         sample_weight = np.ones(X.shape[0], dtype=X.dtype)
1.02 logistic.py(838):     le = LabelEncoder()
1.02 logistic.py(839):     if isinstance(class_weight, dict) or multi_class == 'multinomial':
1.02 logistic.py(845):     if multi_class == 'ovr':
1.02 logistic.py(846):         w0 = np.zeros(n_features + int(fit_intercept), dtype=X.dtype)
1.02 logistic.py(847):         mask_classes = np.array([-1, 1])
1.02 logistic.py(848):         mask = (y == pos_class)
1.02 logistic.py(849):         y_bin = np.ones(y.shape, dtype=X.dtype)
1.02 logistic.py(850):         y_bin[~mask] = -1.
1.02 logistic.py(853):         if class_weight == "balanced":
1.02 logistic.py(872):     if coef is not None:
1.02 logistic.py(901):     if multi_class == 'multinomial':
1.02 logistic.py(914):         target = y_bin
1.02 logistic.py(915):         if solver == 'lbfgs':
1.02 logistic.py(917):         elif solver == 'newton-cg':
1.02 logistic.py(921):         warm_start_sag = {'coef': np.expand_dims(w0, axis=1)}
1.02 logistic.py(923):     coefs = list()
1.02 logistic.py(924):     n_iter = np.zeros(len(Cs), dtype=np.int32)
1.02 logistic.py(925):     for i, C in enumerate(Cs):
1.02 logistic.py(926):         if solver == 'lbfgs':
1.02 logistic.py(939):         elif solver == 'newton-cg':
1.02 logistic.py(943):         elif solver == 'liblinear':
1.02 logistic.py(953):         elif solver in ['sag', 'saga']:
1.02 logistic.py(954):             if multi_class == 'multinomial':
1.02 logistic.py(958):                 loss = 'log'
1.02 logistic.py(960):             if penalty == 'l1':
1.02 logistic.py(963):             elif penalty == 'l2':
1.02 logistic.py(964):                 alpha = 1. / C
1.02 logistic.py(965):                 beta = 0.
1.02 logistic.py(970):             w0, n_iter_i, warm_start_sag = sag_solver(
1.02 logistic.py(971):                 X, target, sample_weight, loss, alpha,
1.02 logistic.py(972):                 beta, max_iter, tol,
1.02 logistic.py(973):                 verbose, random_state, False, max_squared_sum, warm_start_sag,
1.02 logistic.py(974):                 is_saga=(solver == 'saga'))
1.02 logistic.py(980):         if multi_class == 'multinomial':
1.02 logistic.py(987):             coefs.append(w0.copy())
1.02 logistic.py(989):         n_iter[i] = n_iter_i
1.02 logistic.py(925):     for i, C in enumerate(Cs):
1.02 logistic.py(926):         if solver == 'lbfgs':
1.02 logistic.py(939):         elif solver == 'newton-cg':
1.02 logistic.py(943):         elif solver == 'liblinear':
1.02 logistic.py(953):         elif solver in ['sag', 'saga']:
1.02 logistic.py(954):             if multi_class == 'multinomial':
1.02 logistic.py(958):                 loss = 'log'
1.02 logistic.py(960):             if penalty == 'l1':
1.02 logistic.py(963):             elif penalty == 'l2':
1.02 logistic.py(964):                 alpha = 1. / C
1.02 logistic.py(965):                 beta = 0.
1.02 logistic.py(970):             w0, n_iter_i, warm_start_sag = sag_solver(
1.02 logistic.py(971):                 X, target, sample_weight, loss, alpha,
1.02 logistic.py(972):                 beta, max_iter, tol,
1.02 logistic.py(973):                 verbose, random_state, False, max_squared_sum, warm_start_sag,
1.02 logistic.py(974):                 is_saga=(solver == 'saga'))
1.02 logistic.py(980):         if multi_class == 'multinomial':
1.02 logistic.py(987):             coefs.append(w0.copy())
1.02 logistic.py(989):         n_iter[i] = n_iter_i
1.02 logistic.py(925):     for i, C in enumerate(Cs):
1.02 logistic.py(926):         if solver == 'lbfgs':
1.02 logistic.py(939):         elif solver == 'newton-cg':
1.02 logistic.py(943):         elif solver == 'liblinear':
1.02 logistic.py(953):         elif solver in ['sag', 'saga']:
1.02 logistic.py(954):             if multi_class == 'multinomial':
1.02 logistic.py(958):                 loss = 'log'
1.02 logistic.py(960):             if penalty == 'l1':
1.02 logistic.py(963):             elif penalty == 'l2':
1.02 logistic.py(964):                 alpha = 1. / C
1.02 logistic.py(965):                 beta = 0.
1.02 logistic.py(970):             w0, n_iter_i, warm_start_sag = sag_solver(
1.02 logistic.py(971):                 X, target, sample_weight, loss, alpha,
1.02 logistic.py(972):                 beta, max_iter, tol,
1.02 logistic.py(973):                 verbose, random_state, False, max_squared_sum, warm_start_sag,
1.02 logistic.py(974):                 is_saga=(solver == 'saga'))
1.02 logistic.py(980):         if multi_class == 'multinomial':
1.02 logistic.py(987):             coefs.append(w0.copy())
1.02 logistic.py(989):         n_iter[i] = n_iter_i
1.02 logistic.py(925):     for i, C in enumerate(Cs):
1.02 logistic.py(926):         if solver == 'lbfgs':
1.02 logistic.py(939):         elif solver == 'newton-cg':
1.02 logistic.py(943):         elif solver == 'liblinear':
1.02 logistic.py(953):         elif solver in ['sag', 'saga']:
1.02 logistic.py(954):             if multi_class == 'multinomial':
1.02 logistic.py(958):                 loss = 'log'
1.02 logistic.py(960):             if penalty == 'l1':
1.02 logistic.py(963):             elif penalty == 'l2':
1.02 logistic.py(964):                 alpha = 1. / C
1.02 logistic.py(965):                 beta = 0.
1.02 logistic.py(970):             w0, n_iter_i, warm_start_sag = sag_solver(
1.02 logistic.py(971):                 X, target, sample_weight, loss, alpha,
1.02 logistic.py(972):                 beta, max_iter, tol,
1.02 logistic.py(973):                 verbose, random_state, False, max_squared_sum, warm_start_sag,
1.02 logistic.py(974):                 is_saga=(solver == 'saga'))
1.02 logistic.py(980):         if multi_class == 'multinomial':
1.02 logistic.py(987):             coefs.append(w0.copy())
1.02 logistic.py(989):         n_iter[i] = n_iter_i
1.02 logistic.py(925):     for i, C in enumerate(Cs):
1.02 logistic.py(926):         if solver == 'lbfgs':
1.02 logistic.py(939):         elif solver == 'newton-cg':
1.02 logistic.py(943):         elif solver == 'liblinear':
1.02 logistic.py(953):         elif solver in ['sag', 'saga']:
1.02 logistic.py(954):             if multi_class == 'multinomial':
1.02 logistic.py(958):                 loss = 'log'
1.02 logistic.py(960):             if penalty == 'l1':
1.02 logistic.py(963):             elif penalty == 'l2':
1.02 logistic.py(964):                 alpha = 1. / C
1.02 logistic.py(965):                 beta = 0.
1.02 logistic.py(970):             w0, n_iter_i, warm_start_sag = sag_solver(
1.02 logistic.py(971):                 X, target, sample_weight, loss, alpha,
1.02 logistic.py(972):                 beta, max_iter, tol,
1.02 logistic.py(973):                 verbose, random_state, False, max_squared_sum, warm_start_sag,
1.02 logistic.py(974):                 is_saga=(solver == 'saga'))
1.02 logistic.py(980):         if multi_class == 'multinomial':
1.02 logistic.py(987):             coefs.append(w0.copy())
1.02 logistic.py(989):         n_iter[i] = n_iter_i
1.02 logistic.py(925):     for i, C in enumerate(Cs):
1.02 logistic.py(926):         if solver == 'lbfgs':
1.03 logistic.py(939):         elif solver == 'newton-cg':
1.03 logistic.py(943):         elif solver == 'liblinear':
1.03 logistic.py(953):         elif solver in ['sag', 'saga']:
1.03 logistic.py(954):             if multi_class == 'multinomial':
1.03 logistic.py(958):                 loss = 'log'
1.03 logistic.py(960):             if penalty == 'l1':
1.03 logistic.py(963):             elif penalty == 'l2':
1.03 logistic.py(964):                 alpha = 1. / C
1.03 logistic.py(965):                 beta = 0.
1.03 logistic.py(970):             w0, n_iter_i, warm_start_sag = sag_solver(
1.03 logistic.py(971):                 X, target, sample_weight, loss, alpha,
1.03 logistic.py(972):                 beta, max_iter, tol,
1.03 logistic.py(973):                 verbose, random_state, False, max_squared_sum, warm_start_sag,
1.03 logistic.py(974):                 is_saga=(solver == 'saga'))
1.03 logistic.py(980):         if multi_class == 'multinomial':
1.03 logistic.py(987):             coefs.append(w0.copy())
1.03 logistic.py(989):         n_iter[i] = n_iter_i
1.03 logistic.py(925):     for i, C in enumerate(Cs):
1.03 logistic.py(926):         if solver == 'lbfgs':
1.03 logistic.py(939):         elif solver == 'newton-cg':
1.03 logistic.py(943):         elif solver == 'liblinear':
1.03 logistic.py(953):         elif solver in ['sag', 'saga']:
1.03 logistic.py(954):             if multi_class == 'multinomial':
1.03 logistic.py(958):                 loss = 'log'
1.03 logistic.py(960):             if penalty == 'l1':
1.03 logistic.py(963):             elif penalty == 'l2':
1.03 logistic.py(964):                 alpha = 1. / C
1.03 logistic.py(965):                 beta = 0.
1.03 logistic.py(970):             w0, n_iter_i, warm_start_sag = sag_solver(
1.03 logistic.py(971):                 X, target, sample_weight, loss, alpha,
1.03 logistic.py(972):                 beta, max_iter, tol,
1.03 logistic.py(973):                 verbose, random_state, False, max_squared_sum, warm_start_sag,
1.03 logistic.py(974):                 is_saga=(solver == 'saga'))
1.03 logistic.py(980):         if multi_class == 'multinomial':
1.03 logistic.py(987):             coefs.append(w0.copy())
1.03 logistic.py(989):         n_iter[i] = n_iter_i
1.03 logistic.py(925):     for i, C in enumerate(Cs):
1.03 logistic.py(926):         if solver == 'lbfgs':
1.03 logistic.py(939):         elif solver == 'newton-cg':
1.03 logistic.py(943):         elif solver == 'liblinear':
1.03 logistic.py(953):         elif solver in ['sag', 'saga']:
1.03 logistic.py(954):             if multi_class == 'multinomial':
1.03 logistic.py(958):                 loss = 'log'
1.03 logistic.py(960):             if penalty == 'l1':
1.03 logistic.py(963):             elif penalty == 'l2':
1.03 logistic.py(964):                 alpha = 1. / C
1.03 logistic.py(965):                 beta = 0.
1.03 logistic.py(970):             w0, n_iter_i, warm_start_sag = sag_solver(
1.03 logistic.py(971):                 X, target, sample_weight, loss, alpha,
1.03 logistic.py(972):                 beta, max_iter, tol,
1.03 logistic.py(973):                 verbose, random_state, False, max_squared_sum, warm_start_sag,
1.03 logistic.py(974):                 is_saga=(solver == 'saga'))
1.03 logistic.py(980):         if multi_class == 'multinomial':
1.03 logistic.py(987):             coefs.append(w0.copy())
1.03 logistic.py(989):         n_iter[i] = n_iter_i
1.03 logistic.py(925):     for i, C in enumerate(Cs):
1.03 logistic.py(926):         if solver == 'lbfgs':
1.03 logistic.py(939):         elif solver == 'newton-cg':
1.03 logistic.py(943):         elif solver == 'liblinear':
1.03 logistic.py(953):         elif solver in ['sag', 'saga']:
1.03 logistic.py(954):             if multi_class == 'multinomial':
1.03 logistic.py(958):                 loss = 'log'
1.03 logistic.py(960):             if penalty == 'l1':
1.03 logistic.py(963):             elif penalty == 'l2':
1.03 logistic.py(964):                 alpha = 1. / C
1.03 logistic.py(965):                 beta = 0.
1.03 logistic.py(970):             w0, n_iter_i, warm_start_sag = sag_solver(
1.03 logistic.py(971):                 X, target, sample_weight, loss, alpha,
1.03 logistic.py(972):                 beta, max_iter, tol,
1.03 logistic.py(973):                 verbose, random_state, False, max_squared_sum, warm_start_sag,
1.03 logistic.py(974):                 is_saga=(solver == 'saga'))
1.03 logistic.py(980):         if multi_class == 'multinomial':
1.03 logistic.py(987):             coefs.append(w0.copy())
1.03 logistic.py(989):         n_iter[i] = n_iter_i
1.03 logistic.py(925):     for i, C in enumerate(Cs):
1.03 logistic.py(926):         if solver == 'lbfgs':
1.03 logistic.py(939):         elif solver == 'newton-cg':
1.03 logistic.py(943):         elif solver == 'liblinear':
1.03 logistic.py(953):         elif solver in ['sag', 'saga']:
1.03 logistic.py(954):             if multi_class == 'multinomial':
1.03 logistic.py(958):                 loss = 'log'
1.03 logistic.py(960):             if penalty == 'l1':
1.03 logistic.py(963):             elif penalty == 'l2':
1.03 logistic.py(964):                 alpha = 1. / C
1.03 logistic.py(965):                 beta = 0.
1.03 logistic.py(970):             w0, n_iter_i, warm_start_sag = sag_solver(
1.03 logistic.py(971):                 X, target, sample_weight, loss, alpha,
1.03 logistic.py(972):                 beta, max_iter, tol,
1.03 logistic.py(973):                 verbose, random_state, False, max_squared_sum, warm_start_sag,
1.03 logistic.py(974):                 is_saga=(solver == 'saga'))
1.03 logistic.py(980):         if multi_class == 'multinomial':
1.03 logistic.py(987):             coefs.append(w0.copy())
1.03 logistic.py(989):         n_iter[i] = n_iter_i
1.03 logistic.py(925):     for i, C in enumerate(Cs):
1.03 logistic.py(991):     return np.array(coefs), np.array(Cs), n_iter
1.03 logistic.py(1152):     log_reg = LogisticRegression(solver=solver, multi_class=multi_class)
1.03 logistic.py(1437):         self.penalty = penalty
1.03 logistic.py(1438):         self.dual = dual
1.03 logistic.py(1439):         self.tol = tol
1.03 logistic.py(1440):         self.C = C
1.03 logistic.py(1441):         self.fit_intercept = fit_intercept
1.03 logistic.py(1442):         self.intercept_scaling = intercept_scaling
1.03 logistic.py(1443):         self.class_weight = class_weight
1.03 logistic.py(1444):         self.random_state = random_state
1.03 logistic.py(1445):         self.solver = solver
1.03 logistic.py(1446):         self.max_iter = max_iter
1.03 logistic.py(1447):         self.multi_class = multi_class
1.03 logistic.py(1448):         self.verbose = verbose
1.03 logistic.py(1449):         self.warm_start = warm_start
1.03 logistic.py(1450):         self.n_jobs = n_jobs
1.03 logistic.py(1451):         self.l1_ratio = l1_ratio
1.03 logistic.py(1155):     if multi_class == 'ovr':
1.03 logistic.py(1156):         log_reg.classes_ = np.array([-1, 1])
1.03 logistic.py(1163):     if pos_class is not None:
1.03 logistic.py(1164):         mask = (y_test == pos_class)
1.03 logistic.py(1165):         y_test = np.ones(y_test.shape, dtype=np.float64)
1.03 logistic.py(1166):         y_test[~mask] = -1.
1.03 logistic.py(1168):     scores = list()
1.03 logistic.py(1170):     if isinstance(scoring, str):
1.03 logistic.py(1172):     for w in coefs:
1.03 logistic.py(1173):         if multi_class == 'ovr':
1.03 logistic.py(1174):             w = w[np.newaxis, :]
1.03 logistic.py(1175):         if fit_intercept:
1.03 logistic.py(1176):             log_reg.coef_ = w[:, :-1]
1.03 logistic.py(1177):             log_reg.intercept_ = w[:, -1]
1.03 logistic.py(1182):         if scoring is None:
1.03 logistic.py(1183):             scores.append(log_reg.score(X_test, y_test))
1.03 logistic.py(1172):     for w in coefs:
1.03 logistic.py(1173):         if multi_class == 'ovr':
1.03 logistic.py(1174):             w = w[np.newaxis, :]
1.03 logistic.py(1175):         if fit_intercept:
1.03 logistic.py(1176):             log_reg.coef_ = w[:, :-1]
1.03 logistic.py(1177):             log_reg.intercept_ = w[:, -1]
1.03 logistic.py(1182):         if scoring is None:
1.03 logistic.py(1183):             scores.append(log_reg.score(X_test, y_test))
1.03 logistic.py(1172):     for w in coefs:
1.03 logistic.py(1173):         if multi_class == 'ovr':
1.03 logistic.py(1174):             w = w[np.newaxis, :]
1.03 logistic.py(1175):         if fit_intercept:
1.03 logistic.py(1176):             log_reg.coef_ = w[:, :-1]
1.03 logistic.py(1177):             log_reg.intercept_ = w[:, -1]
1.03 logistic.py(1182):         if scoring is None:
1.03 logistic.py(1183):             scores.append(log_reg.score(X_test, y_test))
1.03 logistic.py(1172):     for w in coefs:
1.03 logistic.py(1173):         if multi_class == 'ovr':
1.03 logistic.py(1174):             w = w[np.newaxis, :]
1.03 logistic.py(1175):         if fit_intercept:
1.03 logistic.py(1176):             log_reg.coef_ = w[:, :-1]
1.03 logistic.py(1177):             log_reg.intercept_ = w[:, -1]
1.03 logistic.py(1182):         if scoring is None:
1.03 logistic.py(1183):             scores.append(log_reg.score(X_test, y_test))
1.03 logistic.py(1172):     for w in coefs:
1.03 logistic.py(1173):         if multi_class == 'ovr':
1.03 logistic.py(1174):             w = w[np.newaxis, :]
1.03 logistic.py(1175):         if fit_intercept:
1.03 logistic.py(1176):             log_reg.coef_ = w[:, :-1]
1.03 logistic.py(1177):             log_reg.intercept_ = w[:, -1]
1.03 logistic.py(1182):         if scoring is None:
1.03 logistic.py(1183):             scores.append(log_reg.score(X_test, y_test))
1.03 logistic.py(1172):     for w in coefs:
1.03 logistic.py(1173):         if multi_class == 'ovr':
1.03 logistic.py(1174):             w = w[np.newaxis, :]
1.03 logistic.py(1175):         if fit_intercept:
1.03 logistic.py(1176):             log_reg.coef_ = w[:, :-1]
1.03 logistic.py(1177):             log_reg.intercept_ = w[:, -1]
1.03 logistic.py(1182):         if scoring is None:
1.03 logistic.py(1183):             scores.append(log_reg.score(X_test, y_test))
1.03 logistic.py(1172):     for w in coefs:
1.03 logistic.py(1173):         if multi_class == 'ovr':
1.03 logistic.py(1174):             w = w[np.newaxis, :]
1.03 logistic.py(1175):         if fit_intercept:
1.03 logistic.py(1176):             log_reg.coef_ = w[:, :-1]
1.03 logistic.py(1177):             log_reg.intercept_ = w[:, -1]
1.03 logistic.py(1182):         if scoring is None:
1.03 logistic.py(1183):             scores.append(log_reg.score(X_test, y_test))
1.03 logistic.py(1172):     for w in coefs:
1.03 logistic.py(1173):         if multi_class == 'ovr':
1.03 logistic.py(1174):             w = w[np.newaxis, :]
1.03 logistic.py(1175):         if fit_intercept:
1.03 logistic.py(1176):             log_reg.coef_ = w[:, :-1]
1.03 logistic.py(1177):             log_reg.intercept_ = w[:, -1]
1.03 logistic.py(1182):         if scoring is None:
1.03 logistic.py(1183):             scores.append(log_reg.score(X_test, y_test))
1.03 logistic.py(1172):     for w in coefs:
1.03 logistic.py(1173):         if multi_class == 'ovr':
1.03 logistic.py(1174):             w = w[np.newaxis, :]
1.03 logistic.py(1175):         if fit_intercept:
1.03 logistic.py(1176):             log_reg.coef_ = w[:, :-1]
1.03 logistic.py(1177):             log_reg.intercept_ = w[:, -1]
1.03 logistic.py(1182):         if scoring is None:
1.03 logistic.py(1183):             scores.append(log_reg.score(X_test, y_test))
1.03 logistic.py(1172):     for w in coefs:
1.03 logistic.py(1173):         if multi_class == 'ovr':
1.03 logistic.py(1174):             w = w[np.newaxis, :]
1.03 logistic.py(1175):         if fit_intercept:
1.03 logistic.py(1176):             log_reg.coef_ = w[:, :-1]
1.03 logistic.py(1177):             log_reg.intercept_ = w[:, -1]
1.03 logistic.py(1182):         if scoring is None:
1.03 logistic.py(1183):             scores.append(log_reg.score(X_test, y_test))
1.03 logistic.py(1172):     for w in coefs:
1.03 logistic.py(1187):     return coefs, Cs, np.array(scores), n_iter
1.03 logistic.py(2071):             for l1_ratio in l1_ratios_)
1.03 logistic.py(2070):             for train, test in folds
1.03 logistic.py(2071):             for l1_ratio in l1_ratios_)
1.03 logistic.py(1132):     X_train = X[train]
1.03 logistic.py(1133):     X_test = X[test]
1.03 logistic.py(1134):     y_train = y[train]
1.03 logistic.py(1135):     y_test = y[test]
1.03 logistic.py(1137):     if sample_weight is not None:
1.03 logistic.py(1143):     coefs, Cs, n_iter = _logistic_regression_path(
1.03 logistic.py(1144):         X_train, y_train, Cs=Cs, l1_ratio=l1_ratio,
1.03 logistic.py(1145):         fit_intercept=fit_intercept, solver=solver, max_iter=max_iter,
1.03 logistic.py(1146):         class_weight=class_weight, pos_class=pos_class,
1.03 logistic.py(1147):         multi_class=multi_class, tol=tol, verbose=verbose, dual=dual,
1.03 logistic.py(1148):         penalty=penalty, intercept_scaling=intercept_scaling,
1.03 logistic.py(1149):         random_state=random_state, check_input=False,
1.03 logistic.py(1150):         max_squared_sum=max_squared_sum, sample_weight=sample_weight)
1.03 logistic.py(803):     if isinstance(Cs, numbers.Integral):
1.03 logistic.py(804):         Cs = np.logspace(-4, 4, Cs)
1.03 logistic.py(806):     solver = _check_solver(solver, penalty, dual)
1.03 logistic.py(428):     all_solvers = ['liblinear', 'newton-cg', 'lbfgs', 'sag', 'saga']
1.03 logistic.py(429):     if solver not in all_solvers:
1.03 logistic.py(433):     all_penalties = ['l1', 'l2', 'elasticnet', 'none']
1.03 logistic.py(434):     if penalty not in all_penalties:
1.03 logistic.py(438):     if solver not in ['liblinear', 'saga'] and penalty not in ('l2', 'none'):
1.03 logistic.py(441):     if solver != 'liblinear' and dual:
1.03 logistic.py(445):     if penalty == 'elasticnet' and solver != 'saga':
1.03 logistic.py(449):     if solver == 'liblinear' and penalty == 'none':
1.03 logistic.py(454):     return solver
1.03 logistic.py(809):     if check_input:
1.03 logistic.py(814):     _, n_features = X.shape
1.03 logistic.py(816):     classes = np.unique(y)
1.03 logistic.py(817):     random_state = check_random_state(random_state)
1.03 logistic.py(819):     multi_class = _check_multi_class(multi_class, solver, len(classes))
1.03 logistic.py(458):     if multi_class == 'auto':
1.03 logistic.py(465):     if multi_class not in ('multinomial', 'ovr'):
1.03 logistic.py(468):     if multi_class == 'multinomial' and solver == 'liblinear':
1.03 logistic.py(471):     return multi_class
1.03 logistic.py(820):     if pos_class is None and multi_class != 'multinomial':
1.03 logistic.py(829):     if sample_weight is not None:
1.03 logistic.py(833):         sample_weight = np.ones(X.shape[0], dtype=X.dtype)
1.03 logistic.py(838):     le = LabelEncoder()
1.03 logistic.py(839):     if isinstance(class_weight, dict) or multi_class == 'multinomial':
1.03 logistic.py(845):     if multi_class == 'ovr':
1.03 logistic.py(846):         w0 = np.zeros(n_features + int(fit_intercept), dtype=X.dtype)
1.03 logistic.py(847):         mask_classes = np.array([-1, 1])
1.03 logistic.py(848):         mask = (y == pos_class)
1.03 logistic.py(849):         y_bin = np.ones(y.shape, dtype=X.dtype)
1.03 logistic.py(850):         y_bin[~mask] = -1.
1.03 logistic.py(853):         if class_weight == "balanced":
1.03 logistic.py(872):     if coef is not None:
1.03 logistic.py(901):     if multi_class == 'multinomial':
1.03 logistic.py(914):         target = y_bin
1.03 logistic.py(915):         if solver == 'lbfgs':
1.03 logistic.py(917):         elif solver == 'newton-cg':
1.03 logistic.py(921):         warm_start_sag = {'coef': np.expand_dims(w0, axis=1)}
1.03 logistic.py(923):     coefs = list()
1.03 logistic.py(924):     n_iter = np.zeros(len(Cs), dtype=np.int32)
1.03 logistic.py(925):     for i, C in enumerate(Cs):
1.03 logistic.py(926):         if solver == 'lbfgs':
1.03 logistic.py(939):         elif solver == 'newton-cg':
1.03 logistic.py(943):         elif solver == 'liblinear':
1.03 logistic.py(953):         elif solver in ['sag', 'saga']:
1.03 logistic.py(954):             if multi_class == 'multinomial':
1.03 logistic.py(958):                 loss = 'log'
1.03 logistic.py(960):             if penalty == 'l1':
1.03 logistic.py(963):             elif penalty == 'l2':
1.03 logistic.py(964):                 alpha = 1. / C
1.03 logistic.py(965):                 beta = 0.
1.03 logistic.py(970):             w0, n_iter_i, warm_start_sag = sag_solver(
1.03 logistic.py(971):                 X, target, sample_weight, loss, alpha,
1.03 logistic.py(972):                 beta, max_iter, tol,
1.03 logistic.py(973):                 verbose, random_state, False, max_squared_sum, warm_start_sag,
1.03 logistic.py(974):                 is_saga=(solver == 'saga'))
1.03 logistic.py(980):         if multi_class == 'multinomial':
1.03 logistic.py(987):             coefs.append(w0.copy())
1.03 logistic.py(989):         n_iter[i] = n_iter_i
1.03 logistic.py(925):     for i, C in enumerate(Cs):
1.03 logistic.py(926):         if solver == 'lbfgs':
1.03 logistic.py(939):         elif solver == 'newton-cg':
1.03 logistic.py(943):         elif solver == 'liblinear':
1.03 logistic.py(953):         elif solver in ['sag', 'saga']:
1.03 logistic.py(954):             if multi_class == 'multinomial':
1.03 logistic.py(958):                 loss = 'log'
1.03 logistic.py(960):             if penalty == 'l1':
1.03 logistic.py(963):             elif penalty == 'l2':
1.03 logistic.py(964):                 alpha = 1. / C
1.03 logistic.py(965):                 beta = 0.
1.03 logistic.py(970):             w0, n_iter_i, warm_start_sag = sag_solver(
1.03 logistic.py(971):                 X, target, sample_weight, loss, alpha,
1.03 logistic.py(972):                 beta, max_iter, tol,
1.03 logistic.py(973):                 verbose, random_state, False, max_squared_sum, warm_start_sag,
1.03 logistic.py(974):                 is_saga=(solver == 'saga'))
1.04 logistic.py(980):         if multi_class == 'multinomial':
1.04 logistic.py(987):             coefs.append(w0.copy())
1.04 logistic.py(989):         n_iter[i] = n_iter_i
1.04 logistic.py(925):     for i, C in enumerate(Cs):
1.04 logistic.py(926):         if solver == 'lbfgs':
1.04 logistic.py(939):         elif solver == 'newton-cg':
1.04 logistic.py(943):         elif solver == 'liblinear':
1.04 logistic.py(953):         elif solver in ['sag', 'saga']:
1.04 logistic.py(954):             if multi_class == 'multinomial':
1.04 logistic.py(958):                 loss = 'log'
1.04 logistic.py(960):             if penalty == 'l1':
1.04 logistic.py(963):             elif penalty == 'l2':
1.04 logistic.py(964):                 alpha = 1. / C
1.04 logistic.py(965):                 beta = 0.
1.04 logistic.py(970):             w0, n_iter_i, warm_start_sag = sag_solver(
1.04 logistic.py(971):                 X, target, sample_weight, loss, alpha,
1.04 logistic.py(972):                 beta, max_iter, tol,
1.04 logistic.py(973):                 verbose, random_state, False, max_squared_sum, warm_start_sag,
1.04 logistic.py(974):                 is_saga=(solver == 'saga'))
1.04 logistic.py(980):         if multi_class == 'multinomial':
1.04 logistic.py(987):             coefs.append(w0.copy())
1.04 logistic.py(989):         n_iter[i] = n_iter_i
1.04 logistic.py(925):     for i, C in enumerate(Cs):
1.04 logistic.py(926):         if solver == 'lbfgs':
1.04 logistic.py(939):         elif solver == 'newton-cg':
1.04 logistic.py(943):         elif solver == 'liblinear':
1.04 logistic.py(953):         elif solver in ['sag', 'saga']:
1.04 logistic.py(954):             if multi_class == 'multinomial':
1.04 logistic.py(958):                 loss = 'log'
1.04 logistic.py(960):             if penalty == 'l1':
1.04 logistic.py(963):             elif penalty == 'l2':
1.04 logistic.py(964):                 alpha = 1. / C
1.04 logistic.py(965):                 beta = 0.
1.04 logistic.py(970):             w0, n_iter_i, warm_start_sag = sag_solver(
1.04 logistic.py(971):                 X, target, sample_weight, loss, alpha,
1.04 logistic.py(972):                 beta, max_iter, tol,
1.04 logistic.py(973):                 verbose, random_state, False, max_squared_sum, warm_start_sag,
1.04 logistic.py(974):                 is_saga=(solver == 'saga'))
1.04 logistic.py(980):         if multi_class == 'multinomial':
1.04 logistic.py(987):             coefs.append(w0.copy())
1.04 logistic.py(989):         n_iter[i] = n_iter_i
1.04 logistic.py(925):     for i, C in enumerate(Cs):
1.04 logistic.py(926):         if solver == 'lbfgs':
1.04 logistic.py(939):         elif solver == 'newton-cg':
1.04 logistic.py(943):         elif solver == 'liblinear':
1.04 logistic.py(953):         elif solver in ['sag', 'saga']:
1.04 logistic.py(954):             if multi_class == 'multinomial':
1.04 logistic.py(958):                 loss = 'log'
1.04 logistic.py(960):             if penalty == 'l1':
1.04 logistic.py(963):             elif penalty == 'l2':
1.04 logistic.py(964):                 alpha = 1. / C
1.04 logistic.py(965):                 beta = 0.
1.04 logistic.py(970):             w0, n_iter_i, warm_start_sag = sag_solver(
1.04 logistic.py(971):                 X, target, sample_weight, loss, alpha,
1.04 logistic.py(972):                 beta, max_iter, tol,
1.04 logistic.py(973):                 verbose, random_state, False, max_squared_sum, warm_start_sag,
1.04 logistic.py(974):                 is_saga=(solver == 'saga'))
1.04 logistic.py(980):         if multi_class == 'multinomial':
1.04 logistic.py(987):             coefs.append(w0.copy())
1.04 logistic.py(989):         n_iter[i] = n_iter_i
1.04 logistic.py(925):     for i, C in enumerate(Cs):
1.04 logistic.py(926):         if solver == 'lbfgs':
1.04 logistic.py(939):         elif solver == 'newton-cg':
1.04 logistic.py(943):         elif solver == 'liblinear':
1.04 logistic.py(953):         elif solver in ['sag', 'saga']:
1.04 logistic.py(954):             if multi_class == 'multinomial':
1.04 logistic.py(958):                 loss = 'log'
1.04 logistic.py(960):             if penalty == 'l1':
1.04 logistic.py(963):             elif penalty == 'l2':
1.04 logistic.py(964):                 alpha = 1. / C
1.04 logistic.py(965):                 beta = 0.
1.04 logistic.py(970):             w0, n_iter_i, warm_start_sag = sag_solver(
1.04 logistic.py(971):                 X, target, sample_weight, loss, alpha,
1.04 logistic.py(972):                 beta, max_iter, tol,
1.04 logistic.py(973):                 verbose, random_state, False, max_squared_sum, warm_start_sag,
1.04 logistic.py(974):                 is_saga=(solver == 'saga'))
1.04 logistic.py(980):         if multi_class == 'multinomial':
1.04 logistic.py(987):             coefs.append(w0.copy())
1.04 logistic.py(989):         n_iter[i] = n_iter_i
1.04 logistic.py(925):     for i, C in enumerate(Cs):
1.04 logistic.py(926):         if solver == 'lbfgs':
1.04 logistic.py(939):         elif solver == 'newton-cg':
1.04 logistic.py(943):         elif solver == 'liblinear':
1.04 logistic.py(953):         elif solver in ['sag', 'saga']:
1.04 logistic.py(954):             if multi_class == 'multinomial':
1.04 logistic.py(958):                 loss = 'log'
1.04 logistic.py(960):             if penalty == 'l1':
1.04 logistic.py(963):             elif penalty == 'l2':
1.04 logistic.py(964):                 alpha = 1. / C
1.04 logistic.py(965):                 beta = 0.
1.04 logistic.py(970):             w0, n_iter_i, warm_start_sag = sag_solver(
1.04 logistic.py(971):                 X, target, sample_weight, loss, alpha,
1.04 logistic.py(972):                 beta, max_iter, tol,
1.04 logistic.py(973):                 verbose, random_state, False, max_squared_sum, warm_start_sag,
1.04 logistic.py(974):                 is_saga=(solver == 'saga'))
1.04 logistic.py(980):         if multi_class == 'multinomial':
1.04 logistic.py(987):             coefs.append(w0.copy())
1.04 logistic.py(989):         n_iter[i] = n_iter_i
1.04 logistic.py(925):     for i, C in enumerate(Cs):
1.04 logistic.py(926):         if solver == 'lbfgs':
1.04 logistic.py(939):         elif solver == 'newton-cg':
1.04 logistic.py(943):         elif solver == 'liblinear':
1.04 logistic.py(953):         elif solver in ['sag', 'saga']:
1.04 logistic.py(954):             if multi_class == 'multinomial':
1.04 logistic.py(958):                 loss = 'log'
1.04 logistic.py(960):             if penalty == 'l1':
1.04 logistic.py(963):             elif penalty == 'l2':
1.04 logistic.py(964):                 alpha = 1. / C
1.04 logistic.py(965):                 beta = 0.
1.04 logistic.py(970):             w0, n_iter_i, warm_start_sag = sag_solver(
1.04 logistic.py(971):                 X, target, sample_weight, loss, alpha,
1.04 logistic.py(972):                 beta, max_iter, tol,
1.04 logistic.py(973):                 verbose, random_state, False, max_squared_sum, warm_start_sag,
1.04 logistic.py(974):                 is_saga=(solver == 'saga'))
1.04 logistic.py(980):         if multi_class == 'multinomial':
1.04 logistic.py(987):             coefs.append(w0.copy())
1.04 logistic.py(989):         n_iter[i] = n_iter_i
1.04 logistic.py(925):     for i, C in enumerate(Cs):
1.04 logistic.py(926):         if solver == 'lbfgs':
1.04 logistic.py(939):         elif solver == 'newton-cg':
1.04 logistic.py(943):         elif solver == 'liblinear':
1.04 logistic.py(953):         elif solver in ['sag', 'saga']:
1.04 logistic.py(954):             if multi_class == 'multinomial':
1.04 logistic.py(958):                 loss = 'log'
1.04 logistic.py(960):             if penalty == 'l1':
1.04 logistic.py(963):             elif penalty == 'l2':
1.04 logistic.py(964):                 alpha = 1. / C
1.04 logistic.py(965):                 beta = 0.
1.04 logistic.py(970):             w0, n_iter_i, warm_start_sag = sag_solver(
1.04 logistic.py(971):                 X, target, sample_weight, loss, alpha,
1.04 logistic.py(972):                 beta, max_iter, tol,
1.04 logistic.py(973):                 verbose, random_state, False, max_squared_sum, warm_start_sag,
1.04 logistic.py(974):                 is_saga=(solver == 'saga'))
1.04 logistic.py(980):         if multi_class == 'multinomial':
1.04 logistic.py(987):             coefs.append(w0.copy())
1.04 logistic.py(989):         n_iter[i] = n_iter_i
1.04 logistic.py(925):     for i, C in enumerate(Cs):
1.04 logistic.py(926):         if solver == 'lbfgs':
1.04 logistic.py(939):         elif solver == 'newton-cg':
1.04 logistic.py(943):         elif solver == 'liblinear':
1.04 logistic.py(953):         elif solver in ['sag', 'saga']:
1.04 logistic.py(954):             if multi_class == 'multinomial':
1.04 logistic.py(958):                 loss = 'log'
1.04 logistic.py(960):             if penalty == 'l1':
1.04 logistic.py(963):             elif penalty == 'l2':
1.04 logistic.py(964):                 alpha = 1. / C
1.04 logistic.py(965):                 beta = 0.
1.04 logistic.py(970):             w0, n_iter_i, warm_start_sag = sag_solver(
1.04 logistic.py(971):                 X, target, sample_weight, loss, alpha,
1.04 logistic.py(972):                 beta, max_iter, tol,
1.04 logistic.py(973):                 verbose, random_state, False, max_squared_sum, warm_start_sag,
1.04 logistic.py(974):                 is_saga=(solver == 'saga'))
1.04 logistic.py(980):         if multi_class == 'multinomial':
1.04 logistic.py(987):             coefs.append(w0.copy())
1.04 logistic.py(989):         n_iter[i] = n_iter_i
1.04 logistic.py(925):     for i, C in enumerate(Cs):
1.04 logistic.py(991):     return np.array(coefs), np.array(Cs), n_iter
1.04 logistic.py(1152):     log_reg = LogisticRegression(solver=solver, multi_class=multi_class)
1.04 logistic.py(1437):         self.penalty = penalty
1.04 logistic.py(1438):         self.dual = dual
1.04 logistic.py(1439):         self.tol = tol
1.04 logistic.py(1440):         self.C = C
1.04 logistic.py(1441):         self.fit_intercept = fit_intercept
1.04 logistic.py(1442):         self.intercept_scaling = intercept_scaling
1.04 logistic.py(1443):         self.class_weight = class_weight
1.04 logistic.py(1444):         self.random_state = random_state
1.04 logistic.py(1445):         self.solver = solver
1.04 logistic.py(1446):         self.max_iter = max_iter
1.04 logistic.py(1447):         self.multi_class = multi_class
1.04 logistic.py(1448):         self.verbose = verbose
1.04 logistic.py(1449):         self.warm_start = warm_start
1.04 logistic.py(1450):         self.n_jobs = n_jobs
1.04 logistic.py(1451):         self.l1_ratio = l1_ratio
1.04 logistic.py(1155):     if multi_class == 'ovr':
1.04 logistic.py(1156):         log_reg.classes_ = np.array([-1, 1])
1.04 logistic.py(1163):     if pos_class is not None:
1.04 logistic.py(1164):         mask = (y_test == pos_class)
1.04 logistic.py(1165):         y_test = np.ones(y_test.shape, dtype=np.float64)
1.04 logistic.py(1166):         y_test[~mask] = -1.
1.04 logistic.py(1168):     scores = list()
1.04 logistic.py(1170):     if isinstance(scoring, str):
1.04 logistic.py(1172):     for w in coefs:
1.04 logistic.py(1173):         if multi_class == 'ovr':
1.04 logistic.py(1174):             w = w[np.newaxis, :]
1.04 logistic.py(1175):         if fit_intercept:
1.04 logistic.py(1176):             log_reg.coef_ = w[:, :-1]
1.04 logistic.py(1177):             log_reg.intercept_ = w[:, -1]
1.04 logistic.py(1182):         if scoring is None:
1.04 logistic.py(1183):             scores.append(log_reg.score(X_test, y_test))
1.04 logistic.py(1172):     for w in coefs:
1.04 logistic.py(1173):         if multi_class == 'ovr':
1.04 logistic.py(1174):             w = w[np.newaxis, :]
1.04 logistic.py(1175):         if fit_intercept:
1.04 logistic.py(1176):             log_reg.coef_ = w[:, :-1]
1.04 logistic.py(1177):             log_reg.intercept_ = w[:, -1]
1.04 logistic.py(1182):         if scoring is None:
1.04 logistic.py(1183):             scores.append(log_reg.score(X_test, y_test))
1.04 logistic.py(1172):     for w in coefs:
1.04 logistic.py(1173):         if multi_class == 'ovr':
1.04 logistic.py(1174):             w = w[np.newaxis, :]
1.04 logistic.py(1175):         if fit_intercept:
1.04 logistic.py(1176):             log_reg.coef_ = w[:, :-1]
1.04 logistic.py(1177):             log_reg.intercept_ = w[:, -1]
1.04 logistic.py(1182):         if scoring is None:
1.04 logistic.py(1183):             scores.append(log_reg.score(X_test, y_test))
1.04 logistic.py(1172):     for w in coefs:
1.04 logistic.py(1173):         if multi_class == 'ovr':
1.04 logistic.py(1174):             w = w[np.newaxis, :]
1.04 logistic.py(1175):         if fit_intercept:
1.04 logistic.py(1176):             log_reg.coef_ = w[:, :-1]
1.04 logistic.py(1177):             log_reg.intercept_ = w[:, -1]
1.04 logistic.py(1182):         if scoring is None:
1.04 logistic.py(1183):             scores.append(log_reg.score(X_test, y_test))
1.04 logistic.py(1172):     for w in coefs:
1.04 logistic.py(1173):         if multi_class == 'ovr':
1.04 logistic.py(1174):             w = w[np.newaxis, :]
1.04 logistic.py(1175):         if fit_intercept:
1.04 logistic.py(1176):             log_reg.coef_ = w[:, :-1]
1.04 logistic.py(1177):             log_reg.intercept_ = w[:, -1]
1.04 logistic.py(1182):         if scoring is None:
1.04 logistic.py(1183):             scores.append(log_reg.score(X_test, y_test))
1.04 logistic.py(1172):     for w in coefs:
1.04 logistic.py(1173):         if multi_class == 'ovr':
1.04 logistic.py(1174):             w = w[np.newaxis, :]
1.04 logistic.py(1175):         if fit_intercept:
1.04 logistic.py(1176):             log_reg.coef_ = w[:, :-1]
1.04 logistic.py(1177):             log_reg.intercept_ = w[:, -1]
1.04 logistic.py(1182):         if scoring is None:
1.04 logistic.py(1183):             scores.append(log_reg.score(X_test, y_test))
1.05 logistic.py(1172):     for w in coefs:
1.05 logistic.py(1173):         if multi_class == 'ovr':
1.05 logistic.py(1174):             w = w[np.newaxis, :]
1.05 logistic.py(1175):         if fit_intercept:
1.05 logistic.py(1176):             log_reg.coef_ = w[:, :-1]
1.05 logistic.py(1177):             log_reg.intercept_ = w[:, -1]
1.05 logistic.py(1182):         if scoring is None:
1.05 logistic.py(1183):             scores.append(log_reg.score(X_test, y_test))
1.05 logistic.py(1172):     for w in coefs:
1.05 logistic.py(1173):         if multi_class == 'ovr':
1.05 logistic.py(1174):             w = w[np.newaxis, :]
1.05 logistic.py(1175):         if fit_intercept:
1.05 logistic.py(1176):             log_reg.coef_ = w[:, :-1]
1.05 logistic.py(1177):             log_reg.intercept_ = w[:, -1]
1.05 logistic.py(1182):         if scoring is None:
1.05 logistic.py(1183):             scores.append(log_reg.score(X_test, y_test))
1.05 logistic.py(1172):     for w in coefs:
1.05 logistic.py(1173):         if multi_class == 'ovr':
1.05 logistic.py(1174):             w = w[np.newaxis, :]
1.05 logistic.py(1175):         if fit_intercept:
1.05 logistic.py(1176):             log_reg.coef_ = w[:, :-1]
1.05 logistic.py(1177):             log_reg.intercept_ = w[:, -1]
1.05 logistic.py(1182):         if scoring is None:
1.05 logistic.py(1183):             scores.append(log_reg.score(X_test, y_test))
1.05 logistic.py(1172):     for w in coefs:
1.05 logistic.py(1173):         if multi_class == 'ovr':
1.05 logistic.py(1174):             w = w[np.newaxis, :]
1.05 logistic.py(1175):         if fit_intercept:
1.05 logistic.py(1176):             log_reg.coef_ = w[:, :-1]
1.05 logistic.py(1177):             log_reg.intercept_ = w[:, -1]
1.05 logistic.py(1182):         if scoring is None:
1.05 logistic.py(1183):             scores.append(log_reg.score(X_test, y_test))
1.05 logistic.py(1172):     for w in coefs:
1.05 logistic.py(1187):     return coefs, Cs, np.array(scores), n_iter
1.05 logistic.py(2071):             for l1_ratio in l1_ratios_)
1.05 logistic.py(2070):             for train, test in folds
1.05 logistic.py(2071):             for l1_ratio in l1_ratios_)
1.05 logistic.py(1132):     X_train = X[train]
1.05 logistic.py(1133):     X_test = X[test]
1.05 logistic.py(1134):     y_train = y[train]
1.05 logistic.py(1135):     y_test = y[test]
1.05 logistic.py(1137):     if sample_weight is not None:
1.05 logistic.py(1143):     coefs, Cs, n_iter = _logistic_regression_path(
1.05 logistic.py(1144):         X_train, y_train, Cs=Cs, l1_ratio=l1_ratio,
1.05 logistic.py(1145):         fit_intercept=fit_intercept, solver=solver, max_iter=max_iter,
1.05 logistic.py(1146):         class_weight=class_weight, pos_class=pos_class,
1.05 logistic.py(1147):         multi_class=multi_class, tol=tol, verbose=verbose, dual=dual,
1.05 logistic.py(1148):         penalty=penalty, intercept_scaling=intercept_scaling,
1.05 logistic.py(1149):         random_state=random_state, check_input=False,
1.05 logistic.py(1150):         max_squared_sum=max_squared_sum, sample_weight=sample_weight)
1.05 logistic.py(803):     if isinstance(Cs, numbers.Integral):
1.05 logistic.py(804):         Cs = np.logspace(-4, 4, Cs)
1.05 logistic.py(806):     solver = _check_solver(solver, penalty, dual)
1.05 logistic.py(428):     all_solvers = ['liblinear', 'newton-cg', 'lbfgs', 'sag', 'saga']
1.05 logistic.py(429):     if solver not in all_solvers:
1.05 logistic.py(433):     all_penalties = ['l1', 'l2', 'elasticnet', 'none']
1.05 logistic.py(434):     if penalty not in all_penalties:
1.05 logistic.py(438):     if solver not in ['liblinear', 'saga'] and penalty not in ('l2', 'none'):
1.05 logistic.py(441):     if solver != 'liblinear' and dual:
1.05 logistic.py(445):     if penalty == 'elasticnet' and solver != 'saga':
1.05 logistic.py(449):     if solver == 'liblinear' and penalty == 'none':
1.05 logistic.py(454):     return solver
1.05 logistic.py(809):     if check_input:
1.05 logistic.py(814):     _, n_features = X.shape
1.05 logistic.py(816):     classes = np.unique(y)
1.05 logistic.py(817):     random_state = check_random_state(random_state)
1.05 logistic.py(819):     multi_class = _check_multi_class(multi_class, solver, len(classes))
1.05 logistic.py(458):     if multi_class == 'auto':
1.05 logistic.py(465):     if multi_class not in ('multinomial', 'ovr'):
1.05 logistic.py(468):     if multi_class == 'multinomial' and solver == 'liblinear':
1.05 logistic.py(471):     return multi_class
1.05 logistic.py(820):     if pos_class is None and multi_class != 'multinomial':
1.05 logistic.py(829):     if sample_weight is not None:
1.05 logistic.py(833):         sample_weight = np.ones(X.shape[0], dtype=X.dtype)
1.05 logistic.py(838):     le = LabelEncoder()
1.05 logistic.py(839):     if isinstance(class_weight, dict) or multi_class == 'multinomial':
1.05 logistic.py(845):     if multi_class == 'ovr':
1.05 logistic.py(846):         w0 = np.zeros(n_features + int(fit_intercept), dtype=X.dtype)
1.05 logistic.py(847):         mask_classes = np.array([-1, 1])
1.05 logistic.py(848):         mask = (y == pos_class)
1.05 logistic.py(849):         y_bin = np.ones(y.shape, dtype=X.dtype)
1.05 logistic.py(850):         y_bin[~mask] = -1.
1.05 logistic.py(853):         if class_weight == "balanced":
1.05 logistic.py(872):     if coef is not None:
1.05 logistic.py(901):     if multi_class == 'multinomial':
1.05 logistic.py(914):         target = y_bin
1.05 logistic.py(915):         if solver == 'lbfgs':
1.05 logistic.py(917):         elif solver == 'newton-cg':
1.05 logistic.py(921):         warm_start_sag = {'coef': np.expand_dims(w0, axis=1)}
1.05 logistic.py(923):     coefs = list()
1.05 logistic.py(924):     n_iter = np.zeros(len(Cs), dtype=np.int32)
1.05 logistic.py(925):     for i, C in enumerate(Cs):
1.05 logistic.py(926):         if solver == 'lbfgs':
1.05 logistic.py(939):         elif solver == 'newton-cg':
1.05 logistic.py(943):         elif solver == 'liblinear':
1.05 logistic.py(953):         elif solver in ['sag', 'saga']:
1.05 logistic.py(954):             if multi_class == 'multinomial':
1.05 logistic.py(958):                 loss = 'log'
1.05 logistic.py(960):             if penalty == 'l1':
1.05 logistic.py(963):             elif penalty == 'l2':
1.05 logistic.py(964):                 alpha = 1. / C
1.05 logistic.py(965):                 beta = 0.
1.05 logistic.py(970):             w0, n_iter_i, warm_start_sag = sag_solver(
1.05 logistic.py(971):                 X, target, sample_weight, loss, alpha,
1.05 logistic.py(972):                 beta, max_iter, tol,
1.05 logistic.py(973):                 verbose, random_state, False, max_squared_sum, warm_start_sag,
1.05 logistic.py(974):                 is_saga=(solver == 'saga'))
1.05 logistic.py(980):         if multi_class == 'multinomial':
1.05 logistic.py(987):             coefs.append(w0.copy())
1.05 logistic.py(989):         n_iter[i] = n_iter_i
1.05 logistic.py(925):     for i, C in enumerate(Cs):
1.05 logistic.py(926):         if solver == 'lbfgs':
1.05 logistic.py(939):         elif solver == 'newton-cg':
1.05 logistic.py(943):         elif solver == 'liblinear':
1.05 logistic.py(953):         elif solver in ['sag', 'saga']:
1.05 logistic.py(954):             if multi_class == 'multinomial':
1.05 logistic.py(958):                 loss = 'log'
1.05 logistic.py(960):             if penalty == 'l1':
1.05 logistic.py(963):             elif penalty == 'l2':
1.05 logistic.py(964):                 alpha = 1. / C
1.05 logistic.py(965):                 beta = 0.
1.05 logistic.py(970):             w0, n_iter_i, warm_start_sag = sag_solver(
1.05 logistic.py(971):                 X, target, sample_weight, loss, alpha,
1.05 logistic.py(972):                 beta, max_iter, tol,
1.05 logistic.py(973):                 verbose, random_state, False, max_squared_sum, warm_start_sag,
1.05 logistic.py(974):                 is_saga=(solver == 'saga'))
1.05 logistic.py(980):         if multi_class == 'multinomial':
1.05 logistic.py(987):             coefs.append(w0.copy())
1.05 logistic.py(989):         n_iter[i] = n_iter_i
1.05 logistic.py(925):     for i, C in enumerate(Cs):
1.05 logistic.py(926):         if solver == 'lbfgs':
1.05 logistic.py(939):         elif solver == 'newton-cg':
1.05 logistic.py(943):         elif solver == 'liblinear':
1.05 logistic.py(953):         elif solver in ['sag', 'saga']:
1.05 logistic.py(954):             if multi_class == 'multinomial':
1.05 logistic.py(958):                 loss = 'log'
1.05 logistic.py(960):             if penalty == 'l1':
1.05 logistic.py(963):             elif penalty == 'l2':
1.05 logistic.py(964):                 alpha = 1. / C
1.05 logistic.py(965):                 beta = 0.
1.05 logistic.py(970):             w0, n_iter_i, warm_start_sag = sag_solver(
1.05 logistic.py(971):                 X, target, sample_weight, loss, alpha,
1.05 logistic.py(972):                 beta, max_iter, tol,
1.05 logistic.py(973):                 verbose, random_state, False, max_squared_sum, warm_start_sag,
1.05 logistic.py(974):                 is_saga=(solver == 'saga'))
1.05 logistic.py(980):         if multi_class == 'multinomial':
1.05 logistic.py(987):             coefs.append(w0.copy())
1.05 logistic.py(989):         n_iter[i] = n_iter_i
1.05 logistic.py(925):     for i, C in enumerate(Cs):
1.05 logistic.py(926):         if solver == 'lbfgs':
1.05 logistic.py(939):         elif solver == 'newton-cg':
1.05 logistic.py(943):         elif solver == 'liblinear':
1.05 logistic.py(953):         elif solver in ['sag', 'saga']:
1.05 logistic.py(954):             if multi_class == 'multinomial':
1.05 logistic.py(958):                 loss = 'log'
1.05 logistic.py(960):             if penalty == 'l1':
1.05 logistic.py(963):             elif penalty == 'l2':
1.05 logistic.py(964):                 alpha = 1. / C
1.05 logistic.py(965):                 beta = 0.
1.05 logistic.py(970):             w0, n_iter_i, warm_start_sag = sag_solver(
1.05 logistic.py(971):                 X, target, sample_weight, loss, alpha,
1.05 logistic.py(972):                 beta, max_iter, tol,
1.05 logistic.py(973):                 verbose, random_state, False, max_squared_sum, warm_start_sag,
1.05 logistic.py(974):                 is_saga=(solver == 'saga'))
1.05 logistic.py(980):         if multi_class == 'multinomial':
1.05 logistic.py(987):             coefs.append(w0.copy())
1.05 logistic.py(989):         n_iter[i] = n_iter_i
1.05 logistic.py(925):     for i, C in enumerate(Cs):
1.05 logistic.py(926):         if solver == 'lbfgs':
1.05 logistic.py(939):         elif solver == 'newton-cg':
1.05 logistic.py(943):         elif solver == 'liblinear':
1.05 logistic.py(953):         elif solver in ['sag', 'saga']:
1.05 logistic.py(954):             if multi_class == 'multinomial':
1.05 logistic.py(958):                 loss = 'log'
1.05 logistic.py(960):             if penalty == 'l1':
1.05 logistic.py(963):             elif penalty == 'l2':
1.05 logistic.py(964):                 alpha = 1. / C
1.05 logistic.py(965):                 beta = 0.
1.05 logistic.py(970):             w0, n_iter_i, warm_start_sag = sag_solver(
1.05 logistic.py(971):                 X, target, sample_weight, loss, alpha,
1.05 logistic.py(972):                 beta, max_iter, tol,
1.05 logistic.py(973):                 verbose, random_state, False, max_squared_sum, warm_start_sag,
1.05 logistic.py(974):                 is_saga=(solver == 'saga'))
1.05 logistic.py(980):         if multi_class == 'multinomial':
1.05 logistic.py(987):             coefs.append(w0.copy())
1.05 logistic.py(989):         n_iter[i] = n_iter_i
1.05 logistic.py(925):     for i, C in enumerate(Cs):
1.05 logistic.py(926):         if solver == 'lbfgs':
1.05 logistic.py(939):         elif solver == 'newton-cg':
1.05 logistic.py(943):         elif solver == 'liblinear':
1.05 logistic.py(953):         elif solver in ['sag', 'saga']:
1.05 logistic.py(954):             if multi_class == 'multinomial':
1.05 logistic.py(958):                 loss = 'log'
1.05 logistic.py(960):             if penalty == 'l1':
1.05 logistic.py(963):             elif penalty == 'l2':
1.05 logistic.py(964):                 alpha = 1. / C
1.05 logistic.py(965):                 beta = 0.
1.05 logistic.py(970):             w0, n_iter_i, warm_start_sag = sag_solver(
1.05 logistic.py(971):                 X, target, sample_weight, loss, alpha,
1.05 logistic.py(972):                 beta, max_iter, tol,
1.05 logistic.py(973):                 verbose, random_state, False, max_squared_sum, warm_start_sag,
1.05 logistic.py(974):                 is_saga=(solver == 'saga'))
1.05 logistic.py(980):         if multi_class == 'multinomial':
1.05 logistic.py(987):             coefs.append(w0.copy())
1.05 logistic.py(989):         n_iter[i] = n_iter_i
1.05 logistic.py(925):     for i, C in enumerate(Cs):
1.05 logistic.py(926):         if solver == 'lbfgs':
1.05 logistic.py(939):         elif solver == 'newton-cg':
1.05 logistic.py(943):         elif solver == 'liblinear':
1.05 logistic.py(953):         elif solver in ['sag', 'saga']:
1.05 logistic.py(954):             if multi_class == 'multinomial':
1.05 logistic.py(958):                 loss = 'log'
1.05 logistic.py(960):             if penalty == 'l1':
1.05 logistic.py(963):             elif penalty == 'l2':
1.05 logistic.py(964):                 alpha = 1. / C
1.05 logistic.py(965):                 beta = 0.
1.05 logistic.py(970):             w0, n_iter_i, warm_start_sag = sag_solver(
1.05 logistic.py(971):                 X, target, sample_weight, loss, alpha,
1.05 logistic.py(972):                 beta, max_iter, tol,
1.05 logistic.py(973):                 verbose, random_state, False, max_squared_sum, warm_start_sag,
1.05 logistic.py(974):                 is_saga=(solver == 'saga'))
1.05 logistic.py(980):         if multi_class == 'multinomial':
1.05 logistic.py(987):             coefs.append(w0.copy())
1.05 logistic.py(989):         n_iter[i] = n_iter_i
1.05 logistic.py(925):     for i, C in enumerate(Cs):
1.05 logistic.py(926):         if solver == 'lbfgs':
1.05 logistic.py(939):         elif solver == 'newton-cg':
1.05 logistic.py(943):         elif solver == 'liblinear':
1.05 logistic.py(953):         elif solver in ['sag', 'saga']:
1.05 logistic.py(954):             if multi_class == 'multinomial':
1.05 logistic.py(958):                 loss = 'log'
1.05 logistic.py(960):             if penalty == 'l1':
1.05 logistic.py(963):             elif penalty == 'l2':
1.05 logistic.py(964):                 alpha = 1. / C
1.05 logistic.py(965):                 beta = 0.
1.05 logistic.py(970):             w0, n_iter_i, warm_start_sag = sag_solver(
1.05 logistic.py(971):                 X, target, sample_weight, loss, alpha,
1.05 logistic.py(972):                 beta, max_iter, tol,
1.05 logistic.py(973):                 verbose, random_state, False, max_squared_sum, warm_start_sag,
1.05 logistic.py(974):                 is_saga=(solver == 'saga'))
1.06 logistic.py(980):         if multi_class == 'multinomial':
1.06 logistic.py(987):             coefs.append(w0.copy())
1.06 logistic.py(989):         n_iter[i] = n_iter_i
1.06 logistic.py(925):     for i, C in enumerate(Cs):
1.06 logistic.py(926):         if solver == 'lbfgs':
1.06 logistic.py(939):         elif solver == 'newton-cg':
1.06 logistic.py(943):         elif solver == 'liblinear':
1.06 logistic.py(953):         elif solver in ['sag', 'saga']:
1.06 logistic.py(954):             if multi_class == 'multinomial':
1.06 logistic.py(958):                 loss = 'log'
1.06 logistic.py(960):             if penalty == 'l1':
1.06 logistic.py(963):             elif penalty == 'l2':
1.06 logistic.py(964):                 alpha = 1. / C
1.06 logistic.py(965):                 beta = 0.
1.06 logistic.py(970):             w0, n_iter_i, warm_start_sag = sag_solver(
1.06 logistic.py(971):                 X, target, sample_weight, loss, alpha,
1.06 logistic.py(972):                 beta, max_iter, tol,
1.06 logistic.py(973):                 verbose, random_state, False, max_squared_sum, warm_start_sag,
1.06 logistic.py(974):                 is_saga=(solver == 'saga'))
1.06 logistic.py(980):         if multi_class == 'multinomial':
1.06 logistic.py(987):             coefs.append(w0.copy())
1.06 logistic.py(989):         n_iter[i] = n_iter_i
1.06 logistic.py(925):     for i, C in enumerate(Cs):
1.06 logistic.py(926):         if solver == 'lbfgs':
1.06 logistic.py(939):         elif solver == 'newton-cg':
1.06 logistic.py(943):         elif solver == 'liblinear':
1.06 logistic.py(953):         elif solver in ['sag', 'saga']:
1.06 logistic.py(954):             if multi_class == 'multinomial':
1.06 logistic.py(958):                 loss = 'log'
1.06 logistic.py(960):             if penalty == 'l1':
1.06 logistic.py(963):             elif penalty == 'l2':
1.06 logistic.py(964):                 alpha = 1. / C
1.06 logistic.py(965):                 beta = 0.
1.06 logistic.py(970):             w0, n_iter_i, warm_start_sag = sag_solver(
1.06 logistic.py(971):                 X, target, sample_weight, loss, alpha,
1.06 logistic.py(972):                 beta, max_iter, tol,
1.06 logistic.py(973):                 verbose, random_state, False, max_squared_sum, warm_start_sag,
1.06 logistic.py(974):                 is_saga=(solver == 'saga'))
1.06 logistic.py(980):         if multi_class == 'multinomial':
1.06 logistic.py(987):             coefs.append(w0.copy())
1.06 logistic.py(989):         n_iter[i] = n_iter_i
1.06 logistic.py(925):     for i, C in enumerate(Cs):
1.06 logistic.py(991):     return np.array(coefs), np.array(Cs), n_iter
1.06 logistic.py(1152):     log_reg = LogisticRegression(solver=solver, multi_class=multi_class)
1.06 logistic.py(1437):         self.penalty = penalty
1.06 logistic.py(1438):         self.dual = dual
1.06 logistic.py(1439):         self.tol = tol
1.06 logistic.py(1440):         self.C = C
1.06 logistic.py(1441):         self.fit_intercept = fit_intercept
1.06 logistic.py(1442):         self.intercept_scaling = intercept_scaling
1.06 logistic.py(1443):         self.class_weight = class_weight
1.06 logistic.py(1444):         self.random_state = random_state
1.06 logistic.py(1445):         self.solver = solver
1.06 logistic.py(1446):         self.max_iter = max_iter
1.06 logistic.py(1447):         self.multi_class = multi_class
1.06 logistic.py(1448):         self.verbose = verbose
1.06 logistic.py(1449):         self.warm_start = warm_start
1.06 logistic.py(1450):         self.n_jobs = n_jobs
1.06 logistic.py(1451):         self.l1_ratio = l1_ratio
1.06 logistic.py(1155):     if multi_class == 'ovr':
1.06 logistic.py(1156):         log_reg.classes_ = np.array([-1, 1])
1.06 logistic.py(1163):     if pos_class is not None:
1.06 logistic.py(1164):         mask = (y_test == pos_class)
1.06 logistic.py(1165):         y_test = np.ones(y_test.shape, dtype=np.float64)
1.06 logistic.py(1166):         y_test[~mask] = -1.
1.06 logistic.py(1168):     scores = list()
1.06 logistic.py(1170):     if isinstance(scoring, str):
1.06 logistic.py(1172):     for w in coefs:
1.06 logistic.py(1173):         if multi_class == 'ovr':
1.06 logistic.py(1174):             w = w[np.newaxis, :]
1.06 logistic.py(1175):         if fit_intercept:
1.06 logistic.py(1176):             log_reg.coef_ = w[:, :-1]
1.06 logistic.py(1177):             log_reg.intercept_ = w[:, -1]
1.06 logistic.py(1182):         if scoring is None:
1.06 logistic.py(1183):             scores.append(log_reg.score(X_test, y_test))
1.06 logistic.py(1172):     for w in coefs:
1.06 logistic.py(1173):         if multi_class == 'ovr':
1.06 logistic.py(1174):             w = w[np.newaxis, :]
1.06 logistic.py(1175):         if fit_intercept:
1.06 logistic.py(1176):             log_reg.coef_ = w[:, :-1]
1.06 logistic.py(1177):             log_reg.intercept_ = w[:, -1]
1.06 logistic.py(1182):         if scoring is None:
1.06 logistic.py(1183):             scores.append(log_reg.score(X_test, y_test))
1.06 logistic.py(1172):     for w in coefs:
1.06 logistic.py(1173):         if multi_class == 'ovr':
1.06 logistic.py(1174):             w = w[np.newaxis, :]
1.06 logistic.py(1175):         if fit_intercept:
1.06 logistic.py(1176):             log_reg.coef_ = w[:, :-1]
1.06 logistic.py(1177):             log_reg.intercept_ = w[:, -1]
1.06 logistic.py(1182):         if scoring is None:
1.06 logistic.py(1183):             scores.append(log_reg.score(X_test, y_test))
1.06 logistic.py(1172):     for w in coefs:
1.06 logistic.py(1173):         if multi_class == 'ovr':
1.06 logistic.py(1174):             w = w[np.newaxis, :]
1.06 logistic.py(1175):         if fit_intercept:
1.06 logistic.py(1176):             log_reg.coef_ = w[:, :-1]
1.06 logistic.py(1177):             log_reg.intercept_ = w[:, -1]
1.06 logistic.py(1182):         if scoring is None:
1.06 logistic.py(1183):             scores.append(log_reg.score(X_test, y_test))
1.06 logistic.py(1172):     for w in coefs:
1.06 logistic.py(1173):         if multi_class == 'ovr':
1.06 logistic.py(1174):             w = w[np.newaxis, :]
1.06 logistic.py(1175):         if fit_intercept:
1.06 logistic.py(1176):             log_reg.coef_ = w[:, :-1]
1.06 logistic.py(1177):             log_reg.intercept_ = w[:, -1]
1.06 logistic.py(1182):         if scoring is None:
1.06 logistic.py(1183):             scores.append(log_reg.score(X_test, y_test))
1.06 logistic.py(1172):     for w in coefs:
1.06 logistic.py(1173):         if multi_class == 'ovr':
1.06 logistic.py(1174):             w = w[np.newaxis, :]
1.06 logistic.py(1175):         if fit_intercept:
1.06 logistic.py(1176):             log_reg.coef_ = w[:, :-1]
1.06 logistic.py(1177):             log_reg.intercept_ = w[:, -1]
1.06 logistic.py(1182):         if scoring is None:
1.06 logistic.py(1183):             scores.append(log_reg.score(X_test, y_test))
1.06 logistic.py(1172):     for w in coefs:
1.06 logistic.py(1173):         if multi_class == 'ovr':
1.06 logistic.py(1174):             w = w[np.newaxis, :]
1.06 logistic.py(1175):         if fit_intercept:
1.06 logistic.py(1176):             log_reg.coef_ = w[:, :-1]
1.06 logistic.py(1177):             log_reg.intercept_ = w[:, -1]
1.06 logistic.py(1182):         if scoring is None:
1.06 logistic.py(1183):             scores.append(log_reg.score(X_test, y_test))
1.06 logistic.py(1172):     for w in coefs:
1.06 logistic.py(1173):         if multi_class == 'ovr':
1.06 logistic.py(1174):             w = w[np.newaxis, :]
1.06 logistic.py(1175):         if fit_intercept:
1.06 logistic.py(1176):             log_reg.coef_ = w[:, :-1]
1.06 logistic.py(1177):             log_reg.intercept_ = w[:, -1]
1.06 logistic.py(1182):         if scoring is None:
1.06 logistic.py(1183):             scores.append(log_reg.score(X_test, y_test))
1.06 logistic.py(1172):     for w in coefs:
1.06 logistic.py(1173):         if multi_class == 'ovr':
1.06 logistic.py(1174):             w = w[np.newaxis, :]
1.06 logistic.py(1175):         if fit_intercept:
1.06 logistic.py(1176):             log_reg.coef_ = w[:, :-1]
1.06 logistic.py(1177):             log_reg.intercept_ = w[:, -1]
1.06 logistic.py(1182):         if scoring is None:
1.06 logistic.py(1183):             scores.append(log_reg.score(X_test, y_test))
1.06 logistic.py(1172):     for w in coefs:
1.06 logistic.py(1173):         if multi_class == 'ovr':
1.06 logistic.py(1174):             w = w[np.newaxis, :]
1.06 logistic.py(1175):         if fit_intercept:
1.06 logistic.py(1176):             log_reg.coef_ = w[:, :-1]
1.06 logistic.py(1177):             log_reg.intercept_ = w[:, -1]
1.06 logistic.py(1182):         if scoring is None:
1.06 logistic.py(1183):             scores.append(log_reg.score(X_test, y_test))
1.06 logistic.py(1172):     for w in coefs:
1.06 logistic.py(1187):     return coefs, Cs, np.array(scores), n_iter
1.06 logistic.py(2071):             for l1_ratio in l1_ratios_)
1.06 logistic.py(2070):             for train, test in folds
1.06 logistic.py(2057):             path_func(X, y, train, test, pos_class=label, Cs=self.Cs,
1.06 logistic.py(2084):         coefs_paths, Cs, scores, n_iter_ = zip(*fold_coefs_)
1.06 logistic.py(2085):         self.Cs_ = Cs[0]
1.06 logistic.py(2086):         if multi_class == 'multinomial':
1.06 logistic.py(2102):             coefs_paths = np.reshape(
1.06 logistic.py(2103):                 coefs_paths,
1.06 logistic.py(2104):                 (n_classes, len(folds), len(self.Cs_) * len(l1_ratios_),
1.06 logistic.py(2105):                  -1)
1.06 logistic.py(2107):             self.n_iter_ = np.reshape(
1.06 logistic.py(2108):                 n_iter_,
1.06 logistic.py(2109):                 (n_classes, len(folds), len(self.Cs_) * len(l1_ratios_))
1.06 logistic.py(2111):         scores = np.reshape(scores, (n_classes, len(folds), -1))
1.06 logistic.py(2112):         self.scores_ = dict(zip(classes, scores))
1.06 logistic.py(2113):         self.coefs_paths_ = dict(zip(classes, coefs_paths))
1.06 logistic.py(2115):         self.C_ = list()
1.06 logistic.py(2116):         self.l1_ratio_ = list()
1.06 logistic.py(2117):         self.coef_ = np.empty((n_classes, X.shape[1]))
1.06 logistic.py(2118):         self.intercept_ = np.zeros(n_classes)
1.06 logistic.py(2119):         for index, (cls, encoded_label) in enumerate(
1.06 logistic.py(2120):                 zip(iter_classes, iter_encoded_labels)):
1.06 logistic.py(2122):             if multi_class == 'ovr':
1.06 logistic.py(2123):                 scores = self.scores_[cls]
1.06 logistic.py(2124):                 coefs_paths = self.coefs_paths_[cls]
1.06 logistic.py(2131):             if self.refit:
1.06 logistic.py(2172):                 best_indices = np.argmax(scores, axis=1)
1.06 logistic.py(2173):                 if self.multi_class == 'ovr':
1.06 logistic.py(2177):                     w = np.mean([coefs_paths[:, i, best_indices[i], :]
1.06 logistic.py(2178):                                  for i in range(len(folds))], axis=0)
1.06 logistic.py(2177):                     w = np.mean([coefs_paths[:, i, best_indices[i], :]
1.06 logistic.py(2178):                                  for i in range(len(folds))], axis=0)
=========================== short test summary info ============================
FAILED sklearn/tests/test_coverup_scikit-learn__scikit-learn-14087.py::test_logistic_regression_cv_refit_false_index_error
========================= 1 failed, 1 warning in 0.52s =========================
+ cat coverage.cover
{"/testbed/sklearn/linear_model/logistic.py": {"13": 1, "14": 1, "16": 1, "17": 1, "18": 1, "20": 1, "21": 1, "22": 1, "23": 1, "24": 1, "25": 1, "26": 1, "28": 1, "29": 1, "30": 1, "31": 1, "32": 1, "33": 1, "34": 1, "35": 1, "36": 1, "37": 1, "38": 1, "39": 1, "43": 1, "81": 1, "132": 1, "168": 1, "245": 1, "301": 1, "354": 1, "427": 1, "457": 1, "474": 1, "483": 1, "653": 1, "1002": 1, "1190": 2, "1191": 1, "1670": 2, "1671": 1, "71": 0, "72": 0, "73": 0, "74": 0, "76": 0, "77": 0, "78": 0, "110": 0, "111": 0, "113": 0, "115": 0, "116": 0, "119": 0, "121": 0, "122": 0, "124": 0, "127": 0, "128": 0, "129": 0, "158": 0, "160": 0, "161": 0, "164": 0, "165": 0, "198": 0, "199": 0, "200": 0, "202": 0, "204": 0, "205": 0, "207": 0, "208": 0, "210": 0, "213": 0, "214": 0, "217": 0, "218": 0, "219": 0, "220": 0, "223": 0, "225": 0, "228": 0, "230": 0, "242": 0, "231": 0, "232": 0, "233": 0, "236": 0, "237": 0, "238": 0, "239": 0, "240": 0, "282": 0, "283": 0, "284": 0, "285": 0, "286": 0, "287": 0, "288": 0, "289": 0, "291": 0, "292": 0, "293": 0, "294": 0, "295": 0, "296": 0, "297": 0, "298": 0, "339": 0, "340": 0, "341": 0, "342": 0, "343": 0, "344": 0, "345": 0, "346": 0, "347": 0, "348": 0, "349": 0, "350": 0, "351": 0, "392": 0, "393": 0, "394": 0, "398": 0, "399": 0, "403": 0, "424": 0, "404": 0, "405": 0, "406": 0, "407": 0, "409": 0, "412": 0, "413": 0, "414": 0, "415": 0, "416": 0, "417": 0, "418": 0, "419": 0, "420": 0, "421": 0, "422": 0, "428": 6, "429": 6, "430": 0, "431": 0, "433": 6, "434": 6, "435": 0, "436": 0, "438": 6, "439": 0, "440": 0, "441": 6, "442": 0, "443": 0, "445": 6, "446": 0, "447": 0, "449": 6, "450": 0, "451": 0, "454": 6, "458": 6, "459": 1, "460": 0, "461": 1, "462": 0, "464": 1, "465": 6, "466": 0, "467": 0, "468": 6, "469": 0, "470": 0, "471": 6, "638": 0, "639": 0, "640": 0, "641": 0, "642": 0, "643": 0, "803": 5, "804": 5, "806": 5, "809": 5, "810": 0, "811": 0, "812": 0, "813": 0, "814": 5, "816": 5, "817": 5, "819": 5, "820": 5, "821": 0, "822": 0, "824": 0, "829": 5, "830": 0, "831": 0, "833": 5, "838": 5, "839": 5, "840": 0, "841": 0, "845": 5, "846": 5, "847": 5, "848": 5, "849": 5, "850": 5, "853": 5, "854": 0, "855": 0, "856": 0, "859": 0, "860": 0, "861": 0, "862": 0, "863": 0, "866": 0, "867": 0, "869": 0, "870": 0, "872": 5, "874": 0, "875": 0, "876": 0, "877": 0, "878": 0, "879": 0, "883": 0, "884": 0, "885": 0, "887": 0, "888": 0, "889": 0, "890": 0, "892": 0, "893": 0, "895": 0, "896": 0, "897": 0, "899": 0, "901": 5, "903": 0, "904": 0, "905": 0, "906": 0, "907": 0, "908": 0, "909": 0, "910": 0, "911": 0, "912": 0, "914": 5, "915": 5, "916": 0, "917": 5, "918": 0, "919": 0, "920": 0, "921": 5, "923": 5, "924": 5, "925": 55, "926": 50, "927": 0, "928": 0, "929": 0, "930": 0, "931": 0, "932": 0, "933": 0, "934": 0, "935": 0, "938": 0, "939": 50, "940": 0, "941": 0, "942": 0, "943": 50, "944": 0, "945": 0, "946": 0, "947": 0, "948": 0, "949": 0, "951": 0, "953": 50, "954": 50, "955": 0, "956": 0, "958": 50, "960": 50, "961": 0, "962": 0, "963": 50, "964": 50, "965": 50, "967": 0, "968": 0, "970": 50, "971": 50, "972": 50, "973": 50, "974": 50, "977": 0, "978": 0, "980": 50, "981": 0, "982": 0, "983": 0, "984": 0, "985": 0, "987": 50, "989": 50, "991": 5, "1132": 5, "1133": 5, "1134": 5, "1135": 5, "1137": 5, "1138": 0, "1139": 0, "1141": 0, "1143": 5, "1144": 5, "1145": 5, "1146": 5, "1147": 5, "1148": 5, "1149": 5, "1150": 5, "1152": 5, "1155": 5, "1156": 5, "1157": 0, "1158": 0, "1160": 0, "1161": 0, "1163": 5, "1164": 5, "1165": 5, "1166": 5, "1168": 5, "1170": 5, "1171": 0, "1172": 55, "1173": 50, "1174": 50, "1175": 50, "1176": 50, "1177": 50, "1179": 0, "1180": 0, "1182": 50, "1183": 50, "1185": 0, "1187": 5, "1435": 1, "1453": 1, "1611": 1, "1651": 1, "1437": 5, "1438": 5, "1439": 5, "1440": 5, "1441": 5, "1442": 5, "1443": 5, "1444": 5, "1445": 5, "1446": 5, "1447": 5, "1448": 5, "1449": 5, "1450": 5, "1451": 5, "1480": 0, "1482": 0, "1483": 0, "1484": 0, "1485": 0, "1486": 0, "1487": 0, "1488": 0, "1489": 0, "1490": 0, "1491": 0, "1493": 0, "1494": 0, "1495": 0, "1496": 0, "1497": 0, "1501": 0, "1502": 0, "1504": 0, "1505": 0, "1506": 0, "1507": 0, "1508": 0, "1509": 0, "1510": 0, "1511": 0, "1513": 0, "1514": 0, "1516": 0, "1518": 0, "1519": 0, "1520": 0, "1521": 0, "1522": 0, "1524": 0, "1525": 0, "1527": 0, "1528": 0, "1529": 0, "1531": 0, "1532": 0, "1533": 0, "1534": 0, "1535": 0, "1536": 0, "1537": 0, "1538": 0, "1540": 0, "1541": 0, "1543": 0, "1545": 0, "1546": 0, "1547": 0, "1548": 0, "1550": 0, "1552": 0, "1553": 0, "1554": 0, "1556": 0, "1557": 0, "1559": 0, "1560": 0, "1561": 0, "1562": 0, "1563": 0, "1565": 0, "1566": 0, "1569": 0, "1570": 0, "1571": 0, "1572": 0, "1573": 0, "1575": 0, "1579": 0, "1580": 0, "1582": 0, "1583": 0, "1584": 0, "1585": 0, "1593": 0, "1595": 0, "1596": 0, "1598": 0, "1599": 0, "1601": 0, "1602": 0, "1603": 0, "1605": 0, "1606": 0, "1607": 0, "1609": 0, "1634": 0, "1636": 0, "1637": 0, "1638": 0, "1639": 0, "1640": 0, "1642": 0, "1643": 0, "1646": 0, "1648": 0, "1649": 0, "1667": 0, "1917": 1, "1936": 1, "2214": 1, "1918": 1, "1919": 1, "1920": 1, "1921": 1, "1922": 1, "1923": 1, "1924": 1, "1925": 1, "1926": 1, "1927": 1, "1928": 1, "1929": 1, "1930": 1, "1931": 1, "1932": 1, "1933": 1, "1934": 1, "1956": 1, "1958": 1, "1959": 0, "1960": 0, "1961": 1, "1962": 0, "1963": 0, "1964": 1, "1965": 0, "1966": 0, "1967": 0, "1968": 0, "1970": 0, "1971": 0, "1973": 1, "1974": 0, "1976": 0, "1978": 1, "1980": 1, "1981": 0, "1982": 0, "1986": 1, "1987": 1, "1988": 1, "1989": 1, "1991": 1, "1994": 1, "1995": 1, "1996": 1, "1997": 0, "1998": 0, "2001": 1, "2002": 1, "2004": 1, "2005": 1, "2007": 1, "2008": 1, "2010": 0, "2013": 1, "2014": 1, "2017": 1, "2019": 1, "2020": 0, "2022": 0, "2024": 1, "2027": 1, "2028": 1, "2029": 1, "2033": 1, "2034": 0, "2036": 1, "2037": 1, "2040": 1, "2041": 0, "2042": 0, "2043": 0, "2044": 0, "2046": 1, "2050": 1, "2051": 1, "2053": 0, "2055": 1, "2056": 1, "2057": 3, "2069": 2, "2084": 1, "2085": 1, "2086": 1, "2087": 0, "2088": 0, "2089": 0, "2093": 0, "2094": 0, "2095": 0, "2096": 0, "2097": 0, "2100": 0, "2102": 1, "2103": 1, "2104": 1, "2105": 1, "2107": 1, "2108": 1, "2109": 1, "2111": 1, "2112": 1, "2113": 1, "2115": 1, "2116": 1, "2117": 1, "2118": 1, "2119": 1, "2120": 1, "2122": 1, "2123": 1, "2124": 1, "2127": 0, "2131": 1, "2137": 0, "2139": 0, "2140": 0, "2141": 0, "2143": 0, "2144": 0, "2145": 0, "2147": 0, "2148": 0, "2149": 0, "2151": 0, "2155": 0, "2156": 0, "2157": 0, "2158": 0, "2159": 0, "2160": 0, "2161": 0, "2162": 0, "2163": 0, "2164": 0, "2165": 0, "2166": 0, "2167": 0, "2172": 1, "2173": 1, "2174": 0, "2175": 0, "2177": 2, "2178": 2, "2180": 0, "2181": 0, "2183": 0, "2184": 0, "2186": 0, "2187": 0, "2188": 0, "2189": 0, "2190": 0, "2191": 0, "2193": 0, "2194": 0, "2195": 0, "2197": 0, "2198": 0, "2199": 0, "2202": 0, "2203": 0, "2204": 0, "2205": 0, "2206": 0, "2207": 0, "2208": 0, "2209": 0, "2210": 0, "2212": 0, "2070": 6, "2071": 10, "2236": 0, "2237": 0, "2241": 0, "2242": 0, "2243": 0, "2244": 0, "2246": 0}}
+ git checkout a5743ed36fbd3fbc8e351bdab16561fbfca7dfa1
Note: switching to 'a5743ed36fbd3fbc8e351bdab16561fbfca7dfa1'.

You are in 'detached HEAD' state. You can look around, make experimental
changes and commit them, and you can discard any commits you make in this
state without impacting any branches by switching back to a branch.

If you want to create a new branch to retain commits you create, you may
do so (now or later) by using -c with the switch command. Example:

  git switch -c <new-branch-name>

Or undo this operation with:

  git switch -

Turn off this advice by setting config variable advice.detachedHead to false

HEAD is now at a5743ed36f TST add test for LAD-loss and quantile loss equivalence (old GBDT code) (#14086)
+ git apply /root/pre_state.patch
error: unrecognized input
