+ source /opt/miniconda3/bin/activate
++ _CONDA_ROOT=/opt/miniconda3
++ . /opt/miniconda3/etc/profile.d/conda.sh
+++ export CONDA_EXE=/opt/miniconda3/bin/conda
+++ CONDA_EXE=/opt/miniconda3/bin/conda
+++ export _CE_M=
+++ _CE_M=
+++ export _CE_CONDA=
+++ _CE_CONDA=
+++ export CONDA_PYTHON_EXE=/opt/miniconda3/bin/python
+++ CONDA_PYTHON_EXE=/opt/miniconda3/bin/python
+++ '[' -z '' ']'
+++ export CONDA_SHLVL=0
+++ CONDA_SHLVL=0
+++ '[' -n '' ']'
+++++ dirname /opt/miniconda3/bin/conda
++++ dirname /opt/miniconda3/bin
+++ PATH=/opt/miniconda3/condabin:/opt/miniconda3/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin
+++ export PATH
+++ '[' -z '' ']'
+++ PS1=
++ conda activate
++ local cmd=activate
++ case "$cmd" in
++ __conda_activate activate
++ '[' -n '' ']'
++ local ask_conda
+++ PS1=
+++ __conda_exe shell.posix activate
+++ /opt/miniconda3/bin/conda shell.posix activate
++ ask_conda='PS1='\''(base) '\''
export PATH='\''/opt/miniconda3/bin:/opt/miniconda3/condabin:/opt/miniconda3/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin'\''
export CONDA_PREFIX='\''/opt/miniconda3'\''
export CONDA_SHLVL='\''1'\''
export CONDA_DEFAULT_ENV='\''base'\''
export CONDA_PROMPT_MODIFIER='\''(base) '\''
export CONDA_EXE='\''/opt/miniconda3/bin/conda'\''
export _CE_M='\'''\''
export _CE_CONDA='\'''\''
export CONDA_PYTHON_EXE='\''/opt/miniconda3/bin/python'\'''
++ eval 'PS1='\''(base) '\''
export PATH='\''/opt/miniconda3/bin:/opt/miniconda3/condabin:/opt/miniconda3/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin'\''
export CONDA_PREFIX='\''/opt/miniconda3'\''
export CONDA_SHLVL='\''1'\''
export CONDA_DEFAULT_ENV='\''base'\''
export CONDA_PROMPT_MODIFIER='\''(base) '\''
export CONDA_EXE='\''/opt/miniconda3/bin/conda'\''
export _CE_M='\'''\''
export _CE_CONDA='\'''\''
export CONDA_PYTHON_EXE='\''/opt/miniconda3/bin/python'\'''
+++ PS1='(base) '
+++ export PATH=/opt/miniconda3/bin:/opt/miniconda3/condabin:/opt/miniconda3/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin
+++ PATH=/opt/miniconda3/bin:/opt/miniconda3/condabin:/opt/miniconda3/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin
+++ export CONDA_PREFIX=/opt/miniconda3
+++ CONDA_PREFIX=/opt/miniconda3
+++ export CONDA_SHLVL=1
+++ CONDA_SHLVL=1
+++ export CONDA_DEFAULT_ENV=base
+++ CONDA_DEFAULT_ENV=base
+++ export 'CONDA_PROMPT_MODIFIER=(base) '
+++ CONDA_PROMPT_MODIFIER='(base) '
+++ export CONDA_EXE=/opt/miniconda3/bin/conda
+++ CONDA_EXE=/opt/miniconda3/bin/conda
+++ export _CE_M=
+++ _CE_M=
+++ export _CE_CONDA=
+++ _CE_CONDA=
+++ export CONDA_PYTHON_EXE=/opt/miniconda3/bin/python
+++ CONDA_PYTHON_EXE=/opt/miniconda3/bin/python
++ __conda_hashr
++ '[' -n '' ']'
++ '[' -n '' ']'
++ hash -r
+ conda activate testbed
+ local cmd=activate
+ case "$cmd" in
+ __conda_activate activate testbed
+ '[' -n '' ']'
+ local ask_conda
++ PS1='(base) '
++ __conda_exe shell.posix activate testbed
++ /opt/miniconda3/bin/conda shell.posix activate testbed
+ ask_conda='PS1='\''(testbed) '\''
export PATH='\''/opt/miniconda3/envs/testbed/bin:/opt/miniconda3/condabin:/opt/miniconda3/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin'\''
export CONDA_PREFIX='\''/opt/miniconda3/envs/testbed'\''
export CONDA_SHLVL='\''2'\''
export CONDA_DEFAULT_ENV='\''testbed'\''
export CONDA_PROMPT_MODIFIER='\''(testbed) '\''
export CONDA_PREFIX_1='\''/opt/miniconda3'\''
export CONDA_EXE='\''/opt/miniconda3/bin/conda'\''
export _CE_M='\'''\''
export _CE_CONDA='\'''\''
export CONDA_PYTHON_EXE='\''/opt/miniconda3/bin/python'\'''
+ eval 'PS1='\''(testbed) '\''
export PATH='\''/opt/miniconda3/envs/testbed/bin:/opt/miniconda3/condabin:/opt/miniconda3/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin'\''
export CONDA_PREFIX='\''/opt/miniconda3/envs/testbed'\''
export CONDA_SHLVL='\''2'\''
export CONDA_DEFAULT_ENV='\''testbed'\''
export CONDA_PROMPT_MODIFIER='\''(testbed) '\''
export CONDA_PREFIX_1='\''/opt/miniconda3'\''
export CONDA_EXE='\''/opt/miniconda3/bin/conda'\''
export _CE_M='\'''\''
export _CE_CONDA='\'''\''
export CONDA_PYTHON_EXE='\''/opt/miniconda3/bin/python'\'''
++ PS1='(testbed) '
++ export PATH=/opt/miniconda3/envs/testbed/bin:/opt/miniconda3/condabin:/opt/miniconda3/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin
++ PATH=/opt/miniconda3/envs/testbed/bin:/opt/miniconda3/condabin:/opt/miniconda3/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin
++ export CONDA_PREFIX=/opt/miniconda3/envs/testbed
++ CONDA_PREFIX=/opt/miniconda3/envs/testbed
++ export CONDA_SHLVL=2
++ CONDA_SHLVL=2
++ export CONDA_DEFAULT_ENV=testbed
++ CONDA_DEFAULT_ENV=testbed
++ export 'CONDA_PROMPT_MODIFIER=(testbed) '
++ CONDA_PROMPT_MODIFIER='(testbed) '
++ export CONDA_PREFIX_1=/opt/miniconda3
++ CONDA_PREFIX_1=/opt/miniconda3
++ export CONDA_EXE=/opt/miniconda3/bin/conda
++ CONDA_EXE=/opt/miniconda3/bin/conda
++ export _CE_M=
++ _CE_M=
++ export _CE_CONDA=
++ _CE_CONDA=
++ export CONDA_PYTHON_EXE=/opt/miniconda3/bin/python
++ CONDA_PYTHON_EXE=/opt/miniconda3/bin/python
+ __conda_hashr
+ '[' -n '' ']'
+ '[' -n '' ']'
+ hash -r
+ cd /testbed
+ git diff HEAD 3aefc834dce72e850bff48689bea3c7dff5f3fad
+ git config --global --add safe.directory /testbed
+ cd /testbed
+ git status
On branch main
nothing to commit, working tree clean
+ git show
commit 3aefc834dce72e850bff48689bea3c7dff5f3fad
Author: Loic Esteve <loic.esteve@ymail.com>
Date:   Wed Mar 27 16:01:11 2019 +0100

    MNT CI Fix for sphinx-gallery 0.3.1 + 404 errors on Debian Jessie packages (#13527)
    
    * Fix for sphinx-gallery 0.3.1.
    
    Figure numbering have been changed to ignore matplotlib figure number.
    
    * Use circleci/3.6 image.
    
    This docker image uses Debian stretch and fixes the problems seen with apt-get update with Debian jessie.
    
    * Install additional fonts.
    
    Seems to be needed to convert doc/images/iris.svg.
    
    * Another missed example using figure number 0.
    
    [doc build]

diff --git a/.circleci/config.yml b/.circleci/config.yml
index 14f152cbc9..b1e484269e 100644
--- a/.circleci/config.yml
+++ b/.circleci/config.yml
@@ -3,7 +3,7 @@ version: 2
 jobs:
   doc-min-dependencies:
     docker:
-      - image: circleci/python:3.6.1
+      - image: circleci/python:3.6
     environment:
       - MINICONDA_PATH: ~/miniconda
       - CONDA_ENV_NAME: testenv
@@ -32,7 +32,7 @@ jobs:
 
   doc:
     docker:
-      - image: circleci/python:3.6.1
+      - image: circleci/python:3.6
     environment:
       - MINICONDA_PATH: ~/miniconda
       - CONDA_ENV_NAME: testenv
@@ -61,7 +61,7 @@ jobs:
 
   lint:
     docker:
-      - image: circleci/python:3.6.1
+      - image: circleci/python:3.6
     steps:
       - checkout
       - run: ./build_tools/circle/checkout_merge_commit.sh
@@ -90,7 +90,7 @@ jobs:
 
   deploy:
     docker:
-      - image: circleci/python:3.6.1
+      - image: circleci/python:3.6
     steps:
       - checkout
       - run: ./build_tools/circle/checkout_merge_commit.sh
diff --git a/build_tools/circle/build_doc.sh b/build_tools/circle/build_doc.sh
index d32f7b9000..26c6959e43 100755
--- a/build_tools/circle/build_doc.sh
+++ b/build_tools/circle/build_doc.sh
@@ -101,7 +101,7 @@ sudo -E apt-get -yq remove texlive-binaries --purge
 sudo -E apt-get -yq --no-install-suggests --no-install-recommends --force-yes \
     install dvipng texlive-latex-base texlive-latex-extra \
     texlive-latex-recommended texlive-latex-extra texlive-fonts-recommended\
-    latexmk
+    latexmk gsfonts
 
 # deactivate circleci virtualenv and setup a miniconda env instead
 if [[ `type -t deactivate` ]]; then
diff --git a/doc/modules/calibration.rst b/doc/modules/calibration.rst
index a462ff3229..6fe30c93ff 100644
--- a/doc/modules/calibration.rst
+++ b/doc/modules/calibration.rst
@@ -171,7 +171,7 @@ probability vectors predicted by the same classifier after sigmoid calibration
 on a hold-out validation set. Colors indicate the true class of an instance
 (red: class 1, green: class 2, blue: class 3).
 
-.. figure:: ../auto_examples/calibration/images/sphx_glr_plot_calibration_multiclass_000.png
+.. figure:: ../auto_examples/calibration/images/sphx_glr_plot_calibration_multiclass_001.png
    :target: ../auto_examples/calibration/plot_calibration_multiclass.html
    :align: center
 
@@ -183,7 +183,7 @@ method='sigmoid' on the remaining 200 datapoints reduces the confidence of the
 predictions, i.e., moves the probability vectors from the edges of the simplex
 towards the center:
 
-.. figure:: ../auto_examples/calibration/images/sphx_glr_plot_calibration_multiclass_001.png
+.. figure:: ../auto_examples/calibration/images/sphx_glr_plot_calibration_multiclass_002.png
    :target: ../auto_examples/calibration/plot_calibration_multiclass.html
    :align: center
 
diff --git a/doc/modules/gaussian_process.rst b/doc/modules/gaussian_process.rst
index 4b8950b765..bacbec51b9 100644
--- a/doc/modules/gaussian_process.rst
+++ b/doc/modules/gaussian_process.rst
@@ -88,14 +88,14 @@ estimate the noise level of data. An illustration of the
 log-marginal-likelihood (LML) landscape shows that there exist two local
 maxima of LML.
 
-.. figure:: ../auto_examples/gaussian_process/images/sphx_glr_plot_gpr_noisy_000.png
+.. figure:: ../auto_examples/gaussian_process/images/sphx_glr_plot_gpr_noisy_001.png
    :target: ../auto_examples/gaussian_process/plot_gpr_noisy.html
    :align: center
 
 The first corresponds to a model with a high noise level and a
 large length scale, which explains all variations in the data by noise.
 
-.. figure:: ../auto_examples/gaussian_process/images/sphx_glr_plot_gpr_noisy_001.png
+.. figure:: ../auto_examples/gaussian_process/images/sphx_glr_plot_gpr_noisy_002.png
    :target: ../auto_examples/gaussian_process/plot_gpr_noisy.html
    :align: center
 
@@ -106,7 +106,7 @@ hyperparameters, the gradient-based optimization might also converge to the
 high-noise solution. It is thus important to repeat the optimization several
 times for different initializations.
 
-.. figure:: ../auto_examples/gaussian_process/images/sphx_glr_plot_gpr_noisy_002.png
+.. figure:: ../auto_examples/gaussian_process/images/sphx_glr_plot_gpr_noisy_003.png
    :target: ../auto_examples/gaussian_process/plot_gpr_noisy.html
    :align: center
 
@@ -306,11 +306,11 @@ The second figure shows the log-marginal-likelihood for different choices of
 the kernel's hyperparameters, highlighting the two choices of the
 hyperparameters used in the first figure by black dots.
 
-.. figure:: ../auto_examples/gaussian_process/images/sphx_glr_plot_gpc_000.png
+.. figure:: ../auto_examples/gaussian_process/images/sphx_glr_plot_gpc_001.png
    :target: ../auto_examples/gaussian_process/plot_gpc.html
    :align: center
 
-.. figure:: ../auto_examples/gaussian_process/images/sphx_glr_plot_gpc_001.png
+.. figure:: ../auto_examples/gaussian_process/images/sphx_glr_plot_gpc_002.png
    :target: ../auto_examples/gaussian_process/plot_gpc.html
    :align: center
 
@@ -493,7 +493,7 @@ kernel as covariance function have mean square derivatives of all orders, and ar
 very smooth. The prior and posterior of a GP resulting from an RBF kernel are shown in
 the following figure:
 
-.. figure:: ../auto_examples/gaussian_process/images/sphx_glr_plot_gpr_prior_posterior_000.png
+.. figure:: ../auto_examples/gaussian_process/images/sphx_glr_plot_gpr_prior_posterior_001.png
    :target: ../auto_examples/gaussian_process/plot_gpr_prior_posterior.html
    :align: center
 
@@ -534,7 +534,7 @@ allows adapting to the properties of the true underlying functional relation.
 The prior and posterior of a GP resulting from a Matérn kernel are shown in
 the following figure:
 
-.. figure:: ../auto_examples/gaussian_process/images/sphx_glr_plot_gpr_prior_posterior_004.png
+.. figure:: ../auto_examples/gaussian_process/images/sphx_glr_plot_gpr_prior_posterior_005.png
    :target: ../auto_examples/gaussian_process/plot_gpr_prior_posterior.html
    :align: center
 
@@ -556,7 +556,7 @@ The kernel is given by:
 The prior and posterior of a GP resulting from a :class:`RationalQuadratic` kernel are shown in
 the following figure:
 
-.. figure:: ../auto_examples/gaussian_process/images/sphx_glr_plot_gpr_prior_posterior_001.png
+.. figure:: ../auto_examples/gaussian_process/images/sphx_glr_plot_gpr_prior_posterior_002.png
    :target: ../auto_examples/gaussian_process/plot_gpr_prior_posterior.html
    :align: center
 
@@ -574,7 +574,7 @@ The kernel is given by:
 The prior and posterior of a GP resulting from an ExpSineSquared kernel are shown in
 the following figure:
 
-.. figure:: ../auto_examples/gaussian_process/images/sphx_glr_plot_gpr_prior_posterior_002.png
+.. figure:: ../auto_examples/gaussian_process/images/sphx_glr_plot_gpr_prior_posterior_003.png
    :target: ../auto_examples/gaussian_process/plot_gpr_prior_posterior.html
    :align: center
 
@@ -594,7 +594,7 @@ is called the homogeneous linear kernel, otherwise it is inhomogeneous. The kern
 The :class:`DotProduct` kernel is commonly combined with exponentiation. An example with exponent 2 is
 shown in the following figure:
 
-.. figure:: ../auto_examples/gaussian_process/images/sphx_glr_plot_gpr_prior_posterior_003.png
+.. figure:: ../auto_examples/gaussian_process/images/sphx_glr_plot_gpr_prior_posterior_004.png
    :target: ../auto_examples/gaussian_process/plot_gpr_prior_posterior.html
    :align: center
 
diff --git a/examples/calibration/plot_calibration_multiclass.py b/examples/calibration/plot_calibration_multiclass.py
index 5ddea6194f..8aa6cb9f99 100644
--- a/examples/calibration/plot_calibration_multiclass.py
+++ b/examples/calibration/plot_calibration_multiclass.py
@@ -64,7 +64,7 @@ sig_clf_probs = sig_clf.predict_proba(X_test)
 sig_score = log_loss(y_test, sig_clf_probs)
 
 # Plot changes in predicted probabilities via arrows
-plt.figure(0)
+plt.figure()
 colors = ["r", "g", "b"]
 for i in range(clf_probs.shape[0]):
     plt.arrow(clf_probs[i, 0], clf_probs[i, 1],
@@ -131,7 +131,7 @@ print(" * classifier trained on 600 datapoints and calibrated on "
       "200 datapoint: %.3f" % sig_score)
 
 # Illustrate calibrator
-plt.figure(1)
+plt.figure()
 # generate grid over 2-simplex
 p1d = np.linspace(0, 1, 20)
 p0, p1 = np.meshgrid(p1d, p1d)
diff --git a/examples/exercises/plot_iris_exercise.py b/examples/exercises/plot_iris_exercise.py
index 985858574d..1372fa565d 100644
--- a/examples/exercises/plot_iris_exercise.py
+++ b/examples/exercises/plot_iris_exercise.py
@@ -35,11 +35,11 @@ X_test = X[int(.9 * n_sample):]
 y_test = y[int(.9 * n_sample):]
 
 # fit the model
-for fig_num, kernel in enumerate(('linear', 'rbf', 'poly')):
+for kernel in ('linear', 'rbf', 'poly'):
     clf = svm.SVC(kernel=kernel, gamma=10)
     clf.fit(X_train, y_train)
 
-    plt.figure(fig_num)
+    plt.figure()
     plt.clf()
     plt.scatter(X[:, 0], X[:, 1], c=y, zorder=10, cmap=plt.cm.Paired,
                 edgecolor='k', s=20)
diff --git a/examples/gaussian_process/plot_gpc.py b/examples/gaussian_process/plot_gpc.py
index c0ab9f76d3..edd3f16081 100644
--- a/examples/gaussian_process/plot_gpc.py
+++ b/examples/gaussian_process/plot_gpc.py
@@ -63,7 +63,7 @@ print("Log-loss: %.3f (initial) %.3f (optimized)"
 
 
 # Plot posteriors
-plt.figure(0)
+plt.figure()
 plt.scatter(X[:train_size, 0], y[:train_size], c='k', label="Train data",
             edgecolors=(0, 0, 0))
 plt.scatter(X[train_size:, 0], y[train_size:], c='g', label="Test data",
@@ -80,7 +80,7 @@ plt.ylim(-0.25, 1.5)
 plt.legend(loc="best")
 
 # Plot LML landscape
-plt.figure(1)
+plt.figure()
 theta0 = np.logspace(0, 8, 30)
 theta1 = np.logspace(-1, 1, 29)
 Theta0, Theta1 = np.meshgrid(theta0, theta1)
diff --git a/examples/gaussian_process/plot_gpr_noisy.py b/examples/gaussian_process/plot_gpr_noisy.py
index 9aa4074915..5f8ce2cd0f 100644
--- a/examples/gaussian_process/plot_gpr_noisy.py
+++ b/examples/gaussian_process/plot_gpr_noisy.py
@@ -35,7 +35,7 @@ X = rng.uniform(0, 5, 20)[:, np.newaxis]
 y = 0.5 * np.sin(3 * X[:, 0]) + rng.normal(0, 0.5, X.shape[0])
 
 # First run
-plt.figure(0)
+plt.figure()
 kernel = 1.0 * RBF(length_scale=100.0, length_scale_bounds=(1e-2, 1e3)) \
     + WhiteKernel(noise_level=1, noise_level_bounds=(1e-10, 1e+1))
 gp = GaussianProcessRegressor(kernel=kernel,
@@ -54,7 +54,7 @@ plt.title("Initial: %s\nOptimum: %s\nLog-Marginal-Likelihood: %s"
 plt.tight_layout()
 
 # Second run
-plt.figure(1)
+plt.figure()
 kernel = 1.0 * RBF(length_scale=1.0, length_scale_bounds=(1e-2, 1e3)) \
     + WhiteKernel(noise_level=1e-5, noise_level_bounds=(1e-10, 1e+1))
 gp = GaussianProcessRegressor(kernel=kernel,
@@ -73,7 +73,7 @@ plt.title("Initial: %s\nOptimum: %s\nLog-Marginal-Likelihood: %s"
 plt.tight_layout()
 
 # Plot LML landscape
-plt.figure(2)
+plt.figure()
 theta0 = np.logspace(-2, 3, 49)
 theta1 = np.logspace(-2, 0, 50)
 Theta0, Theta1 = np.meshgrid(theta0, theta1)
diff --git a/examples/gaussian_process/plot_gpr_prior_posterior.py b/examples/gaussian_process/plot_gpr_prior_posterior.py
index 85d18041ad..78eb22df48 100644
--- a/examples/gaussian_process/plot_gpr_prior_posterior.py
+++ b/examples/gaussian_process/plot_gpr_prior_posterior.py
@@ -33,12 +33,12 @@ kernels = [1.0 * RBF(length_scale=1.0, length_scale_bounds=(1e-1, 10.0)),
            1.0 * Matern(length_scale=1.0, length_scale_bounds=(1e-1, 10.0),
                         nu=1.5)]
 
-for fig_index, kernel in enumerate(kernels):
+for kernel in kernels:
     # Specify Gaussian Process
     gp = GaussianProcessRegressor(kernel=kernel)
 
     # Plot prior
-    plt.figure(fig_index, figsize=(8, 8))
+    plt.figure(figsize=(8, 8))
     plt.subplot(2, 1, 1)
     X_ = np.linspace(0, 5, 100)
     y_mean, y_std = gp.predict(X_[:, np.newaxis], return_std=True)
+ git diff 3aefc834dce72e850bff48689bea3c7dff5f3fad
+ source /opt/miniconda3/bin/activate
++ _CONDA_ROOT=/opt/miniconda3
++ . /opt/miniconda3/etc/profile.d/conda.sh
+++ export CONDA_EXE=/opt/miniconda3/bin/conda
+++ CONDA_EXE=/opt/miniconda3/bin/conda
+++ export _CE_M=
+++ _CE_M=
+++ export _CE_CONDA=
+++ _CE_CONDA=
+++ export CONDA_PYTHON_EXE=/opt/miniconda3/bin/python
+++ CONDA_PYTHON_EXE=/opt/miniconda3/bin/python
+++ '[' -z x ']'
++ conda activate
++ local cmd=activate
++ case "$cmd" in
++ __conda_activate activate
++ '[' -n '' ']'
++ local ask_conda
+++ PS1='(testbed) '
+++ __conda_exe shell.posix activate
+++ /opt/miniconda3/bin/conda shell.posix activate
++ ask_conda='PS1='\''(base) '\''
export PATH='\''/opt/miniconda3/bin:/opt/miniconda3/condabin:/opt/miniconda3/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin'\''
export CONDA_PREFIX='\''/opt/miniconda3'\''
export CONDA_SHLVL='\''3'\''
export CONDA_DEFAULT_ENV='\''base'\''
export CONDA_PROMPT_MODIFIER='\''(base) '\''
export CONDA_PREFIX_2='\''/opt/miniconda3/envs/testbed'\''
export CONDA_EXE='\''/opt/miniconda3/bin/conda'\''
export _CE_M='\'''\''
export _CE_CONDA='\'''\''
export CONDA_PYTHON_EXE='\''/opt/miniconda3/bin/python'\'''
++ eval 'PS1='\''(base) '\''
export PATH='\''/opt/miniconda3/bin:/opt/miniconda3/condabin:/opt/miniconda3/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin'\''
export CONDA_PREFIX='\''/opt/miniconda3'\''
export CONDA_SHLVL='\''3'\''
export CONDA_DEFAULT_ENV='\''base'\''
export CONDA_PROMPT_MODIFIER='\''(base) '\''
export CONDA_PREFIX_2='\''/opt/miniconda3/envs/testbed'\''
export CONDA_EXE='\''/opt/miniconda3/bin/conda'\''
export _CE_M='\'''\''
export _CE_CONDA='\'''\''
export CONDA_PYTHON_EXE='\''/opt/miniconda3/bin/python'\'''
+++ PS1='(base) '
+++ export PATH=/opt/miniconda3/bin:/opt/miniconda3/condabin:/opt/miniconda3/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin
+++ PATH=/opt/miniconda3/bin:/opt/miniconda3/condabin:/opt/miniconda3/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin
+++ export CONDA_PREFIX=/opt/miniconda3
+++ CONDA_PREFIX=/opt/miniconda3
+++ export CONDA_SHLVL=3
+++ CONDA_SHLVL=3
+++ export CONDA_DEFAULT_ENV=base
+++ CONDA_DEFAULT_ENV=base
+++ export 'CONDA_PROMPT_MODIFIER=(base) '
+++ CONDA_PROMPT_MODIFIER='(base) '
+++ export CONDA_PREFIX_2=/opt/miniconda3/envs/testbed
+++ CONDA_PREFIX_2=/opt/miniconda3/envs/testbed
+++ export CONDA_EXE=/opt/miniconda3/bin/conda
+++ CONDA_EXE=/opt/miniconda3/bin/conda
+++ export _CE_M=
+++ _CE_M=
+++ export _CE_CONDA=
+++ _CE_CONDA=
+++ export CONDA_PYTHON_EXE=/opt/miniconda3/bin/python
+++ CONDA_PYTHON_EXE=/opt/miniconda3/bin/python
++ __conda_hashr
++ '[' -n '' ']'
++ '[' -n '' ']'
++ hash -r
+ conda activate testbed
+ local cmd=activate
+ case "$cmd" in
+ __conda_activate activate testbed
+ '[' -n '' ']'
+ local ask_conda
++ PS1='(base) '
++ __conda_exe shell.posix activate testbed
++ /opt/miniconda3/bin/conda shell.posix activate testbed
+ ask_conda='PS1='\''(testbed) '\''
export PATH='\''/opt/miniconda3/envs/testbed/bin:/opt/miniconda3/condabin:/opt/miniconda3/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin'\''
export CONDA_PREFIX='\''/opt/miniconda3/envs/testbed'\''
export CONDA_SHLVL='\''4'\''
export CONDA_DEFAULT_ENV='\''testbed'\''
export CONDA_PROMPT_MODIFIER='\''(testbed) '\''
export CONDA_PREFIX_3='\''/opt/miniconda3'\''
export CONDA_EXE='\''/opt/miniconda3/bin/conda'\''
export _CE_M='\'''\''
export _CE_CONDA='\'''\''
export CONDA_PYTHON_EXE='\''/opt/miniconda3/bin/python'\'''
+ eval 'PS1='\''(testbed) '\''
export PATH='\''/opt/miniconda3/envs/testbed/bin:/opt/miniconda3/condabin:/opt/miniconda3/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin'\''
export CONDA_PREFIX='\''/opt/miniconda3/envs/testbed'\''
export CONDA_SHLVL='\''4'\''
export CONDA_DEFAULT_ENV='\''testbed'\''
export CONDA_PROMPT_MODIFIER='\''(testbed) '\''
export CONDA_PREFIX_3='\''/opt/miniconda3'\''
export CONDA_EXE='\''/opt/miniconda3/bin/conda'\''
export _CE_M='\'''\''
export _CE_CONDA='\'''\''
export CONDA_PYTHON_EXE='\''/opt/miniconda3/bin/python'\'''
++ PS1='(testbed) '
++ export PATH=/opt/miniconda3/envs/testbed/bin:/opt/miniconda3/condabin:/opt/miniconda3/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin
++ PATH=/opt/miniconda3/envs/testbed/bin:/opt/miniconda3/condabin:/opt/miniconda3/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin
++ export CONDA_PREFIX=/opt/miniconda3/envs/testbed
++ CONDA_PREFIX=/opt/miniconda3/envs/testbed
++ export CONDA_SHLVL=4
++ CONDA_SHLVL=4
++ export CONDA_DEFAULT_ENV=testbed
++ CONDA_DEFAULT_ENV=testbed
++ export 'CONDA_PROMPT_MODIFIER=(testbed) '
++ CONDA_PROMPT_MODIFIER='(testbed) '
++ export CONDA_PREFIX_3=/opt/miniconda3
++ CONDA_PREFIX_3=/opt/miniconda3
++ export CONDA_EXE=/opt/miniconda3/bin/conda
++ CONDA_EXE=/opt/miniconda3/bin/conda
++ export _CE_M=
++ _CE_M=
++ export _CE_CONDA=
++ _CE_CONDA=
++ export CONDA_PYTHON_EXE=/opt/miniconda3/bin/python
++ CONDA_PYTHON_EXE=/opt/miniconda3/bin/python
+ __conda_hashr
+ '[' -n '' ']'
+ '[' -n '' ']'
+ hash -r
+ python -m pip install -v --no-use-pep517 --no-build-isolation -e .
Using pip 21.2.2 from /opt/miniconda3/envs/testbed/lib/python3.6/site-packages/pip (python 3.6)
Obtaining file:///testbed
    Running command python setup.py egg_info
    running egg_info
    creating /tmp/pip-pip-egg-info-bpndyl07/scikit_learn.egg-info
    writing /tmp/pip-pip-egg-info-bpndyl07/scikit_learn.egg-info/PKG-INFO
    writing dependency_links to /tmp/pip-pip-egg-info-bpndyl07/scikit_learn.egg-info/dependency_links.txt
    writing requirements to /tmp/pip-pip-egg-info-bpndyl07/scikit_learn.egg-info/requires.txt
    writing top-level names to /tmp/pip-pip-egg-info-bpndyl07/scikit_learn.egg-info/top_level.txt
    writing manifest file '/tmp/pip-pip-egg-info-bpndyl07/scikit_learn.egg-info/SOURCES.txt'
    reading manifest file '/tmp/pip-pip-egg-info-bpndyl07/scikit_learn.egg-info/SOURCES.txt'
    reading manifest template 'MANIFEST.in'
    adding license file 'COPYING'
    writing manifest file '/tmp/pip-pip-egg-info-bpndyl07/scikit_learn.egg-info/SOURCES.txt'
    Partial import of sklearn during the build process.
Requirement already satisfied: numpy>=1.11.0 in /opt/miniconda3/envs/testbed/lib/python3.6/site-packages (from scikit-learn==0.21.dev0) (1.19.2)
Requirement already satisfied: scipy>=0.17.0 in /opt/miniconda3/envs/testbed/lib/python3.6/site-packages (from scikit-learn==0.21.dev0) (1.5.2)
Installing collected packages: scikit-learn
  Attempting uninstall: scikit-learn
    Found existing installation: scikit-learn 0.21.dev0
    Uninstalling scikit-learn-0.21.dev0:
      Removing file or directory /opt/miniconda3/envs/testbed/lib/python3.6/site-packages/scikit-learn.egg-link
      Removing pth entries from /opt/miniconda3/envs/testbed/lib/python3.6/site-packages/easy-install.pth:
      Removing entry: /testbed
      Successfully uninstalled scikit-learn-0.21.dev0
  Running setup.py develop for scikit-learn
    Running command /opt/miniconda3/envs/testbed/bin/python -c 'import io, os, sys, setuptools, tokenize; sys.argv[0] = '"'"'/testbed/setup.py'"'"'; __file__='"'"'/testbed/setup.py'"'"';f = getattr(tokenize, '"'"'open'"'"', open)(__file__) if os.path.exists(__file__) else io.StringIO('"'"'from setuptools import setup; setup()'"'"');code = f.read().replace('"'"'\r\n'"'"', '"'"'\n'"'"');f.close();exec(compile(code, __file__, '"'"'exec'"'"'))' develop --no-deps
    blas_opt_info:
    blas_mkl_info:
    customize UnixCCompiler
      libraries mkl_rt not found in ['/opt/miniconda3/envs/testbed/lib', '/usr/local/lib', '/usr/lib64', '/usr/lib', '/usr/lib/x86_64-linux-gnu']
      NOT AVAILABLE

    blis_info:
      libraries blis not found in ['/opt/miniconda3/envs/testbed/lib', '/usr/local/lib', '/usr/lib64', '/usr/lib', '/usr/lib/x86_64-linux-gnu']
      NOT AVAILABLE

    openblas_info:
    C compiler: gcc -pthread -B /opt/miniconda3/envs/testbed/compiler_compat -Wl,--sysroot=/ -Wsign-compare -DNDEBUG -g -fwrapv -O3 -Wall -Wstrict-prototypes -fPIC

    creating /tmp/tmpev_g5z96/tmp
    creating /tmp/tmpev_g5z96/tmp/tmpev_g5z96
    compile options: '-c'
    gcc: /tmp/tmpev_g5z96/source.c
    gcc -pthread -B /opt/miniconda3/envs/testbed/compiler_compat -Wl,--sysroot=/ /tmp/tmpev_g5z96/tmp/tmpev_g5z96/source.o -L/opt/miniconda3/envs/testbed/lib -lopenblas -o /tmp/tmpev_g5z96/a.out
      FOUND:
        libraries = ['openblas', 'openblas']
        library_dirs = ['/opt/miniconda3/envs/testbed/lib']
        language = c
        define_macros = [('HAVE_CBLAS', None)]

      FOUND:
        libraries = ['openblas', 'openblas']
        library_dirs = ['/opt/miniconda3/envs/testbed/lib']
        language = c
        define_macros = [('HAVE_CBLAS', None)]

    running develop
    running build_scripts
    running egg_info
    running build_src
    build_src
    building library "libsvm-skl" sources
    building extension "sklearn.__check_build._check_build" sources
    building extension "sklearn.preprocessing._csr_polynomial_expansion" sources
    building extension "sklearn.cluster._dbscan_inner" sources
    building extension "sklearn.cluster._hierarchical" sources
    building extension "sklearn.cluster._k_means_elkan" sources
    building extension "sklearn.cluster._k_means" sources
    building extension "sklearn.datasets._svmlight_format" sources
    building extension "sklearn.decomposition._online_lda" sources
    building extension "sklearn.decomposition.cdnmf_fast" sources
    building extension "sklearn.ensemble._gradient_boosting" sources
    building extension "sklearn.feature_extraction._hashing" sources
    building extension "sklearn.manifold._utils" sources
    building extension "sklearn.manifold._barnes_hut_tsne" sources
    building extension "sklearn.metrics.cluster.expected_mutual_info_fast" sources
    building extension "sklearn.metrics.pairwise_fast" sources
    building extension "sklearn.neighbors.ball_tree" sources
    building extension "sklearn.neighbors.kd_tree" sources
    building extension "sklearn.neighbors.dist_metrics" sources
    building extension "sklearn.neighbors.typedefs" sources
    building extension "sklearn.neighbors.quad_tree" sources
    building extension "sklearn.tree._tree" sources
    building extension "sklearn.tree._splitter" sources
    building extension "sklearn.tree._criterion" sources
    building extension "sklearn.tree._utils" sources
    building extension "sklearn.utils.sparsefuncs_fast" sources
    building extension "sklearn.utils._cython_blas" sources
    building extension "sklearn.utils.arrayfuncs" sources
    building extension "sklearn.utils.murmurhash" sources
    building extension "sklearn.utils.lgamma" sources
    building extension "sklearn.utils.graph_shortest_path" sources
    building extension "sklearn.utils.fast_dict" sources
    building extension "sklearn.utils.seq_dataset" sources
    building extension "sklearn.utils.weight_vector" sources
    building extension "sklearn.utils._random" sources
    building extension "sklearn.utils._logistic_sigmoid" sources
    building extension "sklearn.svm.libsvm" sources
    building extension "sklearn.svm.liblinear" sources
    building extension "sklearn.svm.libsvm_sparse" sources
    building extension "sklearn.linear_model.cd_fast" sources
    building extension "sklearn.linear_model.sgd_fast" sources
    building extension "sklearn.linear_model.sag_fast" sources
    building extension "sklearn._isotonic" sources
    building data_files sources
    build_src: building npy-pkg config files
    writing scikit_learn.egg-info/PKG-INFO
    writing dependency_links to scikit_learn.egg-info/dependency_links.txt
    writing requirements to scikit_learn.egg-info/requires.txt
    writing top-level names to scikit_learn.egg-info/top_level.txt
    reading manifest file 'scikit_learn.egg-info/SOURCES.txt'
    reading manifest template 'MANIFEST.in'
    adding license file 'COPYING'
    writing manifest file 'scikit_learn.egg-info/SOURCES.txt'
    running build_ext
    customize UnixCCompiler
    customize UnixCCompiler using build_clib
    customize UnixCCompiler
    customize UnixCCompiler using build_ext_subclass
    customize UnixCCompiler
    customize UnixCCompiler using build_ext_subclass
    Creating /opt/miniconda3/envs/testbed/lib/python3.6/site-packages/scikit-learn.egg-link (link to .)
    Adding scikit-learn 0.21.dev0 to easy-install.pth file

    Installed /testbed
    Partial import of sklearn during the build process.
Successfully installed scikit-learn-0.21.dev0
WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv
+ git apply -v -
Checking patch sklearn/tests/test_coverup_scikit-learn__scikit-learn-13496.py...
<stdin>:47: new blank line at EOF.
+
Applied patch sklearn/tests/test_coverup_scikit-learn__scikit-learn-13496.py cleanly.
warning: 1 line adds whitespace errors.
+ python3 /root/trace.py --timing --trace --count -C coverage.cover --include-pattern '/testbed/(sklearn/ensemble/iforest\.py)' -m pytest --no-header -rA -p no:cacheprovider sklearn/tests/test_coverup_scikit-learn__scikit-learn-13496.py
['--timing', '--trace', '--count', '-C', 'coverage.cover', '--include-pattern', '/testbed/(sklearn/ensemble/iforest\\.py)']
============================= test session starts ==============================
collected 1 item

sklearn/tests/test_coverup_scikit-learn__scikit-learn-13496.py F         [100%]

=================================== FAILURES ===================================
__________________ test_isolation_forest_warm_start_behavior ___________________

    def test_isolation_forest_warm_start_behavior():
        # Create a synthetic dataset
        X = np.random.rand(100, 2)
    
        # Initial number of estimators
        n_estimators_initial = 10
        n_estimators_incremented = 20
    
        # Initialize IsolationForest without warm_start in __init__
        model = IsolationForest(n_estimators=n_estimators_initial)
    
        # Manually set warm_start to True
        model.warm_start = True
    
        # Fit the model with the initial number of estimators
        model.fit(X)
    
        # Store the number of estimators after the first fit
        n_estimators_after_first_fit = len(model.estimators_)
    
        # Increment n_estimators
        model.n_estimators = n_estimators_incremented
    
        # Fit the model again with the same data
        model.fit(X)
    
        # Store the number of estimators after the second fit
        n_estimators_after_second_fit = len(model.estimators_)
    
        # Assert that the number of estimators after the first fit is equal to n_estimators_initial
        assert n_estimators_after_first_fit == n_estimators_initial, \
            "BUG: The number of estimators after the first fit should be equal to n_estimators_initial"
    
        # Assert that the number of estimators after the second fit is equal to the sum of initial and incremented
>       assert n_estimators_after_second_fit == n_estimators_initial + n_estimators_incremented, \
            "The number of estimators after the second fit should be equal to the sum of n_estimators_initial and n_estimators_incremented"
E       AssertionError: The number of estimators after the second fit should be equal to the sum of n_estimators_initial and n_estimators_incremented
E       assert 20 == (10 + 20)

sklearn/tests/test_coverup_scikit-learn__scikit-learn-13496.py:39: AssertionError
----------------------------- Captured stdout call -----------------------------
0.80 iforest.py(177):         super().__init__(
0.80 iforest.py(178):             base_estimator=ExtraTreeRegressor(
0.80 iforest.py(179):                 max_features=1,
0.80 iforest.py(180):                 splitter='random',
0.80 iforest.py(181):                 random_state=random_state),
0.80 iforest.py(183):             bootstrap=bootstrap,
0.80 iforest.py(184):             bootstrap_features=False,
0.80 iforest.py(185):             n_estimators=n_estimators,
0.80 iforest.py(186):             max_samples=max_samples,
0.80 iforest.py(187):             max_features=max_features,
0.80 iforest.py(188):             n_jobs=n_jobs,
0.80 iforest.py(189):             random_state=random_state,
0.80 iforest.py(190):             verbose=verbose)
0.80 iforest.py(192):         self.behaviour = behaviour
0.80 iforest.py(193):         self.contamination = contamination
0.80 iforest.py(225):         if self.contamination == "legacy":
0.80 iforest.py(226):             warn('default contamination parameter 0.1 will change '
0.80 iforest.py(229):                  FutureWarning)
0.80 iforest.py(230):             self._contamination = 0.1
0.80 iforest.py(234):         if self.behaviour == 'old':
0.80 iforest.py(235):             warn('behaviour="old" is deprecated and will be removed '
0.80 iforest.py(239):                  FutureWarning)
0.80 iforest.py(241):         X = check_array(X, accept_sparse=['csc'])
0.80 iforest.py(242):         if issparse(X):
0.80 iforest.py(247):         rnd = check_random_state(self.random_state)
0.80 iforest.py(248):         y = rnd.uniform(size=X.shape[0])
0.80 iforest.py(251):         n_samples = X.shape[0]
0.80 iforest.py(253):         if isinstance(self.max_samples, str):
0.80 iforest.py(254):             if self.max_samples == 'auto':
0.80 iforest.py(255):                 max_samples = min(256, n_samples)
0.80 iforest.py(276):         self.max_samples_ = max_samples
0.80 iforest.py(277):         max_depth = int(np.ceil(np.log2(max(max_samples, 2))))
0.80 iforest.py(278):         super()._fit(X, y, max_samples,
0.80 iforest.py(279):                      max_depth=max_depth,
0.80 iforest.py(280):                      sample_weight=sample_weight)
0.80 iforest.py(203):         return _joblib_parallel_args(prefer='threads')
0.81 iforest.py(282):         if self.behaviour == 'old':
0.81 iforest.py(284):             if self._contamination == "auto":
0.81 iforest.py(288):             self.offset_ = -0.5
0.81 iforest.py(289):             self._threshold_ = np.percentile(self.decision_function(X),
0.81 iforest.py(361):         return self.score_samples(X) - self.offset_
0.81 iforest.py(387):         check_is_fitted(self, ["estimators_"])
0.81 iforest.py(390):         X = check_array(X, accept_sparse='csr')
0.81 iforest.py(391):         if self.n_features_ != X.shape[1]:
0.81 iforest.py(399):         return -self._compute_chunked_score_samples(X)
0.81 iforest.py(412):         n_samples = _num_samples(X)
0.81 iforest.py(414):         if self._max_features == X.shape[1]:
0.81 iforest.py(415):             subsample_features = False
0.81 iforest.py(430):         chunk_n_rows = get_chunk_n_rows(row_bytes=16 * self._max_features,
0.81 iforest.py(431):                                         max_n_rows=n_samples)
0.81 iforest.py(432):         slices = gen_batches(n_samples, chunk_n_rows)
0.81 iforest.py(434):         scores = np.zeros(n_samples, order="f")
0.81 iforest.py(436):         for sl in slices:
0.81 iforest.py(438):             scores[sl] = self._compute_score_samples(X[sl], subsample_features)
0.81 iforest.py(452):         n_samples = X.shape[0]
0.81 iforest.py(454):         depths = np.zeros(n_samples, order="f")
0.81 iforest.py(456):         for tree, features in zip(self.estimators_, self.estimators_features_):
0.81 iforest.py(457):             X_subset = X[:, features] if subsample_features else X
0.81 iforest.py(459):             leaves_index = tree.apply(X_subset)
0.81 iforest.py(460):             node_indicator = tree.decision_path(X_subset)
0.81 iforest.py(461):             n_samples_leaf = tree.tree_.n_node_samples[leaves_index]
0.81 iforest.py(463):             depths += (
0.81 iforest.py(466):                 - 1.0
0.81 iforest.py(493):     n_samples_leaf = check_array(n_samples_leaf, ensure_2d=False)
0.81 iforest.py(495):     n_samples_leaf_shape = n_samples_leaf.shape
0.81 iforest.py(496):     n_samples_leaf = n_samples_leaf.reshape((1, -1))
0.81 iforest.py(497):     average_path_length = np.zeros(n_samples_leaf.shape)
0.81 iforest.py(499):     mask_1 = n_samples_leaf <= 1
0.81 iforest.py(500):     mask_2 = n_samples_leaf == 2
0.81 iforest.py(501):     not_mask = ~np.logical_or(mask_1, mask_2)
0.81 iforest.py(503):     average_path_length[mask_1] = 0.
0.81 iforest.py(504):     average_path_length[mask_2] = 1.
0.81 iforest.py(506):         2.0 * (np.log(n_samples_leaf[not_mask] - 1.0) + np.euler_gamma)
0.81 iforest.py(507):         - 2.0 * (n_samples_leaf[not_mask] - 1.0) / n_samples_leaf[not_mask]
0.81 iforest.py(510):     return average_path_length.reshape(n_samples_leaf_shape)
0.81 iforest.py(456):         for tree, features in zip(self.estimators_, self.estimators_features_):
0.81 iforest.py(457):             X_subset = X[:, features] if subsample_features else X
0.81 iforest.py(459):             leaves_index = tree.apply(X_subset)
0.82 iforest.py(460):             node_indicator = tree.decision_path(X_subset)
0.82 iforest.py(461):             n_samples_leaf = tree.tree_.n_node_samples[leaves_index]
0.82 iforest.py(463):             depths += (
0.82 iforest.py(466):                 - 1.0
0.82 iforest.py(493):     n_samples_leaf = check_array(n_samples_leaf, ensure_2d=False)
0.82 iforest.py(495):     n_samples_leaf_shape = n_samples_leaf.shape
0.82 iforest.py(496):     n_samples_leaf = n_samples_leaf.reshape((1, -1))
0.82 iforest.py(497):     average_path_length = np.zeros(n_samples_leaf.shape)
0.82 iforest.py(499):     mask_1 = n_samples_leaf <= 1
0.82 iforest.py(500):     mask_2 = n_samples_leaf == 2
0.82 iforest.py(501):     not_mask = ~np.logical_or(mask_1, mask_2)
0.82 iforest.py(503):     average_path_length[mask_1] = 0.
0.82 iforest.py(504):     average_path_length[mask_2] = 1.
0.82 iforest.py(506):         2.0 * (np.log(n_samples_leaf[not_mask] - 1.0) + np.euler_gamma)
0.82 iforest.py(507):         - 2.0 * (n_samples_leaf[not_mask] - 1.0) / n_samples_leaf[not_mask]
0.82 iforest.py(510):     return average_path_length.reshape(n_samples_leaf_shape)
0.82 iforest.py(456):         for tree, features in zip(self.estimators_, self.estimators_features_):
0.82 iforest.py(457):             X_subset = X[:, features] if subsample_features else X
0.82 iforest.py(459):             leaves_index = tree.apply(X_subset)
0.82 iforest.py(460):             node_indicator = tree.decision_path(X_subset)
0.82 iforest.py(461):             n_samples_leaf = tree.tree_.n_node_samples[leaves_index]
0.82 iforest.py(463):             depths += (
0.82 iforest.py(466):                 - 1.0
0.82 iforest.py(493):     n_samples_leaf = check_array(n_samples_leaf, ensure_2d=False)
0.82 iforest.py(495):     n_samples_leaf_shape = n_samples_leaf.shape
0.82 iforest.py(496):     n_samples_leaf = n_samples_leaf.reshape((1, -1))
0.82 iforest.py(497):     average_path_length = np.zeros(n_samples_leaf.shape)
0.82 iforest.py(499):     mask_1 = n_samples_leaf <= 1
0.82 iforest.py(500):     mask_2 = n_samples_leaf == 2
0.82 iforest.py(501):     not_mask = ~np.logical_or(mask_1, mask_2)
0.82 iforest.py(503):     average_path_length[mask_1] = 0.
0.82 iforest.py(504):     average_path_length[mask_2] = 1.
0.82 iforest.py(506):         2.0 * (np.log(n_samples_leaf[not_mask] - 1.0) + np.euler_gamma)
0.82 iforest.py(507):         - 2.0 * (n_samples_leaf[not_mask] - 1.0) / n_samples_leaf[not_mask]
0.82 iforest.py(510):     return average_path_length.reshape(n_samples_leaf_shape)
0.82 iforest.py(456):         for tree, features in zip(self.estimators_, self.estimators_features_):
0.82 iforest.py(457):             X_subset = X[:, features] if subsample_features else X
0.82 iforest.py(459):             leaves_index = tree.apply(X_subset)
0.82 iforest.py(460):             node_indicator = tree.decision_path(X_subset)
0.82 iforest.py(461):             n_samples_leaf = tree.tree_.n_node_samples[leaves_index]
0.82 iforest.py(463):             depths += (
0.82 iforest.py(466):                 - 1.0
0.82 iforest.py(493):     n_samples_leaf = check_array(n_samples_leaf, ensure_2d=False)
0.82 iforest.py(495):     n_samples_leaf_shape = n_samples_leaf.shape
0.82 iforest.py(496):     n_samples_leaf = n_samples_leaf.reshape((1, -1))
0.82 iforest.py(497):     average_path_length = np.zeros(n_samples_leaf.shape)
0.82 iforest.py(499):     mask_1 = n_samples_leaf <= 1
0.82 iforest.py(500):     mask_2 = n_samples_leaf == 2
0.82 iforest.py(501):     not_mask = ~np.logical_or(mask_1, mask_2)
0.82 iforest.py(503):     average_path_length[mask_1] = 0.
0.82 iforest.py(504):     average_path_length[mask_2] = 1.
0.82 iforest.py(506):         2.0 * (np.log(n_samples_leaf[not_mask] - 1.0) + np.euler_gamma)
0.82 iforest.py(507):         - 2.0 * (n_samples_leaf[not_mask] - 1.0) / n_samples_leaf[not_mask]
0.82 iforest.py(510):     return average_path_length.reshape(n_samples_leaf_shape)
0.82 iforest.py(456):         for tree, features in zip(self.estimators_, self.estimators_features_):
0.82 iforest.py(457):             X_subset = X[:, features] if subsample_features else X
0.82 iforest.py(459):             leaves_index = tree.apply(X_subset)
0.82 iforest.py(460):             node_indicator = tree.decision_path(X_subset)
0.82 iforest.py(461):             n_samples_leaf = tree.tree_.n_node_samples[leaves_index]
0.82 iforest.py(463):             depths += (
0.82 iforest.py(466):                 - 1.0
0.82 iforest.py(493):     n_samples_leaf = check_array(n_samples_leaf, ensure_2d=False)
0.82 iforest.py(495):     n_samples_leaf_shape = n_samples_leaf.shape
0.82 iforest.py(496):     n_samples_leaf = n_samples_leaf.reshape((1, -1))
0.82 iforest.py(497):     average_path_length = np.zeros(n_samples_leaf.shape)
0.82 iforest.py(499):     mask_1 = n_samples_leaf <= 1
0.82 iforest.py(500):     mask_2 = n_samples_leaf == 2
0.82 iforest.py(501):     not_mask = ~np.logical_or(mask_1, mask_2)
0.82 iforest.py(503):     average_path_length[mask_1] = 0.
0.82 iforest.py(504):     average_path_length[mask_2] = 1.
0.82 iforest.py(506):         2.0 * (np.log(n_samples_leaf[not_mask] - 1.0) + np.euler_gamma)
0.82 iforest.py(507):         - 2.0 * (n_samples_leaf[not_mask] - 1.0) / n_samples_leaf[not_mask]
0.82 iforest.py(510):     return average_path_length.reshape(n_samples_leaf_shape)
0.82 iforest.py(456):         for tree, features in zip(self.estimators_, self.estimators_features_):
0.82 iforest.py(457):             X_subset = X[:, features] if subsample_features else X
0.82 iforest.py(459):             leaves_index = tree.apply(X_subset)
0.82 iforest.py(460):             node_indicator = tree.decision_path(X_subset)
0.82 iforest.py(461):             n_samples_leaf = tree.tree_.n_node_samples[leaves_index]
0.82 iforest.py(463):             depths += (
0.82 iforest.py(466):                 - 1.0
0.82 iforest.py(493):     n_samples_leaf = check_array(n_samples_leaf, ensure_2d=False)
0.82 iforest.py(495):     n_samples_leaf_shape = n_samples_leaf.shape
0.82 iforest.py(496):     n_samples_leaf = n_samples_leaf.reshape((1, -1))
0.82 iforest.py(497):     average_path_length = np.zeros(n_samples_leaf.shape)
0.82 iforest.py(499):     mask_1 = n_samples_leaf <= 1
0.82 iforest.py(500):     mask_2 = n_samples_leaf == 2
0.82 iforest.py(501):     not_mask = ~np.logical_or(mask_1, mask_2)
0.82 iforest.py(503):     average_path_length[mask_1] = 0.
0.82 iforest.py(504):     average_path_length[mask_2] = 1.
0.82 iforest.py(506):         2.0 * (np.log(n_samples_leaf[not_mask] - 1.0) + np.euler_gamma)
0.82 iforest.py(507):         - 2.0 * (n_samples_leaf[not_mask] - 1.0) / n_samples_leaf[not_mask]
0.82 iforest.py(510):     return average_path_length.reshape(n_samples_leaf_shape)
0.82 iforest.py(456):         for tree, features in zip(self.estimators_, self.estimators_features_):
0.82 iforest.py(457):             X_subset = X[:, features] if subsample_features else X
0.82 iforest.py(459):             leaves_index = tree.apply(X_subset)
0.82 iforest.py(460):             node_indicator = tree.decision_path(X_subset)
0.82 iforest.py(461):             n_samples_leaf = tree.tree_.n_node_samples[leaves_index]
0.82 iforest.py(463):             depths += (
0.82 iforest.py(466):                 - 1.0
0.82 iforest.py(493):     n_samples_leaf = check_array(n_samples_leaf, ensure_2d=False)
0.82 iforest.py(495):     n_samples_leaf_shape = n_samples_leaf.shape
0.82 iforest.py(496):     n_samples_leaf = n_samples_leaf.reshape((1, -1))
0.82 iforest.py(497):     average_path_length = np.zeros(n_samples_leaf.shape)
0.82 iforest.py(499):     mask_1 = n_samples_leaf <= 1
0.82 iforest.py(500):     mask_2 = n_samples_leaf == 2
0.82 iforest.py(501):     not_mask = ~np.logical_or(mask_1, mask_2)
0.82 iforest.py(503):     average_path_length[mask_1] = 0.
0.82 iforest.py(504):     average_path_length[mask_2] = 1.
0.82 iforest.py(506):         2.0 * (np.log(n_samples_leaf[not_mask] - 1.0) + np.euler_gamma)
0.82 iforest.py(507):         - 2.0 * (n_samples_leaf[not_mask] - 1.0) / n_samples_leaf[not_mask]
0.82 iforest.py(510):     return average_path_length.reshape(n_samples_leaf_shape)
0.82 iforest.py(456):         for tree, features in zip(self.estimators_, self.estimators_features_):
0.82 iforest.py(457):             X_subset = X[:, features] if subsample_features else X
0.82 iforest.py(459):             leaves_index = tree.apply(X_subset)
0.82 iforest.py(460):             node_indicator = tree.decision_path(X_subset)
0.82 iforest.py(461):             n_samples_leaf = tree.tree_.n_node_samples[leaves_index]
0.82 iforest.py(463):             depths += (
0.82 iforest.py(466):                 - 1.0
0.82 iforest.py(493):     n_samples_leaf = check_array(n_samples_leaf, ensure_2d=False)
0.82 iforest.py(495):     n_samples_leaf_shape = n_samples_leaf.shape
0.82 iforest.py(496):     n_samples_leaf = n_samples_leaf.reshape((1, -1))
0.82 iforest.py(497):     average_path_length = np.zeros(n_samples_leaf.shape)
0.82 iforest.py(499):     mask_1 = n_samples_leaf <= 1
0.82 iforest.py(500):     mask_2 = n_samples_leaf == 2
0.82 iforest.py(501):     not_mask = ~np.logical_or(mask_1, mask_2)
0.82 iforest.py(503):     average_path_length[mask_1] = 0.
0.82 iforest.py(504):     average_path_length[mask_2] = 1.
0.82 iforest.py(506):         2.0 * (np.log(n_samples_leaf[not_mask] - 1.0) + np.euler_gamma)
0.82 iforest.py(507):         - 2.0 * (n_samples_leaf[not_mask] - 1.0) / n_samples_leaf[not_mask]
0.82 iforest.py(510):     return average_path_length.reshape(n_samples_leaf_shape)
0.82 iforest.py(456):         for tree, features in zip(self.estimators_, self.estimators_features_):
0.82 iforest.py(457):             X_subset = X[:, features] if subsample_features else X
0.82 iforest.py(459):             leaves_index = tree.apply(X_subset)
0.82 iforest.py(460):             node_indicator = tree.decision_path(X_subset)
0.82 iforest.py(461):             n_samples_leaf = tree.tree_.n_node_samples[leaves_index]
0.82 iforest.py(463):             depths += (
0.82 iforest.py(466):                 - 1.0
0.82 iforest.py(493):     n_samples_leaf = check_array(n_samples_leaf, ensure_2d=False)
0.82 iforest.py(495):     n_samples_leaf_shape = n_samples_leaf.shape
0.82 iforest.py(496):     n_samples_leaf = n_samples_leaf.reshape((1, -1))
0.82 iforest.py(497):     average_path_length = np.zeros(n_samples_leaf.shape)
0.82 iforest.py(499):     mask_1 = n_samples_leaf <= 1
0.82 iforest.py(500):     mask_2 = n_samples_leaf == 2
0.82 iforest.py(501):     not_mask = ~np.logical_or(mask_1, mask_2)
0.82 iforest.py(503):     average_path_length[mask_1] = 0.
0.82 iforest.py(504):     average_path_length[mask_2] = 1.
0.82 iforest.py(506):         2.0 * (np.log(n_samples_leaf[not_mask] - 1.0) + np.euler_gamma)
0.82 iforest.py(507):         - 2.0 * (n_samples_leaf[not_mask] - 1.0) / n_samples_leaf[not_mask]
0.82 iforest.py(510):     return average_path_length.reshape(n_samples_leaf_shape)
0.82 iforest.py(456):         for tree, features in zip(self.estimators_, self.estimators_features_):
0.82 iforest.py(457):             X_subset = X[:, features] if subsample_features else X
0.82 iforest.py(459):             leaves_index = tree.apply(X_subset)
0.82 iforest.py(460):             node_indicator = tree.decision_path(X_subset)
0.82 iforest.py(461):             n_samples_leaf = tree.tree_.n_node_samples[leaves_index]
0.82 iforest.py(463):             depths += (
0.82 iforest.py(466):                 - 1.0
0.82 iforest.py(493):     n_samples_leaf = check_array(n_samples_leaf, ensure_2d=False)
0.82 iforest.py(495):     n_samples_leaf_shape = n_samples_leaf.shape
0.82 iforest.py(496):     n_samples_leaf = n_samples_leaf.reshape((1, -1))
0.82 iforest.py(497):     average_path_length = np.zeros(n_samples_leaf.shape)
0.82 iforest.py(499):     mask_1 = n_samples_leaf <= 1
0.82 iforest.py(500):     mask_2 = n_samples_leaf == 2
0.82 iforest.py(501):     not_mask = ~np.logical_or(mask_1, mask_2)
0.82 iforest.py(503):     average_path_length[mask_1] = 0.
0.82 iforest.py(504):     average_path_length[mask_2] = 1.
0.82 iforest.py(506):         2.0 * (np.log(n_samples_leaf[not_mask] - 1.0) + np.euler_gamma)
0.82 iforest.py(507):         - 2.0 * (n_samples_leaf[not_mask] - 1.0) / n_samples_leaf[not_mask]
0.82 iforest.py(510):     return average_path_length.reshape(n_samples_leaf_shape)
0.82 iforest.py(456):         for tree, features in zip(self.estimators_, self.estimators_features_):
0.82 iforest.py(469):         scores = 2 ** (
0.82 iforest.py(470):             -depths
0.82 iforest.py(471):             / (len(self.estimators_)
0.82 iforest.py(472):                * _average_path_length([self.max_samples_]))
0.82 iforest.py(493):     n_samples_leaf = check_array(n_samples_leaf, ensure_2d=False)
0.82 iforest.py(495):     n_samples_leaf_shape = n_samples_leaf.shape
0.82 iforest.py(496):     n_samples_leaf = n_samples_leaf.reshape((1, -1))
0.82 iforest.py(497):     average_path_length = np.zeros(n_samples_leaf.shape)
0.82 iforest.py(499):     mask_1 = n_samples_leaf <= 1
0.82 iforest.py(500):     mask_2 = n_samples_leaf == 2
0.82 iforest.py(501):     not_mask = ~np.logical_or(mask_1, mask_2)
0.82 iforest.py(503):     average_path_length[mask_1] = 0.
0.82 iforest.py(504):     average_path_length[mask_2] = 1.
0.82 iforest.py(506):         2.0 * (np.log(n_samples_leaf[not_mask] - 1.0) + np.euler_gamma)
0.82 iforest.py(507):         - 2.0 * (n_samples_leaf[not_mask] - 1.0) / n_samples_leaf[not_mask]
0.82 iforest.py(510):     return average_path_length.reshape(n_samples_leaf_shape)
0.82 iforest.py(474):         return scores
0.82 iforest.py(436):         for sl in slices:
0.82 iforest.py(440):         return scores
0.82 iforest.py(290):                                              100. * self._contamination)
0.82 iforest.py(292):             return self
0.82 iforest.py(225):         if self.contamination == "legacy":
0.82 iforest.py(226):             warn('default contamination parameter 0.1 will change '
0.82 iforest.py(229):                  FutureWarning)
0.82 iforest.py(230):             self._contamination = 0.1
0.82 iforest.py(234):         if self.behaviour == 'old':
0.82 iforest.py(235):             warn('behaviour="old" is deprecated and will be removed '
0.82 iforest.py(239):                  FutureWarning)
0.82 iforest.py(241):         X = check_array(X, accept_sparse=['csc'])
0.82 iforest.py(242):         if issparse(X):
0.82 iforest.py(247):         rnd = check_random_state(self.random_state)
0.82 iforest.py(248):         y = rnd.uniform(size=X.shape[0])
0.82 iforest.py(251):         n_samples = X.shape[0]
0.82 iforest.py(253):         if isinstance(self.max_samples, str):
0.82 iforest.py(254):             if self.max_samples == 'auto':
0.82 iforest.py(255):                 max_samples = min(256, n_samples)
0.82 iforest.py(276):         self.max_samples_ = max_samples
0.82 iforest.py(277):         max_depth = int(np.ceil(np.log2(max(max_samples, 2))))
0.82 iforest.py(278):         super()._fit(X, y, max_samples,
0.82 iforest.py(279):                      max_depth=max_depth,
0.82 iforest.py(280):                      sample_weight=sample_weight)
0.82 iforest.py(203):         return _joblib_parallel_args(prefer='threads')
0.84 iforest.py(282):         if self.behaviour == 'old':
0.84 iforest.py(284):             if self._contamination == "auto":
0.84 iforest.py(288):             self.offset_ = -0.5
0.84 iforest.py(289):             self._threshold_ = np.percentile(self.decision_function(X),
0.84 iforest.py(361):         return self.score_samples(X) - self.offset_
0.84 iforest.py(387):         check_is_fitted(self, ["estimators_"])
0.84 iforest.py(390):         X = check_array(X, accept_sparse='csr')
0.84 iforest.py(391):         if self.n_features_ != X.shape[1]:
0.84 iforest.py(399):         return -self._compute_chunked_score_samples(X)
0.84 iforest.py(412):         n_samples = _num_samples(X)
0.84 iforest.py(414):         if self._max_features == X.shape[1]:
0.84 iforest.py(415):             subsample_features = False
0.84 iforest.py(430):         chunk_n_rows = get_chunk_n_rows(row_bytes=16 * self._max_features,
0.84 iforest.py(431):                                         max_n_rows=n_samples)
0.84 iforest.py(432):         slices = gen_batches(n_samples, chunk_n_rows)
0.84 iforest.py(434):         scores = np.zeros(n_samples, order="f")
0.84 iforest.py(436):         for sl in slices:
0.84 iforest.py(438):             scores[sl] = self._compute_score_samples(X[sl], subsample_features)
0.84 iforest.py(452):         n_samples = X.shape[0]
0.84 iforest.py(454):         depths = np.zeros(n_samples, order="f")
0.84 iforest.py(456):         for tree, features in zip(self.estimators_, self.estimators_features_):
0.84 iforest.py(457):             X_subset = X[:, features] if subsample_features else X
0.84 iforest.py(459):             leaves_index = tree.apply(X_subset)
0.84 iforest.py(460):             node_indicator = tree.decision_path(X_subset)
0.84 iforest.py(461):             n_samples_leaf = tree.tree_.n_node_samples[leaves_index]
0.84 iforest.py(463):             depths += (
0.84 iforest.py(466):                 - 1.0
0.84 iforest.py(493):     n_samples_leaf = check_array(n_samples_leaf, ensure_2d=False)
0.84 iforest.py(495):     n_samples_leaf_shape = n_samples_leaf.shape
0.84 iforest.py(496):     n_samples_leaf = n_samples_leaf.reshape((1, -1))
0.84 iforest.py(497):     average_path_length = np.zeros(n_samples_leaf.shape)
0.84 iforest.py(499):     mask_1 = n_samples_leaf <= 1
0.84 iforest.py(500):     mask_2 = n_samples_leaf == 2
0.84 iforest.py(501):     not_mask = ~np.logical_or(mask_1, mask_2)
0.84 iforest.py(503):     average_path_length[mask_1] = 0.
0.84 iforest.py(504):     average_path_length[mask_2] = 1.
0.84 iforest.py(506):         2.0 * (np.log(n_samples_leaf[not_mask] - 1.0) + np.euler_gamma)
0.84 iforest.py(507):         - 2.0 * (n_samples_leaf[not_mask] - 1.0) / n_samples_leaf[not_mask]
0.84 iforest.py(510):     return average_path_length.reshape(n_samples_leaf_shape)
0.84 iforest.py(456):         for tree, features in zip(self.estimators_, self.estimators_features_):
0.84 iforest.py(457):             X_subset = X[:, features] if subsample_features else X
0.84 iforest.py(459):             leaves_index = tree.apply(X_subset)
0.84 iforest.py(460):             node_indicator = tree.decision_path(X_subset)
0.84 iforest.py(461):             n_samples_leaf = tree.tree_.n_node_samples[leaves_index]
0.84 iforest.py(463):             depths += (
0.84 iforest.py(466):                 - 1.0
0.84 iforest.py(493):     n_samples_leaf = check_array(n_samples_leaf, ensure_2d=False)
0.84 iforest.py(495):     n_samples_leaf_shape = n_samples_leaf.shape
0.84 iforest.py(496):     n_samples_leaf = n_samples_leaf.reshape((1, -1))
0.84 iforest.py(497):     average_path_length = np.zeros(n_samples_leaf.shape)
0.84 iforest.py(499):     mask_1 = n_samples_leaf <= 1
0.84 iforest.py(500):     mask_2 = n_samples_leaf == 2
0.84 iforest.py(501):     not_mask = ~np.logical_or(mask_1, mask_2)
0.84 iforest.py(503):     average_path_length[mask_1] = 0.
0.84 iforest.py(504):     average_path_length[mask_2] = 1.
0.84 iforest.py(506):         2.0 * (np.log(n_samples_leaf[not_mask] - 1.0) + np.euler_gamma)
0.84 iforest.py(507):         - 2.0 * (n_samples_leaf[not_mask] - 1.0) / n_samples_leaf[not_mask]
0.84 iforest.py(510):     return average_path_length.reshape(n_samples_leaf_shape)
0.84 iforest.py(456):         for tree, features in zip(self.estimators_, self.estimators_features_):
0.84 iforest.py(457):             X_subset = X[:, features] if subsample_features else X
0.84 iforest.py(459):             leaves_index = tree.apply(X_subset)
0.84 iforest.py(460):             node_indicator = tree.decision_path(X_subset)
0.84 iforest.py(461):             n_samples_leaf = tree.tree_.n_node_samples[leaves_index]
0.84 iforest.py(463):             depths += (
0.84 iforest.py(466):                 - 1.0
0.84 iforest.py(493):     n_samples_leaf = check_array(n_samples_leaf, ensure_2d=False)
0.84 iforest.py(495):     n_samples_leaf_shape = n_samples_leaf.shape
0.84 iforest.py(496):     n_samples_leaf = n_samples_leaf.reshape((1, -1))
0.84 iforest.py(497):     average_path_length = np.zeros(n_samples_leaf.shape)
0.84 iforest.py(499):     mask_1 = n_samples_leaf <= 1
0.84 iforest.py(500):     mask_2 = n_samples_leaf == 2
0.84 iforest.py(501):     not_mask = ~np.logical_or(mask_1, mask_2)
0.84 iforest.py(503):     average_path_length[mask_1] = 0.
0.84 iforest.py(504):     average_path_length[mask_2] = 1.
0.84 iforest.py(506):         2.0 * (np.log(n_samples_leaf[not_mask] - 1.0) + np.euler_gamma)
0.84 iforest.py(507):         - 2.0 * (n_samples_leaf[not_mask] - 1.0) / n_samples_leaf[not_mask]
0.84 iforest.py(510):     return average_path_length.reshape(n_samples_leaf_shape)
0.84 iforest.py(456):         for tree, features in zip(self.estimators_, self.estimators_features_):
0.84 iforest.py(457):             X_subset = X[:, features] if subsample_features else X
0.84 iforest.py(459):             leaves_index = tree.apply(X_subset)
0.84 iforest.py(460):             node_indicator = tree.decision_path(X_subset)
0.84 iforest.py(461):             n_samples_leaf = tree.tree_.n_node_samples[leaves_index]
0.84 iforest.py(463):             depths += (
0.84 iforest.py(466):                 - 1.0
0.84 iforest.py(493):     n_samples_leaf = check_array(n_samples_leaf, ensure_2d=False)
0.84 iforest.py(495):     n_samples_leaf_shape = n_samples_leaf.shape
0.84 iforest.py(496):     n_samples_leaf = n_samples_leaf.reshape((1, -1))
0.84 iforest.py(497):     average_path_length = np.zeros(n_samples_leaf.shape)
0.84 iforest.py(499):     mask_1 = n_samples_leaf <= 1
0.84 iforest.py(500):     mask_2 = n_samples_leaf == 2
0.84 iforest.py(501):     not_mask = ~np.logical_or(mask_1, mask_2)
0.84 iforest.py(503):     average_path_length[mask_1] = 0.
0.84 iforest.py(504):     average_path_length[mask_2] = 1.
0.84 iforest.py(506):         2.0 * (np.log(n_samples_leaf[not_mask] - 1.0) + np.euler_gamma)
0.84 iforest.py(507):         - 2.0 * (n_samples_leaf[not_mask] - 1.0) / n_samples_leaf[not_mask]
0.84 iforest.py(510):     return average_path_length.reshape(n_samples_leaf_shape)
0.84 iforest.py(456):         for tree, features in zip(self.estimators_, self.estimators_features_):
0.84 iforest.py(457):             X_subset = X[:, features] if subsample_features else X
0.84 iforest.py(459):             leaves_index = tree.apply(X_subset)
0.84 iforest.py(460):             node_indicator = tree.decision_path(X_subset)
0.84 iforest.py(461):             n_samples_leaf = tree.tree_.n_node_samples[leaves_index]
0.84 iforest.py(463):             depths += (
0.84 iforest.py(466):                 - 1.0
0.84 iforest.py(493):     n_samples_leaf = check_array(n_samples_leaf, ensure_2d=False)
0.84 iforest.py(495):     n_samples_leaf_shape = n_samples_leaf.shape
0.84 iforest.py(496):     n_samples_leaf = n_samples_leaf.reshape((1, -1))
0.84 iforest.py(497):     average_path_length = np.zeros(n_samples_leaf.shape)
0.84 iforest.py(499):     mask_1 = n_samples_leaf <= 1
0.84 iforest.py(500):     mask_2 = n_samples_leaf == 2
0.84 iforest.py(501):     not_mask = ~np.logical_or(mask_1, mask_2)
0.84 iforest.py(503):     average_path_length[mask_1] = 0.
0.84 iforest.py(504):     average_path_length[mask_2] = 1.
0.84 iforest.py(506):         2.0 * (np.log(n_samples_leaf[not_mask] - 1.0) + np.euler_gamma)
0.84 iforest.py(507):         - 2.0 * (n_samples_leaf[not_mask] - 1.0) / n_samples_leaf[not_mask]
0.84 iforest.py(510):     return average_path_length.reshape(n_samples_leaf_shape)
0.84 iforest.py(456):         for tree, features in zip(self.estimators_, self.estimators_features_):
0.84 iforest.py(457):             X_subset = X[:, features] if subsample_features else X
0.84 iforest.py(459):             leaves_index = tree.apply(X_subset)
0.84 iforest.py(460):             node_indicator = tree.decision_path(X_subset)
0.84 iforest.py(461):             n_samples_leaf = tree.tree_.n_node_samples[leaves_index]
0.84 iforest.py(463):             depths += (
0.84 iforest.py(466):                 - 1.0
0.84 iforest.py(493):     n_samples_leaf = check_array(n_samples_leaf, ensure_2d=False)
0.84 iforest.py(495):     n_samples_leaf_shape = n_samples_leaf.shape
0.84 iforest.py(496):     n_samples_leaf = n_samples_leaf.reshape((1, -1))
0.84 iforest.py(497):     average_path_length = np.zeros(n_samples_leaf.shape)
0.84 iforest.py(499):     mask_1 = n_samples_leaf <= 1
0.84 iforest.py(500):     mask_2 = n_samples_leaf == 2
0.84 iforest.py(501):     not_mask = ~np.logical_or(mask_1, mask_2)
0.84 iforest.py(503):     average_path_length[mask_1] = 0.
0.84 iforest.py(504):     average_path_length[mask_2] = 1.
0.84 iforest.py(506):         2.0 * (np.log(n_samples_leaf[not_mask] - 1.0) + np.euler_gamma)
0.84 iforest.py(507):         - 2.0 * (n_samples_leaf[not_mask] - 1.0) / n_samples_leaf[not_mask]
0.84 iforest.py(510):     return average_path_length.reshape(n_samples_leaf_shape)
0.84 iforest.py(456):         for tree, features in zip(self.estimators_, self.estimators_features_):
0.84 iforest.py(457):             X_subset = X[:, features] if subsample_features else X
0.84 iforest.py(459):             leaves_index = tree.apply(X_subset)
0.84 iforest.py(460):             node_indicator = tree.decision_path(X_subset)
0.84 iforest.py(461):             n_samples_leaf = tree.tree_.n_node_samples[leaves_index]
0.84 iforest.py(463):             depths += (
0.84 iforest.py(466):                 - 1.0
0.84 iforest.py(493):     n_samples_leaf = check_array(n_samples_leaf, ensure_2d=False)
0.84 iforest.py(495):     n_samples_leaf_shape = n_samples_leaf.shape
0.84 iforest.py(496):     n_samples_leaf = n_samples_leaf.reshape((1, -1))
0.84 iforest.py(497):     average_path_length = np.zeros(n_samples_leaf.shape)
0.84 iforest.py(499):     mask_1 = n_samples_leaf <= 1
0.84 iforest.py(500):     mask_2 = n_samples_leaf == 2
0.84 iforest.py(501):     not_mask = ~np.logical_or(mask_1, mask_2)
0.84 iforest.py(503):     average_path_length[mask_1] = 0.
0.84 iforest.py(504):     average_path_length[mask_2] = 1.
0.84 iforest.py(506):         2.0 * (np.log(n_samples_leaf[not_mask] - 1.0) + np.euler_gamma)
0.84 iforest.py(507):         - 2.0 * (n_samples_leaf[not_mask] - 1.0) / n_samples_leaf[not_mask]
0.84 iforest.py(510):     return average_path_length.reshape(n_samples_leaf_shape)
0.84 iforest.py(456):         for tree, features in zip(self.estimators_, self.estimators_features_):
0.85 iforest.py(457):             X_subset = X[:, features] if subsample_features else X
0.85 iforest.py(459):             leaves_index = tree.apply(X_subset)
0.85 iforest.py(460):             node_indicator = tree.decision_path(X_subset)
0.85 iforest.py(461):             n_samples_leaf = tree.tree_.n_node_samples[leaves_index]
0.85 iforest.py(463):             depths += (
0.85 iforest.py(466):                 - 1.0
0.85 iforest.py(493):     n_samples_leaf = check_array(n_samples_leaf, ensure_2d=False)
0.85 iforest.py(495):     n_samples_leaf_shape = n_samples_leaf.shape
0.85 iforest.py(496):     n_samples_leaf = n_samples_leaf.reshape((1, -1))
0.85 iforest.py(497):     average_path_length = np.zeros(n_samples_leaf.shape)
0.85 iforest.py(499):     mask_1 = n_samples_leaf <= 1
0.85 iforest.py(500):     mask_2 = n_samples_leaf == 2
0.85 iforest.py(501):     not_mask = ~np.logical_or(mask_1, mask_2)
0.85 iforest.py(503):     average_path_length[mask_1] = 0.
0.85 iforest.py(504):     average_path_length[mask_2] = 1.
0.85 iforest.py(506):         2.0 * (np.log(n_samples_leaf[not_mask] - 1.0) + np.euler_gamma)
0.85 iforest.py(507):         - 2.0 * (n_samples_leaf[not_mask] - 1.0) / n_samples_leaf[not_mask]
0.85 iforest.py(510):     return average_path_length.reshape(n_samples_leaf_shape)
0.85 iforest.py(456):         for tree, features in zip(self.estimators_, self.estimators_features_):
0.85 iforest.py(457):             X_subset = X[:, features] if subsample_features else X
0.85 iforest.py(459):             leaves_index = tree.apply(X_subset)
0.85 iforest.py(460):             node_indicator = tree.decision_path(X_subset)
0.85 iforest.py(461):             n_samples_leaf = tree.tree_.n_node_samples[leaves_index]
0.85 iforest.py(463):             depths += (
0.85 iforest.py(466):                 - 1.0
0.85 iforest.py(493):     n_samples_leaf = check_array(n_samples_leaf, ensure_2d=False)
0.85 iforest.py(495):     n_samples_leaf_shape = n_samples_leaf.shape
0.85 iforest.py(496):     n_samples_leaf = n_samples_leaf.reshape((1, -1))
0.85 iforest.py(497):     average_path_length = np.zeros(n_samples_leaf.shape)
0.85 iforest.py(499):     mask_1 = n_samples_leaf <= 1
0.85 iforest.py(500):     mask_2 = n_samples_leaf == 2
0.85 iforest.py(501):     not_mask = ~np.logical_or(mask_1, mask_2)
0.85 iforest.py(503):     average_path_length[mask_1] = 0.
0.85 iforest.py(504):     average_path_length[mask_2] = 1.
0.85 iforest.py(506):         2.0 * (np.log(n_samples_leaf[not_mask] - 1.0) + np.euler_gamma)
0.85 iforest.py(507):         - 2.0 * (n_samples_leaf[not_mask] - 1.0) / n_samples_leaf[not_mask]
0.85 iforest.py(510):     return average_path_length.reshape(n_samples_leaf_shape)
0.85 iforest.py(456):         for tree, features in zip(self.estimators_, self.estimators_features_):
0.85 iforest.py(457):             X_subset = X[:, features] if subsample_features else X
0.85 iforest.py(459):             leaves_index = tree.apply(X_subset)
0.85 iforest.py(460):             node_indicator = tree.decision_path(X_subset)
0.85 iforest.py(461):             n_samples_leaf = tree.tree_.n_node_samples[leaves_index]
0.85 iforest.py(463):             depths += (
0.85 iforest.py(466):                 - 1.0
0.85 iforest.py(493):     n_samples_leaf = check_array(n_samples_leaf, ensure_2d=False)
0.85 iforest.py(495):     n_samples_leaf_shape = n_samples_leaf.shape
0.85 iforest.py(496):     n_samples_leaf = n_samples_leaf.reshape((1, -1))
0.85 iforest.py(497):     average_path_length = np.zeros(n_samples_leaf.shape)
0.85 iforest.py(499):     mask_1 = n_samples_leaf <= 1
0.85 iforest.py(500):     mask_2 = n_samples_leaf == 2
0.85 iforest.py(501):     not_mask = ~np.logical_or(mask_1, mask_2)
0.85 iforest.py(503):     average_path_length[mask_1] = 0.
0.85 iforest.py(504):     average_path_length[mask_2] = 1.
0.85 iforest.py(506):         2.0 * (np.log(n_samples_leaf[not_mask] - 1.0) + np.euler_gamma)
0.85 iforest.py(507):         - 2.0 * (n_samples_leaf[not_mask] - 1.0) / n_samples_leaf[not_mask]
0.85 iforest.py(510):     return average_path_length.reshape(n_samples_leaf_shape)
0.85 iforest.py(456):         for tree, features in zip(self.estimators_, self.estimators_features_):
0.85 iforest.py(457):             X_subset = X[:, features] if subsample_features else X
0.85 iforest.py(459):             leaves_index = tree.apply(X_subset)
0.85 iforest.py(460):             node_indicator = tree.decision_path(X_subset)
0.85 iforest.py(461):             n_samples_leaf = tree.tree_.n_node_samples[leaves_index]
0.85 iforest.py(463):             depths += (
0.85 iforest.py(466):                 - 1.0
0.85 iforest.py(493):     n_samples_leaf = check_array(n_samples_leaf, ensure_2d=False)
0.85 iforest.py(495):     n_samples_leaf_shape = n_samples_leaf.shape
0.85 iforest.py(496):     n_samples_leaf = n_samples_leaf.reshape((1, -1))
0.85 iforest.py(497):     average_path_length = np.zeros(n_samples_leaf.shape)
0.85 iforest.py(499):     mask_1 = n_samples_leaf <= 1
0.85 iforest.py(500):     mask_2 = n_samples_leaf == 2
0.85 iforest.py(501):     not_mask = ~np.logical_or(mask_1, mask_2)
0.85 iforest.py(503):     average_path_length[mask_1] = 0.
0.85 iforest.py(504):     average_path_length[mask_2] = 1.
0.85 iforest.py(506):         2.0 * (np.log(n_samples_leaf[not_mask] - 1.0) + np.euler_gamma)
0.85 iforest.py(507):         - 2.0 * (n_samples_leaf[not_mask] - 1.0) / n_samples_leaf[not_mask]
0.85 iforest.py(510):     return average_path_length.reshape(n_samples_leaf_shape)
0.85 iforest.py(456):         for tree, features in zip(self.estimators_, self.estimators_features_):
0.85 iforest.py(457):             X_subset = X[:, features] if subsample_features else X
0.85 iforest.py(459):             leaves_index = tree.apply(X_subset)
0.85 iforest.py(460):             node_indicator = tree.decision_path(X_subset)
0.85 iforest.py(461):             n_samples_leaf = tree.tree_.n_node_samples[leaves_index]
0.85 iforest.py(463):             depths += (
0.85 iforest.py(466):                 - 1.0
0.85 iforest.py(493):     n_samples_leaf = check_array(n_samples_leaf, ensure_2d=False)
0.85 iforest.py(495):     n_samples_leaf_shape = n_samples_leaf.shape
0.85 iforest.py(496):     n_samples_leaf = n_samples_leaf.reshape((1, -1))
0.85 iforest.py(497):     average_path_length = np.zeros(n_samples_leaf.shape)
0.85 iforest.py(499):     mask_1 = n_samples_leaf <= 1
0.85 iforest.py(500):     mask_2 = n_samples_leaf == 2
0.85 iforest.py(501):     not_mask = ~np.logical_or(mask_1, mask_2)
0.85 iforest.py(503):     average_path_length[mask_1] = 0.
0.85 iforest.py(504):     average_path_length[mask_2] = 1.
0.85 iforest.py(506):         2.0 * (np.log(n_samples_leaf[not_mask] - 1.0) + np.euler_gamma)
0.85 iforest.py(507):         - 2.0 * (n_samples_leaf[not_mask] - 1.0) / n_samples_leaf[not_mask]
0.85 iforest.py(510):     return average_path_length.reshape(n_samples_leaf_shape)
0.85 iforest.py(456):         for tree, features in zip(self.estimators_, self.estimators_features_):
0.85 iforest.py(457):             X_subset = X[:, features] if subsample_features else X
0.85 iforest.py(459):             leaves_index = tree.apply(X_subset)
0.85 iforest.py(460):             node_indicator = tree.decision_path(X_subset)
0.85 iforest.py(461):             n_samples_leaf = tree.tree_.n_node_samples[leaves_index]
0.85 iforest.py(463):             depths += (
0.85 iforest.py(466):                 - 1.0
0.85 iforest.py(493):     n_samples_leaf = check_array(n_samples_leaf, ensure_2d=False)
0.85 iforest.py(495):     n_samples_leaf_shape = n_samples_leaf.shape
0.85 iforest.py(496):     n_samples_leaf = n_samples_leaf.reshape((1, -1))
0.85 iforest.py(497):     average_path_length = np.zeros(n_samples_leaf.shape)
0.85 iforest.py(499):     mask_1 = n_samples_leaf <= 1
0.85 iforest.py(500):     mask_2 = n_samples_leaf == 2
0.85 iforest.py(501):     not_mask = ~np.logical_or(mask_1, mask_2)
0.85 iforest.py(503):     average_path_length[mask_1] = 0.
0.85 iforest.py(504):     average_path_length[mask_2] = 1.
0.85 iforest.py(506):         2.0 * (np.log(n_samples_leaf[not_mask] - 1.0) + np.euler_gamma)
0.85 iforest.py(507):         - 2.0 * (n_samples_leaf[not_mask] - 1.0) / n_samples_leaf[not_mask]
0.85 iforest.py(510):     return average_path_length.reshape(n_samples_leaf_shape)
0.85 iforest.py(456):         for tree, features in zip(self.estimators_, self.estimators_features_):
0.85 iforest.py(457):             X_subset = X[:, features] if subsample_features else X
0.85 iforest.py(459):             leaves_index = tree.apply(X_subset)
0.85 iforest.py(460):             node_indicator = tree.decision_path(X_subset)
0.85 iforest.py(461):             n_samples_leaf = tree.tree_.n_node_samples[leaves_index]
0.85 iforest.py(463):             depths += (
0.85 iforest.py(466):                 - 1.0
0.85 iforest.py(493):     n_samples_leaf = check_array(n_samples_leaf, ensure_2d=False)
0.85 iforest.py(495):     n_samples_leaf_shape = n_samples_leaf.shape
0.85 iforest.py(496):     n_samples_leaf = n_samples_leaf.reshape((1, -1))
0.85 iforest.py(497):     average_path_length = np.zeros(n_samples_leaf.shape)
0.85 iforest.py(499):     mask_1 = n_samples_leaf <= 1
0.85 iforest.py(500):     mask_2 = n_samples_leaf == 2
0.85 iforest.py(501):     not_mask = ~np.logical_or(mask_1, mask_2)
0.85 iforest.py(503):     average_path_length[mask_1] = 0.
0.85 iforest.py(504):     average_path_length[mask_2] = 1.
0.85 iforest.py(506):         2.0 * (np.log(n_samples_leaf[not_mask] - 1.0) + np.euler_gamma)
0.85 iforest.py(507):         - 2.0 * (n_samples_leaf[not_mask] - 1.0) / n_samples_leaf[not_mask]
0.85 iforest.py(510):     return average_path_length.reshape(n_samples_leaf_shape)
0.85 iforest.py(456):         for tree, features in zip(self.estimators_, self.estimators_features_):
0.85 iforest.py(457):             X_subset = X[:, features] if subsample_features else X
0.85 iforest.py(459):             leaves_index = tree.apply(X_subset)
0.85 iforest.py(460):             node_indicator = tree.decision_path(X_subset)
0.85 iforest.py(461):             n_samples_leaf = tree.tree_.n_node_samples[leaves_index]
0.85 iforest.py(463):             depths += (
0.85 iforest.py(466):                 - 1.0
0.85 iforest.py(493):     n_samples_leaf = check_array(n_samples_leaf, ensure_2d=False)
0.85 iforest.py(495):     n_samples_leaf_shape = n_samples_leaf.shape
0.85 iforest.py(496):     n_samples_leaf = n_samples_leaf.reshape((1, -1))
0.85 iforest.py(497):     average_path_length = np.zeros(n_samples_leaf.shape)
0.85 iforest.py(499):     mask_1 = n_samples_leaf <= 1
0.85 iforest.py(500):     mask_2 = n_samples_leaf == 2
0.85 iforest.py(501):     not_mask = ~np.logical_or(mask_1, mask_2)
0.85 iforest.py(503):     average_path_length[mask_1] = 0.
0.85 iforest.py(504):     average_path_length[mask_2] = 1.
0.85 iforest.py(506):         2.0 * (np.log(n_samples_leaf[not_mask] - 1.0) + np.euler_gamma)
0.85 iforest.py(507):         - 2.0 * (n_samples_leaf[not_mask] - 1.0) / n_samples_leaf[not_mask]
0.85 iforest.py(510):     return average_path_length.reshape(n_samples_leaf_shape)
0.85 iforest.py(456):         for tree, features in zip(self.estimators_, self.estimators_features_):
0.85 iforest.py(457):             X_subset = X[:, features] if subsample_features else X
0.85 iforest.py(459):             leaves_index = tree.apply(X_subset)
0.85 iforest.py(460):             node_indicator = tree.decision_path(X_subset)
0.85 iforest.py(461):             n_samples_leaf = tree.tree_.n_node_samples[leaves_index]
0.85 iforest.py(463):             depths += (
0.85 iforest.py(466):                 - 1.0
0.85 iforest.py(493):     n_samples_leaf = check_array(n_samples_leaf, ensure_2d=False)
0.85 iforest.py(495):     n_samples_leaf_shape = n_samples_leaf.shape
0.85 iforest.py(496):     n_samples_leaf = n_samples_leaf.reshape((1, -1))
0.85 iforest.py(497):     average_path_length = np.zeros(n_samples_leaf.shape)
0.85 iforest.py(499):     mask_1 = n_samples_leaf <= 1
0.85 iforest.py(500):     mask_2 = n_samples_leaf == 2
0.85 iforest.py(501):     not_mask = ~np.logical_or(mask_1, mask_2)
0.85 iforest.py(503):     average_path_length[mask_1] = 0.
0.85 iforest.py(504):     average_path_length[mask_2] = 1.
0.85 iforest.py(506):         2.0 * (np.log(n_samples_leaf[not_mask] - 1.0) + np.euler_gamma)
0.85 iforest.py(507):         - 2.0 * (n_samples_leaf[not_mask] - 1.0) / n_samples_leaf[not_mask]
0.85 iforest.py(510):     return average_path_length.reshape(n_samples_leaf_shape)
0.85 iforest.py(456):         for tree, features in zip(self.estimators_, self.estimators_features_):
0.85 iforest.py(457):             X_subset = X[:, features] if subsample_features else X
0.85 iforest.py(459):             leaves_index = tree.apply(X_subset)
0.85 iforest.py(460):             node_indicator = tree.decision_path(X_subset)
0.85 iforest.py(461):             n_samples_leaf = tree.tree_.n_node_samples[leaves_index]
0.85 iforest.py(463):             depths += (
0.85 iforest.py(466):                 - 1.0
0.85 iforest.py(493):     n_samples_leaf = check_array(n_samples_leaf, ensure_2d=False)
0.85 iforest.py(495):     n_samples_leaf_shape = n_samples_leaf.shape
0.85 iforest.py(496):     n_samples_leaf = n_samples_leaf.reshape((1, -1))
0.85 iforest.py(497):     average_path_length = np.zeros(n_samples_leaf.shape)
0.85 iforest.py(499):     mask_1 = n_samples_leaf <= 1
0.85 iforest.py(500):     mask_2 = n_samples_leaf == 2
0.85 iforest.py(501):     not_mask = ~np.logical_or(mask_1, mask_2)
0.85 iforest.py(503):     average_path_length[mask_1] = 0.
0.85 iforest.py(504):     average_path_length[mask_2] = 1.
0.85 iforest.py(506):         2.0 * (np.log(n_samples_leaf[not_mask] - 1.0) + np.euler_gamma)
0.85 iforest.py(507):         - 2.0 * (n_samples_leaf[not_mask] - 1.0) / n_samples_leaf[not_mask]
0.85 iforest.py(510):     return average_path_length.reshape(n_samples_leaf_shape)
0.85 iforest.py(456):         for tree, features in zip(self.estimators_, self.estimators_features_):
0.85 iforest.py(457):             X_subset = X[:, features] if subsample_features else X
0.85 iforest.py(459):             leaves_index = tree.apply(X_subset)
0.85 iforest.py(460):             node_indicator = tree.decision_path(X_subset)
0.85 iforest.py(461):             n_samples_leaf = tree.tree_.n_node_samples[leaves_index]
0.85 iforest.py(463):             depths += (
0.85 iforest.py(466):                 - 1.0
0.85 iforest.py(493):     n_samples_leaf = check_array(n_samples_leaf, ensure_2d=False)
0.85 iforest.py(495):     n_samples_leaf_shape = n_samples_leaf.shape
0.85 iforest.py(496):     n_samples_leaf = n_samples_leaf.reshape((1, -1))
0.85 iforest.py(497):     average_path_length = np.zeros(n_samples_leaf.shape)
0.85 iforest.py(499):     mask_1 = n_samples_leaf <= 1
0.85 iforest.py(500):     mask_2 = n_samples_leaf == 2
0.85 iforest.py(501):     not_mask = ~np.logical_or(mask_1, mask_2)
0.85 iforest.py(503):     average_path_length[mask_1] = 0.
0.85 iforest.py(504):     average_path_length[mask_2] = 1.
0.85 iforest.py(506):         2.0 * (np.log(n_samples_leaf[not_mask] - 1.0) + np.euler_gamma)
0.85 iforest.py(507):         - 2.0 * (n_samples_leaf[not_mask] - 1.0) / n_samples_leaf[not_mask]
0.86 iforest.py(510):     return average_path_length.reshape(n_samples_leaf_shape)
0.86 iforest.py(456):         for tree, features in zip(self.estimators_, self.estimators_features_):
0.86 iforest.py(457):             X_subset = X[:, features] if subsample_features else X
0.86 iforest.py(459):             leaves_index = tree.apply(X_subset)
0.86 iforest.py(460):             node_indicator = tree.decision_path(X_subset)
0.86 iforest.py(461):             n_samples_leaf = tree.tree_.n_node_samples[leaves_index]
0.86 iforest.py(463):             depths += (
0.86 iforest.py(466):                 - 1.0
0.86 iforest.py(493):     n_samples_leaf = check_array(n_samples_leaf, ensure_2d=False)
0.86 iforest.py(495):     n_samples_leaf_shape = n_samples_leaf.shape
0.86 iforest.py(496):     n_samples_leaf = n_samples_leaf.reshape((1, -1))
0.86 iforest.py(497):     average_path_length = np.zeros(n_samples_leaf.shape)
0.86 iforest.py(499):     mask_1 = n_samples_leaf <= 1
0.86 iforest.py(500):     mask_2 = n_samples_leaf == 2
0.86 iforest.py(501):     not_mask = ~np.logical_or(mask_1, mask_2)
0.86 iforest.py(503):     average_path_length[mask_1] = 0.
0.86 iforest.py(504):     average_path_length[mask_2] = 1.
0.86 iforest.py(506):         2.0 * (np.log(n_samples_leaf[not_mask] - 1.0) + np.euler_gamma)
0.86 iforest.py(507):         - 2.0 * (n_samples_leaf[not_mask] - 1.0) / n_samples_leaf[not_mask]
0.86 iforest.py(510):     return average_path_length.reshape(n_samples_leaf_shape)
0.86 iforest.py(456):         for tree, features in zip(self.estimators_, self.estimators_features_):
0.86 iforest.py(457):             X_subset = X[:, features] if subsample_features else X
0.86 iforest.py(459):             leaves_index = tree.apply(X_subset)
0.86 iforest.py(460):             node_indicator = tree.decision_path(X_subset)
0.86 iforest.py(461):             n_samples_leaf = tree.tree_.n_node_samples[leaves_index]
0.86 iforest.py(463):             depths += (
0.86 iforest.py(466):                 - 1.0
0.86 iforest.py(493):     n_samples_leaf = check_array(n_samples_leaf, ensure_2d=False)
0.86 iforest.py(495):     n_samples_leaf_shape = n_samples_leaf.shape
0.86 iforest.py(496):     n_samples_leaf = n_samples_leaf.reshape((1, -1))
0.86 iforest.py(497):     average_path_length = np.zeros(n_samples_leaf.shape)
0.86 iforest.py(499):     mask_1 = n_samples_leaf <= 1
0.86 iforest.py(500):     mask_2 = n_samples_leaf == 2
0.86 iforest.py(501):     not_mask = ~np.logical_or(mask_1, mask_2)
0.86 iforest.py(503):     average_path_length[mask_1] = 0.
0.86 iforest.py(504):     average_path_length[mask_2] = 1.
0.86 iforest.py(506):         2.0 * (np.log(n_samples_leaf[not_mask] - 1.0) + np.euler_gamma)
0.86 iforest.py(507):         - 2.0 * (n_samples_leaf[not_mask] - 1.0) / n_samples_leaf[not_mask]
0.86 iforest.py(510):     return average_path_length.reshape(n_samples_leaf_shape)
0.86 iforest.py(456):         for tree, features in zip(self.estimators_, self.estimators_features_):
0.86 iforest.py(469):         scores = 2 ** (
0.86 iforest.py(470):             -depths
0.86 iforest.py(471):             / (len(self.estimators_)
0.86 iforest.py(472):                * _average_path_length([self.max_samples_]))
0.86 iforest.py(493):     n_samples_leaf = check_array(n_samples_leaf, ensure_2d=False)
0.86 iforest.py(495):     n_samples_leaf_shape = n_samples_leaf.shape
0.86 iforest.py(496):     n_samples_leaf = n_samples_leaf.reshape((1, -1))
0.86 iforest.py(497):     average_path_length = np.zeros(n_samples_leaf.shape)
0.86 iforest.py(499):     mask_1 = n_samples_leaf <= 1
0.86 iforest.py(500):     mask_2 = n_samples_leaf == 2
0.86 iforest.py(501):     not_mask = ~np.logical_or(mask_1, mask_2)
0.86 iforest.py(503):     average_path_length[mask_1] = 0.
0.86 iforest.py(504):     average_path_length[mask_2] = 1.
0.86 iforest.py(506):         2.0 * (np.log(n_samples_leaf[not_mask] - 1.0) + np.euler_gamma)
0.86 iforest.py(507):         - 2.0 * (n_samples_leaf[not_mask] - 1.0) / n_samples_leaf[not_mask]
0.86 iforest.py(510):     return average_path_length.reshape(n_samples_leaf_shape)
0.86 iforest.py(474):         return scores
0.86 iforest.py(436):         for sl in slices:
0.86 iforest.py(440):         return scores
0.86 iforest.py(290):                                              100. * self._contamination)
0.86 iforest.py(292):             return self
=========================== short test summary info ============================
FAILED sklearn/tests/test_coverup_scikit-learn__scikit-learn-13496.py::test_isolation_forest_warm_start_behavior
======================== 1 failed, 4 warnings in 0.82s =========================
+ cat coverage.cover
{"/testbed/sklearn/ensemble/iforest.py": {"6": 1, "7": 1, "8": 1, "9": 1, "11": 1, "12": 1, "18": 1, "19": 1, "20": 1, "22": 1, "24": 1, "26": 1, "29": 2, "477": 1, "176": 1, "195": 1, "198": 1, "205": 1, "308": 1, "331": 1, "363": 1, "401": 1, "410": 1, "442": 1, "177": 1, "178": 1, "179": 1, "180": 1, "181": 1, "183": 1, "184": 1, "185": 1, "186": 1, "187": 1, "188": 1, "189": 1, "190": 1, "192": 1, "193": 1, "196": 0, "203": 2, "225": 2, "226": 2, "229": 2, "230": 2, "232": 0, "234": 2, "235": 2, "239": 2, "241": 2, "242": 2, "245": 0, "247": 2, "248": 2, "251": 2, "253": 2, "254": 2, "255": 2, "257": 0, "259": 0, "261": 0, "262": 0, "263": 0, "266": 0, "267": 0, "269": 0, "271": 0, "272": 0, "273": 0, "274": 0, "276": 2, "277": 2, "278": 2, "279": 2, "280": 2, "282": 2, "284": 2, "285": 0, "288": 2, "289": 2, "290": 2, "292": 2, "295": 0, "298": 0, "299": 0, "303": 0, "304": 0, "306": 0, "324": 0, "325": 0, "326": 0, "327": 0, "328": 0, "329": 0, "361": 2, "387": 2, "390": 2, "391": 2, "392": 0, "395": 0, "399": 2, "403": 0, "404": 0, "406": 0, "407": 0, "408": 0, "412": 2, "414": 2, "415": 2, "417": 0, "430": 2, "431": 2, "432": 2, "434": 2, "436": 4, "438": 2, "440": 2, "452": 2, "454": 2, "456": 32, "457": 30, "459": 30, "460": 30, "461": 30, "463": 30, "466": 30, "469": 2, "470": 2, "471": 2, "472": 2, "474": 2, "493": 32, "495": 32, "496": 32, "497": 32, "499": 32, "500": 32, "501": 32, "503": 32, "504": 32, "506": 32, "507": 32, "510": 32}}
+ git checkout 3aefc834dce72e850bff48689bea3c7dff5f3fad
Note: switching to '3aefc834dce72e850bff48689bea3c7dff5f3fad'.

You are in 'detached HEAD' state. You can look around, make experimental
changes and commit them, and you can discard any commits you make in this
state without impacting any branches by switching back to a branch.

If you want to create a new branch to retain commits you create, you may
do so (now or later) by using -c with the switch command. Example:

  git switch -c <new-branch-name>

Or undo this operation with:

  git switch -

Turn off this advice by setting config variable advice.detachedHead to false

HEAD is now at 3aefc834dc MNT CI Fix for sphinx-gallery 0.3.1 + 404 errors on Debian Jessie packages (#13527)
+ git apply /root/pre_state.patch
error: unrecognized input
