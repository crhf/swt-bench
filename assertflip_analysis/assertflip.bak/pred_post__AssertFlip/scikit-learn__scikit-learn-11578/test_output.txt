+ source /opt/miniconda3/bin/activate
++ _CONDA_ROOT=/opt/miniconda3
++ . /opt/miniconda3/etc/profile.d/conda.sh
+++ export CONDA_EXE=/opt/miniconda3/bin/conda
+++ CONDA_EXE=/opt/miniconda3/bin/conda
+++ export _CE_M=
+++ _CE_M=
+++ export _CE_CONDA=
+++ _CE_CONDA=
+++ export CONDA_PYTHON_EXE=/opt/miniconda3/bin/python
+++ CONDA_PYTHON_EXE=/opt/miniconda3/bin/python
+++ '[' -z '' ']'
+++ export CONDA_SHLVL=0
+++ CONDA_SHLVL=0
+++ '[' -n '' ']'
+++++ dirname /opt/miniconda3/bin/conda
++++ dirname /opt/miniconda3/bin
+++ PATH=/opt/miniconda3/condabin:/opt/miniconda3/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin
+++ export PATH
+++ '[' -z '' ']'
+++ PS1=
++ conda activate
++ local cmd=activate
++ case "$cmd" in
++ __conda_activate activate
++ '[' -n '' ']'
++ local ask_conda
+++ PS1=
+++ __conda_exe shell.posix activate
+++ /opt/miniconda3/bin/conda shell.posix activate
++ ask_conda='PS1='\''(base) '\''
export PATH='\''/opt/miniconda3/bin:/opt/miniconda3/condabin:/opt/miniconda3/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin'\''
export CONDA_PREFIX='\''/opt/miniconda3'\''
export CONDA_SHLVL='\''1'\''
export CONDA_DEFAULT_ENV='\''base'\''
export CONDA_PROMPT_MODIFIER='\''(base) '\''
export CONDA_EXE='\''/opt/miniconda3/bin/conda'\''
export _CE_M='\'''\''
export _CE_CONDA='\'''\''
export CONDA_PYTHON_EXE='\''/opt/miniconda3/bin/python'\'''
++ eval 'PS1='\''(base) '\''
export PATH='\''/opt/miniconda3/bin:/opt/miniconda3/condabin:/opt/miniconda3/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin'\''
export CONDA_PREFIX='\''/opt/miniconda3'\''
export CONDA_SHLVL='\''1'\''
export CONDA_DEFAULT_ENV='\''base'\''
export CONDA_PROMPT_MODIFIER='\''(base) '\''
export CONDA_EXE='\''/opt/miniconda3/bin/conda'\''
export _CE_M='\'''\''
export _CE_CONDA='\'''\''
export CONDA_PYTHON_EXE='\''/opt/miniconda3/bin/python'\'''
+++ PS1='(base) '
+++ export PATH=/opt/miniconda3/bin:/opt/miniconda3/condabin:/opt/miniconda3/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin
+++ PATH=/opt/miniconda3/bin:/opt/miniconda3/condabin:/opt/miniconda3/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin
+++ export CONDA_PREFIX=/opt/miniconda3
+++ CONDA_PREFIX=/opt/miniconda3
+++ export CONDA_SHLVL=1
+++ CONDA_SHLVL=1
+++ export CONDA_DEFAULT_ENV=base
+++ CONDA_DEFAULT_ENV=base
+++ export 'CONDA_PROMPT_MODIFIER=(base) '
+++ CONDA_PROMPT_MODIFIER='(base) '
+++ export CONDA_EXE=/opt/miniconda3/bin/conda
+++ CONDA_EXE=/opt/miniconda3/bin/conda
+++ export _CE_M=
+++ _CE_M=
+++ export _CE_CONDA=
+++ _CE_CONDA=
+++ export CONDA_PYTHON_EXE=/opt/miniconda3/bin/python
+++ CONDA_PYTHON_EXE=/opt/miniconda3/bin/python
++ __conda_hashr
++ '[' -n '' ']'
++ '[' -n '' ']'
++ hash -r
+ conda activate testbed
+ local cmd=activate
+ case "$cmd" in
+ __conda_activate activate testbed
+ '[' -n '' ']'
+ local ask_conda
++ PS1='(base) '
++ __conda_exe shell.posix activate testbed
++ /opt/miniconda3/bin/conda shell.posix activate testbed
+ ask_conda='PS1='\''(testbed) '\''
export PATH='\''/opt/miniconda3/envs/testbed/bin:/opt/miniconda3/condabin:/opt/miniconda3/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin'\''
export CONDA_PREFIX='\''/opt/miniconda3/envs/testbed'\''
export CONDA_SHLVL='\''2'\''
export CONDA_DEFAULT_ENV='\''testbed'\''
export CONDA_PROMPT_MODIFIER='\''(testbed) '\''
export CONDA_PREFIX_1='\''/opt/miniconda3'\''
export CONDA_EXE='\''/opt/miniconda3/bin/conda'\''
export _CE_M='\'''\''
export _CE_CONDA='\'''\''
export CONDA_PYTHON_EXE='\''/opt/miniconda3/bin/python'\'''
+ eval 'PS1='\''(testbed) '\''
export PATH='\''/opt/miniconda3/envs/testbed/bin:/opt/miniconda3/condabin:/opt/miniconda3/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin'\''
export CONDA_PREFIX='\''/opt/miniconda3/envs/testbed'\''
export CONDA_SHLVL='\''2'\''
export CONDA_DEFAULT_ENV='\''testbed'\''
export CONDA_PROMPT_MODIFIER='\''(testbed) '\''
export CONDA_PREFIX_1='\''/opt/miniconda3'\''
export CONDA_EXE='\''/opt/miniconda3/bin/conda'\''
export _CE_M='\'''\''
export _CE_CONDA='\'''\''
export CONDA_PYTHON_EXE='\''/opt/miniconda3/bin/python'\'''
++ PS1='(testbed) '
++ export PATH=/opt/miniconda3/envs/testbed/bin:/opt/miniconda3/condabin:/opt/miniconda3/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin
++ PATH=/opt/miniconda3/envs/testbed/bin:/opt/miniconda3/condabin:/opt/miniconda3/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin
++ export CONDA_PREFIX=/opt/miniconda3/envs/testbed
++ CONDA_PREFIX=/opt/miniconda3/envs/testbed
++ export CONDA_SHLVL=2
++ CONDA_SHLVL=2
++ export CONDA_DEFAULT_ENV=testbed
++ CONDA_DEFAULT_ENV=testbed
++ export 'CONDA_PROMPT_MODIFIER=(testbed) '
++ CONDA_PROMPT_MODIFIER='(testbed) '
++ export CONDA_PREFIX_1=/opt/miniconda3
++ CONDA_PREFIX_1=/opt/miniconda3
++ export CONDA_EXE=/opt/miniconda3/bin/conda
++ CONDA_EXE=/opt/miniconda3/bin/conda
++ export _CE_M=
++ _CE_M=
++ export _CE_CONDA=
++ _CE_CONDA=
++ export CONDA_PYTHON_EXE=/opt/miniconda3/bin/python
++ CONDA_PYTHON_EXE=/opt/miniconda3/bin/python
+ __conda_hashr
+ '[' -n '' ']'
+ '[' -n '' ']'
+ hash -r
+ cd /testbed
+ git diff HEAD dd69361a0d9c6ccde0d2353b00b86e0e7541a3e3
+ git config --global --add safe.directory /testbed
+ cd /testbed
+ git status
On branch main
nothing to commit, working tree clean
+ git show
commit dd69361a0d9c6ccde0d2353b00b86e0e7541a3e3
Author: Hanmin Qin <qinhanmin2005@sina.com>
Date:   Tue Jul 17 06:21:17 2018 +0800

    [MRG+2] ENH&BUG Add pos_label parameter and fix a bug in average_precision_score (#9980)
    
    <!--
    Thanks for contributing a pull request! Please ensure you have taken a look at
    the contribution guidelines: https://github.com/scikit-learn/scikit-learn/blob/master/CONTRIBUTING.md#pull-request-checklist
    -->
    
    #### Reference Issues/PRs
    <!--
    Example: Fixes #1234. See also #3456.
    Please use keywords (e.g., Fixes) to create link to the issues or pull requests
    you resolved, so that they will automatically be closed when your pull request
    is merged. See https://github.com/blog/1506-closing-issues-via-pull-requests
    -->
    part of #9829
    
    #### What does this implement/fix? Explain your changes.
    (1)add pos_label parameter to average_precision_score (Although we finally decide not to introduce pos_label in roc_auc_score, I think we might need pos_label here. Because there are no relationship between the results if we reverse the true labels, also, precision/recall all support pos_label)
    (2)fix a bug where average_precision_score will sometimes return nan when sample_weight contains 0
    ```python
    y_true = np.array([0, 0, 0, 1, 1, 1])
    y_score = np.array([0.1, 0.4, 0.85, 0.35, 0.8, 0.9])
    average_precision_score(y_true, y_score, sample_weight=[1, 1, 0, 1, 1, 0])
    # output:nan
    ```
    I do it here because of (3)
    (3)move average_precision scores out of METRIC_UNDEFINED_BINARY (this should contain the regression test for (1) and (2))
    
    Some comments:
    (1)For the underlying method(precision_recall_curve), the default value of pos_label is None, but I choose to set the default value of pos_label to 1 because this is what P/R/F is doing. What's more, the meaning of pos_label=None is not clear even in scikit-learn itself (see #10010)
    (2)I slightly modified the common test. Currently, the part I modified is only designed for brier_score_loss(I'm doing the same thing in #9562) . I think it is right because as a common test, it seems not good to force metrics to accept str y_true without pos_label.
    
    #### Any other comments?
    cc @jnothman Could you please take some time to review or at least judge whether this is the right way to go? Thanks a lot :)
    
    <!--
    Please be aware that we are a loose team of volunteers so patience is
    necessary; assistance handling other issues is very welcome. We value
    all user contributions, no matter how minor they are. If we are slow to
    review, either the pull request needs some benchmarking, tinkering,
    convincing, etc. or more likely the reviewers are simply busy. In either
    case, we ask for your understanding during the review process.
    For more information, see our FAQ on this topic:
    http://scikit-learn.org/dev/faq.html#why-is-my-pull-request-not-getting-any-attention.
    
    Thanks for contributing!
    -->

diff --git a/doc/whats_new/v0.20.rst b/doc/whats_new/v0.20.rst
index f9b76dc355..28ed5f821e 100644
--- a/doc/whats_new/v0.20.rst
+++ b/doc/whats_new/v0.20.rst
@@ -392,6 +392,10 @@ Metrics
   faster. This avoids some reported freezes and MemoryErrors.
   :issue:`11135` by `Joel Nothman`_.
 
+- :func:`metrics.average_precision_score` now supports binary ``y_true``
+  other than ``{0, 1}`` or ``{-1, 1}`` through ``pos_label`` parameter.
+  :issue:`9980` by :user:`Hanmin Qin <qinhanmin2014>`.
+
 Linear, kernelized and related models
 
 - Deprecate ``random_state`` parameter in :class:`svm.OneClassSVM` as the
@@ -628,6 +632,10 @@ Metrics
   :func:`metrics.mutual_info_score`.
   :issue:`9772` by :user:`Kumar Ashutosh <thechargedneutron>`.
 
+- Fixed a bug where :func:`metrics.average_precision_score` will sometimes return
+  ``nan`` when ``sample_weight`` contains 0.
+  :issue:`9980` by :user:`Hanmin Qin <qinhanmin2014>`.
+
 - Fixed a bug in :func:`metrics.fowlkes_mallows_score` to avoid integer
   overflow. Casted return value of `contingency_matrix` to `int64` and computed
   product of square roots rather than square root of product.
diff --git a/sklearn/metrics/ranking.py b/sklearn/metrics/ranking.py
index 5039c5f874..fd6e28a20a 100644
--- a/sklearn/metrics/ranking.py
+++ b/sklearn/metrics/ranking.py
@@ -20,6 +20,8 @@ the lower the better
 from __future__ import division
 
 import warnings
+from functools import partial
+
 import numpy as np
 from scipy.sparse import csr_matrix
 from scipy.stats import rankdata
@@ -125,7 +127,7 @@ def auc(x, y, reorder='deprecated'):
     return area
 
 
-def average_precision_score(y_true, y_score, average="macro",
+def average_precision_score(y_true, y_score, average="macro", pos_label=1,
                             sample_weight=None):
     """Compute average precision (AP) from prediction scores
 
@@ -150,7 +152,7 @@ def average_precision_score(y_true, y_score, average="macro",
     Parameters
     ----------
     y_true : array, shape = [n_samples] or [n_samples, n_classes]
-        True binary labels (either {0, 1} or {-1, 1}).
+        True binary labels or binary label indicators.
 
     y_score : array, shape = [n_samples] or [n_samples, n_classes]
         Target scores, can either be probability estimates of the positive
@@ -173,6 +175,10 @@ def average_precision_score(y_true, y_score, average="macro",
         ``'samples'``:
             Calculate metrics for each instance, and find their average.
 
+    pos_label : int or str (default=1)
+        The label of the positive class. Only applied to binary ``y_true``.
+        For multilabel-indicator ``y_true``, ``pos_label`` is fixed to 1.
+
     sample_weight : array-like of shape = [n_samples], optional
         Sample weights.
 
@@ -209,17 +215,23 @@ def average_precision_score(y_true, y_score, average="macro",
       are weighted by the change in recall since the last operating point.
     """
     def _binary_uninterpolated_average_precision(
-            y_true, y_score, sample_weight=None):
+            y_true, y_score, pos_label=1, sample_weight=None):
         precision, recall, _ = precision_recall_curve(
-            y_true, y_score, sample_weight=sample_weight)
+            y_true, y_score, pos_label=pos_label, sample_weight=sample_weight)
         # Return the step function integral
         # The following works because the last entry of precision is
         # guaranteed to be 1, as returned by precision_recall_curve
         return -np.sum(np.diff(recall) * np.array(precision)[:-1])
 
-    return _average_binary_score(_binary_uninterpolated_average_precision,
-                                 y_true, y_score, average,
-                                 sample_weight=sample_weight)
+    y_type = type_of_target(y_true)
+    if y_type == "multilabel-indicator" and pos_label != 1:
+        raise ValueError("Parameter pos_label is fixed to 1 for "
+                         "multilabel-indicator y_true. Do not set "
+                         "pos_label or set pos_label to 1.")
+    average_precision = partial(_binary_uninterpolated_average_precision,
+                                pos_label=pos_label)
+    return _average_binary_score(average_precision, y_true, y_score,
+                                 average, sample_weight=sample_weight)
 
 
 def roc_auc_score(y_true, y_score, average="macro", sample_weight=None,
@@ -501,6 +513,7 @@ def precision_recall_curve(y_true, probas_pred, pos_label=None,
                                              sample_weight=sample_weight)
 
     precision = tps / (tps + fps)
+    precision[np.isnan(precision)] = 0
     recall = tps / tps[-1]
 
     # stop when full recall attained
diff --git a/sklearn/metrics/tests/test_common.py b/sklearn/metrics/tests/test_common.py
index b858868a74..51ebd00f00 100644
--- a/sklearn/metrics/tests/test_common.py
+++ b/sklearn/metrics/tests/test_common.py
@@ -241,13 +241,6 @@ METRIC_UNDEFINED_BINARY = {
     "samples_precision_score",
     "samples_recall_score",
     "coverage_error",
-
-    "average_precision_score",
-    "weighted_average_precision_score",
-    "micro_average_precision_score",
-    "macro_average_precision_score",
-    "samples_average_precision_score",
-
     "label_ranking_loss",
     "label_ranking_average_precision_score",
 }
@@ -264,6 +257,12 @@ METRIC_UNDEFINED_MULTICLASS = {
     "samples_roc_auc",
     "partial_roc_auc",
 
+    "average_precision_score",
+    "weighted_average_precision_score",
+    "micro_average_precision_score",
+    "macro_average_precision_score",
+    "samples_average_precision_score",
+
     # with default average='binary', multiclass is prohibited
     "precision_score",
     "recall_score",
@@ -299,6 +298,12 @@ METRICS_WITH_POS_LABEL = {
 
     "precision_score", "recall_score", "f1_score", "f2_score", "f0.5_score",
 
+    "average_precision_score",
+    "weighted_average_precision_score",
+    "micro_average_precision_score",
+    "macro_average_precision_score",
+    "samples_average_precision_score",
+
     # pos_label support deprecated; to be removed in 0.18:
     "weighted_f0.5_score", "weighted_f1_score", "weighted_f2_score",
     "weighted_precision_score", "weighted_recall_score",
@@ -667,7 +672,7 @@ def test_thresholded_invariance_string_vs_numbers_labels(name):
                                err_msg="{0} failed string vs number "
                                        "invariance test".format(name))
 
-            measure_with_strobj = metric(y1_str.astype('O'), y2)
+            measure_with_strobj = metric_str(y1_str.astype('O'), y2)
             assert_array_equal(measure_with_number, measure_with_strobj,
                                err_msg="{0} failed string object vs number "
                                        "invariance test".format(name))
diff --git a/sklearn/metrics/tests/test_ranking.py b/sklearn/metrics/tests/test_ranking.py
index 5e9a8a0c84..d7915eab60 100644
--- a/sklearn/metrics/tests/test_ranking.py
+++ b/sklearn/metrics/tests/test_ranking.py
@@ -681,6 +681,18 @@ def test_average_precision_constant_values():
     assert_equal(average_precision_score(y_true, y_score), .25)
 
 
+def test_average_precision_score_pos_label_multilabel_indicator():
+    # Raise an error for multilabel-indicator y_true with
+    # pos_label other than 1
+    y_true = np.array([[1, 0], [0, 1], [0, 1], [1, 0]])
+    y_pred = np.array([[0.9, 0.1], [0.1, 0.9], [0.8, 0.2], [0.2, 0.8]])
+    erorr_message = ("Parameter pos_label is fixed to 1 for multilabel"
+                     "-indicator y_true. Do not set pos_label or set "
+                     "pos_label to 1.")
+    assert_raise_message(ValueError, erorr_message, average_precision_score,
+                         y_true, y_pred, pos_label=0)
+
+
 def test_score_scale_invariance():
     # Test that average_precision_score and roc_auc_score are invariant by
     # the scaling or shifting of probabilities
+ git diff dd69361a0d9c6ccde0d2353b00b86e0e7541a3e3
+ source /opt/miniconda3/bin/activate
++ _CONDA_ROOT=/opt/miniconda3
++ . /opt/miniconda3/etc/profile.d/conda.sh
+++ export CONDA_EXE=/opt/miniconda3/bin/conda
+++ CONDA_EXE=/opt/miniconda3/bin/conda
+++ export _CE_M=
+++ _CE_M=
+++ export _CE_CONDA=
+++ _CE_CONDA=
+++ export CONDA_PYTHON_EXE=/opt/miniconda3/bin/python
+++ CONDA_PYTHON_EXE=/opt/miniconda3/bin/python
+++ '[' -z x ']'
++ conda activate
++ local cmd=activate
++ case "$cmd" in
++ __conda_activate activate
++ '[' -n '' ']'
++ local ask_conda
+++ PS1='(testbed) '
+++ __conda_exe shell.posix activate
+++ /opt/miniconda3/bin/conda shell.posix activate
++ ask_conda='PS1='\''(base) '\''
export PATH='\''/opt/miniconda3/bin:/opt/miniconda3/condabin:/opt/miniconda3/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin'\''
export CONDA_PREFIX='\''/opt/miniconda3'\''
export CONDA_SHLVL='\''3'\''
export CONDA_DEFAULT_ENV='\''base'\''
export CONDA_PROMPT_MODIFIER='\''(base) '\''
export CONDA_PREFIX_2='\''/opt/miniconda3/envs/testbed'\''
export CONDA_EXE='\''/opt/miniconda3/bin/conda'\''
export _CE_M='\'''\''
export _CE_CONDA='\'''\''
export CONDA_PYTHON_EXE='\''/opt/miniconda3/bin/python'\'''
++ eval 'PS1='\''(base) '\''
export PATH='\''/opt/miniconda3/bin:/opt/miniconda3/condabin:/opt/miniconda3/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin'\''
export CONDA_PREFIX='\''/opt/miniconda3'\''
export CONDA_SHLVL='\''3'\''
export CONDA_DEFAULT_ENV='\''base'\''
export CONDA_PROMPT_MODIFIER='\''(base) '\''
export CONDA_PREFIX_2='\''/opt/miniconda3/envs/testbed'\''
export CONDA_EXE='\''/opt/miniconda3/bin/conda'\''
export _CE_M='\'''\''
export _CE_CONDA='\'''\''
export CONDA_PYTHON_EXE='\''/opt/miniconda3/bin/python'\'''
+++ PS1='(base) '
+++ export PATH=/opt/miniconda3/bin:/opt/miniconda3/condabin:/opt/miniconda3/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin
+++ PATH=/opt/miniconda3/bin:/opt/miniconda3/condabin:/opt/miniconda3/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin
+++ export CONDA_PREFIX=/opt/miniconda3
+++ CONDA_PREFIX=/opt/miniconda3
+++ export CONDA_SHLVL=3
+++ CONDA_SHLVL=3
+++ export CONDA_DEFAULT_ENV=base
+++ CONDA_DEFAULT_ENV=base
+++ export 'CONDA_PROMPT_MODIFIER=(base) '
+++ CONDA_PROMPT_MODIFIER='(base) '
+++ export CONDA_PREFIX_2=/opt/miniconda3/envs/testbed
+++ CONDA_PREFIX_2=/opt/miniconda3/envs/testbed
+++ export CONDA_EXE=/opt/miniconda3/bin/conda
+++ CONDA_EXE=/opt/miniconda3/bin/conda
+++ export _CE_M=
+++ _CE_M=
+++ export _CE_CONDA=
+++ _CE_CONDA=
+++ export CONDA_PYTHON_EXE=/opt/miniconda3/bin/python
+++ CONDA_PYTHON_EXE=/opt/miniconda3/bin/python
++ __conda_hashr
++ '[' -n '' ']'
++ '[' -n '' ']'
++ hash -r
+ conda activate testbed
+ local cmd=activate
+ case "$cmd" in
+ __conda_activate activate testbed
+ '[' -n '' ']'
+ local ask_conda
++ PS1='(base) '
++ __conda_exe shell.posix activate testbed
++ /opt/miniconda3/bin/conda shell.posix activate testbed
+ ask_conda='PS1='\''(testbed) '\''
export PATH='\''/opt/miniconda3/envs/testbed/bin:/opt/miniconda3/condabin:/opt/miniconda3/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin'\''
export CONDA_PREFIX='\''/opt/miniconda3/envs/testbed'\''
export CONDA_SHLVL='\''4'\''
export CONDA_DEFAULT_ENV='\''testbed'\''
export CONDA_PROMPT_MODIFIER='\''(testbed) '\''
export CONDA_PREFIX_3='\''/opt/miniconda3'\''
export CONDA_EXE='\''/opt/miniconda3/bin/conda'\''
export _CE_M='\'''\''
export _CE_CONDA='\'''\''
export CONDA_PYTHON_EXE='\''/opt/miniconda3/bin/python'\'''
+ eval 'PS1='\''(testbed) '\''
export PATH='\''/opt/miniconda3/envs/testbed/bin:/opt/miniconda3/condabin:/opt/miniconda3/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin'\''
export CONDA_PREFIX='\''/opt/miniconda3/envs/testbed'\''
export CONDA_SHLVL='\''4'\''
export CONDA_DEFAULT_ENV='\''testbed'\''
export CONDA_PROMPT_MODIFIER='\''(testbed) '\''
export CONDA_PREFIX_3='\''/opt/miniconda3'\''
export CONDA_EXE='\''/opt/miniconda3/bin/conda'\''
export _CE_M='\'''\''
export _CE_CONDA='\'''\''
export CONDA_PYTHON_EXE='\''/opt/miniconda3/bin/python'\'''
++ PS1='(testbed) '
++ export PATH=/opt/miniconda3/envs/testbed/bin:/opt/miniconda3/condabin:/opt/miniconda3/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin
++ PATH=/opt/miniconda3/envs/testbed/bin:/opt/miniconda3/condabin:/opt/miniconda3/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin
++ export CONDA_PREFIX=/opt/miniconda3/envs/testbed
++ CONDA_PREFIX=/opt/miniconda3/envs/testbed
++ export CONDA_SHLVL=4
++ CONDA_SHLVL=4
++ export CONDA_DEFAULT_ENV=testbed
++ CONDA_DEFAULT_ENV=testbed
++ export 'CONDA_PROMPT_MODIFIER=(testbed) '
++ CONDA_PROMPT_MODIFIER='(testbed) '
++ export CONDA_PREFIX_3=/opt/miniconda3
++ CONDA_PREFIX_3=/opt/miniconda3
++ export CONDA_EXE=/opt/miniconda3/bin/conda
++ CONDA_EXE=/opt/miniconda3/bin/conda
++ export _CE_M=
++ _CE_M=
++ export _CE_CONDA=
++ _CE_CONDA=
++ export CONDA_PYTHON_EXE=/opt/miniconda3/bin/python
++ CONDA_PYTHON_EXE=/opt/miniconda3/bin/python
+ __conda_hashr
+ '[' -n '' ']'
+ '[' -n '' ']'
+ hash -r
+ python -m pip install -v --no-use-pep517 --no-build-isolation -e .
Using pip 21.2.2 from /opt/miniconda3/envs/testbed/lib/python3.6/site-packages/pip (python 3.6)
Obtaining file:///testbed
    Running command python setup.py egg_info
    running egg_info
    creating /tmp/pip-pip-egg-info-8naagwc8/scikit_learn.egg-info
    writing /tmp/pip-pip-egg-info-8naagwc8/scikit_learn.egg-info/PKG-INFO
    writing dependency_links to /tmp/pip-pip-egg-info-8naagwc8/scikit_learn.egg-info/dependency_links.txt
    writing requirements to /tmp/pip-pip-egg-info-8naagwc8/scikit_learn.egg-info/requires.txt
    writing top-level names to /tmp/pip-pip-egg-info-8naagwc8/scikit_learn.egg-info/top_level.txt
    writing manifest file '/tmp/pip-pip-egg-info-8naagwc8/scikit_learn.egg-info/SOURCES.txt'
    reading manifest file '/tmp/pip-pip-egg-info-8naagwc8/scikit_learn.egg-info/SOURCES.txt'
    reading manifest template 'MANIFEST.in'
    adding license file 'COPYING'
    adding license file 'AUTHORS.rst'
    writing manifest file '/tmp/pip-pip-egg-info-8naagwc8/scikit_learn.egg-info/SOURCES.txt'
    Partial import of sklearn during the build process.
Requirement already satisfied: numpy>=1.8.2 in /opt/miniconda3/envs/testbed/lib/python3.6/site-packages (from scikit-learn==0.20.dev0) (1.19.2)
Requirement already satisfied: scipy>=0.13.3 in /opt/miniconda3/envs/testbed/lib/python3.6/site-packages (from scikit-learn==0.20.dev0) (1.5.2)
Installing collected packages: scikit-learn
  Attempting uninstall: scikit-learn
    Found existing installation: scikit-learn 0.20.dev0
    Uninstalling scikit-learn-0.20.dev0:
      Removing file or directory /opt/miniconda3/envs/testbed/lib/python3.6/site-packages/scikit-learn.egg-link
      Removing pth entries from /opt/miniconda3/envs/testbed/lib/python3.6/site-packages/easy-install.pth:
      Removing entry: /testbed
      Successfully uninstalled scikit-learn-0.20.dev0
  Running setup.py develop for scikit-learn
    Running command /opt/miniconda3/envs/testbed/bin/python -c 'import io, os, sys, setuptools, tokenize; sys.argv[0] = '"'"'/testbed/setup.py'"'"'; __file__='"'"'/testbed/setup.py'"'"';f = getattr(tokenize, '"'"'open'"'"', open)(__file__) if os.path.exists(__file__) else io.StringIO('"'"'from setuptools import setup; setup()'"'"');code = f.read().replace('"'"'\r\n'"'"', '"'"'\n'"'"');f.close();exec(compile(code, __file__, '"'"'exec'"'"'))' develop --no-deps
    blas_opt_info:
    blas_mkl_info:
    customize UnixCCompiler
      libraries mkl_rt not found in ['/opt/miniconda3/envs/testbed/lib', '/usr/local/lib', '/usr/lib64', '/usr/lib', '/usr/lib/x86_64-linux-gnu']
      NOT AVAILABLE

    blis_info:
      libraries blis not found in ['/opt/miniconda3/envs/testbed/lib', '/usr/local/lib', '/usr/lib64', '/usr/lib', '/usr/lib/x86_64-linux-gnu']
      NOT AVAILABLE

    openblas_info:
    C compiler: gcc -pthread -B /opt/miniconda3/envs/testbed/compiler_compat -Wl,--sysroot=/ -Wsign-compare -DNDEBUG -g -fwrapv -O3 -Wall -Wstrict-prototypes -fPIC

    creating /tmp/tmpv4nv21sp/tmp
    creating /tmp/tmpv4nv21sp/tmp/tmpv4nv21sp
    compile options: '-c'
    gcc: /tmp/tmpv4nv21sp/source.c
    gcc -pthread -B /opt/miniconda3/envs/testbed/compiler_compat -Wl,--sysroot=/ /tmp/tmpv4nv21sp/tmp/tmpv4nv21sp/source.o -L/opt/miniconda3/envs/testbed/lib -lopenblas -o /tmp/tmpv4nv21sp/a.out
      FOUND:
        libraries = ['openblas', 'openblas']
        library_dirs = ['/opt/miniconda3/envs/testbed/lib']
        language = c
        define_macros = [('HAVE_CBLAS', None)]

      FOUND:
        libraries = ['openblas', 'openblas']
        library_dirs = ['/opt/miniconda3/envs/testbed/lib']
        language = c
        define_macros = [('HAVE_CBLAS', None)]

    running develop
    running build_scripts
    running egg_info
    running build_src
    build_src
    building library "libsvm-skl" sources
    building extension "sklearn.__check_build._check_build" sources
    building extension "sklearn.cluster._dbscan_inner" sources
    building extension "sklearn.cluster._optics_inner" sources
    building extension "sklearn.cluster._hierarchical" sources
    building extension "sklearn.cluster._k_means_elkan" sources
    building extension "sklearn.cluster._k_means" sources
    building extension "sklearn.datasets._svmlight_format" sources
    building extension "sklearn.decomposition._online_lda" sources
    building extension "sklearn.decomposition.cdnmf_fast" sources
    building extension "sklearn.ensemble._gradient_boosting" sources
    building extension "sklearn.feature_extraction._hashing" sources
    building extension "sklearn.manifold._utils" sources
    building extension "sklearn.manifold._barnes_hut_tsne" sources
    building extension "sklearn.metrics.cluster.expected_mutual_info_fast" sources
    building extension "sklearn.metrics.pairwise_fast" sources
    building extension "sklearn.neighbors.ball_tree" sources
    building extension "sklearn.neighbors.kd_tree" sources
    building extension "sklearn.neighbors.dist_metrics" sources
    building extension "sklearn.neighbors.typedefs" sources
    building extension "sklearn.neighbors.quad_tree" sources
    building extension "sklearn.tree._tree" sources
    building extension "sklearn.tree._splitter" sources
    building extension "sklearn.tree._criterion" sources
    building extension "sklearn.tree._utils" sources
    building extension "sklearn.svm.libsvm" sources
    building extension "sklearn.svm.liblinear" sources
    building extension "sklearn.svm.libsvm_sparse" sources
    building extension "sklearn._isotonic" sources
    building extension "sklearn.linear_model.cd_fast" sources
    building extension "sklearn.linear_model.sgd_fast" sources
    building extension "sklearn.linear_model.sag_fast" sources
    building extension "sklearn.utils.sparsefuncs_fast" sources
    building extension "sklearn.utils.arrayfuncs" sources
    building extension "sklearn.utils.murmurhash" sources
    building extension "sklearn.utils.lgamma" sources
    building extension "sklearn.utils.graph_shortest_path" sources
    building extension "sklearn.utils.fast_dict" sources
    building extension "sklearn.utils.seq_dataset" sources
    building extension "sklearn.utils.weight_vector" sources
    building extension "sklearn.utils._random" sources
    building extension "sklearn.utils._logistic_sigmoid" sources
    building data_files sources
    build_src: building npy-pkg config files
    writing scikit_learn.egg-info/PKG-INFO
    writing dependency_links to scikit_learn.egg-info/dependency_links.txt
    writing requirements to scikit_learn.egg-info/requires.txt
    writing top-level names to scikit_learn.egg-info/top_level.txt
    reading manifest file 'scikit_learn.egg-info/SOURCES.txt'
    reading manifest template 'MANIFEST.in'
    adding license file 'COPYING'
    adding license file 'AUTHORS.rst'
    writing manifest file 'scikit_learn.egg-info/SOURCES.txt'
    running build_ext
    customize UnixCCompiler
    customize UnixCCompiler using build_clib
    customize UnixCCompiler
    customize UnixCCompiler using build_ext
    resetting extension 'sklearn.svm.liblinear' language from 'c' to 'c++'.
    customize UnixCCompiler
    customize UnixCCompiler using build_ext
    Creating /opt/miniconda3/envs/testbed/lib/python3.6/site-packages/scikit-learn.egg-link (link to .)
    Adding scikit-learn 0.20.dev0 to easy-install.pth file

    Installed /testbed
    Partial import of sklearn during the build process.
Successfully installed scikit-learn-0.20.dev0
WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv
+ git apply -v -
Checking patch sklearn/linear_model/logistic.py...
Applied patch sklearn/linear_model/logistic.py cleanly.
+ git apply -v -
Checking patch sklearn/tests/test_coverup_scikit-learn__scikit-learn-11578.py...
Applied patch sklearn/tests/test_coverup_scikit-learn__scikit-learn-11578.py cleanly.
+ python3 /root/trace.py --timing --trace --count -C coverage.cover --include-pattern '/testbed/(sklearn/linear_model/logistic\.py)' -m pytest --no-header -rA -p no:cacheprovider sklearn/tests/test_coverup_scikit-learn__scikit-learn-11578.py
['--timing', '--trace', '--count', '-C', 'coverage.cover', '--include-pattern', '/testbed/(sklearn/linear_model/logistic\\.py)']
============================= test session starts ==============================
collected 1 item

sklearn/tests/test_coverup_scikit-learn__scikit-learn-11578.py F         [100%]

=================================== FAILURES ===================================
__________________ test_log_reg_scoring_path_multinomial_bug ___________________

    def test_log_reg_scoring_path_multinomial_bug():
        # Setup synthetic data
        np.random.seed(1234)
        samples = 200
        features = 5
        folds = 10
    
        X = np.random.random(size=(samples, features))
        y = np.random.choice(['a', 'b', 'c'], size=samples)
    
        test_indices = np.random.choice(range(samples), size=int(samples / folds), replace=False)
        train_indices = [idx for idx in range(samples) if idx not in test_indices]
    
        # Binarize the labels for y[test]
        lb = LabelBinarizer()
        lb.fit(y[test_indices])
        y_bin = lb.transform(y[test_indices])
    
        # Use LogisticRegressionCV to get expected multinomial scores
        log_reg_cv = LogisticRegressionCV(multi_class='multinomial', solver='lbfgs', cv=folds)
        log_reg_cv.fit(X[train_indices], y[train_indices])
        expected_probs = log_reg_cv.predict_proba(X[test_indices])
    
        # Call _log_reg_scoring_path with multi_class='multinomial'
        coefs, _, scores, _ = _log_reg_scoring_path(X, y, train_indices, test_indices, fit_intercept=True, scoring='neg_log_loss', multi_class='multinomial')
    
        # Initialize a LogisticRegression instance with the same coefficients
        log_reg = LogisticRegression(fit_intercept=True, multi_class='multinomial')
        log_reg.coef_ = coefs[0][:, :-1]
        log_reg.intercept_ = coefs[0][:, -1]
    
        # Get probabilities using predict_proba
        actual_probs = log_reg.predict_proba(X[test_indices])
    
        # Assert that the probabilities match the expected multinomial probabilities
>       assert np.allclose(actual_probs, expected_probs), "Probabilities should match when the bug is fixed"
E       AssertionError: Probabilities should match when the bug is fixed
E       assert False
E        +  where False = <function allclose at 0x7f0b249fdb70>(array([[0.29442022, 0.3555607 , 0.35001908],\n       [0.29446448, 0.35554677, 0.34998875],\n       [0.29446811, 0.355538...4 , 0.35558579, 0.35004881],\n       [0.29443708, 0.35558854, 0.34997438],\n       [0.29443579, 0.35556763, 0.34999659]]), array([[0.29441389, 0.35556492, 0.35002119],\n       [0.29445816, 0.35555099, 0.34999085],\n       [0.29446177, 0.355543...07, 0.35559002, 0.35005092],\n       [0.29443075, 0.35559276, 0.34997649],\n       [0.29442944, 0.35557187, 0.34999869]]))
E        +    where <function allclose at 0x7f0b249fdb70> = np.allclose

sklearn/tests/test_coverup_scikit-learn__scikit-learn-11578.py:43: AssertionError
----------------------------- Captured stdout call -----------------------------
0.71 logistic.py(1577):         self.Cs = Cs
0.71 logistic.py(1578):         self.fit_intercept = fit_intercept
0.71 logistic.py(1579):         self.cv = cv
0.71 logistic.py(1580):         self.dual = dual
0.71 logistic.py(1581):         self.penalty = penalty
0.71 logistic.py(1582):         self.scoring = scoring
0.71 logistic.py(1583):         self.tol = tol
0.71 logistic.py(1584):         self.max_iter = max_iter
0.71 logistic.py(1585):         self.class_weight = class_weight
0.71 logistic.py(1586):         self.n_jobs = n_jobs
0.71 logistic.py(1587):         self.verbose = verbose
0.71 logistic.py(1588):         self.solver = solver
0.71 logistic.py(1589):         self.refit = refit
0.71 logistic.py(1590):         self.intercept_scaling = intercept_scaling
0.71 logistic.py(1591):         self.multi_class = multi_class
0.71 logistic.py(1592):         self.random_state = random_state
0.71 logistic.py(1614):         _check_solver_option(self.solver, self.multi_class, self.penalty,
0.71 logistic.py(1615):                              self.dual)
0.71 logistic.py(428):     if solver not in ['liblinear', 'newton-cg', 'lbfgs', 'sag', 'saga']:
0.71 logistic.py(433):     if multi_class not in ['multinomial', 'ovr']:
0.71 logistic.py(437):     if multi_class == 'multinomial' and solver == 'liblinear':
0.71 logistic.py(441):     if solver not in ['liblinear', 'saga']:
0.71 logistic.py(442):         if penalty != 'l2':
0.71 logistic.py(445):     if solver != 'liblinear':
0.71 logistic.py(446):         if dual:
0.71 logistic.py(1617):         if not isinstance(self.max_iter, numbers.Number) or self.max_iter < 0:
0.71 logistic.py(1620):         if not isinstance(self.tol, numbers.Number) or self.tol < 0:
0.71 logistic.py(1624):         X, y = check_X_y(X, y, accept_sparse='csr', dtype=np.float64,
0.71 logistic.py(1625):                          order="C",
0.71 logistic.py(1626):                          accept_large_sparse=self.solver != 'liblinear')
0.71 logistic.py(1627):         check_classification_targets(y)
0.71 logistic.py(1629):         class_weight = self.class_weight
0.71 logistic.py(1632):         label_encoder = LabelEncoder().fit(y)
0.71 logistic.py(1633):         y = label_encoder.transform(y)
0.71 logistic.py(1634):         if isinstance(class_weight, dict):
0.71 logistic.py(1639):         classes = self.classes_ = label_encoder.classes_
0.71 logistic.py(1640):         encoded_labels = label_encoder.transform(label_encoder.classes_)
0.71 logistic.py(1642):         if self.solver in ['sag', 'saga']:
0.71 logistic.py(1645):             max_squared_sum = None
0.71 logistic.py(1648):         cv = check_cv(self.cv, y, classifier=True)
0.71 logistic.py(1649):         folds = list(cv.split(X, y))
0.71 logistic.py(1652):         n_classes = len(encoded_labels)
0.71 logistic.py(1654):         if n_classes < 2:
0.71 logistic.py(1659):         if n_classes == 2:
0.71 logistic.py(1668):         if self.multi_class == 'multinomial':
0.71 logistic.py(1669):             iter_encoded_labels = iter_classes = [None]
0.71 logistic.py(1675):         if class_weight == "balanced":
0.71 logistic.py(1681):         path_func = delayed(_log_reg_scoring_path)
0.71 logistic.py(1685):         if self.solver in ['sag', 'saga']:
0.71 logistic.py(1688):             backend = 'multiprocessing'
0.71 logistic.py(1689):         fold_coefs_ = Parallel(n_jobs=self.n_jobs, verbose=self.verbose,
0.71 logistic.py(1690):                                backend=backend)(
0.71 logistic.py(1691):             path_func(X, y, train, test, pos_class=label, Cs=self.Cs,
0.71 logistic.py(1702):             for label in iter_encoded_labels
0.71 logistic.py(1691):             path_func(X, y, train, test, pos_class=label, Cs=self.Cs,
0.71 logistic.py(1702):             for label in iter_encoded_labels
0.71 logistic.py(1703):             for train, test in folds)
0.71 logistic.py(903):     _check_solver_option(solver, multi_class, penalty, dual)
0.71 logistic.py(428):     if solver not in ['liblinear', 'newton-cg', 'lbfgs', 'sag', 'saga']:
0.71 logistic.py(433):     if multi_class not in ['multinomial', 'ovr']:
0.71 logistic.py(437):     if multi_class == 'multinomial' and solver == 'liblinear':
0.71 logistic.py(441):     if solver not in ['liblinear', 'saga']:
0.71 logistic.py(442):         if penalty != 'l2':
0.71 logistic.py(445):     if solver != 'liblinear':
0.71 logistic.py(446):         if dual:
0.71 logistic.py(905):     X_train = X[train]
0.71 logistic.py(906):     X_test = X[test]
0.71 logistic.py(907):     y_train = y[train]
0.71 logistic.py(908):     y_test = y[test]
0.71 logistic.py(910):     if sample_weight is not None:
0.71 logistic.py(916):     coefs, Cs, n_iter = logistic_regression_path(
0.71 logistic.py(917):         X_train, y_train, Cs=Cs, fit_intercept=fit_intercept,
0.71 logistic.py(918):         solver=solver, max_iter=max_iter, class_weight=class_weight,
0.71 logistic.py(919):         pos_class=pos_class, multi_class=multi_class,
0.71 logistic.py(920):         tol=tol, verbose=verbose, dual=dual, penalty=penalty,
0.71 logistic.py(921):         intercept_scaling=intercept_scaling, random_state=random_state,
0.71 logistic.py(922):         check_input=False, max_squared_sum=max_squared_sum,
0.71 logistic.py(923):         sample_weight=sample_weight)
0.71 logistic.py(591):     if isinstance(Cs, numbers.Integral):
0.71 logistic.py(592):         Cs = np.logspace(-4, 4, Cs)
0.71 logistic.py(594):     _check_solver_option(solver, multi_class, penalty, dual)
0.71 logistic.py(428):     if solver not in ['liblinear', 'newton-cg', 'lbfgs', 'sag', 'saga']:
0.71 logistic.py(433):     if multi_class not in ['multinomial', 'ovr']:
0.71 logistic.py(437):     if multi_class == 'multinomial' and solver == 'liblinear':
0.71 logistic.py(441):     if solver not in ['liblinear', 'saga']:
0.71 logistic.py(442):         if penalty != 'l2':
0.71 logistic.py(445):     if solver != 'liblinear':
0.71 logistic.py(446):         if dual:
0.71 logistic.py(597):     if check_input:
0.71 logistic.py(602):     _, n_features = X.shape
0.71 logistic.py(603):     classes = np.unique(y)
0.71 logistic.py(604):     random_state = check_random_state(random_state)
0.71 logistic.py(606):     if pos_class is None and multi_class != 'multinomial':
0.71 logistic.py(615):     if sample_weight is not None:
0.71 logistic.py(619):         sample_weight = np.ones(X.shape[0], dtype=X.dtype)
0.71 logistic.py(624):     le = LabelEncoder()
0.71 logistic.py(625):     if isinstance(class_weight, dict) or multi_class == 'multinomial':
0.71 logistic.py(626):         class_weight_ = compute_class_weight(class_weight, classes, y)
0.71 logistic.py(627):         sample_weight *= class_weight_[le.fit_transform(y)]
0.71 logistic.py(631):     if multi_class == 'ovr':
0.71 logistic.py(645):         if solver not in ['sag', 'saga']:
0.71 logistic.py(646):             lbin = LabelBinarizer()
0.71 logistic.py(647):             Y_multi = lbin.fit_transform(y)
0.71 logistic.py(648):             if Y_multi.shape[1] == 1:
0.71 logistic.py(655):         w0 = np.zeros((classes.size, n_features + int(fit_intercept)),
0.71 logistic.py(656):                       order='F', dtype=X.dtype)
0.71 logistic.py(658):     if coef is not None:
0.71 logistic.py(688):     if multi_class == 'multinomial':
0.71 logistic.py(690):         if solver in ['lbfgs', 'newton-cg']:
0.71 logistic.py(691):             w0 = w0.ravel()
0.71 logistic.py(692):         target = Y_multi
0.71 logistic.py(693):         if solver == 'lbfgs':
0.71 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
0.71 logistic.py(699):         warm_start_sag = {'coef': w0.T}
0.71 logistic.py(710):     coefs = list()
0.71 logistic.py(711):     n_iter = np.zeros(len(Cs), dtype=np.int32)
0.71 logistic.py(712):     for i, C in enumerate(Cs):
0.71 logistic.py(713):         if solver == 'lbfgs':
0.71 logistic.py(714):             w0, loss, info = optimize.fmin_l_bfgs_b(
0.71 logistic.py(715):                 func, w0, fprime=None,
0.71 logistic.py(716):                 args=(X, target, 1. / C, sample_weight),
0.71 logistic.py(717):                 iprint=(verbose > 0) - 1, pgtol=tol, maxiter=max_iter)
0.71 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
0.71 logistic.py(339):     n_classes = Y.shape[1]
0.71 logistic.py(340):     n_features = X.shape[1]
0.71 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
0.71 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
0.71 logistic.py(343):                     dtype=X.dtype)
0.71 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
0.71 logistic.py(282):     n_classes = Y.shape[1]
0.71 logistic.py(283):     n_features = X.shape[1]
0.71 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
0.71 logistic.py(285):     w = w.reshape(n_classes, -1)
0.71 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
0.71 logistic.py(287):     if fit_intercept:
0.71 logistic.py(288):         intercept = w[:, -1]
0.71 logistic.py(289):         w = w[:, :-1]
0.71 logistic.py(292):     p = safe_sparse_dot(X, w.T)
0.71 logistic.py(293):     p += intercept
0.71 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
0.71 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
0.71 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
0.71 logistic.py(297):     p = np.exp(p, p)
0.71 logistic.py(298):     return loss, p, w
0.71 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
0.71 logistic.py(346):     diff = sample_weight * (p - Y)
0.71 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
0.71 logistic.py(348):     grad[:, :n_features] += alpha * w
0.71 logistic.py(349):     if fit_intercept:
0.71 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
0.71 logistic.py(351):     return loss, grad.ravel(), p
0.71 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
0.71 logistic.py(339):     n_classes = Y.shape[1]
0.71 logistic.py(340):     n_features = X.shape[1]
0.71 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
0.71 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
0.71 logistic.py(343):                     dtype=X.dtype)
0.71 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
0.71 logistic.py(282):     n_classes = Y.shape[1]
0.71 logistic.py(283):     n_features = X.shape[1]
0.71 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
0.71 logistic.py(285):     w = w.reshape(n_classes, -1)
0.71 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
0.71 logistic.py(287):     if fit_intercept:
0.71 logistic.py(288):         intercept = w[:, -1]
0.71 logistic.py(289):         w = w[:, :-1]
0.71 logistic.py(292):     p = safe_sparse_dot(X, w.T)
0.71 logistic.py(293):     p += intercept
0.71 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
0.71 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
0.71 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
0.71 logistic.py(297):     p = np.exp(p, p)
0.71 logistic.py(298):     return loss, p, w
0.71 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
0.71 logistic.py(346):     diff = sample_weight * (p - Y)
0.71 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
0.71 logistic.py(348):     grad[:, :n_features] += alpha * w
0.71 logistic.py(349):     if fit_intercept:
0.71 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
0.71 logistic.py(351):     return loss, grad.ravel(), p
0.71 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
0.71 logistic.py(339):     n_classes = Y.shape[1]
0.71 logistic.py(340):     n_features = X.shape[1]
0.71 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
0.71 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
0.71 logistic.py(343):                     dtype=X.dtype)
0.71 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
0.71 logistic.py(282):     n_classes = Y.shape[1]
0.71 logistic.py(283):     n_features = X.shape[1]
0.71 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
0.71 logistic.py(285):     w = w.reshape(n_classes, -1)
0.71 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
0.71 logistic.py(287):     if fit_intercept:
0.71 logistic.py(288):         intercept = w[:, -1]
0.71 logistic.py(289):         w = w[:, :-1]
0.71 logistic.py(292):     p = safe_sparse_dot(X, w.T)
0.71 logistic.py(293):     p += intercept
0.71 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
0.71 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
0.71 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
0.71 logistic.py(297):     p = np.exp(p, p)
0.71 logistic.py(298):     return loss, p, w
0.71 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
0.71 logistic.py(346):     diff = sample_weight * (p - Y)
0.71 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
0.71 logistic.py(348):     grad[:, :n_features] += alpha * w
0.71 logistic.py(349):     if fit_intercept:
0.71 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
0.71 logistic.py(351):     return loss, grad.ravel(), p
0.71 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
0.71 logistic.py(339):     n_classes = Y.shape[1]
0.71 logistic.py(340):     n_features = X.shape[1]
0.71 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
0.71 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
0.71 logistic.py(343):                     dtype=X.dtype)
0.71 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
0.71 logistic.py(282):     n_classes = Y.shape[1]
0.71 logistic.py(283):     n_features = X.shape[1]
0.71 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
0.71 logistic.py(285):     w = w.reshape(n_classes, -1)
0.71 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
0.71 logistic.py(287):     if fit_intercept:
0.71 logistic.py(288):         intercept = w[:, -1]
0.71 logistic.py(289):         w = w[:, :-1]
0.71 logistic.py(292):     p = safe_sparse_dot(X, w.T)
0.71 logistic.py(293):     p += intercept
0.71 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
0.71 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
0.71 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
0.71 logistic.py(297):     p = np.exp(p, p)
0.71 logistic.py(298):     return loss, p, w
0.71 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
0.71 logistic.py(346):     diff = sample_weight * (p - Y)
0.71 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
0.71 logistic.py(348):     grad[:, :n_features] += alpha * w
0.71 logistic.py(349):     if fit_intercept:
0.71 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
0.71 logistic.py(351):     return loss, grad.ravel(), p
0.71 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
0.71 logistic.py(339):     n_classes = Y.shape[1]
0.71 logistic.py(340):     n_features = X.shape[1]
0.71 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
0.71 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
0.71 logistic.py(343):                     dtype=X.dtype)
0.71 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
0.71 logistic.py(282):     n_classes = Y.shape[1]
0.71 logistic.py(283):     n_features = X.shape[1]
0.71 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
0.71 logistic.py(285):     w = w.reshape(n_classes, -1)
0.71 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
0.71 logistic.py(287):     if fit_intercept:
0.71 logistic.py(288):         intercept = w[:, -1]
0.71 logistic.py(289):         w = w[:, :-1]
0.71 logistic.py(292):     p = safe_sparse_dot(X, w.T)
0.71 logistic.py(293):     p += intercept
0.71 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
0.71 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
0.71 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
0.71 logistic.py(297):     p = np.exp(p, p)
0.71 logistic.py(298):     return loss, p, w
0.71 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
0.71 logistic.py(346):     diff = sample_weight * (p - Y)
0.71 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
0.71 logistic.py(348):     grad[:, :n_features] += alpha * w
0.71 logistic.py(349):     if fit_intercept:
0.71 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
0.71 logistic.py(351):     return loss, grad.ravel(), p
0.71 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
0.71 logistic.py(339):     n_classes = Y.shape[1]
0.71 logistic.py(340):     n_features = X.shape[1]
0.71 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
0.71 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
0.71 logistic.py(343):                     dtype=X.dtype)
0.71 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
0.71 logistic.py(282):     n_classes = Y.shape[1]
0.71 logistic.py(283):     n_features = X.shape[1]
0.71 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
0.71 logistic.py(285):     w = w.reshape(n_classes, -1)
0.71 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
0.71 logistic.py(287):     if fit_intercept:
0.71 logistic.py(288):         intercept = w[:, -1]
0.71 logistic.py(289):         w = w[:, :-1]
0.71 logistic.py(292):     p = safe_sparse_dot(X, w.T)
0.71 logistic.py(293):     p += intercept
0.71 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
0.71 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
0.71 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
0.71 logistic.py(297):     p = np.exp(p, p)
0.71 logistic.py(298):     return loss, p, w
0.71 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
0.71 logistic.py(346):     diff = sample_weight * (p - Y)
0.71 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
0.71 logistic.py(348):     grad[:, :n_features] += alpha * w
0.71 logistic.py(349):     if fit_intercept:
0.71 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
0.71 logistic.py(351):     return loss, grad.ravel(), p
0.71 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
0.72 logistic.py(339):     n_classes = Y.shape[1]
0.72 logistic.py(340):     n_features = X.shape[1]
0.72 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
0.72 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
0.72 logistic.py(343):                     dtype=X.dtype)
0.72 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
0.72 logistic.py(282):     n_classes = Y.shape[1]
0.72 logistic.py(283):     n_features = X.shape[1]
0.72 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
0.72 logistic.py(285):     w = w.reshape(n_classes, -1)
0.72 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
0.72 logistic.py(287):     if fit_intercept:
0.72 logistic.py(288):         intercept = w[:, -1]
0.72 logistic.py(289):         w = w[:, :-1]
0.72 logistic.py(292):     p = safe_sparse_dot(X, w.T)
0.72 logistic.py(293):     p += intercept
0.72 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
0.72 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
0.72 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
0.72 logistic.py(297):     p = np.exp(p, p)
0.72 logistic.py(298):     return loss, p, w
0.72 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
0.72 logistic.py(346):     diff = sample_weight * (p - Y)
0.72 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
0.72 logistic.py(348):     grad[:, :n_features] += alpha * w
0.72 logistic.py(349):     if fit_intercept:
0.72 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
0.72 logistic.py(351):     return loss, grad.ravel(), p
0.72 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
0.72 logistic.py(339):     n_classes = Y.shape[1]
0.72 logistic.py(340):     n_features = X.shape[1]
0.72 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
0.72 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
0.72 logistic.py(343):                     dtype=X.dtype)
0.72 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
0.72 logistic.py(282):     n_classes = Y.shape[1]
0.72 logistic.py(283):     n_features = X.shape[1]
0.72 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
0.72 logistic.py(285):     w = w.reshape(n_classes, -1)
0.72 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
0.72 logistic.py(287):     if fit_intercept:
0.72 logistic.py(288):         intercept = w[:, -1]
0.72 logistic.py(289):         w = w[:, :-1]
0.72 logistic.py(292):     p = safe_sparse_dot(X, w.T)
0.72 logistic.py(293):     p += intercept
0.72 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
0.72 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
0.72 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
0.72 logistic.py(297):     p = np.exp(p, p)
0.72 logistic.py(298):     return loss, p, w
0.72 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
0.72 logistic.py(346):     diff = sample_weight * (p - Y)
0.72 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
0.72 logistic.py(348):     grad[:, :n_features] += alpha * w
0.72 logistic.py(349):     if fit_intercept:
0.72 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
0.72 logistic.py(351):     return loss, grad.ravel(), p
0.72 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
0.72 logistic.py(339):     n_classes = Y.shape[1]
0.72 logistic.py(340):     n_features = X.shape[1]
0.72 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
0.72 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
0.72 logistic.py(343):                     dtype=X.dtype)
0.72 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
0.72 logistic.py(282):     n_classes = Y.shape[1]
0.72 logistic.py(283):     n_features = X.shape[1]
0.72 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
0.72 logistic.py(285):     w = w.reshape(n_classes, -1)
0.72 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
0.72 logistic.py(287):     if fit_intercept:
0.72 logistic.py(288):         intercept = w[:, -1]
0.72 logistic.py(289):         w = w[:, :-1]
0.72 logistic.py(292):     p = safe_sparse_dot(X, w.T)
0.72 logistic.py(293):     p += intercept
0.72 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
0.72 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
0.72 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
0.72 logistic.py(297):     p = np.exp(p, p)
0.72 logistic.py(298):     return loss, p, w
0.72 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
0.72 logistic.py(346):     diff = sample_weight * (p - Y)
0.72 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
0.72 logistic.py(348):     grad[:, :n_features] += alpha * w
0.72 logistic.py(349):     if fit_intercept:
0.72 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
0.72 logistic.py(351):     return loss, grad.ravel(), p
0.72 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
0.72 logistic.py(339):     n_classes = Y.shape[1]
0.72 logistic.py(340):     n_features = X.shape[1]
0.72 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
0.72 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
0.72 logistic.py(343):                     dtype=X.dtype)
0.72 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
0.72 logistic.py(282):     n_classes = Y.shape[1]
0.72 logistic.py(283):     n_features = X.shape[1]
0.72 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
0.72 logistic.py(285):     w = w.reshape(n_classes, -1)
0.72 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
0.72 logistic.py(287):     if fit_intercept:
0.72 logistic.py(288):         intercept = w[:, -1]
0.72 logistic.py(289):         w = w[:, :-1]
0.72 logistic.py(292):     p = safe_sparse_dot(X, w.T)
0.72 logistic.py(293):     p += intercept
0.72 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
0.72 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
0.72 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
0.72 logistic.py(297):     p = np.exp(p, p)
0.72 logistic.py(298):     return loss, p, w
0.72 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
0.72 logistic.py(346):     diff = sample_weight * (p - Y)
0.72 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
0.72 logistic.py(348):     grad[:, :n_features] += alpha * w
0.72 logistic.py(349):     if fit_intercept:
0.72 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
0.72 logistic.py(351):     return loss, grad.ravel(), p
0.72 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
0.72 logistic.py(339):     n_classes = Y.shape[1]
0.72 logistic.py(340):     n_features = X.shape[1]
0.72 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
0.72 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
0.72 logistic.py(343):                     dtype=X.dtype)
0.72 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
0.72 logistic.py(282):     n_classes = Y.shape[1]
0.72 logistic.py(283):     n_features = X.shape[1]
0.72 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
0.72 logistic.py(285):     w = w.reshape(n_classes, -1)
0.72 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
0.72 logistic.py(287):     if fit_intercept:
0.72 logistic.py(288):         intercept = w[:, -1]
0.72 logistic.py(289):         w = w[:, :-1]
0.72 logistic.py(292):     p = safe_sparse_dot(X, w.T)
0.72 logistic.py(293):     p += intercept
0.72 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
0.72 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
0.72 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
0.72 logistic.py(297):     p = np.exp(p, p)
0.72 logistic.py(298):     return loss, p, w
0.72 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
0.72 logistic.py(346):     diff = sample_weight * (p - Y)
0.72 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
0.72 logistic.py(348):     grad[:, :n_features] += alpha * w
0.72 logistic.py(349):     if fit_intercept:
0.72 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
0.72 logistic.py(351):     return loss, grad.ravel(), p
0.72 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
0.72 logistic.py(339):     n_classes = Y.shape[1]
0.72 logistic.py(340):     n_features = X.shape[1]
0.72 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
0.72 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
0.72 logistic.py(343):                     dtype=X.dtype)
0.72 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
0.72 logistic.py(282):     n_classes = Y.shape[1]
0.72 logistic.py(283):     n_features = X.shape[1]
0.72 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
0.72 logistic.py(285):     w = w.reshape(n_classes, -1)
0.72 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
0.72 logistic.py(287):     if fit_intercept:
0.72 logistic.py(288):         intercept = w[:, -1]
0.72 logistic.py(289):         w = w[:, :-1]
0.72 logistic.py(292):     p = safe_sparse_dot(X, w.T)
0.72 logistic.py(293):     p += intercept
0.72 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
0.72 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
0.72 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
0.72 logistic.py(297):     p = np.exp(p, p)
0.72 logistic.py(298):     return loss, p, w
0.72 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
0.72 logistic.py(346):     diff = sample_weight * (p - Y)
0.72 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
0.72 logistic.py(348):     grad[:, :n_features] += alpha * w
0.72 logistic.py(349):     if fit_intercept:
0.72 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
0.72 logistic.py(351):     return loss, grad.ravel(), p
0.72 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
0.72 logistic.py(339):     n_classes = Y.shape[1]
0.72 logistic.py(340):     n_features = X.shape[1]
0.72 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
0.72 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
0.72 logistic.py(343):                     dtype=X.dtype)
0.72 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
0.72 logistic.py(282):     n_classes = Y.shape[1]
0.72 logistic.py(283):     n_features = X.shape[1]
0.72 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
0.72 logistic.py(285):     w = w.reshape(n_classes, -1)
0.72 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
0.72 logistic.py(287):     if fit_intercept:
0.72 logistic.py(288):         intercept = w[:, -1]
0.72 logistic.py(289):         w = w[:, :-1]
0.72 logistic.py(292):     p = safe_sparse_dot(X, w.T)
0.72 logistic.py(293):     p += intercept
0.72 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
0.72 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
0.72 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
0.72 logistic.py(297):     p = np.exp(p, p)
0.72 logistic.py(298):     return loss, p, w
0.72 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
0.72 logistic.py(346):     diff = sample_weight * (p - Y)
0.72 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
0.72 logistic.py(348):     grad[:, :n_features] += alpha * w
0.72 logistic.py(349):     if fit_intercept:
0.72 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
0.72 logistic.py(351):     return loss, grad.ravel(), p
0.72 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
0.72 logistic.py(339):     n_classes = Y.shape[1]
0.72 logistic.py(340):     n_features = X.shape[1]
0.72 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
0.72 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
0.72 logistic.py(343):                     dtype=X.dtype)
0.72 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
0.72 logistic.py(282):     n_classes = Y.shape[1]
0.72 logistic.py(283):     n_features = X.shape[1]
0.72 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
0.72 logistic.py(285):     w = w.reshape(n_classes, -1)
0.72 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
0.72 logistic.py(287):     if fit_intercept:
0.72 logistic.py(288):         intercept = w[:, -1]
0.72 logistic.py(289):         w = w[:, :-1]
0.72 logistic.py(292):     p = safe_sparse_dot(X, w.T)
0.72 logistic.py(293):     p += intercept
0.72 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
0.72 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
0.72 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
0.72 logistic.py(297):     p = np.exp(p, p)
0.72 logistic.py(298):     return loss, p, w
0.72 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
0.72 logistic.py(346):     diff = sample_weight * (p - Y)
0.72 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
0.72 logistic.py(348):     grad[:, :n_features] += alpha * w
0.72 logistic.py(349):     if fit_intercept:
0.72 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
0.72 logistic.py(351):     return loss, grad.ravel(), p
0.72 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
0.72 logistic.py(339):     n_classes = Y.shape[1]
0.72 logistic.py(340):     n_features = X.shape[1]
0.72 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
0.72 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
0.72 logistic.py(343):                     dtype=X.dtype)
0.72 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
0.72 logistic.py(282):     n_classes = Y.shape[1]
0.72 logistic.py(283):     n_features = X.shape[1]
0.72 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
0.72 logistic.py(285):     w = w.reshape(n_classes, -1)
0.72 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
0.72 logistic.py(287):     if fit_intercept:
0.72 logistic.py(288):         intercept = w[:, -1]
0.72 logistic.py(289):         w = w[:, :-1]
0.72 logistic.py(292):     p = safe_sparse_dot(X, w.T)
0.72 logistic.py(293):     p += intercept
0.72 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
0.72 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
0.72 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
0.72 logistic.py(297):     p = np.exp(p, p)
0.72 logistic.py(298):     return loss, p, w
0.72 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
0.72 logistic.py(346):     diff = sample_weight * (p - Y)
0.72 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
0.72 logistic.py(348):     grad[:, :n_features] += alpha * w
0.72 logistic.py(349):     if fit_intercept:
0.72 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
0.72 logistic.py(351):     return loss, grad.ravel(), p
0.72 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
0.72 logistic.py(339):     n_classes = Y.shape[1]
0.72 logistic.py(340):     n_features = X.shape[1]
0.72 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
0.72 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
0.72 logistic.py(343):                     dtype=X.dtype)
0.72 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
0.72 logistic.py(282):     n_classes = Y.shape[1]
0.72 logistic.py(283):     n_features = X.shape[1]
0.72 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
0.72 logistic.py(285):     w = w.reshape(n_classes, -1)
0.72 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
0.72 logistic.py(287):     if fit_intercept:
0.72 logistic.py(288):         intercept = w[:, -1]
0.72 logistic.py(289):         w = w[:, :-1]
0.72 logistic.py(292):     p = safe_sparse_dot(X, w.T)
0.72 logistic.py(293):     p += intercept
0.72 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
0.72 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
0.72 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
0.72 logistic.py(297):     p = np.exp(p, p)
0.72 logistic.py(298):     return loss, p, w
0.72 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
0.72 logistic.py(346):     diff = sample_weight * (p - Y)
0.72 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
0.72 logistic.py(348):     grad[:, :n_features] += alpha * w
0.72 logistic.py(349):     if fit_intercept:
0.72 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
0.72 logistic.py(351):     return loss, grad.ravel(), p
0.72 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
0.72 logistic.py(339):     n_classes = Y.shape[1]
0.72 logistic.py(340):     n_features = X.shape[1]
0.72 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
0.72 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
0.72 logistic.py(343):                     dtype=X.dtype)
0.72 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
0.72 logistic.py(282):     n_classes = Y.shape[1]
0.72 logistic.py(283):     n_features = X.shape[1]
0.72 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
0.72 logistic.py(285):     w = w.reshape(n_classes, -1)
0.72 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
0.72 logistic.py(287):     if fit_intercept:
0.72 logistic.py(288):         intercept = w[:, -1]
0.72 logistic.py(289):         w = w[:, :-1]
0.72 logistic.py(292):     p = safe_sparse_dot(X, w.T)
0.72 logistic.py(293):     p += intercept
0.72 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
0.72 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
0.72 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
0.72 logistic.py(297):     p = np.exp(p, p)
0.72 logistic.py(298):     return loss, p, w
0.72 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
0.72 logistic.py(346):     diff = sample_weight * (p - Y)
0.72 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
0.72 logistic.py(348):     grad[:, :n_features] += alpha * w
0.72 logistic.py(349):     if fit_intercept:
0.72 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
0.72 logistic.py(351):     return loss, grad.ravel(), p
0.72 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
0.72 logistic.py(339):     n_classes = Y.shape[1]
0.72 logistic.py(340):     n_features = X.shape[1]
0.72 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
0.72 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
0.72 logistic.py(343):                     dtype=X.dtype)
0.72 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
0.72 logistic.py(282):     n_classes = Y.shape[1]
0.72 logistic.py(283):     n_features = X.shape[1]
0.72 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
0.72 logistic.py(285):     w = w.reshape(n_classes, -1)
0.72 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
0.72 logistic.py(287):     if fit_intercept:
0.72 logistic.py(288):         intercept = w[:, -1]
0.72 logistic.py(289):         w = w[:, :-1]
0.72 logistic.py(292):     p = safe_sparse_dot(X, w.T)
0.72 logistic.py(293):     p += intercept
0.72 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
0.72 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
0.72 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
0.72 logistic.py(297):     p = np.exp(p, p)
0.72 logistic.py(298):     return loss, p, w
0.72 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
0.72 logistic.py(346):     diff = sample_weight * (p - Y)
0.72 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
0.72 logistic.py(348):     grad[:, :n_features] += alpha * w
0.72 logistic.py(349):     if fit_intercept:
0.72 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
0.72 logistic.py(351):     return loss, grad.ravel(), p
0.72 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
0.72 logistic.py(339):     n_classes = Y.shape[1]
0.72 logistic.py(340):     n_features = X.shape[1]
0.72 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
0.72 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
0.72 logistic.py(343):                     dtype=X.dtype)
0.72 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
0.72 logistic.py(282):     n_classes = Y.shape[1]
0.72 logistic.py(283):     n_features = X.shape[1]
0.72 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
0.72 logistic.py(285):     w = w.reshape(n_classes, -1)
0.72 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
0.72 logistic.py(287):     if fit_intercept:
0.72 logistic.py(288):         intercept = w[:, -1]
0.72 logistic.py(289):         w = w[:, :-1]
0.72 logistic.py(292):     p = safe_sparse_dot(X, w.T)
0.72 logistic.py(293):     p += intercept
0.72 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
0.72 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
0.72 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
0.72 logistic.py(297):     p = np.exp(p, p)
0.72 logistic.py(298):     return loss, p, w
0.72 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
0.72 logistic.py(346):     diff = sample_weight * (p - Y)
0.72 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
0.72 logistic.py(348):     grad[:, :n_features] += alpha * w
0.72 logistic.py(349):     if fit_intercept:
0.72 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
0.72 logistic.py(351):     return loss, grad.ravel(), p
0.72 logistic.py(718):             if info["warnflag"] == 1:
0.72 logistic.py(723):             n_iter_i = min(info['nit'], max_iter)
0.72 logistic.py(760):         if multi_class == 'multinomial':
0.72 logistic.py(761):             multi_w0 = np.reshape(w0, (classes.size, -1))
0.72 logistic.py(762):             if classes.size == 2:
0.72 logistic.py(764):             coefs.append(multi_w0)
0.72 logistic.py(768):         n_iter[i] = n_iter_i
0.72 logistic.py(712):     for i, C in enumerate(Cs):
0.72 logistic.py(713):         if solver == 'lbfgs':
0.72 logistic.py(714):             w0, loss, info = optimize.fmin_l_bfgs_b(
0.72 logistic.py(715):                 func, w0, fprime=None,
0.72 logistic.py(716):                 args=(X, target, 1. / C, sample_weight),
0.72 logistic.py(717):                 iprint=(verbose > 0) - 1, pgtol=tol, maxiter=max_iter)
0.72 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
0.72 logistic.py(339):     n_classes = Y.shape[1]
0.72 logistic.py(340):     n_features = X.shape[1]
0.72 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
0.72 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
0.72 logistic.py(343):                     dtype=X.dtype)
0.72 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
0.72 logistic.py(282):     n_classes = Y.shape[1]
0.72 logistic.py(283):     n_features = X.shape[1]
0.72 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
0.72 logistic.py(285):     w = w.reshape(n_classes, -1)
0.72 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
0.72 logistic.py(287):     if fit_intercept:
0.72 logistic.py(288):         intercept = w[:, -1]
0.72 logistic.py(289):         w = w[:, :-1]
0.72 logistic.py(292):     p = safe_sparse_dot(X, w.T)
0.72 logistic.py(293):     p += intercept
0.72 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
0.72 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
0.72 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
0.72 logistic.py(297):     p = np.exp(p, p)
0.72 logistic.py(298):     return loss, p, w
0.72 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
0.72 logistic.py(346):     diff = sample_weight * (p - Y)
0.72 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
0.72 logistic.py(348):     grad[:, :n_features] += alpha * w
0.72 logistic.py(349):     if fit_intercept:
0.72 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
0.72 logistic.py(351):     return loss, grad.ravel(), p
0.72 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
0.72 logistic.py(339):     n_classes = Y.shape[1]
0.72 logistic.py(340):     n_features = X.shape[1]
0.72 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
0.72 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
0.72 logistic.py(343):                     dtype=X.dtype)
0.72 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
0.72 logistic.py(282):     n_classes = Y.shape[1]
0.72 logistic.py(283):     n_features = X.shape[1]
0.72 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
0.72 logistic.py(285):     w = w.reshape(n_classes, -1)
0.72 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
0.72 logistic.py(287):     if fit_intercept:
0.72 logistic.py(288):         intercept = w[:, -1]
0.72 logistic.py(289):         w = w[:, :-1]
0.72 logistic.py(292):     p = safe_sparse_dot(X, w.T)
0.72 logistic.py(293):     p += intercept
0.72 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
0.72 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
0.72 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
0.72 logistic.py(297):     p = np.exp(p, p)
0.72 logistic.py(298):     return loss, p, w
0.72 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
0.72 logistic.py(346):     diff = sample_weight * (p - Y)
0.72 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
0.72 logistic.py(348):     grad[:, :n_features] += alpha * w
0.72 logistic.py(349):     if fit_intercept:
0.72 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
0.72 logistic.py(351):     return loss, grad.ravel(), p
0.72 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
0.72 logistic.py(339):     n_classes = Y.shape[1]
0.72 logistic.py(340):     n_features = X.shape[1]
0.72 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
0.72 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
0.72 logistic.py(343):                     dtype=X.dtype)
0.72 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
0.72 logistic.py(282):     n_classes = Y.shape[1]
0.72 logistic.py(283):     n_features = X.shape[1]
0.72 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
0.72 logistic.py(285):     w = w.reshape(n_classes, -1)
0.72 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
0.72 logistic.py(287):     if fit_intercept:
0.72 logistic.py(288):         intercept = w[:, -1]
0.72 logistic.py(289):         w = w[:, :-1]
0.72 logistic.py(292):     p = safe_sparse_dot(X, w.T)
0.72 logistic.py(293):     p += intercept
0.72 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
0.72 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
0.72 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
0.72 logistic.py(297):     p = np.exp(p, p)
0.72 logistic.py(298):     return loss, p, w
0.72 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
0.72 logistic.py(346):     diff = sample_weight * (p - Y)
0.72 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
0.72 logistic.py(348):     grad[:, :n_features] += alpha * w
0.73 logistic.py(349):     if fit_intercept:
0.73 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
0.73 logistic.py(351):     return loss, grad.ravel(), p
0.73 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
0.73 logistic.py(339):     n_classes = Y.shape[1]
0.73 logistic.py(340):     n_features = X.shape[1]
0.73 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
0.73 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
0.73 logistic.py(343):                     dtype=X.dtype)
0.73 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
0.73 logistic.py(282):     n_classes = Y.shape[1]
0.73 logistic.py(283):     n_features = X.shape[1]
0.73 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
0.73 logistic.py(285):     w = w.reshape(n_classes, -1)
0.73 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
0.73 logistic.py(287):     if fit_intercept:
0.73 logistic.py(288):         intercept = w[:, -1]
0.73 logistic.py(289):         w = w[:, :-1]
0.73 logistic.py(292):     p = safe_sparse_dot(X, w.T)
0.73 logistic.py(293):     p += intercept
0.73 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
0.73 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
0.73 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
0.73 logistic.py(297):     p = np.exp(p, p)
0.73 logistic.py(298):     return loss, p, w
0.73 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
0.73 logistic.py(346):     diff = sample_weight * (p - Y)
0.73 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
0.73 logistic.py(348):     grad[:, :n_features] += alpha * w
0.73 logistic.py(349):     if fit_intercept:
0.73 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
0.73 logistic.py(351):     return loss, grad.ravel(), p
0.73 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
0.73 logistic.py(339):     n_classes = Y.shape[1]
0.73 logistic.py(340):     n_features = X.shape[1]
0.73 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
0.73 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
0.73 logistic.py(343):                     dtype=X.dtype)
0.73 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
0.73 logistic.py(282):     n_classes = Y.shape[1]
0.73 logistic.py(283):     n_features = X.shape[1]
0.73 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
0.73 logistic.py(285):     w = w.reshape(n_classes, -1)
0.73 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
0.73 logistic.py(287):     if fit_intercept:
0.73 logistic.py(288):         intercept = w[:, -1]
0.73 logistic.py(289):         w = w[:, :-1]
0.73 logistic.py(292):     p = safe_sparse_dot(X, w.T)
0.73 logistic.py(293):     p += intercept
0.73 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
0.73 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
0.73 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
0.73 logistic.py(297):     p = np.exp(p, p)
0.73 logistic.py(298):     return loss, p, w
0.73 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
0.73 logistic.py(346):     diff = sample_weight * (p - Y)
0.73 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
0.73 logistic.py(348):     grad[:, :n_features] += alpha * w
0.73 logistic.py(349):     if fit_intercept:
0.73 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
0.73 logistic.py(351):     return loss, grad.ravel(), p
0.73 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
0.73 logistic.py(339):     n_classes = Y.shape[1]
0.73 logistic.py(340):     n_features = X.shape[1]
0.73 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
0.73 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
0.73 logistic.py(343):                     dtype=X.dtype)
0.73 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
0.73 logistic.py(282):     n_classes = Y.shape[1]
0.73 logistic.py(283):     n_features = X.shape[1]
0.73 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
0.73 logistic.py(285):     w = w.reshape(n_classes, -1)
0.73 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
0.73 logistic.py(287):     if fit_intercept:
0.73 logistic.py(288):         intercept = w[:, -1]
0.73 logistic.py(289):         w = w[:, :-1]
0.73 logistic.py(292):     p = safe_sparse_dot(X, w.T)
0.73 logistic.py(293):     p += intercept
0.73 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
0.73 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
0.73 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
0.73 logistic.py(297):     p = np.exp(p, p)
0.73 logistic.py(298):     return loss, p, w
0.73 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
0.73 logistic.py(346):     diff = sample_weight * (p - Y)
0.73 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
0.73 logistic.py(348):     grad[:, :n_features] += alpha * w
0.73 logistic.py(349):     if fit_intercept:
0.73 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
0.73 logistic.py(351):     return loss, grad.ravel(), p
0.73 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
0.73 logistic.py(339):     n_classes = Y.shape[1]
0.73 logistic.py(340):     n_features = X.shape[1]
0.73 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
0.73 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
0.73 logistic.py(343):                     dtype=X.dtype)
0.73 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
0.73 logistic.py(282):     n_classes = Y.shape[1]
0.73 logistic.py(283):     n_features = X.shape[1]
0.73 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
0.73 logistic.py(285):     w = w.reshape(n_classes, -1)
0.73 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
0.73 logistic.py(287):     if fit_intercept:
0.73 logistic.py(288):         intercept = w[:, -1]
0.73 logistic.py(289):         w = w[:, :-1]
0.73 logistic.py(292):     p = safe_sparse_dot(X, w.T)
0.73 logistic.py(293):     p += intercept
0.73 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
0.73 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
0.73 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
0.73 logistic.py(297):     p = np.exp(p, p)
0.73 logistic.py(298):     return loss, p, w
0.73 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
0.73 logistic.py(346):     diff = sample_weight * (p - Y)
0.73 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
0.73 logistic.py(348):     grad[:, :n_features] += alpha * w
0.73 logistic.py(349):     if fit_intercept:
0.73 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
0.73 logistic.py(351):     return loss, grad.ravel(), p
0.73 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
0.73 logistic.py(339):     n_classes = Y.shape[1]
0.73 logistic.py(340):     n_features = X.shape[1]
0.73 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
0.73 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
0.73 logistic.py(343):                     dtype=X.dtype)
0.73 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
0.73 logistic.py(282):     n_classes = Y.shape[1]
0.73 logistic.py(283):     n_features = X.shape[1]
0.73 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
0.73 logistic.py(285):     w = w.reshape(n_classes, -1)
0.73 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
0.73 logistic.py(287):     if fit_intercept:
0.73 logistic.py(288):         intercept = w[:, -1]
0.73 logistic.py(289):         w = w[:, :-1]
0.73 logistic.py(292):     p = safe_sparse_dot(X, w.T)
0.73 logistic.py(293):     p += intercept
0.73 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
0.73 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
0.73 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
0.73 logistic.py(297):     p = np.exp(p, p)
0.73 logistic.py(298):     return loss, p, w
0.73 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
0.73 logistic.py(346):     diff = sample_weight * (p - Y)
0.73 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
0.73 logistic.py(348):     grad[:, :n_features] += alpha * w
0.73 logistic.py(349):     if fit_intercept:
0.73 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
0.73 logistic.py(351):     return loss, grad.ravel(), p
0.73 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
0.73 logistic.py(339):     n_classes = Y.shape[1]
0.73 logistic.py(340):     n_features = X.shape[1]
0.73 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
0.73 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
0.73 logistic.py(343):                     dtype=X.dtype)
0.73 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
0.73 logistic.py(282):     n_classes = Y.shape[1]
0.73 logistic.py(283):     n_features = X.shape[1]
0.73 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
0.73 logistic.py(285):     w = w.reshape(n_classes, -1)
0.73 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
0.73 logistic.py(287):     if fit_intercept:
0.73 logistic.py(288):         intercept = w[:, -1]
0.73 logistic.py(289):         w = w[:, :-1]
0.73 logistic.py(292):     p = safe_sparse_dot(X, w.T)
0.73 logistic.py(293):     p += intercept
0.73 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
0.73 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
0.73 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
0.73 logistic.py(297):     p = np.exp(p, p)
0.73 logistic.py(298):     return loss, p, w
0.73 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
0.73 logistic.py(346):     diff = sample_weight * (p - Y)
0.73 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
0.73 logistic.py(348):     grad[:, :n_features] += alpha * w
0.73 logistic.py(349):     if fit_intercept:
0.73 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
0.73 logistic.py(351):     return loss, grad.ravel(), p
0.73 logistic.py(718):             if info["warnflag"] == 1:
0.73 logistic.py(723):             n_iter_i = min(info['nit'], max_iter)
0.73 logistic.py(760):         if multi_class == 'multinomial':
0.73 logistic.py(761):             multi_w0 = np.reshape(w0, (classes.size, -1))
0.73 logistic.py(762):             if classes.size == 2:
0.73 logistic.py(764):             coefs.append(multi_w0)
0.73 logistic.py(768):         n_iter[i] = n_iter_i
0.73 logistic.py(712):     for i, C in enumerate(Cs):
0.73 logistic.py(713):         if solver == 'lbfgs':
0.73 logistic.py(714):             w0, loss, info = optimize.fmin_l_bfgs_b(
0.73 logistic.py(715):                 func, w0, fprime=None,
0.73 logistic.py(716):                 args=(X, target, 1. / C, sample_weight),
0.73 logistic.py(717):                 iprint=(verbose > 0) - 1, pgtol=tol, maxiter=max_iter)
0.73 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
0.73 logistic.py(339):     n_classes = Y.shape[1]
0.73 logistic.py(340):     n_features = X.shape[1]
0.73 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
0.73 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
0.73 logistic.py(343):                     dtype=X.dtype)
0.73 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
0.73 logistic.py(282):     n_classes = Y.shape[1]
0.73 logistic.py(283):     n_features = X.shape[1]
0.73 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
0.73 logistic.py(285):     w = w.reshape(n_classes, -1)
0.73 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
0.73 logistic.py(287):     if fit_intercept:
0.73 logistic.py(288):         intercept = w[:, -1]
0.73 logistic.py(289):         w = w[:, :-1]
0.73 logistic.py(292):     p = safe_sparse_dot(X, w.T)
0.73 logistic.py(293):     p += intercept
0.73 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
0.73 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
0.73 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
0.73 logistic.py(297):     p = np.exp(p, p)
0.73 logistic.py(298):     return loss, p, w
0.73 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
0.73 logistic.py(346):     diff = sample_weight * (p - Y)
0.73 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
0.73 logistic.py(348):     grad[:, :n_features] += alpha * w
0.73 logistic.py(349):     if fit_intercept:
0.73 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
0.73 logistic.py(351):     return loss, grad.ravel(), p
0.73 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
0.73 logistic.py(339):     n_classes = Y.shape[1]
0.73 logistic.py(340):     n_features = X.shape[1]
0.73 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
0.73 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
0.73 logistic.py(343):                     dtype=X.dtype)
0.73 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
0.73 logistic.py(282):     n_classes = Y.shape[1]
0.73 logistic.py(283):     n_features = X.shape[1]
0.73 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
0.73 logistic.py(285):     w = w.reshape(n_classes, -1)
0.73 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
0.73 logistic.py(287):     if fit_intercept:
0.73 logistic.py(288):         intercept = w[:, -1]
0.73 logistic.py(289):         w = w[:, :-1]
0.73 logistic.py(292):     p = safe_sparse_dot(X, w.T)
0.73 logistic.py(293):     p += intercept
0.73 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
0.73 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
0.73 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
0.73 logistic.py(297):     p = np.exp(p, p)
0.73 logistic.py(298):     return loss, p, w
0.73 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
0.73 logistic.py(346):     diff = sample_weight * (p - Y)
0.73 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
0.73 logistic.py(348):     grad[:, :n_features] += alpha * w
0.73 logistic.py(349):     if fit_intercept:
0.73 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
0.73 logistic.py(351):     return loss, grad.ravel(), p
0.73 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
0.73 logistic.py(339):     n_classes = Y.shape[1]
0.73 logistic.py(340):     n_features = X.shape[1]
0.73 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
0.73 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
0.73 logistic.py(343):                     dtype=X.dtype)
0.73 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
0.73 logistic.py(282):     n_classes = Y.shape[1]
0.73 logistic.py(283):     n_features = X.shape[1]
0.73 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
0.73 logistic.py(285):     w = w.reshape(n_classes, -1)
0.73 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
0.73 logistic.py(287):     if fit_intercept:
0.73 logistic.py(288):         intercept = w[:, -1]
0.73 logistic.py(289):         w = w[:, :-1]
0.73 logistic.py(292):     p = safe_sparse_dot(X, w.T)
0.73 logistic.py(293):     p += intercept
0.73 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
0.73 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
0.73 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
0.73 logistic.py(297):     p = np.exp(p, p)
0.73 logistic.py(298):     return loss, p, w
0.73 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
0.73 logistic.py(346):     diff = sample_weight * (p - Y)
0.73 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
0.73 logistic.py(348):     grad[:, :n_features] += alpha * w
0.73 logistic.py(349):     if fit_intercept:
0.73 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
0.73 logistic.py(351):     return loss, grad.ravel(), p
0.73 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
0.73 logistic.py(339):     n_classes = Y.shape[1]
0.73 logistic.py(340):     n_features = X.shape[1]
0.73 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
0.73 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
0.73 logistic.py(343):                     dtype=X.dtype)
0.73 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
0.73 logistic.py(282):     n_classes = Y.shape[1]
0.73 logistic.py(283):     n_features = X.shape[1]
0.73 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
0.73 logistic.py(285):     w = w.reshape(n_classes, -1)
0.73 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
0.73 logistic.py(287):     if fit_intercept:
0.73 logistic.py(288):         intercept = w[:, -1]
0.73 logistic.py(289):         w = w[:, :-1]
0.73 logistic.py(292):     p = safe_sparse_dot(X, w.T)
0.73 logistic.py(293):     p += intercept
0.73 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
0.73 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
0.73 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
0.73 logistic.py(297):     p = np.exp(p, p)
0.73 logistic.py(298):     return loss, p, w
0.73 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
0.73 logistic.py(346):     diff = sample_weight * (p - Y)
0.73 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
0.73 logistic.py(348):     grad[:, :n_features] += alpha * w
0.73 logistic.py(349):     if fit_intercept:
0.73 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
0.73 logistic.py(351):     return loss, grad.ravel(), p
0.73 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
0.73 logistic.py(339):     n_classes = Y.shape[1]
0.73 logistic.py(340):     n_features = X.shape[1]
0.73 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
0.73 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
0.73 logistic.py(343):                     dtype=X.dtype)
0.73 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
0.73 logistic.py(282):     n_classes = Y.shape[1]
0.73 logistic.py(283):     n_features = X.shape[1]
0.73 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
0.73 logistic.py(285):     w = w.reshape(n_classes, -1)
0.73 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
0.73 logistic.py(287):     if fit_intercept:
0.73 logistic.py(288):         intercept = w[:, -1]
0.73 logistic.py(289):         w = w[:, :-1]
0.73 logistic.py(292):     p = safe_sparse_dot(X, w.T)
0.73 logistic.py(293):     p += intercept
0.73 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
0.73 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
0.73 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
0.73 logistic.py(297):     p = np.exp(p, p)
0.73 logistic.py(298):     return loss, p, w
0.73 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
0.73 logistic.py(346):     diff = sample_weight * (p - Y)
0.73 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
0.73 logistic.py(348):     grad[:, :n_features] += alpha * w
0.73 logistic.py(349):     if fit_intercept:
0.73 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
0.73 logistic.py(351):     return loss, grad.ravel(), p
0.73 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
0.73 logistic.py(339):     n_classes = Y.shape[1]
0.73 logistic.py(340):     n_features = X.shape[1]
0.73 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
0.73 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
0.73 logistic.py(343):                     dtype=X.dtype)
0.73 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
0.73 logistic.py(282):     n_classes = Y.shape[1]
0.73 logistic.py(283):     n_features = X.shape[1]
0.73 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
0.73 logistic.py(285):     w = w.reshape(n_classes, -1)
0.73 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
0.73 logistic.py(287):     if fit_intercept:
0.73 logistic.py(288):         intercept = w[:, -1]
0.73 logistic.py(289):         w = w[:, :-1]
0.73 logistic.py(292):     p = safe_sparse_dot(X, w.T)
0.73 logistic.py(293):     p += intercept
0.73 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
0.73 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
0.73 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
0.73 logistic.py(297):     p = np.exp(p, p)
0.73 logistic.py(298):     return loss, p, w
0.73 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
0.73 logistic.py(346):     diff = sample_weight * (p - Y)
0.73 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
0.73 logistic.py(348):     grad[:, :n_features] += alpha * w
0.73 logistic.py(349):     if fit_intercept:
0.73 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
0.73 logistic.py(351):     return loss, grad.ravel(), p
0.73 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
0.73 logistic.py(339):     n_classes = Y.shape[1]
0.73 logistic.py(340):     n_features = X.shape[1]
0.73 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
0.73 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
0.73 logistic.py(343):                     dtype=X.dtype)
0.73 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
0.73 logistic.py(282):     n_classes = Y.shape[1]
0.73 logistic.py(283):     n_features = X.shape[1]
0.73 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
0.73 logistic.py(285):     w = w.reshape(n_classes, -1)
0.73 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
0.73 logistic.py(287):     if fit_intercept:
0.73 logistic.py(288):         intercept = w[:, -1]
0.73 logistic.py(289):         w = w[:, :-1]
0.73 logistic.py(292):     p = safe_sparse_dot(X, w.T)
0.73 logistic.py(293):     p += intercept
0.73 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
0.73 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
0.73 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
0.73 logistic.py(297):     p = np.exp(p, p)
0.73 logistic.py(298):     return loss, p, w
0.73 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
0.73 logistic.py(346):     diff = sample_weight * (p - Y)
0.73 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
0.73 logistic.py(348):     grad[:, :n_features] += alpha * w
0.73 logistic.py(349):     if fit_intercept:
0.73 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
0.73 logistic.py(351):     return loss, grad.ravel(), p
0.73 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
0.73 logistic.py(339):     n_classes = Y.shape[1]
0.73 logistic.py(340):     n_features = X.shape[1]
0.73 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
0.73 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
0.73 logistic.py(343):                     dtype=X.dtype)
0.73 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
0.73 logistic.py(282):     n_classes = Y.shape[1]
0.73 logistic.py(283):     n_features = X.shape[1]
0.73 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
0.73 logistic.py(285):     w = w.reshape(n_classes, -1)
0.73 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
0.73 logistic.py(287):     if fit_intercept:
0.73 logistic.py(288):         intercept = w[:, -1]
0.73 logistic.py(289):         w = w[:, :-1]
0.73 logistic.py(292):     p = safe_sparse_dot(X, w.T)
0.73 logistic.py(293):     p += intercept
0.73 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
0.73 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
0.73 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
0.73 logistic.py(297):     p = np.exp(p, p)
0.73 logistic.py(298):     return loss, p, w
0.73 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
0.73 logistic.py(346):     diff = sample_weight * (p - Y)
0.73 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
0.73 logistic.py(348):     grad[:, :n_features] += alpha * w
0.73 logistic.py(349):     if fit_intercept:
0.73 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
0.73 logistic.py(351):     return loss, grad.ravel(), p
0.73 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
0.73 logistic.py(339):     n_classes = Y.shape[1]
0.73 logistic.py(340):     n_features = X.shape[1]
0.73 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
0.73 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
0.73 logistic.py(343):                     dtype=X.dtype)
0.73 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
0.73 logistic.py(282):     n_classes = Y.shape[1]
0.73 logistic.py(283):     n_features = X.shape[1]
0.73 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
0.73 logistic.py(285):     w = w.reshape(n_classes, -1)
0.73 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
0.73 logistic.py(287):     if fit_intercept:
0.73 logistic.py(288):         intercept = w[:, -1]
0.73 logistic.py(289):         w = w[:, :-1]
0.73 logistic.py(292):     p = safe_sparse_dot(X, w.T)
0.73 logistic.py(293):     p += intercept
0.73 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
0.73 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
0.73 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
0.73 logistic.py(297):     p = np.exp(p, p)
0.73 logistic.py(298):     return loss, p, w
0.73 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
0.73 logistic.py(346):     diff = sample_weight * (p - Y)
0.73 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
0.73 logistic.py(348):     grad[:, :n_features] += alpha * w
0.73 logistic.py(349):     if fit_intercept:
0.73 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
0.73 logistic.py(351):     return loss, grad.ravel(), p
0.73 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
0.73 logistic.py(339):     n_classes = Y.shape[1]
0.73 logistic.py(340):     n_features = X.shape[1]
0.73 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
0.73 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
0.73 logistic.py(343):                     dtype=X.dtype)
0.73 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
0.73 logistic.py(282):     n_classes = Y.shape[1]
0.73 logistic.py(283):     n_features = X.shape[1]
0.73 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
0.73 logistic.py(285):     w = w.reshape(n_classes, -1)
0.73 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
0.73 logistic.py(287):     if fit_intercept:
0.73 logistic.py(288):         intercept = w[:, -1]
0.73 logistic.py(289):         w = w[:, :-1]
0.73 logistic.py(292):     p = safe_sparse_dot(X, w.T)
0.73 logistic.py(293):     p += intercept
0.73 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
0.73 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
0.73 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
0.73 logistic.py(297):     p = np.exp(p, p)
0.73 logistic.py(298):     return loss, p, w
0.73 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
0.73 logistic.py(346):     diff = sample_weight * (p - Y)
0.73 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
0.73 logistic.py(348):     grad[:, :n_features] += alpha * w
0.73 logistic.py(349):     if fit_intercept:
0.73 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
0.74 logistic.py(351):     return loss, grad.ravel(), p
0.74 logistic.py(718):             if info["warnflag"] == 1:
0.74 logistic.py(723):             n_iter_i = min(info['nit'], max_iter)
0.74 logistic.py(760):         if multi_class == 'multinomial':
0.74 logistic.py(761):             multi_w0 = np.reshape(w0, (classes.size, -1))
0.74 logistic.py(762):             if classes.size == 2:
0.74 logistic.py(764):             coefs.append(multi_w0)
0.74 logistic.py(768):         n_iter[i] = n_iter_i
0.74 logistic.py(712):     for i, C in enumerate(Cs):
0.74 logistic.py(713):         if solver == 'lbfgs':
0.74 logistic.py(714):             w0, loss, info = optimize.fmin_l_bfgs_b(
0.74 logistic.py(715):                 func, w0, fprime=None,
0.74 logistic.py(716):                 args=(X, target, 1. / C, sample_weight),
0.74 logistic.py(717):                 iprint=(verbose > 0) - 1, pgtol=tol, maxiter=max_iter)
0.74 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
0.74 logistic.py(339):     n_classes = Y.shape[1]
0.74 logistic.py(340):     n_features = X.shape[1]
0.74 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
0.74 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
0.74 logistic.py(343):                     dtype=X.dtype)
0.74 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
0.74 logistic.py(282):     n_classes = Y.shape[1]
0.74 logistic.py(283):     n_features = X.shape[1]
0.74 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
0.74 logistic.py(285):     w = w.reshape(n_classes, -1)
0.74 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
0.74 logistic.py(287):     if fit_intercept:
0.74 logistic.py(288):         intercept = w[:, -1]
0.74 logistic.py(289):         w = w[:, :-1]
0.74 logistic.py(292):     p = safe_sparse_dot(X, w.T)
0.74 logistic.py(293):     p += intercept
0.74 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
0.74 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
0.74 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
0.74 logistic.py(297):     p = np.exp(p, p)
0.74 logistic.py(298):     return loss, p, w
0.74 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
0.74 logistic.py(346):     diff = sample_weight * (p - Y)
0.74 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
0.74 logistic.py(348):     grad[:, :n_features] += alpha * w
0.74 logistic.py(349):     if fit_intercept:
0.74 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
0.74 logistic.py(351):     return loss, grad.ravel(), p
0.74 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
0.74 logistic.py(339):     n_classes = Y.shape[1]
0.74 logistic.py(340):     n_features = X.shape[1]
0.74 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
0.74 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
0.74 logistic.py(343):                     dtype=X.dtype)
0.74 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
0.74 logistic.py(282):     n_classes = Y.shape[1]
0.74 logistic.py(283):     n_features = X.shape[1]
0.74 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
0.74 logistic.py(285):     w = w.reshape(n_classes, -1)
0.74 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
0.74 logistic.py(287):     if fit_intercept:
0.74 logistic.py(288):         intercept = w[:, -1]
0.74 logistic.py(289):         w = w[:, :-1]
0.74 logistic.py(292):     p = safe_sparse_dot(X, w.T)
0.74 logistic.py(293):     p += intercept
0.74 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
0.74 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
0.74 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
0.74 logistic.py(297):     p = np.exp(p, p)
0.74 logistic.py(298):     return loss, p, w
0.74 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
0.74 logistic.py(346):     diff = sample_weight * (p - Y)
0.74 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
0.74 logistic.py(348):     grad[:, :n_features] += alpha * w
0.74 logistic.py(349):     if fit_intercept:
0.74 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
0.74 logistic.py(351):     return loss, grad.ravel(), p
0.74 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
0.74 logistic.py(339):     n_classes = Y.shape[1]
0.74 logistic.py(340):     n_features = X.shape[1]
0.74 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
0.74 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
0.74 logistic.py(343):                     dtype=X.dtype)
0.74 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
0.74 logistic.py(282):     n_classes = Y.shape[1]
0.74 logistic.py(283):     n_features = X.shape[1]
0.74 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
0.74 logistic.py(285):     w = w.reshape(n_classes, -1)
0.74 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
0.74 logistic.py(287):     if fit_intercept:
0.74 logistic.py(288):         intercept = w[:, -1]
0.74 logistic.py(289):         w = w[:, :-1]
0.74 logistic.py(292):     p = safe_sparse_dot(X, w.T)
0.74 logistic.py(293):     p += intercept
0.74 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
0.74 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
0.74 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
0.74 logistic.py(297):     p = np.exp(p, p)
0.74 logistic.py(298):     return loss, p, w
0.74 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
0.74 logistic.py(346):     diff = sample_weight * (p - Y)
0.74 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
0.74 logistic.py(348):     grad[:, :n_features] += alpha * w
0.74 logistic.py(349):     if fit_intercept:
0.74 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
0.74 logistic.py(351):     return loss, grad.ravel(), p
0.74 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
0.74 logistic.py(339):     n_classes = Y.shape[1]
0.74 logistic.py(340):     n_features = X.shape[1]
0.74 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
0.74 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
0.74 logistic.py(343):                     dtype=X.dtype)
0.74 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
0.74 logistic.py(282):     n_classes = Y.shape[1]
0.74 logistic.py(283):     n_features = X.shape[1]
0.74 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
0.74 logistic.py(285):     w = w.reshape(n_classes, -1)
0.74 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
0.74 logistic.py(287):     if fit_intercept:
0.74 logistic.py(288):         intercept = w[:, -1]
0.74 logistic.py(289):         w = w[:, :-1]
0.74 logistic.py(292):     p = safe_sparse_dot(X, w.T)
0.74 logistic.py(293):     p += intercept
0.74 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
0.74 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
0.74 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
0.74 logistic.py(297):     p = np.exp(p, p)
0.74 logistic.py(298):     return loss, p, w
0.74 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
0.74 logistic.py(346):     diff = sample_weight * (p - Y)
0.74 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
0.74 logistic.py(348):     grad[:, :n_features] += alpha * w
0.74 logistic.py(349):     if fit_intercept:
0.74 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
0.74 logistic.py(351):     return loss, grad.ravel(), p
0.74 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
0.74 logistic.py(339):     n_classes = Y.shape[1]
0.74 logistic.py(340):     n_features = X.shape[1]
0.74 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
0.74 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
0.74 logistic.py(343):                     dtype=X.dtype)
0.74 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
0.74 logistic.py(282):     n_classes = Y.shape[1]
0.74 logistic.py(283):     n_features = X.shape[1]
0.74 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
0.74 logistic.py(285):     w = w.reshape(n_classes, -1)
0.74 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
0.74 logistic.py(287):     if fit_intercept:
0.74 logistic.py(288):         intercept = w[:, -1]
0.74 logistic.py(289):         w = w[:, :-1]
0.74 logistic.py(292):     p = safe_sparse_dot(X, w.T)
0.74 logistic.py(293):     p += intercept
0.74 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
0.74 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
0.74 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
0.74 logistic.py(297):     p = np.exp(p, p)
0.74 logistic.py(298):     return loss, p, w
0.74 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
0.74 logistic.py(346):     diff = sample_weight * (p - Y)
0.74 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
0.74 logistic.py(348):     grad[:, :n_features] += alpha * w
0.74 logistic.py(349):     if fit_intercept:
0.74 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
0.74 logistic.py(351):     return loss, grad.ravel(), p
0.74 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
0.74 logistic.py(339):     n_classes = Y.shape[1]
0.74 logistic.py(340):     n_features = X.shape[1]
0.74 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
0.74 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
0.74 logistic.py(343):                     dtype=X.dtype)
0.74 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
0.74 logistic.py(282):     n_classes = Y.shape[1]
0.74 logistic.py(283):     n_features = X.shape[1]
0.74 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
0.74 logistic.py(285):     w = w.reshape(n_classes, -1)
0.74 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
0.74 logistic.py(287):     if fit_intercept:
0.74 logistic.py(288):         intercept = w[:, -1]
0.74 logistic.py(289):         w = w[:, :-1]
0.74 logistic.py(292):     p = safe_sparse_dot(X, w.T)
0.74 logistic.py(293):     p += intercept
0.74 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
0.74 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
0.74 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
0.74 logistic.py(297):     p = np.exp(p, p)
0.74 logistic.py(298):     return loss, p, w
0.74 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
0.74 logistic.py(346):     diff = sample_weight * (p - Y)
0.74 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
0.74 logistic.py(348):     grad[:, :n_features] += alpha * w
0.74 logistic.py(349):     if fit_intercept:
0.74 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
0.74 logistic.py(351):     return loss, grad.ravel(), p
0.74 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
0.74 logistic.py(339):     n_classes = Y.shape[1]
0.74 logistic.py(340):     n_features = X.shape[1]
0.74 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
0.74 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
0.74 logistic.py(343):                     dtype=X.dtype)
0.74 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
0.74 logistic.py(282):     n_classes = Y.shape[1]
0.74 logistic.py(283):     n_features = X.shape[1]
0.74 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
0.74 logistic.py(285):     w = w.reshape(n_classes, -1)
0.74 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
0.74 logistic.py(287):     if fit_intercept:
0.74 logistic.py(288):         intercept = w[:, -1]
0.74 logistic.py(289):         w = w[:, :-1]
0.74 logistic.py(292):     p = safe_sparse_dot(X, w.T)
0.74 logistic.py(293):     p += intercept
0.74 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
0.74 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
0.74 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
0.74 logistic.py(297):     p = np.exp(p, p)
0.74 logistic.py(298):     return loss, p, w
0.74 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
0.74 logistic.py(346):     diff = sample_weight * (p - Y)
0.74 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
0.74 logistic.py(348):     grad[:, :n_features] += alpha * w
0.74 logistic.py(349):     if fit_intercept:
0.74 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
0.74 logistic.py(351):     return loss, grad.ravel(), p
0.74 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
0.74 logistic.py(339):     n_classes = Y.shape[1]
0.74 logistic.py(340):     n_features = X.shape[1]
0.74 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
0.74 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
0.74 logistic.py(343):                     dtype=X.dtype)
0.74 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
0.74 logistic.py(282):     n_classes = Y.shape[1]
0.74 logistic.py(283):     n_features = X.shape[1]
0.74 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
0.74 logistic.py(285):     w = w.reshape(n_classes, -1)
0.74 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
0.74 logistic.py(287):     if fit_intercept:
0.74 logistic.py(288):         intercept = w[:, -1]
0.74 logistic.py(289):         w = w[:, :-1]
0.74 logistic.py(292):     p = safe_sparse_dot(X, w.T)
0.74 logistic.py(293):     p += intercept
0.74 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
0.74 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
0.74 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
0.74 logistic.py(297):     p = np.exp(p, p)
0.74 logistic.py(298):     return loss, p, w
0.74 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
0.74 logistic.py(346):     diff = sample_weight * (p - Y)
0.74 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
0.74 logistic.py(348):     grad[:, :n_features] += alpha * w
0.74 logistic.py(349):     if fit_intercept:
0.74 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
0.74 logistic.py(351):     return loss, grad.ravel(), p
0.74 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
0.74 logistic.py(339):     n_classes = Y.shape[1]
0.74 logistic.py(340):     n_features = X.shape[1]
0.74 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
0.74 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
0.74 logistic.py(343):                     dtype=X.dtype)
0.74 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
0.74 logistic.py(282):     n_classes = Y.shape[1]
0.74 logistic.py(283):     n_features = X.shape[1]
0.74 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
0.74 logistic.py(285):     w = w.reshape(n_classes, -1)
0.74 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
0.74 logistic.py(287):     if fit_intercept:
0.74 logistic.py(288):         intercept = w[:, -1]
0.74 logistic.py(289):         w = w[:, :-1]
0.74 logistic.py(292):     p = safe_sparse_dot(X, w.T)
0.74 logistic.py(293):     p += intercept
0.74 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
0.74 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
0.74 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
0.74 logistic.py(297):     p = np.exp(p, p)
0.74 logistic.py(298):     return loss, p, w
0.74 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
0.74 logistic.py(346):     diff = sample_weight * (p - Y)
0.74 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
0.74 logistic.py(348):     grad[:, :n_features] += alpha * w
0.74 logistic.py(349):     if fit_intercept:
0.74 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
0.74 logistic.py(351):     return loss, grad.ravel(), p
0.74 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
0.74 logistic.py(339):     n_classes = Y.shape[1]
0.74 logistic.py(340):     n_features = X.shape[1]
0.74 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
0.74 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
0.74 logistic.py(343):                     dtype=X.dtype)
0.74 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
0.74 logistic.py(282):     n_classes = Y.shape[1]
0.74 logistic.py(283):     n_features = X.shape[1]
0.74 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
0.74 logistic.py(285):     w = w.reshape(n_classes, -1)
0.74 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
0.74 logistic.py(287):     if fit_intercept:
0.74 logistic.py(288):         intercept = w[:, -1]
0.74 logistic.py(289):         w = w[:, :-1]
0.74 logistic.py(292):     p = safe_sparse_dot(X, w.T)
0.74 logistic.py(293):     p += intercept
0.74 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
0.74 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
0.74 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
0.74 logistic.py(297):     p = np.exp(p, p)
0.74 logistic.py(298):     return loss, p, w
0.74 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
0.74 logistic.py(346):     diff = sample_weight * (p - Y)
0.74 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
0.74 logistic.py(348):     grad[:, :n_features] += alpha * w
0.74 logistic.py(349):     if fit_intercept:
0.74 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
0.74 logistic.py(351):     return loss, grad.ravel(), p
0.74 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
0.74 logistic.py(339):     n_classes = Y.shape[1]
0.74 logistic.py(340):     n_features = X.shape[1]
0.74 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
0.74 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
0.74 logistic.py(343):                     dtype=X.dtype)
0.74 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
0.74 logistic.py(282):     n_classes = Y.shape[1]
0.74 logistic.py(283):     n_features = X.shape[1]
0.74 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
0.74 logistic.py(285):     w = w.reshape(n_classes, -1)
0.74 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
0.74 logistic.py(287):     if fit_intercept:
0.74 logistic.py(288):         intercept = w[:, -1]
0.74 logistic.py(289):         w = w[:, :-1]
0.74 logistic.py(292):     p = safe_sparse_dot(X, w.T)
0.74 logistic.py(293):     p += intercept
0.74 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
0.74 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
0.74 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
0.74 logistic.py(297):     p = np.exp(p, p)
0.74 logistic.py(298):     return loss, p, w
0.74 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
0.74 logistic.py(346):     diff = sample_weight * (p - Y)
0.74 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
0.74 logistic.py(348):     grad[:, :n_features] += alpha * w
0.74 logistic.py(349):     if fit_intercept:
0.74 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
0.74 logistic.py(351):     return loss, grad.ravel(), p
0.74 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
0.74 logistic.py(339):     n_classes = Y.shape[1]
0.74 logistic.py(340):     n_features = X.shape[1]
0.74 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
0.74 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
0.74 logistic.py(343):                     dtype=X.dtype)
0.74 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
0.74 logistic.py(282):     n_classes = Y.shape[1]
0.74 logistic.py(283):     n_features = X.shape[1]
0.74 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
0.74 logistic.py(285):     w = w.reshape(n_classes, -1)
0.74 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
0.74 logistic.py(287):     if fit_intercept:
0.74 logistic.py(288):         intercept = w[:, -1]
0.74 logistic.py(289):         w = w[:, :-1]
0.74 logistic.py(292):     p = safe_sparse_dot(X, w.T)
0.74 logistic.py(293):     p += intercept
0.74 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
0.74 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
0.74 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
0.74 logistic.py(297):     p = np.exp(p, p)
0.74 logistic.py(298):     return loss, p, w
0.74 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
0.74 logistic.py(346):     diff = sample_weight * (p - Y)
0.74 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
0.74 logistic.py(348):     grad[:, :n_features] += alpha * w
0.74 logistic.py(349):     if fit_intercept:
0.74 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
0.74 logistic.py(351):     return loss, grad.ravel(), p
0.74 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
0.74 logistic.py(339):     n_classes = Y.shape[1]
0.74 logistic.py(340):     n_features = X.shape[1]
0.74 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
0.74 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
0.74 logistic.py(343):                     dtype=X.dtype)
0.74 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
0.74 logistic.py(282):     n_classes = Y.shape[1]
0.74 logistic.py(283):     n_features = X.shape[1]
0.74 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
0.74 logistic.py(285):     w = w.reshape(n_classes, -1)
0.74 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
0.74 logistic.py(287):     if fit_intercept:
0.74 logistic.py(288):         intercept = w[:, -1]
0.74 logistic.py(289):         w = w[:, :-1]
0.74 logistic.py(292):     p = safe_sparse_dot(X, w.T)
0.74 logistic.py(293):     p += intercept
0.74 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
0.74 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
0.74 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
0.74 logistic.py(297):     p = np.exp(p, p)
0.74 logistic.py(298):     return loss, p, w
0.74 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
0.74 logistic.py(346):     diff = sample_weight * (p - Y)
0.74 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
0.74 logistic.py(348):     grad[:, :n_features] += alpha * w
0.74 logistic.py(349):     if fit_intercept:
0.74 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
0.74 logistic.py(351):     return loss, grad.ravel(), p
0.74 logistic.py(718):             if info["warnflag"] == 1:
0.74 logistic.py(723):             n_iter_i = min(info['nit'], max_iter)
0.74 logistic.py(760):         if multi_class == 'multinomial':
0.74 logistic.py(761):             multi_w0 = np.reshape(w0, (classes.size, -1))
0.74 logistic.py(762):             if classes.size == 2:
0.74 logistic.py(764):             coefs.append(multi_w0)
0.74 logistic.py(768):         n_iter[i] = n_iter_i
0.74 logistic.py(712):     for i, C in enumerate(Cs):
0.74 logistic.py(713):         if solver == 'lbfgs':
0.74 logistic.py(714):             w0, loss, info = optimize.fmin_l_bfgs_b(
0.74 logistic.py(715):                 func, w0, fprime=None,
0.74 logistic.py(716):                 args=(X, target, 1. / C, sample_weight),
0.74 logistic.py(717):                 iprint=(verbose > 0) - 1, pgtol=tol, maxiter=max_iter)
0.74 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
0.74 logistic.py(339):     n_classes = Y.shape[1]
0.74 logistic.py(340):     n_features = X.shape[1]
0.74 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
0.74 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
0.74 logistic.py(343):                     dtype=X.dtype)
0.74 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
0.74 logistic.py(282):     n_classes = Y.shape[1]
0.74 logistic.py(283):     n_features = X.shape[1]
0.74 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
0.74 logistic.py(285):     w = w.reshape(n_classes, -1)
0.74 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
0.74 logistic.py(287):     if fit_intercept:
0.74 logistic.py(288):         intercept = w[:, -1]
0.74 logistic.py(289):         w = w[:, :-1]
0.74 logistic.py(292):     p = safe_sparse_dot(X, w.T)
0.74 logistic.py(293):     p += intercept
0.74 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
0.74 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
0.74 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
0.74 logistic.py(297):     p = np.exp(p, p)
0.74 logistic.py(298):     return loss, p, w
0.74 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
0.74 logistic.py(346):     diff = sample_weight * (p - Y)
0.74 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
0.74 logistic.py(348):     grad[:, :n_features] += alpha * w
0.74 logistic.py(349):     if fit_intercept:
0.74 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
0.74 logistic.py(351):     return loss, grad.ravel(), p
0.74 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
0.74 logistic.py(339):     n_classes = Y.shape[1]
0.74 logistic.py(340):     n_features = X.shape[1]
0.74 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
0.74 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
0.74 logistic.py(343):                     dtype=X.dtype)
0.74 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
0.74 logistic.py(282):     n_classes = Y.shape[1]
0.74 logistic.py(283):     n_features = X.shape[1]
0.74 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
0.74 logistic.py(285):     w = w.reshape(n_classes, -1)
0.74 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
0.74 logistic.py(287):     if fit_intercept:
0.74 logistic.py(288):         intercept = w[:, -1]
0.74 logistic.py(289):         w = w[:, :-1]
0.74 logistic.py(292):     p = safe_sparse_dot(X, w.T)
0.74 logistic.py(293):     p += intercept
0.74 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
0.74 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
0.74 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
0.74 logistic.py(297):     p = np.exp(p, p)
0.74 logistic.py(298):     return loss, p, w
0.74 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
0.74 logistic.py(346):     diff = sample_weight * (p - Y)
0.74 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
0.74 logistic.py(348):     grad[:, :n_features] += alpha * w
0.74 logistic.py(349):     if fit_intercept:
0.74 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
0.74 logistic.py(351):     return loss, grad.ravel(), p
0.74 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
0.74 logistic.py(339):     n_classes = Y.shape[1]
0.75 logistic.py(340):     n_features = X.shape[1]
0.75 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
0.75 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
0.75 logistic.py(343):                     dtype=X.dtype)
0.75 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
0.75 logistic.py(282):     n_classes = Y.shape[1]
0.75 logistic.py(283):     n_features = X.shape[1]
0.75 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
0.75 logistic.py(285):     w = w.reshape(n_classes, -1)
0.75 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
0.75 logistic.py(287):     if fit_intercept:
0.75 logistic.py(288):         intercept = w[:, -1]
0.75 logistic.py(289):         w = w[:, :-1]
0.75 logistic.py(292):     p = safe_sparse_dot(X, w.T)
0.75 logistic.py(293):     p += intercept
0.75 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
0.75 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
0.75 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
0.75 logistic.py(297):     p = np.exp(p, p)
0.75 logistic.py(298):     return loss, p, w
0.75 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
0.75 logistic.py(346):     diff = sample_weight * (p - Y)
0.75 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
0.75 logistic.py(348):     grad[:, :n_features] += alpha * w
0.75 logistic.py(349):     if fit_intercept:
0.75 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
0.75 logistic.py(351):     return loss, grad.ravel(), p
0.75 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
0.75 logistic.py(339):     n_classes = Y.shape[1]
0.75 logistic.py(340):     n_features = X.shape[1]
0.75 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
0.75 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
0.75 logistic.py(343):                     dtype=X.dtype)
0.75 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
0.75 logistic.py(282):     n_classes = Y.shape[1]
0.75 logistic.py(283):     n_features = X.shape[1]
0.75 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
0.75 logistic.py(285):     w = w.reshape(n_classes, -1)
0.75 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
0.75 logistic.py(287):     if fit_intercept:
0.75 logistic.py(288):         intercept = w[:, -1]
0.75 logistic.py(289):         w = w[:, :-1]
0.75 logistic.py(292):     p = safe_sparse_dot(X, w.T)
0.75 logistic.py(293):     p += intercept
0.75 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
0.75 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
0.75 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
0.75 logistic.py(297):     p = np.exp(p, p)
0.75 logistic.py(298):     return loss, p, w
0.75 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
0.75 logistic.py(346):     diff = sample_weight * (p - Y)
0.75 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
0.75 logistic.py(348):     grad[:, :n_features] += alpha * w
0.75 logistic.py(349):     if fit_intercept:
0.75 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
0.75 logistic.py(351):     return loss, grad.ravel(), p
0.75 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
0.75 logistic.py(339):     n_classes = Y.shape[1]
0.75 logistic.py(340):     n_features = X.shape[1]
0.75 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
0.75 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
0.75 logistic.py(343):                     dtype=X.dtype)
0.75 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
0.75 logistic.py(282):     n_classes = Y.shape[1]
0.75 logistic.py(283):     n_features = X.shape[1]
0.75 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
0.75 logistic.py(285):     w = w.reshape(n_classes, -1)
0.75 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
0.75 logistic.py(287):     if fit_intercept:
0.75 logistic.py(288):         intercept = w[:, -1]
0.75 logistic.py(289):         w = w[:, :-1]
0.75 logistic.py(292):     p = safe_sparse_dot(X, w.T)
0.75 logistic.py(293):     p += intercept
0.75 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
0.75 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
0.75 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
0.75 logistic.py(297):     p = np.exp(p, p)
0.75 logistic.py(298):     return loss, p, w
0.75 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
0.75 logistic.py(346):     diff = sample_weight * (p - Y)
0.75 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
0.75 logistic.py(348):     grad[:, :n_features] += alpha * w
0.75 logistic.py(349):     if fit_intercept:
0.75 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
0.75 logistic.py(351):     return loss, grad.ravel(), p
0.75 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
0.75 logistic.py(339):     n_classes = Y.shape[1]
0.75 logistic.py(340):     n_features = X.shape[1]
0.75 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
0.75 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
0.75 logistic.py(343):                     dtype=X.dtype)
0.75 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
0.75 logistic.py(282):     n_classes = Y.shape[1]
0.75 logistic.py(283):     n_features = X.shape[1]
0.75 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
0.75 logistic.py(285):     w = w.reshape(n_classes, -1)
0.75 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
0.75 logistic.py(287):     if fit_intercept:
0.75 logistic.py(288):         intercept = w[:, -1]
0.75 logistic.py(289):         w = w[:, :-1]
0.75 logistic.py(292):     p = safe_sparse_dot(X, w.T)
0.75 logistic.py(293):     p += intercept
0.75 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
0.75 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
0.75 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
0.75 logistic.py(297):     p = np.exp(p, p)
0.75 logistic.py(298):     return loss, p, w
0.75 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
0.75 logistic.py(346):     diff = sample_weight * (p - Y)
0.75 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
0.75 logistic.py(348):     grad[:, :n_features] += alpha * w
0.75 logistic.py(349):     if fit_intercept:
0.75 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
0.75 logistic.py(351):     return loss, grad.ravel(), p
0.75 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
0.75 logistic.py(339):     n_classes = Y.shape[1]
0.75 logistic.py(340):     n_features = X.shape[1]
0.75 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
0.75 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
0.75 logistic.py(343):                     dtype=X.dtype)
0.75 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
0.75 logistic.py(282):     n_classes = Y.shape[1]
0.75 logistic.py(283):     n_features = X.shape[1]
0.75 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
0.75 logistic.py(285):     w = w.reshape(n_classes, -1)
0.75 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
0.75 logistic.py(287):     if fit_intercept:
0.75 logistic.py(288):         intercept = w[:, -1]
0.75 logistic.py(289):         w = w[:, :-1]
0.75 logistic.py(292):     p = safe_sparse_dot(X, w.T)
0.75 logistic.py(293):     p += intercept
0.75 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
0.75 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
0.75 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
0.75 logistic.py(297):     p = np.exp(p, p)
0.75 logistic.py(298):     return loss, p, w
0.75 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
0.75 logistic.py(346):     diff = sample_weight * (p - Y)
0.75 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
0.75 logistic.py(348):     grad[:, :n_features] += alpha * w
0.75 logistic.py(349):     if fit_intercept:
0.75 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
0.75 logistic.py(351):     return loss, grad.ravel(), p
0.75 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
0.75 logistic.py(339):     n_classes = Y.shape[1]
0.75 logistic.py(340):     n_features = X.shape[1]
0.75 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
0.75 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
0.75 logistic.py(343):                     dtype=X.dtype)
0.75 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
0.75 logistic.py(282):     n_classes = Y.shape[1]
0.75 logistic.py(283):     n_features = X.shape[1]
0.75 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
0.75 logistic.py(285):     w = w.reshape(n_classes, -1)
0.75 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
0.75 logistic.py(287):     if fit_intercept:
0.75 logistic.py(288):         intercept = w[:, -1]
0.75 logistic.py(289):         w = w[:, :-1]
0.75 logistic.py(292):     p = safe_sparse_dot(X, w.T)
0.75 logistic.py(293):     p += intercept
0.75 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
0.75 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
0.75 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
0.75 logistic.py(297):     p = np.exp(p, p)
0.75 logistic.py(298):     return loss, p, w
0.75 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
0.75 logistic.py(346):     diff = sample_weight * (p - Y)
0.75 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
0.75 logistic.py(348):     grad[:, :n_features] += alpha * w
0.75 logistic.py(349):     if fit_intercept:
0.75 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
0.75 logistic.py(351):     return loss, grad.ravel(), p
0.75 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
0.75 logistic.py(339):     n_classes = Y.shape[1]
0.75 logistic.py(340):     n_features = X.shape[1]
0.75 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
0.75 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
0.75 logistic.py(343):                     dtype=X.dtype)
0.75 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
0.75 logistic.py(282):     n_classes = Y.shape[1]
0.75 logistic.py(283):     n_features = X.shape[1]
0.75 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
0.75 logistic.py(285):     w = w.reshape(n_classes, -1)
0.75 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
0.75 logistic.py(287):     if fit_intercept:
0.75 logistic.py(288):         intercept = w[:, -1]
0.75 logistic.py(289):         w = w[:, :-1]
0.75 logistic.py(292):     p = safe_sparse_dot(X, w.T)
0.75 logistic.py(293):     p += intercept
0.75 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
0.75 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
0.75 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
0.75 logistic.py(297):     p = np.exp(p, p)
0.75 logistic.py(298):     return loss, p, w
0.75 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
0.75 logistic.py(346):     diff = sample_weight * (p - Y)
0.75 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
0.75 logistic.py(348):     grad[:, :n_features] += alpha * w
0.75 logistic.py(349):     if fit_intercept:
0.75 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
0.75 logistic.py(351):     return loss, grad.ravel(), p
0.75 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
0.75 logistic.py(339):     n_classes = Y.shape[1]
0.75 logistic.py(340):     n_features = X.shape[1]
0.75 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
0.75 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
0.75 logistic.py(343):                     dtype=X.dtype)
0.75 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
0.75 logistic.py(282):     n_classes = Y.shape[1]
0.75 logistic.py(283):     n_features = X.shape[1]
0.75 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
0.75 logistic.py(285):     w = w.reshape(n_classes, -1)
0.75 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
0.75 logistic.py(287):     if fit_intercept:
0.75 logistic.py(288):         intercept = w[:, -1]
0.75 logistic.py(289):         w = w[:, :-1]
0.75 logistic.py(292):     p = safe_sparse_dot(X, w.T)
0.75 logistic.py(293):     p += intercept
0.75 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
0.75 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
0.75 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
0.75 logistic.py(297):     p = np.exp(p, p)
0.75 logistic.py(298):     return loss, p, w
0.75 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
0.75 logistic.py(346):     diff = sample_weight * (p - Y)
0.75 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
0.75 logistic.py(348):     grad[:, :n_features] += alpha * w
0.75 logistic.py(349):     if fit_intercept:
0.75 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
0.75 logistic.py(351):     return loss, grad.ravel(), p
0.75 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
0.75 logistic.py(339):     n_classes = Y.shape[1]
0.75 logistic.py(340):     n_features = X.shape[1]
0.75 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
0.75 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
0.75 logistic.py(343):                     dtype=X.dtype)
0.75 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
0.75 logistic.py(282):     n_classes = Y.shape[1]
0.75 logistic.py(283):     n_features = X.shape[1]
0.75 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
0.75 logistic.py(285):     w = w.reshape(n_classes, -1)
0.75 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
0.75 logistic.py(287):     if fit_intercept:
0.75 logistic.py(288):         intercept = w[:, -1]
0.75 logistic.py(289):         w = w[:, :-1]
0.75 logistic.py(292):     p = safe_sparse_dot(X, w.T)
0.75 logistic.py(293):     p += intercept
0.75 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
0.75 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
0.75 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
0.75 logistic.py(297):     p = np.exp(p, p)
0.75 logistic.py(298):     return loss, p, w
0.75 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
0.75 logistic.py(346):     diff = sample_weight * (p - Y)
0.75 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
0.75 logistic.py(348):     grad[:, :n_features] += alpha * w
0.75 logistic.py(349):     if fit_intercept:
0.75 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
0.75 logistic.py(351):     return loss, grad.ravel(), p
0.75 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
0.75 logistic.py(339):     n_classes = Y.shape[1]
0.75 logistic.py(340):     n_features = X.shape[1]
0.75 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
0.75 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
0.75 logistic.py(343):                     dtype=X.dtype)
0.75 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
0.75 logistic.py(282):     n_classes = Y.shape[1]
0.75 logistic.py(283):     n_features = X.shape[1]
0.75 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
0.75 logistic.py(285):     w = w.reshape(n_classes, -1)
0.75 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
0.75 logistic.py(287):     if fit_intercept:
0.75 logistic.py(288):         intercept = w[:, -1]
0.75 logistic.py(289):         w = w[:, :-1]
0.75 logistic.py(292):     p = safe_sparse_dot(X, w.T)
0.75 logistic.py(293):     p += intercept
0.75 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
0.75 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
0.75 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
0.75 logistic.py(297):     p = np.exp(p, p)
0.75 logistic.py(298):     return loss, p, w
0.75 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
0.75 logistic.py(346):     diff = sample_weight * (p - Y)
0.75 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
0.75 logistic.py(348):     grad[:, :n_features] += alpha * w
0.75 logistic.py(349):     if fit_intercept:
0.75 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
0.75 logistic.py(351):     return loss, grad.ravel(), p
0.75 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
0.75 logistic.py(339):     n_classes = Y.shape[1]
0.75 logistic.py(340):     n_features = X.shape[1]
0.75 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
0.75 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
0.75 logistic.py(343):                     dtype=X.dtype)
0.75 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
0.75 logistic.py(282):     n_classes = Y.shape[1]
0.75 logistic.py(283):     n_features = X.shape[1]
0.75 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
0.75 logistic.py(285):     w = w.reshape(n_classes, -1)
0.75 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
0.75 logistic.py(287):     if fit_intercept:
0.75 logistic.py(288):         intercept = w[:, -1]
0.75 logistic.py(289):         w = w[:, :-1]
0.75 logistic.py(292):     p = safe_sparse_dot(X, w.T)
0.75 logistic.py(293):     p += intercept
0.75 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
0.75 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
0.75 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
0.75 logistic.py(297):     p = np.exp(p, p)
0.75 logistic.py(298):     return loss, p, w
0.75 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
0.75 logistic.py(346):     diff = sample_weight * (p - Y)
0.75 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
0.75 logistic.py(348):     grad[:, :n_features] += alpha * w
0.75 logistic.py(349):     if fit_intercept:
0.75 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
0.75 logistic.py(351):     return loss, grad.ravel(), p
0.75 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
0.75 logistic.py(339):     n_classes = Y.shape[1]
0.75 logistic.py(340):     n_features = X.shape[1]
0.75 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
0.75 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
0.75 logistic.py(343):                     dtype=X.dtype)
0.75 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
0.75 logistic.py(282):     n_classes = Y.shape[1]
0.75 logistic.py(283):     n_features = X.shape[1]
0.75 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
0.75 logistic.py(285):     w = w.reshape(n_classes, -1)
0.75 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
0.75 logistic.py(287):     if fit_intercept:
0.75 logistic.py(288):         intercept = w[:, -1]
0.75 logistic.py(289):         w = w[:, :-1]
0.75 logistic.py(292):     p = safe_sparse_dot(X, w.T)
0.75 logistic.py(293):     p += intercept
0.75 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
0.75 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
0.75 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
0.75 logistic.py(297):     p = np.exp(p, p)
0.75 logistic.py(298):     return loss, p, w
0.75 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
0.75 logistic.py(346):     diff = sample_weight * (p - Y)
0.75 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
0.75 logistic.py(348):     grad[:, :n_features] += alpha * w
0.75 logistic.py(349):     if fit_intercept:
0.75 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
0.75 logistic.py(351):     return loss, grad.ravel(), p
0.75 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
0.75 logistic.py(339):     n_classes = Y.shape[1]
0.75 logistic.py(340):     n_features = X.shape[1]
0.75 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
0.75 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
0.75 logistic.py(343):                     dtype=X.dtype)
0.75 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
0.75 logistic.py(282):     n_classes = Y.shape[1]
0.75 logistic.py(283):     n_features = X.shape[1]
0.75 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
0.75 logistic.py(285):     w = w.reshape(n_classes, -1)
0.75 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
0.75 logistic.py(287):     if fit_intercept:
0.75 logistic.py(288):         intercept = w[:, -1]
0.75 logistic.py(289):         w = w[:, :-1]
0.75 logistic.py(292):     p = safe_sparse_dot(X, w.T)
0.75 logistic.py(293):     p += intercept
0.75 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
0.75 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
0.75 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
0.75 logistic.py(297):     p = np.exp(p, p)
0.75 logistic.py(298):     return loss, p, w
0.75 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
0.75 logistic.py(346):     diff = sample_weight * (p - Y)
0.75 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
0.75 logistic.py(348):     grad[:, :n_features] += alpha * w
0.75 logistic.py(349):     if fit_intercept:
0.75 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
0.75 logistic.py(351):     return loss, grad.ravel(), p
0.75 logistic.py(718):             if info["warnflag"] == 1:
0.75 logistic.py(723):             n_iter_i = min(info['nit'], max_iter)
0.75 logistic.py(760):         if multi_class == 'multinomial':
0.75 logistic.py(761):             multi_w0 = np.reshape(w0, (classes.size, -1))
0.75 logistic.py(762):             if classes.size == 2:
0.75 logistic.py(764):             coefs.append(multi_w0)
0.75 logistic.py(768):         n_iter[i] = n_iter_i
0.75 logistic.py(712):     for i, C in enumerate(Cs):
0.75 logistic.py(713):         if solver == 'lbfgs':
0.75 logistic.py(714):             w0, loss, info = optimize.fmin_l_bfgs_b(
0.75 logistic.py(715):                 func, w0, fprime=None,
0.75 logistic.py(716):                 args=(X, target, 1. / C, sample_weight),
0.75 logistic.py(717):                 iprint=(verbose > 0) - 1, pgtol=tol, maxiter=max_iter)
0.75 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
0.75 logistic.py(339):     n_classes = Y.shape[1]
0.75 logistic.py(340):     n_features = X.shape[1]
0.75 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
0.75 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
0.75 logistic.py(343):                     dtype=X.dtype)
0.75 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
0.75 logistic.py(282):     n_classes = Y.shape[1]
0.75 logistic.py(283):     n_features = X.shape[1]
0.75 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
0.75 logistic.py(285):     w = w.reshape(n_classes, -1)
0.75 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
0.75 logistic.py(287):     if fit_intercept:
0.75 logistic.py(288):         intercept = w[:, -1]
0.75 logistic.py(289):         w = w[:, :-1]
0.75 logistic.py(292):     p = safe_sparse_dot(X, w.T)
0.75 logistic.py(293):     p += intercept
0.75 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
0.75 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
0.75 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
0.75 logistic.py(297):     p = np.exp(p, p)
0.75 logistic.py(298):     return loss, p, w
0.75 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
0.75 logistic.py(346):     diff = sample_weight * (p - Y)
0.75 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
0.75 logistic.py(348):     grad[:, :n_features] += alpha * w
0.75 logistic.py(349):     if fit_intercept:
0.75 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
0.75 logistic.py(351):     return loss, grad.ravel(), p
0.75 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
0.75 logistic.py(339):     n_classes = Y.shape[1]
0.75 logistic.py(340):     n_features = X.shape[1]
0.75 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
0.75 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
0.75 logistic.py(343):                     dtype=X.dtype)
0.75 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
0.75 logistic.py(282):     n_classes = Y.shape[1]
0.75 logistic.py(283):     n_features = X.shape[1]
0.75 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
0.75 logistic.py(285):     w = w.reshape(n_classes, -1)
0.75 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
0.75 logistic.py(287):     if fit_intercept:
0.75 logistic.py(288):         intercept = w[:, -1]
0.75 logistic.py(289):         w = w[:, :-1]
0.75 logistic.py(292):     p = safe_sparse_dot(X, w.T)
0.75 logistic.py(293):     p += intercept
0.75 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
0.75 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
0.75 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
0.75 logistic.py(297):     p = np.exp(p, p)
0.75 logistic.py(298):     return loss, p, w
0.75 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
0.75 logistic.py(346):     diff = sample_weight * (p - Y)
0.75 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
0.75 logistic.py(348):     grad[:, :n_features] += alpha * w
0.75 logistic.py(349):     if fit_intercept:
0.75 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
0.75 logistic.py(351):     return loss, grad.ravel(), p
0.75 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
0.75 logistic.py(339):     n_classes = Y.shape[1]
0.75 logistic.py(340):     n_features = X.shape[1]
0.75 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
0.75 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
0.75 logistic.py(343):                     dtype=X.dtype)
0.75 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
0.75 logistic.py(282):     n_classes = Y.shape[1]
0.75 logistic.py(283):     n_features = X.shape[1]
0.75 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
0.75 logistic.py(285):     w = w.reshape(n_classes, -1)
0.75 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
0.75 logistic.py(287):     if fit_intercept:
0.75 logistic.py(288):         intercept = w[:, -1]
0.75 logistic.py(289):         w = w[:, :-1]
0.75 logistic.py(292):     p = safe_sparse_dot(X, w.T)
0.75 logistic.py(293):     p += intercept
0.75 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
0.75 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
0.75 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
0.75 logistic.py(297):     p = np.exp(p, p)
0.75 logistic.py(298):     return loss, p, w
0.75 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
0.75 logistic.py(346):     diff = sample_weight * (p - Y)
0.75 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
0.75 logistic.py(348):     grad[:, :n_features] += alpha * w
0.75 logistic.py(349):     if fit_intercept:
0.75 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
0.75 logistic.py(351):     return loss, grad.ravel(), p
0.75 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
0.75 logistic.py(339):     n_classes = Y.shape[1]
0.75 logistic.py(340):     n_features = X.shape[1]
0.75 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
0.75 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
0.75 logistic.py(343):                     dtype=X.dtype)
0.75 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
0.76 logistic.py(282):     n_classes = Y.shape[1]
0.76 logistic.py(283):     n_features = X.shape[1]
0.76 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
0.76 logistic.py(285):     w = w.reshape(n_classes, -1)
0.76 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
0.76 logistic.py(287):     if fit_intercept:
0.76 logistic.py(288):         intercept = w[:, -1]
0.76 logistic.py(289):         w = w[:, :-1]
0.76 logistic.py(292):     p = safe_sparse_dot(X, w.T)
0.76 logistic.py(293):     p += intercept
0.76 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
0.76 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
0.76 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
0.76 logistic.py(297):     p = np.exp(p, p)
0.76 logistic.py(298):     return loss, p, w
0.76 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
0.76 logistic.py(346):     diff = sample_weight * (p - Y)
0.76 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
0.76 logistic.py(348):     grad[:, :n_features] += alpha * w
0.76 logistic.py(349):     if fit_intercept:
0.76 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
0.76 logistic.py(351):     return loss, grad.ravel(), p
0.76 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
0.76 logistic.py(339):     n_classes = Y.shape[1]
0.76 logistic.py(340):     n_features = X.shape[1]
0.76 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
0.76 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
0.76 logistic.py(343):                     dtype=X.dtype)
0.76 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
0.76 logistic.py(282):     n_classes = Y.shape[1]
0.76 logistic.py(283):     n_features = X.shape[1]
0.76 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
0.76 logistic.py(285):     w = w.reshape(n_classes, -1)
0.76 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
0.76 logistic.py(287):     if fit_intercept:
0.76 logistic.py(288):         intercept = w[:, -1]
0.76 logistic.py(289):         w = w[:, :-1]
0.76 logistic.py(292):     p = safe_sparse_dot(X, w.T)
0.76 logistic.py(293):     p += intercept
0.76 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
0.76 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
0.76 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
0.76 logistic.py(297):     p = np.exp(p, p)
0.76 logistic.py(298):     return loss, p, w
0.76 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
0.76 logistic.py(346):     diff = sample_weight * (p - Y)
0.76 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
0.76 logistic.py(348):     grad[:, :n_features] += alpha * w
0.76 logistic.py(349):     if fit_intercept:
0.76 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
0.76 logistic.py(351):     return loss, grad.ravel(), p
0.76 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
0.76 logistic.py(339):     n_classes = Y.shape[1]
0.76 logistic.py(340):     n_features = X.shape[1]
0.76 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
0.76 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
0.76 logistic.py(343):                     dtype=X.dtype)
0.76 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
0.76 logistic.py(282):     n_classes = Y.shape[1]
0.76 logistic.py(283):     n_features = X.shape[1]
0.76 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
0.76 logistic.py(285):     w = w.reshape(n_classes, -1)
0.76 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
0.76 logistic.py(287):     if fit_intercept:
0.76 logistic.py(288):         intercept = w[:, -1]
0.76 logistic.py(289):         w = w[:, :-1]
0.76 logistic.py(292):     p = safe_sparse_dot(X, w.T)
0.76 logistic.py(293):     p += intercept
0.76 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
0.76 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
0.76 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
0.76 logistic.py(297):     p = np.exp(p, p)
0.76 logistic.py(298):     return loss, p, w
0.76 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
0.76 logistic.py(346):     diff = sample_weight * (p - Y)
0.76 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
0.76 logistic.py(348):     grad[:, :n_features] += alpha * w
0.76 logistic.py(349):     if fit_intercept:
0.76 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
0.76 logistic.py(351):     return loss, grad.ravel(), p
0.76 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
0.76 logistic.py(339):     n_classes = Y.shape[1]
0.76 logistic.py(340):     n_features = X.shape[1]
0.76 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
0.76 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
0.76 logistic.py(343):                     dtype=X.dtype)
0.76 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
0.76 logistic.py(282):     n_classes = Y.shape[1]
0.76 logistic.py(283):     n_features = X.shape[1]
0.76 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
0.76 logistic.py(285):     w = w.reshape(n_classes, -1)
0.76 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
0.76 logistic.py(287):     if fit_intercept:
0.76 logistic.py(288):         intercept = w[:, -1]
0.76 logistic.py(289):         w = w[:, :-1]
0.76 logistic.py(292):     p = safe_sparse_dot(X, w.T)
0.76 logistic.py(293):     p += intercept
0.76 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
0.76 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
0.76 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
0.76 logistic.py(297):     p = np.exp(p, p)
0.76 logistic.py(298):     return loss, p, w
0.76 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
0.76 logistic.py(346):     diff = sample_weight * (p - Y)
0.76 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
0.76 logistic.py(348):     grad[:, :n_features] += alpha * w
0.76 logistic.py(349):     if fit_intercept:
0.76 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
0.76 logistic.py(351):     return loss, grad.ravel(), p
0.76 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
0.76 logistic.py(339):     n_classes = Y.shape[1]
0.76 logistic.py(340):     n_features = X.shape[1]
0.76 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
0.76 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
0.76 logistic.py(343):                     dtype=X.dtype)
0.76 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
0.76 logistic.py(282):     n_classes = Y.shape[1]
0.76 logistic.py(283):     n_features = X.shape[1]
0.76 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
0.76 logistic.py(285):     w = w.reshape(n_classes, -1)
0.76 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
0.76 logistic.py(287):     if fit_intercept:
0.76 logistic.py(288):         intercept = w[:, -1]
0.76 logistic.py(289):         w = w[:, :-1]
0.76 logistic.py(292):     p = safe_sparse_dot(X, w.T)
0.76 logistic.py(293):     p += intercept
0.76 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
0.76 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
0.76 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
0.76 logistic.py(297):     p = np.exp(p, p)
0.76 logistic.py(298):     return loss, p, w
0.76 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
0.76 logistic.py(346):     diff = sample_weight * (p - Y)
0.76 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
0.76 logistic.py(348):     grad[:, :n_features] += alpha * w
0.76 logistic.py(349):     if fit_intercept:
0.76 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
0.76 logistic.py(351):     return loss, grad.ravel(), p
0.76 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
0.76 logistic.py(339):     n_classes = Y.shape[1]
0.76 logistic.py(340):     n_features = X.shape[1]
0.76 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
0.76 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
0.76 logistic.py(343):                     dtype=X.dtype)
0.76 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
0.76 logistic.py(282):     n_classes = Y.shape[1]
0.76 logistic.py(283):     n_features = X.shape[1]
0.76 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
0.76 logistic.py(285):     w = w.reshape(n_classes, -1)
0.76 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
0.76 logistic.py(287):     if fit_intercept:
0.76 logistic.py(288):         intercept = w[:, -1]
0.76 logistic.py(289):         w = w[:, :-1]
0.76 logistic.py(292):     p = safe_sparse_dot(X, w.T)
0.76 logistic.py(293):     p += intercept
0.76 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
0.76 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
0.76 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
0.76 logistic.py(297):     p = np.exp(p, p)
0.76 logistic.py(298):     return loss, p, w
0.76 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
0.76 logistic.py(346):     diff = sample_weight * (p - Y)
0.76 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
0.76 logistic.py(348):     grad[:, :n_features] += alpha * w
0.76 logistic.py(349):     if fit_intercept:
0.76 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
0.76 logistic.py(351):     return loss, grad.ravel(), p
0.76 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
0.76 logistic.py(339):     n_classes = Y.shape[1]
0.76 logistic.py(340):     n_features = X.shape[1]
0.76 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
0.76 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
0.76 logistic.py(343):                     dtype=X.dtype)
0.76 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
0.76 logistic.py(282):     n_classes = Y.shape[1]
0.76 logistic.py(283):     n_features = X.shape[1]
0.76 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
0.76 logistic.py(285):     w = w.reshape(n_classes, -1)
0.76 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
0.76 logistic.py(287):     if fit_intercept:
0.76 logistic.py(288):         intercept = w[:, -1]
0.76 logistic.py(289):         w = w[:, :-1]
0.76 logistic.py(292):     p = safe_sparse_dot(X, w.T)
0.76 logistic.py(293):     p += intercept
0.76 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
0.76 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
0.76 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
0.76 logistic.py(297):     p = np.exp(p, p)
0.76 logistic.py(298):     return loss, p, w
0.76 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
0.76 logistic.py(346):     diff = sample_weight * (p - Y)
0.76 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
0.76 logistic.py(348):     grad[:, :n_features] += alpha * w
0.76 logistic.py(349):     if fit_intercept:
0.76 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
0.76 logistic.py(351):     return loss, grad.ravel(), p
0.76 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
0.76 logistic.py(339):     n_classes = Y.shape[1]
0.76 logistic.py(340):     n_features = X.shape[1]
0.76 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
0.76 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
0.76 logistic.py(343):                     dtype=X.dtype)
0.76 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
0.76 logistic.py(282):     n_classes = Y.shape[1]
0.76 logistic.py(283):     n_features = X.shape[1]
0.76 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
0.76 logistic.py(285):     w = w.reshape(n_classes, -1)
0.76 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
0.76 logistic.py(287):     if fit_intercept:
0.76 logistic.py(288):         intercept = w[:, -1]
0.76 logistic.py(289):         w = w[:, :-1]
0.76 logistic.py(292):     p = safe_sparse_dot(X, w.T)
0.76 logistic.py(293):     p += intercept
0.76 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
0.76 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
0.76 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
0.76 logistic.py(297):     p = np.exp(p, p)
0.76 logistic.py(298):     return loss, p, w
0.76 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
0.76 logistic.py(346):     diff = sample_weight * (p - Y)
0.76 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
0.76 logistic.py(348):     grad[:, :n_features] += alpha * w
0.76 logistic.py(349):     if fit_intercept:
0.76 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
0.76 logistic.py(351):     return loss, grad.ravel(), p
0.76 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
0.76 logistic.py(339):     n_classes = Y.shape[1]
0.76 logistic.py(340):     n_features = X.shape[1]
0.76 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
0.76 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
0.76 logistic.py(343):                     dtype=X.dtype)
0.76 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
0.76 logistic.py(282):     n_classes = Y.shape[1]
0.76 logistic.py(283):     n_features = X.shape[1]
0.76 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
0.76 logistic.py(285):     w = w.reshape(n_classes, -1)
0.76 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
0.76 logistic.py(287):     if fit_intercept:
0.76 logistic.py(288):         intercept = w[:, -1]
0.76 logistic.py(289):         w = w[:, :-1]
0.76 logistic.py(292):     p = safe_sparse_dot(X, w.T)
0.76 logistic.py(293):     p += intercept
0.76 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
0.76 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
0.76 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
0.76 logistic.py(297):     p = np.exp(p, p)
0.76 logistic.py(298):     return loss, p, w
0.76 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
0.76 logistic.py(346):     diff = sample_weight * (p - Y)
0.76 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
0.76 logistic.py(348):     grad[:, :n_features] += alpha * w
0.76 logistic.py(349):     if fit_intercept:
0.76 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
0.76 logistic.py(351):     return loss, grad.ravel(), p
0.76 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
0.76 logistic.py(339):     n_classes = Y.shape[1]
0.76 logistic.py(340):     n_features = X.shape[1]
0.76 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
0.76 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
0.76 logistic.py(343):                     dtype=X.dtype)
0.76 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
0.76 logistic.py(282):     n_classes = Y.shape[1]
0.76 logistic.py(283):     n_features = X.shape[1]
0.76 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
0.76 logistic.py(285):     w = w.reshape(n_classes, -1)
0.76 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
0.76 logistic.py(287):     if fit_intercept:
0.76 logistic.py(288):         intercept = w[:, -1]
0.76 logistic.py(289):         w = w[:, :-1]
0.76 logistic.py(292):     p = safe_sparse_dot(X, w.T)
0.76 logistic.py(293):     p += intercept
0.76 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
0.76 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
0.76 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
0.76 logistic.py(297):     p = np.exp(p, p)
0.76 logistic.py(298):     return loss, p, w
0.76 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
0.76 logistic.py(346):     diff = sample_weight * (p - Y)
0.76 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
0.76 logistic.py(348):     grad[:, :n_features] += alpha * w
0.76 logistic.py(349):     if fit_intercept:
0.76 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
0.76 logistic.py(351):     return loss, grad.ravel(), p
0.76 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
0.76 logistic.py(339):     n_classes = Y.shape[1]
0.76 logistic.py(340):     n_features = X.shape[1]
0.76 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
0.76 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
0.76 logistic.py(343):                     dtype=X.dtype)
0.76 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
0.76 logistic.py(282):     n_classes = Y.shape[1]
0.76 logistic.py(283):     n_features = X.shape[1]
0.76 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
0.76 logistic.py(285):     w = w.reshape(n_classes, -1)
0.76 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
0.76 logistic.py(287):     if fit_intercept:
0.76 logistic.py(288):         intercept = w[:, -1]
0.76 logistic.py(289):         w = w[:, :-1]
0.76 logistic.py(292):     p = safe_sparse_dot(X, w.T)
0.76 logistic.py(293):     p += intercept
0.76 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
0.76 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
0.76 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
0.76 logistic.py(297):     p = np.exp(p, p)
0.76 logistic.py(298):     return loss, p, w
0.76 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
0.76 logistic.py(346):     diff = sample_weight * (p - Y)
0.76 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
0.76 logistic.py(348):     grad[:, :n_features] += alpha * w
0.76 logistic.py(349):     if fit_intercept:
0.76 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
0.76 logistic.py(351):     return loss, grad.ravel(), p
0.76 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
0.76 logistic.py(339):     n_classes = Y.shape[1]
0.76 logistic.py(340):     n_features = X.shape[1]
0.76 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
0.76 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
0.76 logistic.py(343):                     dtype=X.dtype)
0.76 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
0.76 logistic.py(282):     n_classes = Y.shape[1]
0.76 logistic.py(283):     n_features = X.shape[1]
0.76 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
0.76 logistic.py(285):     w = w.reshape(n_classes, -1)
0.76 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
0.76 logistic.py(287):     if fit_intercept:
0.76 logistic.py(288):         intercept = w[:, -1]
0.76 logistic.py(289):         w = w[:, :-1]
0.76 logistic.py(292):     p = safe_sparse_dot(X, w.T)
0.76 logistic.py(293):     p += intercept
0.76 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
0.76 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
0.76 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
0.76 logistic.py(297):     p = np.exp(p, p)
0.76 logistic.py(298):     return loss, p, w
0.76 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
0.76 logistic.py(346):     diff = sample_weight * (p - Y)
0.76 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
0.76 logistic.py(348):     grad[:, :n_features] += alpha * w
0.76 logistic.py(349):     if fit_intercept:
0.76 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
0.76 logistic.py(351):     return loss, grad.ravel(), p
0.76 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
0.76 logistic.py(339):     n_classes = Y.shape[1]
0.76 logistic.py(340):     n_features = X.shape[1]
0.76 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
0.76 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
0.76 logistic.py(343):                     dtype=X.dtype)
0.76 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
0.76 logistic.py(282):     n_classes = Y.shape[1]
0.76 logistic.py(283):     n_features = X.shape[1]
0.76 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
0.76 logistic.py(285):     w = w.reshape(n_classes, -1)
0.76 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
0.76 logistic.py(287):     if fit_intercept:
0.76 logistic.py(288):         intercept = w[:, -1]
0.76 logistic.py(289):         w = w[:, :-1]
0.76 logistic.py(292):     p = safe_sparse_dot(X, w.T)
0.76 logistic.py(293):     p += intercept
0.76 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
0.76 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
0.76 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
0.76 logistic.py(297):     p = np.exp(p, p)
0.76 logistic.py(298):     return loss, p, w
0.76 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
0.76 logistic.py(346):     diff = sample_weight * (p - Y)
0.76 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
0.76 logistic.py(348):     grad[:, :n_features] += alpha * w
0.76 logistic.py(349):     if fit_intercept:
0.76 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
0.76 logistic.py(351):     return loss, grad.ravel(), p
0.76 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
0.76 logistic.py(339):     n_classes = Y.shape[1]
0.76 logistic.py(340):     n_features = X.shape[1]
0.76 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
0.76 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
0.76 logistic.py(343):                     dtype=X.dtype)
0.76 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
0.76 logistic.py(282):     n_classes = Y.shape[1]
0.76 logistic.py(283):     n_features = X.shape[1]
0.76 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
0.76 logistic.py(285):     w = w.reshape(n_classes, -1)
0.76 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
0.76 logistic.py(287):     if fit_intercept:
0.76 logistic.py(288):         intercept = w[:, -1]
0.76 logistic.py(289):         w = w[:, :-1]
0.76 logistic.py(292):     p = safe_sparse_dot(X, w.T)
0.76 logistic.py(293):     p += intercept
0.76 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
0.76 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
0.76 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
0.76 logistic.py(297):     p = np.exp(p, p)
0.76 logistic.py(298):     return loss, p, w
0.76 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
0.76 logistic.py(346):     diff = sample_weight * (p - Y)
0.76 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
0.76 logistic.py(348):     grad[:, :n_features] += alpha * w
0.76 logistic.py(349):     if fit_intercept:
0.76 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
0.76 logistic.py(351):     return loss, grad.ravel(), p
0.76 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
0.76 logistic.py(339):     n_classes = Y.shape[1]
0.76 logistic.py(340):     n_features = X.shape[1]
0.76 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
0.76 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
0.76 logistic.py(343):                     dtype=X.dtype)
0.76 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
0.76 logistic.py(282):     n_classes = Y.shape[1]
0.76 logistic.py(283):     n_features = X.shape[1]
0.76 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
0.76 logistic.py(285):     w = w.reshape(n_classes, -1)
0.76 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
0.76 logistic.py(287):     if fit_intercept:
0.76 logistic.py(288):         intercept = w[:, -1]
0.76 logistic.py(289):         w = w[:, :-1]
0.76 logistic.py(292):     p = safe_sparse_dot(X, w.T)
0.76 logistic.py(293):     p += intercept
0.76 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
0.76 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
0.76 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
0.76 logistic.py(297):     p = np.exp(p, p)
0.76 logistic.py(298):     return loss, p, w
0.76 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
0.76 logistic.py(346):     diff = sample_weight * (p - Y)
0.76 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
0.76 logistic.py(348):     grad[:, :n_features] += alpha * w
0.76 logistic.py(349):     if fit_intercept:
0.76 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
0.76 logistic.py(351):     return loss, grad.ravel(), p
0.76 logistic.py(718):             if info["warnflag"] == 1:
0.76 logistic.py(723):             n_iter_i = min(info['nit'], max_iter)
0.76 logistic.py(760):         if multi_class == 'multinomial':
0.76 logistic.py(761):             multi_w0 = np.reshape(w0, (classes.size, -1))
0.76 logistic.py(762):             if classes.size == 2:
0.76 logistic.py(764):             coefs.append(multi_w0)
0.76 logistic.py(768):         n_iter[i] = n_iter_i
0.76 logistic.py(712):     for i, C in enumerate(Cs):
0.76 logistic.py(713):         if solver == 'lbfgs':
0.76 logistic.py(714):             w0, loss, info = optimize.fmin_l_bfgs_b(
0.76 logistic.py(715):                 func, w0, fprime=None,
0.76 logistic.py(716):                 args=(X, target, 1. / C, sample_weight),
0.76 logistic.py(717):                 iprint=(verbose > 0) - 1, pgtol=tol, maxiter=max_iter)
0.76 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
0.76 logistic.py(339):     n_classes = Y.shape[1]
0.76 logistic.py(340):     n_features = X.shape[1]
0.76 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
0.76 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
0.76 logistic.py(343):                     dtype=X.dtype)
0.76 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
0.76 logistic.py(282):     n_classes = Y.shape[1]
0.76 logistic.py(283):     n_features = X.shape[1]
0.76 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
0.76 logistic.py(285):     w = w.reshape(n_classes, -1)
0.76 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
0.76 logistic.py(287):     if fit_intercept:
0.76 logistic.py(288):         intercept = w[:, -1]
0.76 logistic.py(289):         w = w[:, :-1]
0.76 logistic.py(292):     p = safe_sparse_dot(X, w.T)
0.76 logistic.py(293):     p += intercept
0.76 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
0.76 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
0.76 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
0.76 logistic.py(297):     p = np.exp(p, p)
0.76 logistic.py(298):     return loss, p, w
0.76 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
0.76 logistic.py(346):     diff = sample_weight * (p - Y)
0.76 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
0.76 logistic.py(348):     grad[:, :n_features] += alpha * w
0.76 logistic.py(349):     if fit_intercept:
0.76 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
0.76 logistic.py(351):     return loss, grad.ravel(), p
0.76 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
0.76 logistic.py(339):     n_classes = Y.shape[1]
0.76 logistic.py(340):     n_features = X.shape[1]
0.76 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
0.77 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
0.77 logistic.py(343):                     dtype=X.dtype)
0.77 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
0.77 logistic.py(282):     n_classes = Y.shape[1]
0.77 logistic.py(283):     n_features = X.shape[1]
0.77 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
0.77 logistic.py(285):     w = w.reshape(n_classes, -1)
0.77 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
0.77 logistic.py(287):     if fit_intercept:
0.77 logistic.py(288):         intercept = w[:, -1]
0.77 logistic.py(289):         w = w[:, :-1]
0.77 logistic.py(292):     p = safe_sparse_dot(X, w.T)
0.77 logistic.py(293):     p += intercept
0.77 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
0.77 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
0.77 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
0.77 logistic.py(297):     p = np.exp(p, p)
0.77 logistic.py(298):     return loss, p, w
0.77 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
0.77 logistic.py(346):     diff = sample_weight * (p - Y)
0.77 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
0.77 logistic.py(348):     grad[:, :n_features] += alpha * w
0.77 logistic.py(349):     if fit_intercept:
0.77 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
0.77 logistic.py(351):     return loss, grad.ravel(), p
0.77 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
0.77 logistic.py(339):     n_classes = Y.shape[1]
0.77 logistic.py(340):     n_features = X.shape[1]
0.77 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
0.77 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
0.77 logistic.py(343):                     dtype=X.dtype)
0.77 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
0.77 logistic.py(282):     n_classes = Y.shape[1]
0.77 logistic.py(283):     n_features = X.shape[1]
0.77 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
0.77 logistic.py(285):     w = w.reshape(n_classes, -1)
0.77 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
0.77 logistic.py(287):     if fit_intercept:
0.77 logistic.py(288):         intercept = w[:, -1]
0.77 logistic.py(289):         w = w[:, :-1]
0.77 logistic.py(292):     p = safe_sparse_dot(X, w.T)
0.77 logistic.py(293):     p += intercept
0.77 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
0.77 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
0.77 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
0.77 logistic.py(297):     p = np.exp(p, p)
0.77 logistic.py(298):     return loss, p, w
0.77 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
0.77 logistic.py(346):     diff = sample_weight * (p - Y)
0.77 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
0.77 logistic.py(348):     grad[:, :n_features] += alpha * w
0.77 logistic.py(349):     if fit_intercept:
0.77 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
0.77 logistic.py(351):     return loss, grad.ravel(), p
0.77 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
0.77 logistic.py(339):     n_classes = Y.shape[1]
0.77 logistic.py(340):     n_features = X.shape[1]
0.77 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
0.77 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
0.77 logistic.py(343):                     dtype=X.dtype)
0.77 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
0.77 logistic.py(282):     n_classes = Y.shape[1]
0.77 logistic.py(283):     n_features = X.shape[1]
0.77 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
0.77 logistic.py(285):     w = w.reshape(n_classes, -1)
0.77 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
0.77 logistic.py(287):     if fit_intercept:
0.77 logistic.py(288):         intercept = w[:, -1]
0.77 logistic.py(289):         w = w[:, :-1]
0.77 logistic.py(292):     p = safe_sparse_dot(X, w.T)
0.77 logistic.py(293):     p += intercept
0.77 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
0.77 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
0.77 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
0.77 logistic.py(297):     p = np.exp(p, p)
0.77 logistic.py(298):     return loss, p, w
0.77 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
0.77 logistic.py(346):     diff = sample_weight * (p - Y)
0.77 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
0.77 logistic.py(348):     grad[:, :n_features] += alpha * w
0.77 logistic.py(349):     if fit_intercept:
0.77 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
0.77 logistic.py(351):     return loss, grad.ravel(), p
0.77 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
0.77 logistic.py(339):     n_classes = Y.shape[1]
0.77 logistic.py(340):     n_features = X.shape[1]
0.77 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
0.77 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
0.77 logistic.py(343):                     dtype=X.dtype)
0.77 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
0.77 logistic.py(282):     n_classes = Y.shape[1]
0.77 logistic.py(283):     n_features = X.shape[1]
0.77 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
0.77 logistic.py(285):     w = w.reshape(n_classes, -1)
0.77 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
0.77 logistic.py(287):     if fit_intercept:
0.77 logistic.py(288):         intercept = w[:, -1]
0.77 logistic.py(289):         w = w[:, :-1]
0.77 logistic.py(292):     p = safe_sparse_dot(X, w.T)
0.77 logistic.py(293):     p += intercept
0.77 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
0.77 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
0.77 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
0.77 logistic.py(297):     p = np.exp(p, p)
0.77 logistic.py(298):     return loss, p, w
0.77 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
0.77 logistic.py(346):     diff = sample_weight * (p - Y)
0.77 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
0.77 logistic.py(348):     grad[:, :n_features] += alpha * w
0.77 logistic.py(349):     if fit_intercept:
0.77 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
0.77 logistic.py(351):     return loss, grad.ravel(), p
0.77 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
0.77 logistic.py(339):     n_classes = Y.shape[1]
0.77 logistic.py(340):     n_features = X.shape[1]
0.77 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
0.77 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
0.77 logistic.py(343):                     dtype=X.dtype)
0.77 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
0.77 logistic.py(282):     n_classes = Y.shape[1]
0.77 logistic.py(283):     n_features = X.shape[1]
0.77 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
0.77 logistic.py(285):     w = w.reshape(n_classes, -1)
0.77 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
0.77 logistic.py(287):     if fit_intercept:
0.77 logistic.py(288):         intercept = w[:, -1]
0.77 logistic.py(289):         w = w[:, :-1]
0.77 logistic.py(292):     p = safe_sparse_dot(X, w.T)
0.77 logistic.py(293):     p += intercept
0.77 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
0.77 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
0.77 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
0.77 logistic.py(297):     p = np.exp(p, p)
0.77 logistic.py(298):     return loss, p, w
0.77 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
0.77 logistic.py(346):     diff = sample_weight * (p - Y)
0.77 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
0.77 logistic.py(348):     grad[:, :n_features] += alpha * w
0.77 logistic.py(349):     if fit_intercept:
0.77 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
0.77 logistic.py(351):     return loss, grad.ravel(), p
0.77 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
0.77 logistic.py(339):     n_classes = Y.shape[1]
0.77 logistic.py(340):     n_features = X.shape[1]
0.77 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
0.77 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
0.77 logistic.py(343):                     dtype=X.dtype)
0.77 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
0.77 logistic.py(282):     n_classes = Y.shape[1]
0.77 logistic.py(283):     n_features = X.shape[1]
0.77 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
0.77 logistic.py(285):     w = w.reshape(n_classes, -1)
0.77 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
0.77 logistic.py(287):     if fit_intercept:
0.77 logistic.py(288):         intercept = w[:, -1]
0.77 logistic.py(289):         w = w[:, :-1]
0.77 logistic.py(292):     p = safe_sparse_dot(X, w.T)
0.77 logistic.py(293):     p += intercept
0.77 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
0.77 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
0.77 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
0.77 logistic.py(297):     p = np.exp(p, p)
0.77 logistic.py(298):     return loss, p, w
0.77 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
0.77 logistic.py(346):     diff = sample_weight * (p - Y)
0.77 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
0.77 logistic.py(348):     grad[:, :n_features] += alpha * w
0.77 logistic.py(349):     if fit_intercept:
0.77 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
0.77 logistic.py(351):     return loss, grad.ravel(), p
0.77 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
0.77 logistic.py(339):     n_classes = Y.shape[1]
0.77 logistic.py(340):     n_features = X.shape[1]
0.77 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
0.77 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
0.77 logistic.py(343):                     dtype=X.dtype)
0.77 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
0.77 logistic.py(282):     n_classes = Y.shape[1]
0.77 logistic.py(283):     n_features = X.shape[1]
0.77 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
0.77 logistic.py(285):     w = w.reshape(n_classes, -1)
0.77 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
0.77 logistic.py(287):     if fit_intercept:
0.77 logistic.py(288):         intercept = w[:, -1]
0.77 logistic.py(289):         w = w[:, :-1]
0.77 logistic.py(292):     p = safe_sparse_dot(X, w.T)
0.77 logistic.py(293):     p += intercept
0.77 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
0.77 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
0.77 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
0.77 logistic.py(297):     p = np.exp(p, p)
0.77 logistic.py(298):     return loss, p, w
0.77 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
0.77 logistic.py(346):     diff = sample_weight * (p - Y)
0.77 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
0.77 logistic.py(348):     grad[:, :n_features] += alpha * w
0.77 logistic.py(349):     if fit_intercept:
0.77 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
0.77 logistic.py(351):     return loss, grad.ravel(), p
0.77 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
0.77 logistic.py(339):     n_classes = Y.shape[1]
0.77 logistic.py(340):     n_features = X.shape[1]
0.77 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
0.77 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
0.77 logistic.py(343):                     dtype=X.dtype)
0.77 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
0.77 logistic.py(282):     n_classes = Y.shape[1]
0.77 logistic.py(283):     n_features = X.shape[1]
0.77 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
0.77 logistic.py(285):     w = w.reshape(n_classes, -1)
0.77 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
0.77 logistic.py(287):     if fit_intercept:
0.77 logistic.py(288):         intercept = w[:, -1]
0.77 logistic.py(289):         w = w[:, :-1]
0.77 logistic.py(292):     p = safe_sparse_dot(X, w.T)
0.77 logistic.py(293):     p += intercept
0.77 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
0.77 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
0.77 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
0.77 logistic.py(297):     p = np.exp(p, p)
0.77 logistic.py(298):     return loss, p, w
0.77 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
0.77 logistic.py(346):     diff = sample_weight * (p - Y)
0.77 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
0.77 logistic.py(348):     grad[:, :n_features] += alpha * w
0.77 logistic.py(349):     if fit_intercept:
0.77 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
0.77 logistic.py(351):     return loss, grad.ravel(), p
0.77 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
0.77 logistic.py(339):     n_classes = Y.shape[1]
0.77 logistic.py(340):     n_features = X.shape[1]
0.77 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
0.77 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
0.77 logistic.py(343):                     dtype=X.dtype)
0.77 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
0.77 logistic.py(282):     n_classes = Y.shape[1]
0.77 logistic.py(283):     n_features = X.shape[1]
0.77 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
0.77 logistic.py(285):     w = w.reshape(n_classes, -1)
0.77 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
0.77 logistic.py(287):     if fit_intercept:
0.77 logistic.py(288):         intercept = w[:, -1]
0.77 logistic.py(289):         w = w[:, :-1]
0.77 logistic.py(292):     p = safe_sparse_dot(X, w.T)
0.77 logistic.py(293):     p += intercept
0.77 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
0.77 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
0.77 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
0.77 logistic.py(297):     p = np.exp(p, p)
0.77 logistic.py(298):     return loss, p, w
0.77 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
0.77 logistic.py(346):     diff = sample_weight * (p - Y)
0.77 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
0.77 logistic.py(348):     grad[:, :n_features] += alpha * w
0.77 logistic.py(349):     if fit_intercept:
0.77 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
0.77 logistic.py(351):     return loss, grad.ravel(), p
0.77 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
0.77 logistic.py(339):     n_classes = Y.shape[1]
0.77 logistic.py(340):     n_features = X.shape[1]
0.77 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
0.77 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
0.77 logistic.py(343):                     dtype=X.dtype)
0.77 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
0.77 logistic.py(282):     n_classes = Y.shape[1]
0.77 logistic.py(283):     n_features = X.shape[1]
0.77 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
0.77 logistic.py(285):     w = w.reshape(n_classes, -1)
0.77 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
0.77 logistic.py(287):     if fit_intercept:
0.77 logistic.py(288):         intercept = w[:, -1]
0.77 logistic.py(289):         w = w[:, :-1]
0.77 logistic.py(292):     p = safe_sparse_dot(X, w.T)
0.77 logistic.py(293):     p += intercept
0.77 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
0.77 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
0.77 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
0.77 logistic.py(297):     p = np.exp(p, p)
0.77 logistic.py(298):     return loss, p, w
0.77 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
0.77 logistic.py(346):     diff = sample_weight * (p - Y)
0.77 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
0.77 logistic.py(348):     grad[:, :n_features] += alpha * w
0.77 logistic.py(349):     if fit_intercept:
0.77 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
0.77 logistic.py(351):     return loss, grad.ravel(), p
0.77 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
0.77 logistic.py(339):     n_classes = Y.shape[1]
0.77 logistic.py(340):     n_features = X.shape[1]
0.77 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
0.77 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
0.77 logistic.py(343):                     dtype=X.dtype)
0.77 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
0.77 logistic.py(282):     n_classes = Y.shape[1]
0.77 logistic.py(283):     n_features = X.shape[1]
0.77 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
0.77 logistic.py(285):     w = w.reshape(n_classes, -1)
0.77 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
0.77 logistic.py(287):     if fit_intercept:
0.77 logistic.py(288):         intercept = w[:, -1]
0.77 logistic.py(289):         w = w[:, :-1]
0.77 logistic.py(292):     p = safe_sparse_dot(X, w.T)
0.77 logistic.py(293):     p += intercept
0.77 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
0.77 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
0.77 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
0.77 logistic.py(297):     p = np.exp(p, p)
0.77 logistic.py(298):     return loss, p, w
0.77 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
0.77 logistic.py(346):     diff = sample_weight * (p - Y)
0.77 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
0.77 logistic.py(348):     grad[:, :n_features] += alpha * w
0.77 logistic.py(349):     if fit_intercept:
0.77 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
0.77 logistic.py(351):     return loss, grad.ravel(), p
0.77 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
0.77 logistic.py(339):     n_classes = Y.shape[1]
0.77 logistic.py(340):     n_features = X.shape[1]
0.77 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
0.77 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
0.77 logistic.py(343):                     dtype=X.dtype)
0.77 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
0.77 logistic.py(282):     n_classes = Y.shape[1]
0.77 logistic.py(283):     n_features = X.shape[1]
0.77 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
0.77 logistic.py(285):     w = w.reshape(n_classes, -1)
0.77 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
0.77 logistic.py(287):     if fit_intercept:
0.77 logistic.py(288):         intercept = w[:, -1]
0.77 logistic.py(289):         w = w[:, :-1]
0.77 logistic.py(292):     p = safe_sparse_dot(X, w.T)
0.77 logistic.py(293):     p += intercept
0.77 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
0.77 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
0.77 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
0.77 logistic.py(297):     p = np.exp(p, p)
0.77 logistic.py(298):     return loss, p, w
0.77 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
0.77 logistic.py(346):     diff = sample_weight * (p - Y)
0.77 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
0.77 logistic.py(348):     grad[:, :n_features] += alpha * w
0.77 logistic.py(349):     if fit_intercept:
0.77 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
0.77 logistic.py(351):     return loss, grad.ravel(), p
0.77 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
0.77 logistic.py(339):     n_classes = Y.shape[1]
0.77 logistic.py(340):     n_features = X.shape[1]
0.77 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
0.77 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
0.77 logistic.py(343):                     dtype=X.dtype)
0.77 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
0.77 logistic.py(282):     n_classes = Y.shape[1]
0.77 logistic.py(283):     n_features = X.shape[1]
0.77 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
0.77 logistic.py(285):     w = w.reshape(n_classes, -1)
0.77 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
0.77 logistic.py(287):     if fit_intercept:
0.77 logistic.py(288):         intercept = w[:, -1]
0.77 logistic.py(289):         w = w[:, :-1]
0.77 logistic.py(292):     p = safe_sparse_dot(X, w.T)
0.77 logistic.py(293):     p += intercept
0.77 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
0.77 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
0.77 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
0.77 logistic.py(297):     p = np.exp(p, p)
0.77 logistic.py(298):     return loss, p, w
0.77 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
0.77 logistic.py(346):     diff = sample_weight * (p - Y)
0.77 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
0.77 logistic.py(348):     grad[:, :n_features] += alpha * w
0.77 logistic.py(349):     if fit_intercept:
0.77 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
0.77 logistic.py(351):     return loss, grad.ravel(), p
0.77 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
0.77 logistic.py(339):     n_classes = Y.shape[1]
0.77 logistic.py(340):     n_features = X.shape[1]
0.77 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
0.77 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
0.77 logistic.py(343):                     dtype=X.dtype)
0.77 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
0.77 logistic.py(282):     n_classes = Y.shape[1]
0.77 logistic.py(283):     n_features = X.shape[1]
0.77 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
0.77 logistic.py(285):     w = w.reshape(n_classes, -1)
0.77 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
0.77 logistic.py(287):     if fit_intercept:
0.77 logistic.py(288):         intercept = w[:, -1]
0.77 logistic.py(289):         w = w[:, :-1]
0.77 logistic.py(292):     p = safe_sparse_dot(X, w.T)
0.77 logistic.py(293):     p += intercept
0.77 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
0.77 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
0.77 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
0.77 logistic.py(297):     p = np.exp(p, p)
0.77 logistic.py(298):     return loss, p, w
0.77 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
0.77 logistic.py(346):     diff = sample_weight * (p - Y)
0.77 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
0.77 logistic.py(348):     grad[:, :n_features] += alpha * w
0.77 logistic.py(349):     if fit_intercept:
0.77 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
0.77 logistic.py(351):     return loss, grad.ravel(), p
0.77 logistic.py(718):             if info["warnflag"] == 1:
0.77 logistic.py(723):             n_iter_i = min(info['nit'], max_iter)
0.77 logistic.py(760):         if multi_class == 'multinomial':
0.77 logistic.py(761):             multi_w0 = np.reshape(w0, (classes.size, -1))
0.77 logistic.py(762):             if classes.size == 2:
0.77 logistic.py(764):             coefs.append(multi_w0)
0.77 logistic.py(768):         n_iter[i] = n_iter_i
0.77 logistic.py(712):     for i, C in enumerate(Cs):
0.77 logistic.py(713):         if solver == 'lbfgs':
0.77 logistic.py(714):             w0, loss, info = optimize.fmin_l_bfgs_b(
0.77 logistic.py(715):                 func, w0, fprime=None,
0.77 logistic.py(716):                 args=(X, target, 1. / C, sample_weight),
0.77 logistic.py(717):                 iprint=(verbose > 0) - 1, pgtol=tol, maxiter=max_iter)
0.77 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
0.77 logistic.py(339):     n_classes = Y.shape[1]
0.77 logistic.py(340):     n_features = X.shape[1]
0.77 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
0.77 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
0.77 logistic.py(343):                     dtype=X.dtype)
0.77 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
0.77 logistic.py(282):     n_classes = Y.shape[1]
0.77 logistic.py(283):     n_features = X.shape[1]
0.77 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
0.77 logistic.py(285):     w = w.reshape(n_classes, -1)
0.77 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
0.77 logistic.py(287):     if fit_intercept:
0.77 logistic.py(288):         intercept = w[:, -1]
0.77 logistic.py(289):         w = w[:, :-1]
0.77 logistic.py(292):     p = safe_sparse_dot(X, w.T)
0.77 logistic.py(293):     p += intercept
0.77 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
0.77 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
0.77 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
0.77 logistic.py(297):     p = np.exp(p, p)
0.77 logistic.py(298):     return loss, p, w
0.77 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
0.77 logistic.py(346):     diff = sample_weight * (p - Y)
0.77 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
0.77 logistic.py(348):     grad[:, :n_features] += alpha * w
0.77 logistic.py(349):     if fit_intercept:
0.77 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
0.77 logistic.py(351):     return loss, grad.ravel(), p
0.77 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
0.77 logistic.py(339):     n_classes = Y.shape[1]
0.77 logistic.py(340):     n_features = X.shape[1]
0.77 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
0.77 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
0.77 logistic.py(343):                     dtype=X.dtype)
0.77 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
0.77 logistic.py(282):     n_classes = Y.shape[1]
0.77 logistic.py(283):     n_features = X.shape[1]
0.77 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
0.77 logistic.py(285):     w = w.reshape(n_classes, -1)
0.77 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
0.77 logistic.py(287):     if fit_intercept:
0.77 logistic.py(288):         intercept = w[:, -1]
0.77 logistic.py(289):         w = w[:, :-1]
0.77 logistic.py(292):     p = safe_sparse_dot(X, w.T)
0.77 logistic.py(293):     p += intercept
0.77 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
0.77 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
0.77 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
0.77 logistic.py(297):     p = np.exp(p, p)
0.78 logistic.py(298):     return loss, p, w
0.78 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
0.78 logistic.py(346):     diff = sample_weight * (p - Y)
0.78 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
0.78 logistic.py(348):     grad[:, :n_features] += alpha * w
0.78 logistic.py(349):     if fit_intercept:
0.78 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
0.78 logistic.py(351):     return loss, grad.ravel(), p
0.78 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
0.78 logistic.py(339):     n_classes = Y.shape[1]
0.78 logistic.py(340):     n_features = X.shape[1]
0.78 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
0.78 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
0.78 logistic.py(343):                     dtype=X.dtype)
0.78 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
0.78 logistic.py(282):     n_classes = Y.shape[1]
0.78 logistic.py(283):     n_features = X.shape[1]
0.78 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
0.78 logistic.py(285):     w = w.reshape(n_classes, -1)
0.78 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
0.78 logistic.py(287):     if fit_intercept:
0.78 logistic.py(288):         intercept = w[:, -1]
0.78 logistic.py(289):         w = w[:, :-1]
0.78 logistic.py(292):     p = safe_sparse_dot(X, w.T)
0.78 logistic.py(293):     p += intercept
0.78 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
0.78 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
0.78 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
0.78 logistic.py(297):     p = np.exp(p, p)
0.78 logistic.py(298):     return loss, p, w
0.78 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
0.78 logistic.py(346):     diff = sample_weight * (p - Y)
0.78 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
0.78 logistic.py(348):     grad[:, :n_features] += alpha * w
0.78 logistic.py(349):     if fit_intercept:
0.78 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
0.78 logistic.py(351):     return loss, grad.ravel(), p
0.78 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
0.78 logistic.py(339):     n_classes = Y.shape[1]
0.78 logistic.py(340):     n_features = X.shape[1]
0.78 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
0.78 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
0.78 logistic.py(343):                     dtype=X.dtype)
0.78 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
0.78 logistic.py(282):     n_classes = Y.shape[1]
0.78 logistic.py(283):     n_features = X.shape[1]
0.78 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
0.78 logistic.py(285):     w = w.reshape(n_classes, -1)
0.78 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
0.78 logistic.py(287):     if fit_intercept:
0.78 logistic.py(288):         intercept = w[:, -1]
0.78 logistic.py(289):         w = w[:, :-1]
0.78 logistic.py(292):     p = safe_sparse_dot(X, w.T)
0.78 logistic.py(293):     p += intercept
0.78 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
0.78 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
0.78 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
0.78 logistic.py(297):     p = np.exp(p, p)
0.78 logistic.py(298):     return loss, p, w
0.78 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
0.78 logistic.py(346):     diff = sample_weight * (p - Y)
0.78 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
0.78 logistic.py(348):     grad[:, :n_features] += alpha * w
0.78 logistic.py(349):     if fit_intercept:
0.78 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
0.78 logistic.py(351):     return loss, grad.ravel(), p
0.78 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
0.78 logistic.py(339):     n_classes = Y.shape[1]
0.78 logistic.py(340):     n_features = X.shape[1]
0.78 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
0.78 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
0.78 logistic.py(343):                     dtype=X.dtype)
0.78 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
0.78 logistic.py(282):     n_classes = Y.shape[1]
0.78 logistic.py(283):     n_features = X.shape[1]
0.78 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
0.78 logistic.py(285):     w = w.reshape(n_classes, -1)
0.78 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
0.78 logistic.py(287):     if fit_intercept:
0.78 logistic.py(288):         intercept = w[:, -1]
0.78 logistic.py(289):         w = w[:, :-1]
0.78 logistic.py(292):     p = safe_sparse_dot(X, w.T)
0.78 logistic.py(293):     p += intercept
0.78 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
0.78 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
0.78 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
0.78 logistic.py(297):     p = np.exp(p, p)
0.78 logistic.py(298):     return loss, p, w
0.78 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
0.78 logistic.py(346):     diff = sample_weight * (p - Y)
0.78 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
0.78 logistic.py(348):     grad[:, :n_features] += alpha * w
0.78 logistic.py(349):     if fit_intercept:
0.78 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
0.78 logistic.py(351):     return loss, grad.ravel(), p
0.78 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
0.78 logistic.py(339):     n_classes = Y.shape[1]
0.78 logistic.py(340):     n_features = X.shape[1]
0.78 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
0.78 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
0.78 logistic.py(343):                     dtype=X.dtype)
0.78 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
0.78 logistic.py(282):     n_classes = Y.shape[1]
0.78 logistic.py(283):     n_features = X.shape[1]
0.78 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
0.78 logistic.py(285):     w = w.reshape(n_classes, -1)
0.78 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
0.78 logistic.py(287):     if fit_intercept:
0.78 logistic.py(288):         intercept = w[:, -1]
0.78 logistic.py(289):         w = w[:, :-1]
0.78 logistic.py(292):     p = safe_sparse_dot(X, w.T)
0.78 logistic.py(293):     p += intercept
0.78 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
0.78 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
0.78 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
0.78 logistic.py(297):     p = np.exp(p, p)
0.78 logistic.py(298):     return loss, p, w
0.78 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
0.78 logistic.py(346):     diff = sample_weight * (p - Y)
0.78 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
0.78 logistic.py(348):     grad[:, :n_features] += alpha * w
0.78 logistic.py(349):     if fit_intercept:
0.78 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
0.78 logistic.py(351):     return loss, grad.ravel(), p
0.78 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
0.78 logistic.py(339):     n_classes = Y.shape[1]
0.78 logistic.py(340):     n_features = X.shape[1]
0.78 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
0.78 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
0.78 logistic.py(343):                     dtype=X.dtype)
0.78 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
0.78 logistic.py(282):     n_classes = Y.shape[1]
0.78 logistic.py(283):     n_features = X.shape[1]
0.78 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
0.78 logistic.py(285):     w = w.reshape(n_classes, -1)
0.78 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
0.78 logistic.py(287):     if fit_intercept:
0.78 logistic.py(288):         intercept = w[:, -1]
0.78 logistic.py(289):         w = w[:, :-1]
0.78 logistic.py(292):     p = safe_sparse_dot(X, w.T)
0.78 logistic.py(293):     p += intercept
0.78 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
0.78 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
0.78 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
0.78 logistic.py(297):     p = np.exp(p, p)
0.78 logistic.py(298):     return loss, p, w
0.78 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
0.78 logistic.py(346):     diff = sample_weight * (p - Y)
0.78 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
0.78 logistic.py(348):     grad[:, :n_features] += alpha * w
0.78 logistic.py(349):     if fit_intercept:
0.78 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
0.78 logistic.py(351):     return loss, grad.ravel(), p
0.78 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
0.78 logistic.py(339):     n_classes = Y.shape[1]
0.78 logistic.py(340):     n_features = X.shape[1]
0.78 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
0.78 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
0.78 logistic.py(343):                     dtype=X.dtype)
0.78 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
0.78 logistic.py(282):     n_classes = Y.shape[1]
0.78 logistic.py(283):     n_features = X.shape[1]
0.78 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
0.78 logistic.py(285):     w = w.reshape(n_classes, -1)
0.78 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
0.78 logistic.py(287):     if fit_intercept:
0.78 logistic.py(288):         intercept = w[:, -1]
0.78 logistic.py(289):         w = w[:, :-1]
0.78 logistic.py(292):     p = safe_sparse_dot(X, w.T)
0.78 logistic.py(293):     p += intercept
0.78 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
0.78 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
0.78 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
0.78 logistic.py(297):     p = np.exp(p, p)
0.78 logistic.py(298):     return loss, p, w
0.78 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
0.78 logistic.py(346):     diff = sample_weight * (p - Y)
0.78 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
0.78 logistic.py(348):     grad[:, :n_features] += alpha * w
0.78 logistic.py(349):     if fit_intercept:
0.78 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
0.78 logistic.py(351):     return loss, grad.ravel(), p
0.78 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
0.78 logistic.py(339):     n_classes = Y.shape[1]
0.78 logistic.py(340):     n_features = X.shape[1]
0.78 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
0.78 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
0.78 logistic.py(343):                     dtype=X.dtype)
0.78 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
0.78 logistic.py(282):     n_classes = Y.shape[1]
0.78 logistic.py(283):     n_features = X.shape[1]
0.78 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
0.78 logistic.py(285):     w = w.reshape(n_classes, -1)
0.78 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
0.78 logistic.py(287):     if fit_intercept:
0.78 logistic.py(288):         intercept = w[:, -1]
0.78 logistic.py(289):         w = w[:, :-1]
0.78 logistic.py(292):     p = safe_sparse_dot(X, w.T)
0.78 logistic.py(293):     p += intercept
0.78 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
0.78 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
0.78 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
0.78 logistic.py(297):     p = np.exp(p, p)
0.78 logistic.py(298):     return loss, p, w
0.78 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
0.78 logistic.py(346):     diff = sample_weight * (p - Y)
0.78 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
0.78 logistic.py(348):     grad[:, :n_features] += alpha * w
0.78 logistic.py(349):     if fit_intercept:
0.78 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
0.78 logistic.py(351):     return loss, grad.ravel(), p
0.78 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
0.78 logistic.py(339):     n_classes = Y.shape[1]
0.78 logistic.py(340):     n_features = X.shape[1]
0.78 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
0.78 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
0.78 logistic.py(343):                     dtype=X.dtype)
0.78 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
0.78 logistic.py(282):     n_classes = Y.shape[1]
0.78 logistic.py(283):     n_features = X.shape[1]
0.78 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
0.78 logistic.py(285):     w = w.reshape(n_classes, -1)
0.78 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
0.78 logistic.py(287):     if fit_intercept:
0.78 logistic.py(288):         intercept = w[:, -1]
0.78 logistic.py(289):         w = w[:, :-1]
0.78 logistic.py(292):     p = safe_sparse_dot(X, w.T)
0.78 logistic.py(293):     p += intercept
0.78 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
0.78 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
0.78 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
0.78 logistic.py(297):     p = np.exp(p, p)
0.78 logistic.py(298):     return loss, p, w
0.78 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
0.78 logistic.py(346):     diff = sample_weight * (p - Y)
0.78 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
0.78 logistic.py(348):     grad[:, :n_features] += alpha * w
0.78 logistic.py(349):     if fit_intercept:
0.78 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
0.78 logistic.py(351):     return loss, grad.ravel(), p
0.78 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
0.78 logistic.py(339):     n_classes = Y.shape[1]
0.78 logistic.py(340):     n_features = X.shape[1]
0.78 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
0.78 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
0.78 logistic.py(343):                     dtype=X.dtype)
0.78 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
0.78 logistic.py(282):     n_classes = Y.shape[1]
0.78 logistic.py(283):     n_features = X.shape[1]
0.78 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
0.78 logistic.py(285):     w = w.reshape(n_classes, -1)
0.78 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
0.78 logistic.py(287):     if fit_intercept:
0.78 logistic.py(288):         intercept = w[:, -1]
0.78 logistic.py(289):         w = w[:, :-1]
0.78 logistic.py(292):     p = safe_sparse_dot(X, w.T)
0.78 logistic.py(293):     p += intercept
0.78 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
0.78 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
0.78 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
0.78 logistic.py(297):     p = np.exp(p, p)
0.78 logistic.py(298):     return loss, p, w
0.78 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
0.78 logistic.py(346):     diff = sample_weight * (p - Y)
0.78 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
0.78 logistic.py(348):     grad[:, :n_features] += alpha * w
0.78 logistic.py(349):     if fit_intercept:
0.78 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
0.78 logistic.py(351):     return loss, grad.ravel(), p
0.78 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
0.78 logistic.py(339):     n_classes = Y.shape[1]
0.78 logistic.py(340):     n_features = X.shape[1]
0.78 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
0.78 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
0.78 logistic.py(343):                     dtype=X.dtype)
0.78 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
0.78 logistic.py(282):     n_classes = Y.shape[1]
0.78 logistic.py(283):     n_features = X.shape[1]
0.78 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
0.78 logistic.py(285):     w = w.reshape(n_classes, -1)
0.78 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
0.78 logistic.py(287):     if fit_intercept:
0.78 logistic.py(288):         intercept = w[:, -1]
0.78 logistic.py(289):         w = w[:, :-1]
0.78 logistic.py(292):     p = safe_sparse_dot(X, w.T)
0.78 logistic.py(293):     p += intercept
0.78 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
0.78 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
0.78 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
0.78 logistic.py(297):     p = np.exp(p, p)
0.78 logistic.py(298):     return loss, p, w
0.78 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
0.78 logistic.py(346):     diff = sample_weight * (p - Y)
0.78 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
0.78 logistic.py(348):     grad[:, :n_features] += alpha * w
0.78 logistic.py(349):     if fit_intercept:
0.78 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
0.78 logistic.py(351):     return loss, grad.ravel(), p
0.78 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
0.78 logistic.py(339):     n_classes = Y.shape[1]
0.78 logistic.py(340):     n_features = X.shape[1]
0.78 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
0.78 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
0.78 logistic.py(343):                     dtype=X.dtype)
0.78 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
0.78 logistic.py(282):     n_classes = Y.shape[1]
0.78 logistic.py(283):     n_features = X.shape[1]
0.78 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
0.78 logistic.py(285):     w = w.reshape(n_classes, -1)
0.78 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
0.78 logistic.py(287):     if fit_intercept:
0.78 logistic.py(288):         intercept = w[:, -1]
0.78 logistic.py(289):         w = w[:, :-1]
0.78 logistic.py(292):     p = safe_sparse_dot(X, w.T)
0.78 logistic.py(293):     p += intercept
0.78 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
0.78 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
0.78 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
0.78 logistic.py(297):     p = np.exp(p, p)
0.78 logistic.py(298):     return loss, p, w
0.78 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
0.78 logistic.py(346):     diff = sample_weight * (p - Y)
0.78 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
0.78 logistic.py(348):     grad[:, :n_features] += alpha * w
0.78 logistic.py(349):     if fit_intercept:
0.78 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
0.78 logistic.py(351):     return loss, grad.ravel(), p
0.78 logistic.py(718):             if info["warnflag"] == 1:
0.78 logistic.py(723):             n_iter_i = min(info['nit'], max_iter)
0.78 logistic.py(760):         if multi_class == 'multinomial':
0.78 logistic.py(761):             multi_w0 = np.reshape(w0, (classes.size, -1))
0.78 logistic.py(762):             if classes.size == 2:
0.78 logistic.py(764):             coefs.append(multi_w0)
0.78 logistic.py(768):         n_iter[i] = n_iter_i
0.78 logistic.py(712):     for i, C in enumerate(Cs):
0.78 logistic.py(713):         if solver == 'lbfgs':
0.78 logistic.py(714):             w0, loss, info = optimize.fmin_l_bfgs_b(
0.78 logistic.py(715):                 func, w0, fprime=None,
0.78 logistic.py(716):                 args=(X, target, 1. / C, sample_weight),
0.78 logistic.py(717):                 iprint=(verbose > 0) - 1, pgtol=tol, maxiter=max_iter)
0.78 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
0.78 logistic.py(339):     n_classes = Y.shape[1]
0.78 logistic.py(340):     n_features = X.shape[1]
0.78 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
0.78 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
0.78 logistic.py(343):                     dtype=X.dtype)
0.78 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
0.78 logistic.py(282):     n_classes = Y.shape[1]
0.78 logistic.py(283):     n_features = X.shape[1]
0.78 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
0.78 logistic.py(285):     w = w.reshape(n_classes, -1)
0.78 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
0.78 logistic.py(287):     if fit_intercept:
0.78 logistic.py(288):         intercept = w[:, -1]
0.78 logistic.py(289):         w = w[:, :-1]
0.78 logistic.py(292):     p = safe_sparse_dot(X, w.T)
0.78 logistic.py(293):     p += intercept
0.78 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
0.78 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
0.78 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
0.78 logistic.py(297):     p = np.exp(p, p)
0.78 logistic.py(298):     return loss, p, w
0.78 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
0.78 logistic.py(346):     diff = sample_weight * (p - Y)
0.78 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
0.78 logistic.py(348):     grad[:, :n_features] += alpha * w
0.78 logistic.py(349):     if fit_intercept:
0.78 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
0.78 logistic.py(351):     return loss, grad.ravel(), p
0.78 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
0.78 logistic.py(339):     n_classes = Y.shape[1]
0.78 logistic.py(340):     n_features = X.shape[1]
0.78 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
0.78 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
0.78 logistic.py(343):                     dtype=X.dtype)
0.78 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
0.78 logistic.py(282):     n_classes = Y.shape[1]
0.78 logistic.py(283):     n_features = X.shape[1]
0.78 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
0.78 logistic.py(285):     w = w.reshape(n_classes, -1)
0.78 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
0.78 logistic.py(287):     if fit_intercept:
0.78 logistic.py(288):         intercept = w[:, -1]
0.78 logistic.py(289):         w = w[:, :-1]
0.78 logistic.py(292):     p = safe_sparse_dot(X, w.T)
0.78 logistic.py(293):     p += intercept
0.78 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
0.78 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
0.78 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
0.78 logistic.py(297):     p = np.exp(p, p)
0.78 logistic.py(298):     return loss, p, w
0.78 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
0.78 logistic.py(346):     diff = sample_weight * (p - Y)
0.78 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
0.78 logistic.py(348):     grad[:, :n_features] += alpha * w
0.78 logistic.py(349):     if fit_intercept:
0.78 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
0.78 logistic.py(351):     return loss, grad.ravel(), p
0.78 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
0.78 logistic.py(339):     n_classes = Y.shape[1]
0.78 logistic.py(340):     n_features = X.shape[1]
0.78 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
0.78 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
0.78 logistic.py(343):                     dtype=X.dtype)
0.78 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
0.78 logistic.py(282):     n_classes = Y.shape[1]
0.78 logistic.py(283):     n_features = X.shape[1]
0.78 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
0.78 logistic.py(285):     w = w.reshape(n_classes, -1)
0.78 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
0.78 logistic.py(287):     if fit_intercept:
0.78 logistic.py(288):         intercept = w[:, -1]
0.78 logistic.py(289):         w = w[:, :-1]
0.78 logistic.py(292):     p = safe_sparse_dot(X, w.T)
0.78 logistic.py(293):     p += intercept
0.78 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
0.78 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
0.78 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
0.78 logistic.py(297):     p = np.exp(p, p)
0.78 logistic.py(298):     return loss, p, w
0.78 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
0.78 logistic.py(346):     diff = sample_weight * (p - Y)
0.78 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
0.78 logistic.py(348):     grad[:, :n_features] += alpha * w
0.78 logistic.py(349):     if fit_intercept:
0.78 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
0.78 logistic.py(351):     return loss, grad.ravel(), p
0.78 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
0.78 logistic.py(339):     n_classes = Y.shape[1]
0.78 logistic.py(340):     n_features = X.shape[1]
0.78 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
0.78 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
0.78 logistic.py(343):                     dtype=X.dtype)
0.78 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
0.78 logistic.py(282):     n_classes = Y.shape[1]
0.78 logistic.py(283):     n_features = X.shape[1]
0.78 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
0.78 logistic.py(285):     w = w.reshape(n_classes, -1)
0.78 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
0.78 logistic.py(287):     if fit_intercept:
0.78 logistic.py(288):         intercept = w[:, -1]
0.78 logistic.py(289):         w = w[:, :-1]
0.78 logistic.py(292):     p = safe_sparse_dot(X, w.T)
0.78 logistic.py(293):     p += intercept
0.78 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
0.78 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
0.78 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
0.78 logistic.py(297):     p = np.exp(p, p)
0.78 logistic.py(298):     return loss, p, w
0.78 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
0.78 logistic.py(346):     diff = sample_weight * (p - Y)
0.78 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
0.78 logistic.py(348):     grad[:, :n_features] += alpha * w
0.78 logistic.py(349):     if fit_intercept:
0.78 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
0.78 logistic.py(351):     return loss, grad.ravel(), p
0.78 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
0.78 logistic.py(339):     n_classes = Y.shape[1]
0.78 logistic.py(340):     n_features = X.shape[1]
0.78 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
0.78 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
0.78 logistic.py(343):                     dtype=X.dtype)
0.78 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
0.78 logistic.py(282):     n_classes = Y.shape[1]
0.78 logistic.py(283):     n_features = X.shape[1]
0.78 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
0.78 logistic.py(285):     w = w.reshape(n_classes, -1)
0.78 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
0.78 logistic.py(287):     if fit_intercept:
0.78 logistic.py(288):         intercept = w[:, -1]
0.78 logistic.py(289):         w = w[:, :-1]
0.78 logistic.py(292):     p = safe_sparse_dot(X, w.T)
0.78 logistic.py(293):     p += intercept
0.78 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
0.78 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
0.79 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
0.79 logistic.py(297):     p = np.exp(p, p)
0.79 logistic.py(298):     return loss, p, w
0.79 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
0.79 logistic.py(346):     diff = sample_weight * (p - Y)
0.79 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
0.79 logistic.py(348):     grad[:, :n_features] += alpha * w
0.79 logistic.py(349):     if fit_intercept:
0.79 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
0.79 logistic.py(351):     return loss, grad.ravel(), p
0.79 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
0.79 logistic.py(339):     n_classes = Y.shape[1]
0.79 logistic.py(340):     n_features = X.shape[1]
0.79 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
0.79 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
0.79 logistic.py(343):                     dtype=X.dtype)
0.79 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
0.79 logistic.py(282):     n_classes = Y.shape[1]
0.79 logistic.py(283):     n_features = X.shape[1]
0.79 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
0.79 logistic.py(285):     w = w.reshape(n_classes, -1)
0.79 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
0.79 logistic.py(287):     if fit_intercept:
0.79 logistic.py(288):         intercept = w[:, -1]
0.79 logistic.py(289):         w = w[:, :-1]
0.79 logistic.py(292):     p = safe_sparse_dot(X, w.T)
0.79 logistic.py(293):     p += intercept
0.79 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
0.79 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
0.79 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
0.79 logistic.py(297):     p = np.exp(p, p)
0.79 logistic.py(298):     return loss, p, w
0.79 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
0.79 logistic.py(346):     diff = sample_weight * (p - Y)
0.79 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
0.79 logistic.py(348):     grad[:, :n_features] += alpha * w
0.79 logistic.py(349):     if fit_intercept:
0.79 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
0.79 logistic.py(351):     return loss, grad.ravel(), p
0.79 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
0.79 logistic.py(339):     n_classes = Y.shape[1]
0.79 logistic.py(340):     n_features = X.shape[1]
0.79 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
0.79 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
0.79 logistic.py(343):                     dtype=X.dtype)
0.79 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
0.79 logistic.py(282):     n_classes = Y.shape[1]
0.79 logistic.py(283):     n_features = X.shape[1]
0.79 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
0.79 logistic.py(285):     w = w.reshape(n_classes, -1)
0.79 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
0.79 logistic.py(287):     if fit_intercept:
0.79 logistic.py(288):         intercept = w[:, -1]
0.79 logistic.py(289):         w = w[:, :-1]
0.79 logistic.py(292):     p = safe_sparse_dot(X, w.T)
0.79 logistic.py(293):     p += intercept
0.79 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
0.79 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
0.79 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
0.79 logistic.py(297):     p = np.exp(p, p)
0.79 logistic.py(298):     return loss, p, w
0.79 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
0.79 logistic.py(346):     diff = sample_weight * (p - Y)
0.79 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
0.79 logistic.py(348):     grad[:, :n_features] += alpha * w
0.79 logistic.py(349):     if fit_intercept:
0.79 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
0.79 logistic.py(351):     return loss, grad.ravel(), p
0.79 logistic.py(718):             if info["warnflag"] == 1:
0.79 logistic.py(723):             n_iter_i = min(info['nit'], max_iter)
0.79 logistic.py(760):         if multi_class == 'multinomial':
0.79 logistic.py(761):             multi_w0 = np.reshape(w0, (classes.size, -1))
0.79 logistic.py(762):             if classes.size == 2:
0.79 logistic.py(764):             coefs.append(multi_w0)
0.79 logistic.py(768):         n_iter[i] = n_iter_i
0.79 logistic.py(712):     for i, C in enumerate(Cs):
0.79 logistic.py(713):         if solver == 'lbfgs':
0.79 logistic.py(714):             w0, loss, info = optimize.fmin_l_bfgs_b(
0.79 logistic.py(715):                 func, w0, fprime=None,
0.79 logistic.py(716):                 args=(X, target, 1. / C, sample_weight),
0.79 logistic.py(717):                 iprint=(verbose > 0) - 1, pgtol=tol, maxiter=max_iter)
0.79 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
0.79 logistic.py(339):     n_classes = Y.shape[1]
0.79 logistic.py(340):     n_features = X.shape[1]
0.79 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
0.79 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
0.79 logistic.py(343):                     dtype=X.dtype)
0.79 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
0.79 logistic.py(282):     n_classes = Y.shape[1]
0.79 logistic.py(283):     n_features = X.shape[1]
0.79 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
0.79 logistic.py(285):     w = w.reshape(n_classes, -1)
0.79 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
0.79 logistic.py(287):     if fit_intercept:
0.79 logistic.py(288):         intercept = w[:, -1]
0.79 logistic.py(289):         w = w[:, :-1]
0.79 logistic.py(292):     p = safe_sparse_dot(X, w.T)
0.79 logistic.py(293):     p += intercept
0.79 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
0.79 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
0.79 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
0.79 logistic.py(297):     p = np.exp(p, p)
0.79 logistic.py(298):     return loss, p, w
0.79 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
0.79 logistic.py(346):     diff = sample_weight * (p - Y)
0.79 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
0.79 logistic.py(348):     grad[:, :n_features] += alpha * w
0.79 logistic.py(349):     if fit_intercept:
0.79 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
0.79 logistic.py(351):     return loss, grad.ravel(), p
0.79 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
0.79 logistic.py(339):     n_classes = Y.shape[1]
0.79 logistic.py(340):     n_features = X.shape[1]
0.79 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
0.79 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
0.79 logistic.py(343):                     dtype=X.dtype)
0.79 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
0.79 logistic.py(282):     n_classes = Y.shape[1]
0.79 logistic.py(283):     n_features = X.shape[1]
0.79 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
0.79 logistic.py(285):     w = w.reshape(n_classes, -1)
0.79 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
0.79 logistic.py(287):     if fit_intercept:
0.79 logistic.py(288):         intercept = w[:, -1]
0.79 logistic.py(289):         w = w[:, :-1]
0.79 logistic.py(292):     p = safe_sparse_dot(X, w.T)
0.79 logistic.py(293):     p += intercept
0.79 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
0.79 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
0.79 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
0.79 logistic.py(297):     p = np.exp(p, p)
0.79 logistic.py(298):     return loss, p, w
0.79 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
0.79 logistic.py(346):     diff = sample_weight * (p - Y)
0.79 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
0.79 logistic.py(348):     grad[:, :n_features] += alpha * w
0.79 logistic.py(349):     if fit_intercept:
0.79 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
0.79 logistic.py(351):     return loss, grad.ravel(), p
0.79 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
0.79 logistic.py(339):     n_classes = Y.shape[1]
0.79 logistic.py(340):     n_features = X.shape[1]
0.79 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
0.79 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
0.79 logistic.py(343):                     dtype=X.dtype)
0.79 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
0.79 logistic.py(282):     n_classes = Y.shape[1]
0.79 logistic.py(283):     n_features = X.shape[1]
0.79 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
0.79 logistic.py(285):     w = w.reshape(n_classes, -1)
0.79 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
0.79 logistic.py(287):     if fit_intercept:
0.79 logistic.py(288):         intercept = w[:, -1]
0.79 logistic.py(289):         w = w[:, :-1]
0.79 logistic.py(292):     p = safe_sparse_dot(X, w.T)
0.79 logistic.py(293):     p += intercept
0.79 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
0.79 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
0.79 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
0.79 logistic.py(297):     p = np.exp(p, p)
0.79 logistic.py(298):     return loss, p, w
0.79 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
0.79 logistic.py(346):     diff = sample_weight * (p - Y)
0.79 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
0.79 logistic.py(348):     grad[:, :n_features] += alpha * w
0.79 logistic.py(349):     if fit_intercept:
0.79 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
0.79 logistic.py(351):     return loss, grad.ravel(), p
0.79 logistic.py(718):             if info["warnflag"] == 1:
0.79 logistic.py(723):             n_iter_i = min(info['nit'], max_iter)
0.79 logistic.py(760):         if multi_class == 'multinomial':
0.79 logistic.py(761):             multi_w0 = np.reshape(w0, (classes.size, -1))
0.79 logistic.py(762):             if classes.size == 2:
0.79 logistic.py(764):             coefs.append(multi_w0)
0.79 logistic.py(768):         n_iter[i] = n_iter_i
0.79 logistic.py(712):     for i, C in enumerate(Cs):
0.79 logistic.py(770):     return coefs, np.array(Cs), n_iter
0.79 logistic.py(925):     log_reg = LogisticRegression(multi_class=multi_class)
0.79 logistic.py(1171):         self.penalty = penalty
0.79 logistic.py(1172):         self.dual = dual
0.79 logistic.py(1173):         self.tol = tol
0.79 logistic.py(1174):         self.C = C
0.79 logistic.py(1175):         self.fit_intercept = fit_intercept
0.79 logistic.py(1176):         self.intercept_scaling = intercept_scaling
0.79 logistic.py(1177):         self.class_weight = class_weight
0.79 logistic.py(1178):         self.random_state = random_state
0.79 logistic.py(1179):         self.solver = solver
0.79 logistic.py(1180):         self.max_iter = max_iter
0.79 logistic.py(1181):         self.multi_class = multi_class
0.79 logistic.py(1182):         self.verbose = verbose
0.79 logistic.py(1183):         self.warm_start = warm_start
0.79 logistic.py(1184):         self.n_jobs = n_jobs
0.79 logistic.py(928):     if multi_class == 'ovr':
0.79 logistic.py(930):     elif multi_class == 'multinomial':
0.79 logistic.py(931):         log_reg.classes_ = np.unique(y_train)
0.79 logistic.py(936):     if pos_class is not None:
0.79 logistic.py(941):     scores = list()
0.79 logistic.py(943):     if isinstance(scoring, six.string_types):
0.79 logistic.py(945):     for w in coefs:
0.79 logistic.py(946):         if multi_class == 'ovr':
0.79 logistic.py(948):         if fit_intercept:
0.79 logistic.py(949):             log_reg.coef_ = w[:, :-1]
0.79 logistic.py(950):             log_reg.intercept_ = w[:, -1]
0.79 logistic.py(955):         if scoring is None:
0.79 logistic.py(956):             scores.append(log_reg.score(X_test, y_test))
0.79 logistic.py(945):     for w in coefs:
0.79 logistic.py(946):         if multi_class == 'ovr':
0.79 logistic.py(948):         if fit_intercept:
0.79 logistic.py(949):             log_reg.coef_ = w[:, :-1]
0.79 logistic.py(950):             log_reg.intercept_ = w[:, -1]
0.79 logistic.py(955):         if scoring is None:
0.79 logistic.py(956):             scores.append(log_reg.score(X_test, y_test))
0.79 logistic.py(945):     for w in coefs:
0.79 logistic.py(946):         if multi_class == 'ovr':
0.79 logistic.py(948):         if fit_intercept:
0.79 logistic.py(949):             log_reg.coef_ = w[:, :-1]
0.79 logistic.py(950):             log_reg.intercept_ = w[:, -1]
0.79 logistic.py(955):         if scoring is None:
0.79 logistic.py(956):             scores.append(log_reg.score(X_test, y_test))
0.79 logistic.py(945):     for w in coefs:
0.79 logistic.py(946):         if multi_class == 'ovr':
0.79 logistic.py(948):         if fit_intercept:
0.79 logistic.py(949):             log_reg.coef_ = w[:, :-1]
0.79 logistic.py(950):             log_reg.intercept_ = w[:, -1]
0.79 logistic.py(955):         if scoring is None:
0.79 logistic.py(956):             scores.append(log_reg.score(X_test, y_test))
0.79 logistic.py(945):     for w in coefs:
0.79 logistic.py(946):         if multi_class == 'ovr':
0.79 logistic.py(948):         if fit_intercept:
0.79 logistic.py(949):             log_reg.coef_ = w[:, :-1]
0.79 logistic.py(950):             log_reg.intercept_ = w[:, -1]
0.79 logistic.py(955):         if scoring is None:
0.79 logistic.py(956):             scores.append(log_reg.score(X_test, y_test))
0.79 logistic.py(945):     for w in coefs:
0.79 logistic.py(946):         if multi_class == 'ovr':
0.79 logistic.py(948):         if fit_intercept:
0.79 logistic.py(949):             log_reg.coef_ = w[:, :-1]
0.79 logistic.py(950):             log_reg.intercept_ = w[:, -1]
0.79 logistic.py(955):         if scoring is None:
0.79 logistic.py(956):             scores.append(log_reg.score(X_test, y_test))
0.79 logistic.py(945):     for w in coefs:
0.79 logistic.py(946):         if multi_class == 'ovr':
0.79 logistic.py(948):         if fit_intercept:
0.79 logistic.py(949):             log_reg.coef_ = w[:, :-1]
0.79 logistic.py(950):             log_reg.intercept_ = w[:, -1]
0.79 logistic.py(955):         if scoring is None:
0.79 logistic.py(956):             scores.append(log_reg.score(X_test, y_test))
0.79 logistic.py(945):     for w in coefs:
0.79 logistic.py(946):         if multi_class == 'ovr':
0.79 logistic.py(948):         if fit_intercept:
0.79 logistic.py(949):             log_reg.coef_ = w[:, :-1]
0.79 logistic.py(950):             log_reg.intercept_ = w[:, -1]
0.79 logistic.py(955):         if scoring is None:
0.79 logistic.py(956):             scores.append(log_reg.score(X_test, y_test))
0.79 logistic.py(945):     for w in coefs:
0.79 logistic.py(946):         if multi_class == 'ovr':
0.79 logistic.py(948):         if fit_intercept:
0.79 logistic.py(949):             log_reg.coef_ = w[:, :-1]
0.79 logistic.py(950):             log_reg.intercept_ = w[:, -1]
0.79 logistic.py(955):         if scoring is None:
0.79 logistic.py(956):             scores.append(log_reg.score(X_test, y_test))
0.79 logistic.py(945):     for w in coefs:
0.79 logistic.py(946):         if multi_class == 'ovr':
0.79 logistic.py(948):         if fit_intercept:
0.79 logistic.py(949):             log_reg.coef_ = w[:, :-1]
0.79 logistic.py(950):             log_reg.intercept_ = w[:, -1]
0.79 logistic.py(955):         if scoring is None:
0.79 logistic.py(956):             scores.append(log_reg.score(X_test, y_test))
0.79 logistic.py(945):     for w in coefs:
0.79 logistic.py(959):     return coefs, Cs, np.array(scores), n_iter
0.79 logistic.py(1703):             for train, test in folds)
0.79 logistic.py(903):     _check_solver_option(solver, multi_class, penalty, dual)
0.79 logistic.py(428):     if solver not in ['liblinear', 'newton-cg', 'lbfgs', 'sag', 'saga']:
0.79 logistic.py(433):     if multi_class not in ['multinomial', 'ovr']:
0.79 logistic.py(437):     if multi_class == 'multinomial' and solver == 'liblinear':
0.79 logistic.py(441):     if solver not in ['liblinear', 'saga']:
0.79 logistic.py(442):         if penalty != 'l2':
0.79 logistic.py(445):     if solver != 'liblinear':
0.79 logistic.py(446):         if dual:
0.79 logistic.py(905):     X_train = X[train]
0.79 logistic.py(906):     X_test = X[test]
0.79 logistic.py(907):     y_train = y[train]
0.79 logistic.py(908):     y_test = y[test]
0.79 logistic.py(910):     if sample_weight is not None:
0.79 logistic.py(916):     coefs, Cs, n_iter = logistic_regression_path(
0.79 logistic.py(917):         X_train, y_train, Cs=Cs, fit_intercept=fit_intercept,
0.79 logistic.py(918):         solver=solver, max_iter=max_iter, class_weight=class_weight,
0.79 logistic.py(919):         pos_class=pos_class, multi_class=multi_class,
0.79 logistic.py(920):         tol=tol, verbose=verbose, dual=dual, penalty=penalty,
0.79 logistic.py(921):         intercept_scaling=intercept_scaling, random_state=random_state,
0.79 logistic.py(922):         check_input=False, max_squared_sum=max_squared_sum,
0.79 logistic.py(923):         sample_weight=sample_weight)
0.79 logistic.py(591):     if isinstance(Cs, numbers.Integral):
0.79 logistic.py(592):         Cs = np.logspace(-4, 4, Cs)
0.79 logistic.py(594):     _check_solver_option(solver, multi_class, penalty, dual)
0.79 logistic.py(428):     if solver not in ['liblinear', 'newton-cg', 'lbfgs', 'sag', 'saga']:
0.79 logistic.py(433):     if multi_class not in ['multinomial', 'ovr']:
0.79 logistic.py(437):     if multi_class == 'multinomial' and solver == 'liblinear':
0.79 logistic.py(441):     if solver not in ['liblinear', 'saga']:
0.79 logistic.py(442):         if penalty != 'l2':
0.79 logistic.py(445):     if solver != 'liblinear':
0.79 logistic.py(446):         if dual:
0.79 logistic.py(597):     if check_input:
0.79 logistic.py(602):     _, n_features = X.shape
0.79 logistic.py(603):     classes = np.unique(y)
0.79 logistic.py(604):     random_state = check_random_state(random_state)
0.79 logistic.py(606):     if pos_class is None and multi_class != 'multinomial':
0.79 logistic.py(615):     if sample_weight is not None:
0.79 logistic.py(619):         sample_weight = np.ones(X.shape[0], dtype=X.dtype)
0.79 logistic.py(624):     le = LabelEncoder()
0.79 logistic.py(625):     if isinstance(class_weight, dict) or multi_class == 'multinomial':
0.79 logistic.py(626):         class_weight_ = compute_class_weight(class_weight, classes, y)
0.79 logistic.py(627):         sample_weight *= class_weight_[le.fit_transform(y)]
0.79 logistic.py(631):     if multi_class == 'ovr':
0.79 logistic.py(645):         if solver not in ['sag', 'saga']:
0.79 logistic.py(646):             lbin = LabelBinarizer()
0.79 logistic.py(647):             Y_multi = lbin.fit_transform(y)
0.79 logistic.py(648):             if Y_multi.shape[1] == 1:
0.79 logistic.py(655):         w0 = np.zeros((classes.size, n_features + int(fit_intercept)),
0.79 logistic.py(656):                       order='F', dtype=X.dtype)
0.79 logistic.py(658):     if coef is not None:
0.79 logistic.py(688):     if multi_class == 'multinomial':
0.79 logistic.py(690):         if solver in ['lbfgs', 'newton-cg']:
0.79 logistic.py(691):             w0 = w0.ravel()
0.79 logistic.py(692):         target = Y_multi
0.79 logistic.py(693):         if solver == 'lbfgs':
0.79 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
0.79 logistic.py(699):         warm_start_sag = {'coef': w0.T}
0.79 logistic.py(710):     coefs = list()
0.79 logistic.py(711):     n_iter = np.zeros(len(Cs), dtype=np.int32)
0.79 logistic.py(712):     for i, C in enumerate(Cs):
0.79 logistic.py(713):         if solver == 'lbfgs':
0.79 logistic.py(714):             w0, loss, info = optimize.fmin_l_bfgs_b(
0.79 logistic.py(715):                 func, w0, fprime=None,
0.79 logistic.py(716):                 args=(X, target, 1. / C, sample_weight),
0.79 logistic.py(717):                 iprint=(verbose > 0) - 1, pgtol=tol, maxiter=max_iter)
0.79 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
0.79 logistic.py(339):     n_classes = Y.shape[1]
0.79 logistic.py(340):     n_features = X.shape[1]
0.79 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
0.79 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
0.79 logistic.py(343):                     dtype=X.dtype)
0.79 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
0.79 logistic.py(282):     n_classes = Y.shape[1]
0.79 logistic.py(283):     n_features = X.shape[1]
0.79 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
0.79 logistic.py(285):     w = w.reshape(n_classes, -1)
0.79 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
0.79 logistic.py(287):     if fit_intercept:
0.79 logistic.py(288):         intercept = w[:, -1]
0.79 logistic.py(289):         w = w[:, :-1]
0.79 logistic.py(292):     p = safe_sparse_dot(X, w.T)
0.79 logistic.py(293):     p += intercept
0.79 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
0.79 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
0.79 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
0.79 logistic.py(297):     p = np.exp(p, p)
0.79 logistic.py(298):     return loss, p, w
0.79 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
0.79 logistic.py(346):     diff = sample_weight * (p - Y)
0.79 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
0.79 logistic.py(348):     grad[:, :n_features] += alpha * w
0.79 logistic.py(349):     if fit_intercept:
0.79 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
0.79 logistic.py(351):     return loss, grad.ravel(), p
0.79 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
0.79 logistic.py(339):     n_classes = Y.shape[1]
0.79 logistic.py(340):     n_features = X.shape[1]
0.79 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
0.79 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
0.79 logistic.py(343):                     dtype=X.dtype)
0.79 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
0.79 logistic.py(282):     n_classes = Y.shape[1]
0.79 logistic.py(283):     n_features = X.shape[1]
0.79 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
0.79 logistic.py(285):     w = w.reshape(n_classes, -1)
0.79 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
0.79 logistic.py(287):     if fit_intercept:
0.79 logistic.py(288):         intercept = w[:, -1]
0.79 logistic.py(289):         w = w[:, :-1]
0.79 logistic.py(292):     p = safe_sparse_dot(X, w.T)
0.79 logistic.py(293):     p += intercept
0.79 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
0.79 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
0.79 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
0.79 logistic.py(297):     p = np.exp(p, p)
0.79 logistic.py(298):     return loss, p, w
0.79 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
0.79 logistic.py(346):     diff = sample_weight * (p - Y)
0.79 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
0.79 logistic.py(348):     grad[:, :n_features] += alpha * w
0.79 logistic.py(349):     if fit_intercept:
0.79 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
0.79 logistic.py(351):     return loss, grad.ravel(), p
0.80 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
0.80 logistic.py(339):     n_classes = Y.shape[1]
0.80 logistic.py(340):     n_features = X.shape[1]
0.80 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
0.80 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
0.80 logistic.py(343):                     dtype=X.dtype)
0.80 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
0.80 logistic.py(282):     n_classes = Y.shape[1]
0.80 logistic.py(283):     n_features = X.shape[1]
0.80 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
0.80 logistic.py(285):     w = w.reshape(n_classes, -1)
0.80 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
0.80 logistic.py(287):     if fit_intercept:
0.80 logistic.py(288):         intercept = w[:, -1]
0.80 logistic.py(289):         w = w[:, :-1]
0.80 logistic.py(292):     p = safe_sparse_dot(X, w.T)
0.80 logistic.py(293):     p += intercept
0.80 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
0.80 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
0.80 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
0.80 logistic.py(297):     p = np.exp(p, p)
0.80 logistic.py(298):     return loss, p, w
0.80 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
0.80 logistic.py(346):     diff = sample_weight * (p - Y)
0.80 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
0.80 logistic.py(348):     grad[:, :n_features] += alpha * w
0.80 logistic.py(349):     if fit_intercept:
0.80 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
0.80 logistic.py(351):     return loss, grad.ravel(), p
0.80 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
0.80 logistic.py(339):     n_classes = Y.shape[1]
0.80 logistic.py(340):     n_features = X.shape[1]
0.80 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
0.80 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
0.80 logistic.py(343):                     dtype=X.dtype)
0.80 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
0.80 logistic.py(282):     n_classes = Y.shape[1]
0.80 logistic.py(283):     n_features = X.shape[1]
0.80 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
0.80 logistic.py(285):     w = w.reshape(n_classes, -1)
0.80 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
0.80 logistic.py(287):     if fit_intercept:
0.80 logistic.py(288):         intercept = w[:, -1]
0.80 logistic.py(289):         w = w[:, :-1]
0.80 logistic.py(292):     p = safe_sparse_dot(X, w.T)
0.80 logistic.py(293):     p += intercept
0.80 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
0.80 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
0.80 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
0.80 logistic.py(297):     p = np.exp(p, p)
0.80 logistic.py(298):     return loss, p, w
0.80 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
0.80 logistic.py(346):     diff = sample_weight * (p - Y)
0.80 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
0.80 logistic.py(348):     grad[:, :n_features] += alpha * w
0.80 logistic.py(349):     if fit_intercept:
0.80 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
0.80 logistic.py(351):     return loss, grad.ravel(), p
0.80 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
0.80 logistic.py(339):     n_classes = Y.shape[1]
0.80 logistic.py(340):     n_features = X.shape[1]
0.80 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
0.80 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
0.80 logistic.py(343):                     dtype=X.dtype)
0.80 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
0.80 logistic.py(282):     n_classes = Y.shape[1]
0.80 logistic.py(283):     n_features = X.shape[1]
0.80 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
0.80 logistic.py(285):     w = w.reshape(n_classes, -1)
0.80 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
0.80 logistic.py(287):     if fit_intercept:
0.80 logistic.py(288):         intercept = w[:, -1]
0.80 logistic.py(289):         w = w[:, :-1]
0.80 logistic.py(292):     p = safe_sparse_dot(X, w.T)
0.80 logistic.py(293):     p += intercept
0.80 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
0.80 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
0.80 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
0.80 logistic.py(297):     p = np.exp(p, p)
0.80 logistic.py(298):     return loss, p, w
0.80 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
0.80 logistic.py(346):     diff = sample_weight * (p - Y)
0.80 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
0.80 logistic.py(348):     grad[:, :n_features] += alpha * w
0.80 logistic.py(349):     if fit_intercept:
0.80 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
0.80 logistic.py(351):     return loss, grad.ravel(), p
0.80 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
0.80 logistic.py(339):     n_classes = Y.shape[1]
0.80 logistic.py(340):     n_features = X.shape[1]
0.80 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
0.80 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
0.80 logistic.py(343):                     dtype=X.dtype)
0.80 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
0.80 logistic.py(282):     n_classes = Y.shape[1]
0.80 logistic.py(283):     n_features = X.shape[1]
0.80 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
0.80 logistic.py(285):     w = w.reshape(n_classes, -1)
0.80 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
0.80 logistic.py(287):     if fit_intercept:
0.80 logistic.py(288):         intercept = w[:, -1]
0.80 logistic.py(289):         w = w[:, :-1]
0.80 logistic.py(292):     p = safe_sparse_dot(X, w.T)
0.80 logistic.py(293):     p += intercept
0.80 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
0.80 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
0.80 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
0.80 logistic.py(297):     p = np.exp(p, p)
0.80 logistic.py(298):     return loss, p, w
0.80 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
0.80 logistic.py(346):     diff = sample_weight * (p - Y)
0.80 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
0.80 logistic.py(348):     grad[:, :n_features] += alpha * w
0.80 logistic.py(349):     if fit_intercept:
0.80 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
0.80 logistic.py(351):     return loss, grad.ravel(), p
0.80 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
0.80 logistic.py(339):     n_classes = Y.shape[1]
0.80 logistic.py(340):     n_features = X.shape[1]
0.80 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
0.80 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
0.80 logistic.py(343):                     dtype=X.dtype)
0.80 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
0.80 logistic.py(282):     n_classes = Y.shape[1]
0.80 logistic.py(283):     n_features = X.shape[1]
0.80 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
0.80 logistic.py(285):     w = w.reshape(n_classes, -1)
0.80 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
0.80 logistic.py(287):     if fit_intercept:
0.80 logistic.py(288):         intercept = w[:, -1]
0.80 logistic.py(289):         w = w[:, :-1]
0.80 logistic.py(292):     p = safe_sparse_dot(X, w.T)
0.80 logistic.py(293):     p += intercept
0.80 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
0.80 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
0.80 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
0.80 logistic.py(297):     p = np.exp(p, p)
0.80 logistic.py(298):     return loss, p, w
0.80 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
0.80 logistic.py(346):     diff = sample_weight * (p - Y)
0.80 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
0.80 logistic.py(348):     grad[:, :n_features] += alpha * w
0.80 logistic.py(349):     if fit_intercept:
0.80 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
0.80 logistic.py(351):     return loss, grad.ravel(), p
0.80 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
0.80 logistic.py(339):     n_classes = Y.shape[1]
0.80 logistic.py(340):     n_features = X.shape[1]
0.80 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
0.80 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
0.80 logistic.py(343):                     dtype=X.dtype)
0.80 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
0.80 logistic.py(282):     n_classes = Y.shape[1]
0.80 logistic.py(283):     n_features = X.shape[1]
0.80 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
0.80 logistic.py(285):     w = w.reshape(n_classes, -1)
0.80 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
0.80 logistic.py(287):     if fit_intercept:
0.80 logistic.py(288):         intercept = w[:, -1]
0.80 logistic.py(289):         w = w[:, :-1]
0.80 logistic.py(292):     p = safe_sparse_dot(X, w.T)
0.80 logistic.py(293):     p += intercept
0.80 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
0.80 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
0.80 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
0.80 logistic.py(297):     p = np.exp(p, p)
0.80 logistic.py(298):     return loss, p, w
0.80 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
0.80 logistic.py(346):     diff = sample_weight * (p - Y)
0.80 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
0.80 logistic.py(348):     grad[:, :n_features] += alpha * w
0.80 logistic.py(349):     if fit_intercept:
0.80 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
0.80 logistic.py(351):     return loss, grad.ravel(), p
0.80 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
0.80 logistic.py(339):     n_classes = Y.shape[1]
0.80 logistic.py(340):     n_features = X.shape[1]
0.80 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
0.80 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
0.80 logistic.py(343):                     dtype=X.dtype)
0.80 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
0.80 logistic.py(282):     n_classes = Y.shape[1]
0.80 logistic.py(283):     n_features = X.shape[1]
0.80 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
0.80 logistic.py(285):     w = w.reshape(n_classes, -1)
0.80 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
0.80 logistic.py(287):     if fit_intercept:
0.80 logistic.py(288):         intercept = w[:, -1]
0.80 logistic.py(289):         w = w[:, :-1]
0.80 logistic.py(292):     p = safe_sparse_dot(X, w.T)
0.80 logistic.py(293):     p += intercept
0.80 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
0.80 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
0.80 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
0.80 logistic.py(297):     p = np.exp(p, p)
0.80 logistic.py(298):     return loss, p, w
0.80 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
0.80 logistic.py(346):     diff = sample_weight * (p - Y)
0.80 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
0.80 logistic.py(348):     grad[:, :n_features] += alpha * w
0.80 logistic.py(349):     if fit_intercept:
0.80 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
0.80 logistic.py(351):     return loss, grad.ravel(), p
0.80 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
0.80 logistic.py(339):     n_classes = Y.shape[1]
0.80 logistic.py(340):     n_features = X.shape[1]
0.80 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
0.80 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
0.80 logistic.py(343):                     dtype=X.dtype)
0.80 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
0.80 logistic.py(282):     n_classes = Y.shape[1]
0.80 logistic.py(283):     n_features = X.shape[1]
0.80 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
0.80 logistic.py(285):     w = w.reshape(n_classes, -1)
0.80 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
0.80 logistic.py(287):     if fit_intercept:
0.80 logistic.py(288):         intercept = w[:, -1]
0.80 logistic.py(289):         w = w[:, :-1]
0.80 logistic.py(292):     p = safe_sparse_dot(X, w.T)
0.80 logistic.py(293):     p += intercept
0.80 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
0.80 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
0.80 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
0.80 logistic.py(297):     p = np.exp(p, p)
0.80 logistic.py(298):     return loss, p, w
0.80 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
0.80 logistic.py(346):     diff = sample_weight * (p - Y)
0.80 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
0.80 logistic.py(348):     grad[:, :n_features] += alpha * w
0.80 logistic.py(349):     if fit_intercept:
0.80 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
0.80 logistic.py(351):     return loss, grad.ravel(), p
0.80 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
0.80 logistic.py(339):     n_classes = Y.shape[1]
0.80 logistic.py(340):     n_features = X.shape[1]
0.80 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
0.80 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
0.80 logistic.py(343):                     dtype=X.dtype)
0.80 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
0.80 logistic.py(282):     n_classes = Y.shape[1]
0.80 logistic.py(283):     n_features = X.shape[1]
0.80 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
0.80 logistic.py(285):     w = w.reshape(n_classes, -1)
0.80 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
0.80 logistic.py(287):     if fit_intercept:
0.80 logistic.py(288):         intercept = w[:, -1]
0.80 logistic.py(289):         w = w[:, :-1]
0.80 logistic.py(292):     p = safe_sparse_dot(X, w.T)
0.80 logistic.py(293):     p += intercept
0.80 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
0.80 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
0.80 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
0.80 logistic.py(297):     p = np.exp(p, p)
0.80 logistic.py(298):     return loss, p, w
0.80 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
0.80 logistic.py(346):     diff = sample_weight * (p - Y)
0.80 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
0.80 logistic.py(348):     grad[:, :n_features] += alpha * w
0.80 logistic.py(349):     if fit_intercept:
0.80 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
0.80 logistic.py(351):     return loss, grad.ravel(), p
0.80 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
0.80 logistic.py(339):     n_classes = Y.shape[1]
0.80 logistic.py(340):     n_features = X.shape[1]
0.80 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
0.80 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
0.80 logistic.py(343):                     dtype=X.dtype)
0.80 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
0.80 logistic.py(282):     n_classes = Y.shape[1]
0.80 logistic.py(283):     n_features = X.shape[1]
0.80 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
0.80 logistic.py(285):     w = w.reshape(n_classes, -1)
0.80 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
0.80 logistic.py(287):     if fit_intercept:
0.80 logistic.py(288):         intercept = w[:, -1]
0.80 logistic.py(289):         w = w[:, :-1]
0.80 logistic.py(292):     p = safe_sparse_dot(X, w.T)
0.80 logistic.py(293):     p += intercept
0.80 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
0.80 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
0.80 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
0.80 logistic.py(297):     p = np.exp(p, p)
0.80 logistic.py(298):     return loss, p, w
0.80 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
0.80 logistic.py(346):     diff = sample_weight * (p - Y)
0.80 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
0.80 logistic.py(348):     grad[:, :n_features] += alpha * w
0.80 logistic.py(349):     if fit_intercept:
0.80 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
0.80 logistic.py(351):     return loss, grad.ravel(), p
0.80 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
0.80 logistic.py(339):     n_classes = Y.shape[1]
0.80 logistic.py(340):     n_features = X.shape[1]
0.80 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
0.80 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
0.80 logistic.py(343):                     dtype=X.dtype)
0.80 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
0.80 logistic.py(282):     n_classes = Y.shape[1]
0.80 logistic.py(283):     n_features = X.shape[1]
0.80 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
0.80 logistic.py(285):     w = w.reshape(n_classes, -1)
0.80 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
0.80 logistic.py(287):     if fit_intercept:
0.80 logistic.py(288):         intercept = w[:, -1]
0.80 logistic.py(289):         w = w[:, :-1]
0.80 logistic.py(292):     p = safe_sparse_dot(X, w.T)
0.80 logistic.py(293):     p += intercept
0.80 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
0.80 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
0.80 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
0.80 logistic.py(297):     p = np.exp(p, p)
0.80 logistic.py(298):     return loss, p, w
0.80 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
0.80 logistic.py(346):     diff = sample_weight * (p - Y)
0.80 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
0.80 logistic.py(348):     grad[:, :n_features] += alpha * w
0.80 logistic.py(349):     if fit_intercept:
0.80 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
0.80 logistic.py(351):     return loss, grad.ravel(), p
0.80 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
0.80 logistic.py(339):     n_classes = Y.shape[1]
0.80 logistic.py(340):     n_features = X.shape[1]
0.80 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
0.80 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
0.80 logistic.py(343):                     dtype=X.dtype)
0.80 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
0.80 logistic.py(282):     n_classes = Y.shape[1]
0.80 logistic.py(283):     n_features = X.shape[1]
0.80 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
0.80 logistic.py(285):     w = w.reshape(n_classes, -1)
0.80 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
0.80 logistic.py(287):     if fit_intercept:
0.80 logistic.py(288):         intercept = w[:, -1]
0.80 logistic.py(289):         w = w[:, :-1]
0.80 logistic.py(292):     p = safe_sparse_dot(X, w.T)
0.80 logistic.py(293):     p += intercept
0.80 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
0.80 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
0.80 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
0.80 logistic.py(297):     p = np.exp(p, p)
0.80 logistic.py(298):     return loss, p, w
0.80 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
0.80 logistic.py(346):     diff = sample_weight * (p - Y)
0.80 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
0.80 logistic.py(348):     grad[:, :n_features] += alpha * w
0.80 logistic.py(349):     if fit_intercept:
0.80 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
0.80 logistic.py(351):     return loss, grad.ravel(), p
0.80 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
0.80 logistic.py(339):     n_classes = Y.shape[1]
0.80 logistic.py(340):     n_features = X.shape[1]
0.80 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
0.80 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
0.80 logistic.py(343):                     dtype=X.dtype)
0.80 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
0.80 logistic.py(282):     n_classes = Y.shape[1]
0.80 logistic.py(283):     n_features = X.shape[1]
0.80 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
0.80 logistic.py(285):     w = w.reshape(n_classes, -1)
0.80 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
0.80 logistic.py(287):     if fit_intercept:
0.80 logistic.py(288):         intercept = w[:, -1]
0.80 logistic.py(289):         w = w[:, :-1]
0.80 logistic.py(292):     p = safe_sparse_dot(X, w.T)
0.80 logistic.py(293):     p += intercept
0.80 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
0.80 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
0.80 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
0.80 logistic.py(297):     p = np.exp(p, p)
0.80 logistic.py(298):     return loss, p, w
0.80 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
0.80 logistic.py(346):     diff = sample_weight * (p - Y)
0.80 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
0.80 logistic.py(348):     grad[:, :n_features] += alpha * w
0.80 logistic.py(349):     if fit_intercept:
0.80 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
0.80 logistic.py(351):     return loss, grad.ravel(), p
0.80 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
0.80 logistic.py(339):     n_classes = Y.shape[1]
0.80 logistic.py(340):     n_features = X.shape[1]
0.80 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
0.80 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
0.80 logistic.py(343):                     dtype=X.dtype)
0.80 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
0.80 logistic.py(282):     n_classes = Y.shape[1]
0.80 logistic.py(283):     n_features = X.shape[1]
0.80 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
0.80 logistic.py(285):     w = w.reshape(n_classes, -1)
0.80 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
0.80 logistic.py(287):     if fit_intercept:
0.80 logistic.py(288):         intercept = w[:, -1]
0.80 logistic.py(289):         w = w[:, :-1]
0.80 logistic.py(292):     p = safe_sparse_dot(X, w.T)
0.80 logistic.py(293):     p += intercept
0.80 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
0.80 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
0.80 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
0.80 logistic.py(297):     p = np.exp(p, p)
0.80 logistic.py(298):     return loss, p, w
0.80 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
0.80 logistic.py(346):     diff = sample_weight * (p - Y)
0.80 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
0.80 logistic.py(348):     grad[:, :n_features] += alpha * w
0.80 logistic.py(349):     if fit_intercept:
0.80 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
0.80 logistic.py(351):     return loss, grad.ravel(), p
0.80 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
0.80 logistic.py(339):     n_classes = Y.shape[1]
0.80 logistic.py(340):     n_features = X.shape[1]
0.80 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
0.80 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
0.80 logistic.py(343):                     dtype=X.dtype)
0.80 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
0.80 logistic.py(282):     n_classes = Y.shape[1]
0.80 logistic.py(283):     n_features = X.shape[1]
0.80 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
0.80 logistic.py(285):     w = w.reshape(n_classes, -1)
0.80 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
0.80 logistic.py(287):     if fit_intercept:
0.80 logistic.py(288):         intercept = w[:, -1]
0.80 logistic.py(289):         w = w[:, :-1]
0.80 logistic.py(292):     p = safe_sparse_dot(X, w.T)
0.80 logistic.py(293):     p += intercept
0.80 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
0.80 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
0.80 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
0.80 logistic.py(297):     p = np.exp(p, p)
0.80 logistic.py(298):     return loss, p, w
0.80 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
0.80 logistic.py(346):     diff = sample_weight * (p - Y)
0.80 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
0.80 logistic.py(348):     grad[:, :n_features] += alpha * w
0.80 logistic.py(349):     if fit_intercept:
0.80 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
0.80 logistic.py(351):     return loss, grad.ravel(), p
0.80 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
0.80 logistic.py(339):     n_classes = Y.shape[1]
0.80 logistic.py(340):     n_features = X.shape[1]
0.80 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
0.80 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
0.80 logistic.py(343):                     dtype=X.dtype)
0.80 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
0.80 logistic.py(282):     n_classes = Y.shape[1]
0.80 logistic.py(283):     n_features = X.shape[1]
0.80 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
0.80 logistic.py(285):     w = w.reshape(n_classes, -1)
0.80 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
0.80 logistic.py(287):     if fit_intercept:
0.80 logistic.py(288):         intercept = w[:, -1]
0.80 logistic.py(289):         w = w[:, :-1]
0.80 logistic.py(292):     p = safe_sparse_dot(X, w.T)
0.80 logistic.py(293):     p += intercept
0.80 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
0.80 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
0.80 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
0.80 logistic.py(297):     p = np.exp(p, p)
0.80 logistic.py(298):     return loss, p, w
0.80 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
0.80 logistic.py(346):     diff = sample_weight * (p - Y)
0.80 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
0.80 logistic.py(348):     grad[:, :n_features] += alpha * w
0.80 logistic.py(349):     if fit_intercept:
0.80 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
0.80 logistic.py(351):     return loss, grad.ravel(), p
0.80 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
0.80 logistic.py(339):     n_classes = Y.shape[1]
0.80 logistic.py(340):     n_features = X.shape[1]
0.80 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
0.80 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
0.80 logistic.py(343):                     dtype=X.dtype)
0.80 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
0.80 logistic.py(282):     n_classes = Y.shape[1]
0.80 logistic.py(283):     n_features = X.shape[1]
0.80 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
0.80 logistic.py(285):     w = w.reshape(n_classes, -1)
0.80 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
0.80 logistic.py(287):     if fit_intercept:
0.80 logistic.py(288):         intercept = w[:, -1]
0.80 logistic.py(289):         w = w[:, :-1]
0.80 logistic.py(292):     p = safe_sparse_dot(X, w.T)
0.80 logistic.py(293):     p += intercept
0.80 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
0.80 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
0.80 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
0.80 logistic.py(297):     p = np.exp(p, p)
0.80 logistic.py(298):     return loss, p, w
0.80 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
0.80 logistic.py(346):     diff = sample_weight * (p - Y)
0.80 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
0.81 logistic.py(348):     grad[:, :n_features] += alpha * w
0.81 logistic.py(349):     if fit_intercept:
0.81 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
0.81 logistic.py(351):     return loss, grad.ravel(), p
0.81 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
0.81 logistic.py(339):     n_classes = Y.shape[1]
0.81 logistic.py(340):     n_features = X.shape[1]
0.81 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
0.81 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
0.81 logistic.py(343):                     dtype=X.dtype)
0.81 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
0.81 logistic.py(282):     n_classes = Y.shape[1]
0.81 logistic.py(283):     n_features = X.shape[1]
0.81 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
0.81 logistic.py(285):     w = w.reshape(n_classes, -1)
0.81 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
0.81 logistic.py(287):     if fit_intercept:
0.81 logistic.py(288):         intercept = w[:, -1]
0.81 logistic.py(289):         w = w[:, :-1]
0.81 logistic.py(292):     p = safe_sparse_dot(X, w.T)
0.81 logistic.py(293):     p += intercept
0.81 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
0.81 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
0.81 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
0.81 logistic.py(297):     p = np.exp(p, p)
0.81 logistic.py(298):     return loss, p, w
0.81 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
0.81 logistic.py(346):     diff = sample_weight * (p - Y)
0.81 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
0.81 logistic.py(348):     grad[:, :n_features] += alpha * w
0.81 logistic.py(349):     if fit_intercept:
0.81 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
0.81 logistic.py(351):     return loss, grad.ravel(), p
0.81 logistic.py(718):             if info["warnflag"] == 1:
0.81 logistic.py(723):             n_iter_i = min(info['nit'], max_iter)
0.81 logistic.py(760):         if multi_class == 'multinomial':
0.81 logistic.py(761):             multi_w0 = np.reshape(w0, (classes.size, -1))
0.81 logistic.py(762):             if classes.size == 2:
0.81 logistic.py(764):             coefs.append(multi_w0)
0.81 logistic.py(768):         n_iter[i] = n_iter_i
0.81 logistic.py(712):     for i, C in enumerate(Cs):
0.81 logistic.py(713):         if solver == 'lbfgs':
0.81 logistic.py(714):             w0, loss, info = optimize.fmin_l_bfgs_b(
0.81 logistic.py(715):                 func, w0, fprime=None,
0.81 logistic.py(716):                 args=(X, target, 1. / C, sample_weight),
0.81 logistic.py(717):                 iprint=(verbose > 0) - 1, pgtol=tol, maxiter=max_iter)
0.81 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
0.81 logistic.py(339):     n_classes = Y.shape[1]
0.81 logistic.py(340):     n_features = X.shape[1]
0.81 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
0.81 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
0.81 logistic.py(343):                     dtype=X.dtype)
0.81 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
0.81 logistic.py(282):     n_classes = Y.shape[1]
0.81 logistic.py(283):     n_features = X.shape[1]
0.81 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
0.81 logistic.py(285):     w = w.reshape(n_classes, -1)
0.81 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
0.81 logistic.py(287):     if fit_intercept:
0.81 logistic.py(288):         intercept = w[:, -1]
0.81 logistic.py(289):         w = w[:, :-1]
0.81 logistic.py(292):     p = safe_sparse_dot(X, w.T)
0.81 logistic.py(293):     p += intercept
0.81 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
0.81 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
0.81 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
0.81 logistic.py(297):     p = np.exp(p, p)
0.81 logistic.py(298):     return loss, p, w
0.81 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
0.81 logistic.py(346):     diff = sample_weight * (p - Y)
0.81 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
0.81 logistic.py(348):     grad[:, :n_features] += alpha * w
0.81 logistic.py(349):     if fit_intercept:
0.81 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
0.81 logistic.py(351):     return loss, grad.ravel(), p
0.81 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
0.81 logistic.py(339):     n_classes = Y.shape[1]
0.81 logistic.py(340):     n_features = X.shape[1]
0.81 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
0.81 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
0.81 logistic.py(343):                     dtype=X.dtype)
0.81 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
0.81 logistic.py(282):     n_classes = Y.shape[1]
0.81 logistic.py(283):     n_features = X.shape[1]
0.81 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
0.81 logistic.py(285):     w = w.reshape(n_classes, -1)
0.81 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
0.81 logistic.py(287):     if fit_intercept:
0.81 logistic.py(288):         intercept = w[:, -1]
0.81 logistic.py(289):         w = w[:, :-1]
0.81 logistic.py(292):     p = safe_sparse_dot(X, w.T)
0.81 logistic.py(293):     p += intercept
0.81 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
0.81 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
0.81 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
0.81 logistic.py(297):     p = np.exp(p, p)
0.81 logistic.py(298):     return loss, p, w
0.81 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
0.81 logistic.py(346):     diff = sample_weight * (p - Y)
0.81 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
0.81 logistic.py(348):     grad[:, :n_features] += alpha * w
0.81 logistic.py(349):     if fit_intercept:
0.81 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
0.81 logistic.py(351):     return loss, grad.ravel(), p
0.81 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
0.81 logistic.py(339):     n_classes = Y.shape[1]
0.81 logistic.py(340):     n_features = X.shape[1]
0.81 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
0.81 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
0.81 logistic.py(343):                     dtype=X.dtype)
0.81 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
0.81 logistic.py(282):     n_classes = Y.shape[1]
0.81 logistic.py(283):     n_features = X.shape[1]
0.81 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
0.81 logistic.py(285):     w = w.reshape(n_classes, -1)
0.81 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
0.81 logistic.py(287):     if fit_intercept:
0.81 logistic.py(288):         intercept = w[:, -1]
0.81 logistic.py(289):         w = w[:, :-1]
0.81 logistic.py(292):     p = safe_sparse_dot(X, w.T)
0.81 logistic.py(293):     p += intercept
0.81 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
0.81 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
0.81 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
0.81 logistic.py(297):     p = np.exp(p, p)
0.81 logistic.py(298):     return loss, p, w
0.81 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
0.81 logistic.py(346):     diff = sample_weight * (p - Y)
0.81 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
0.81 logistic.py(348):     grad[:, :n_features] += alpha * w
0.81 logistic.py(349):     if fit_intercept:
0.81 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
0.81 logistic.py(351):     return loss, grad.ravel(), p
0.81 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
0.81 logistic.py(339):     n_classes = Y.shape[1]
0.81 logistic.py(340):     n_features = X.shape[1]
0.81 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
0.81 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
0.81 logistic.py(343):                     dtype=X.dtype)
0.81 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
0.81 logistic.py(282):     n_classes = Y.shape[1]
0.81 logistic.py(283):     n_features = X.shape[1]
0.81 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
0.81 logistic.py(285):     w = w.reshape(n_classes, -1)
0.81 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
0.81 logistic.py(287):     if fit_intercept:
0.81 logistic.py(288):         intercept = w[:, -1]
0.81 logistic.py(289):         w = w[:, :-1]
0.81 logistic.py(292):     p = safe_sparse_dot(X, w.T)
0.81 logistic.py(293):     p += intercept
0.81 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
0.81 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
0.81 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
0.81 logistic.py(297):     p = np.exp(p, p)
0.81 logistic.py(298):     return loss, p, w
0.81 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
0.81 logistic.py(346):     diff = sample_weight * (p - Y)
0.81 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
0.81 logistic.py(348):     grad[:, :n_features] += alpha * w
0.81 logistic.py(349):     if fit_intercept:
0.81 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
0.81 logistic.py(351):     return loss, grad.ravel(), p
0.81 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
0.81 logistic.py(339):     n_classes = Y.shape[1]
0.81 logistic.py(340):     n_features = X.shape[1]
0.81 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
0.81 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
0.81 logistic.py(343):                     dtype=X.dtype)
0.81 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
0.81 logistic.py(282):     n_classes = Y.shape[1]
0.81 logistic.py(283):     n_features = X.shape[1]
0.81 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
0.81 logistic.py(285):     w = w.reshape(n_classes, -1)
0.81 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
0.81 logistic.py(287):     if fit_intercept:
0.81 logistic.py(288):         intercept = w[:, -1]
0.81 logistic.py(289):         w = w[:, :-1]
0.81 logistic.py(292):     p = safe_sparse_dot(X, w.T)
0.81 logistic.py(293):     p += intercept
0.81 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
0.81 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
0.81 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
0.81 logistic.py(297):     p = np.exp(p, p)
0.81 logistic.py(298):     return loss, p, w
0.81 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
0.81 logistic.py(346):     diff = sample_weight * (p - Y)
0.81 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
0.81 logistic.py(348):     grad[:, :n_features] += alpha * w
0.81 logistic.py(349):     if fit_intercept:
0.81 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
0.81 logistic.py(351):     return loss, grad.ravel(), p
0.81 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
0.81 logistic.py(339):     n_classes = Y.shape[1]
0.81 logistic.py(340):     n_features = X.shape[1]
0.81 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
0.81 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
0.81 logistic.py(343):                     dtype=X.dtype)
0.81 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
0.81 logistic.py(282):     n_classes = Y.shape[1]
0.81 logistic.py(283):     n_features = X.shape[1]
0.81 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
0.81 logistic.py(285):     w = w.reshape(n_classes, -1)
0.81 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
0.81 logistic.py(287):     if fit_intercept:
0.81 logistic.py(288):         intercept = w[:, -1]
0.81 logistic.py(289):         w = w[:, :-1]
0.81 logistic.py(292):     p = safe_sparse_dot(X, w.T)
0.81 logistic.py(293):     p += intercept
0.81 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
0.81 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
0.81 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
0.81 logistic.py(297):     p = np.exp(p, p)
0.81 logistic.py(298):     return loss, p, w
0.81 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
0.81 logistic.py(346):     diff = sample_weight * (p - Y)
0.81 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
0.81 logistic.py(348):     grad[:, :n_features] += alpha * w
0.81 logistic.py(349):     if fit_intercept:
0.81 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
0.81 logistic.py(351):     return loss, grad.ravel(), p
0.81 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
0.81 logistic.py(339):     n_classes = Y.shape[1]
0.81 logistic.py(340):     n_features = X.shape[1]
0.81 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
0.81 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
0.81 logistic.py(343):                     dtype=X.dtype)
0.81 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
0.81 logistic.py(282):     n_classes = Y.shape[1]
0.81 logistic.py(283):     n_features = X.shape[1]
0.81 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
0.81 logistic.py(285):     w = w.reshape(n_classes, -1)
0.81 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
0.81 logistic.py(287):     if fit_intercept:
0.81 logistic.py(288):         intercept = w[:, -1]
0.81 logistic.py(289):         w = w[:, :-1]
0.81 logistic.py(292):     p = safe_sparse_dot(X, w.T)
0.81 logistic.py(293):     p += intercept
0.81 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
0.81 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
0.81 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
0.81 logistic.py(297):     p = np.exp(p, p)
0.81 logistic.py(298):     return loss, p, w
0.81 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
0.81 logistic.py(346):     diff = sample_weight * (p - Y)
0.81 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
0.81 logistic.py(348):     grad[:, :n_features] += alpha * w
0.81 logistic.py(349):     if fit_intercept:
0.81 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
0.81 logistic.py(351):     return loss, grad.ravel(), p
0.81 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
0.81 logistic.py(339):     n_classes = Y.shape[1]
0.81 logistic.py(340):     n_features = X.shape[1]
0.81 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
0.81 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
0.81 logistic.py(343):                     dtype=X.dtype)
0.81 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
0.81 logistic.py(282):     n_classes = Y.shape[1]
0.81 logistic.py(283):     n_features = X.shape[1]
0.81 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
0.81 logistic.py(285):     w = w.reshape(n_classes, -1)
0.81 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
0.81 logistic.py(287):     if fit_intercept:
0.81 logistic.py(288):         intercept = w[:, -1]
0.81 logistic.py(289):         w = w[:, :-1]
0.81 logistic.py(292):     p = safe_sparse_dot(X, w.T)
0.81 logistic.py(293):     p += intercept
0.81 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
0.81 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
0.81 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
0.81 logistic.py(297):     p = np.exp(p, p)
0.81 logistic.py(298):     return loss, p, w
0.81 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
0.81 logistic.py(346):     diff = sample_weight * (p - Y)
0.81 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
0.81 logistic.py(348):     grad[:, :n_features] += alpha * w
0.81 logistic.py(349):     if fit_intercept:
0.81 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
0.81 logistic.py(351):     return loss, grad.ravel(), p
0.81 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
0.81 logistic.py(339):     n_classes = Y.shape[1]
0.81 logistic.py(340):     n_features = X.shape[1]
0.81 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
0.81 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
0.81 logistic.py(343):                     dtype=X.dtype)
0.81 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
0.81 logistic.py(282):     n_classes = Y.shape[1]
0.81 logistic.py(283):     n_features = X.shape[1]
0.81 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
0.81 logistic.py(285):     w = w.reshape(n_classes, -1)
0.81 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
0.81 logistic.py(287):     if fit_intercept:
0.81 logistic.py(288):         intercept = w[:, -1]
0.81 logistic.py(289):         w = w[:, :-1]
0.81 logistic.py(292):     p = safe_sparse_dot(X, w.T)
0.81 logistic.py(293):     p += intercept
0.81 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
0.81 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
0.81 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
0.81 logistic.py(297):     p = np.exp(p, p)
0.81 logistic.py(298):     return loss, p, w
0.81 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
0.81 logistic.py(346):     diff = sample_weight * (p - Y)
0.81 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
0.81 logistic.py(348):     grad[:, :n_features] += alpha * w
0.81 logistic.py(349):     if fit_intercept:
0.81 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
0.81 logistic.py(351):     return loss, grad.ravel(), p
0.81 logistic.py(718):             if info["warnflag"] == 1:
0.81 logistic.py(723):             n_iter_i = min(info['nit'], max_iter)
0.81 logistic.py(760):         if multi_class == 'multinomial':
0.81 logistic.py(761):             multi_w0 = np.reshape(w0, (classes.size, -1))
0.81 logistic.py(762):             if classes.size == 2:
0.81 logistic.py(764):             coefs.append(multi_w0)
0.81 logistic.py(768):         n_iter[i] = n_iter_i
0.81 logistic.py(712):     for i, C in enumerate(Cs):
0.81 logistic.py(713):         if solver == 'lbfgs':
0.81 logistic.py(714):             w0, loss, info = optimize.fmin_l_bfgs_b(
0.81 logistic.py(715):                 func, w0, fprime=None,
0.81 logistic.py(716):                 args=(X, target, 1. / C, sample_weight),
0.81 logistic.py(717):                 iprint=(verbose > 0) - 1, pgtol=tol, maxiter=max_iter)
0.81 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
0.81 logistic.py(339):     n_classes = Y.shape[1]
0.81 logistic.py(340):     n_features = X.shape[1]
0.81 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
0.81 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
0.81 logistic.py(343):                     dtype=X.dtype)
0.81 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
0.81 logistic.py(282):     n_classes = Y.shape[1]
0.81 logistic.py(283):     n_features = X.shape[1]
0.81 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
0.81 logistic.py(285):     w = w.reshape(n_classes, -1)
0.81 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
0.81 logistic.py(287):     if fit_intercept:
0.81 logistic.py(288):         intercept = w[:, -1]
0.81 logistic.py(289):         w = w[:, :-1]
0.81 logistic.py(292):     p = safe_sparse_dot(X, w.T)
0.81 logistic.py(293):     p += intercept
0.81 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
0.81 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
0.81 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
0.81 logistic.py(297):     p = np.exp(p, p)
0.81 logistic.py(298):     return loss, p, w
0.81 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
0.81 logistic.py(346):     diff = sample_weight * (p - Y)
0.81 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
0.81 logistic.py(348):     grad[:, :n_features] += alpha * w
0.81 logistic.py(349):     if fit_intercept:
0.81 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
0.81 logistic.py(351):     return loss, grad.ravel(), p
0.81 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
0.81 logistic.py(339):     n_classes = Y.shape[1]
0.81 logistic.py(340):     n_features = X.shape[1]
0.81 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
0.81 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
0.81 logistic.py(343):                     dtype=X.dtype)
0.81 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
0.81 logistic.py(282):     n_classes = Y.shape[1]
0.81 logistic.py(283):     n_features = X.shape[1]
0.81 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
0.81 logistic.py(285):     w = w.reshape(n_classes, -1)
0.81 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
0.81 logistic.py(287):     if fit_intercept:
0.81 logistic.py(288):         intercept = w[:, -1]
0.81 logistic.py(289):         w = w[:, :-1]
0.81 logistic.py(292):     p = safe_sparse_dot(X, w.T)
0.81 logistic.py(293):     p += intercept
0.81 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
0.81 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
0.81 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
0.81 logistic.py(297):     p = np.exp(p, p)
0.81 logistic.py(298):     return loss, p, w
0.81 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
0.81 logistic.py(346):     diff = sample_weight * (p - Y)
0.81 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
0.81 logistic.py(348):     grad[:, :n_features] += alpha * w
0.81 logistic.py(349):     if fit_intercept:
0.81 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
0.81 logistic.py(351):     return loss, grad.ravel(), p
0.81 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
0.81 logistic.py(339):     n_classes = Y.shape[1]
0.81 logistic.py(340):     n_features = X.shape[1]
0.81 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
0.81 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
0.81 logistic.py(343):                     dtype=X.dtype)
0.81 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
0.81 logistic.py(282):     n_classes = Y.shape[1]
0.81 logistic.py(283):     n_features = X.shape[1]
0.81 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
0.81 logistic.py(285):     w = w.reshape(n_classes, -1)
0.81 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
0.81 logistic.py(287):     if fit_intercept:
0.81 logistic.py(288):         intercept = w[:, -1]
0.81 logistic.py(289):         w = w[:, :-1]
0.81 logistic.py(292):     p = safe_sparse_dot(X, w.T)
0.81 logistic.py(293):     p += intercept
0.81 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
0.81 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
0.81 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
0.81 logistic.py(297):     p = np.exp(p, p)
0.81 logistic.py(298):     return loss, p, w
0.81 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
0.81 logistic.py(346):     diff = sample_weight * (p - Y)
0.81 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
0.81 logistic.py(348):     grad[:, :n_features] += alpha * w
0.81 logistic.py(349):     if fit_intercept:
0.81 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
0.81 logistic.py(351):     return loss, grad.ravel(), p
0.81 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
0.81 logistic.py(339):     n_classes = Y.shape[1]
0.81 logistic.py(340):     n_features = X.shape[1]
0.81 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
0.81 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
0.81 logistic.py(343):                     dtype=X.dtype)
0.81 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
0.81 logistic.py(282):     n_classes = Y.shape[1]
0.81 logistic.py(283):     n_features = X.shape[1]
0.81 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
0.81 logistic.py(285):     w = w.reshape(n_classes, -1)
0.81 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
0.81 logistic.py(287):     if fit_intercept:
0.81 logistic.py(288):         intercept = w[:, -1]
0.81 logistic.py(289):         w = w[:, :-1]
0.81 logistic.py(292):     p = safe_sparse_dot(X, w.T)
0.81 logistic.py(293):     p += intercept
0.81 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
0.81 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
0.81 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
0.81 logistic.py(297):     p = np.exp(p, p)
0.81 logistic.py(298):     return loss, p, w
0.81 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
0.81 logistic.py(346):     diff = sample_weight * (p - Y)
0.81 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
0.81 logistic.py(348):     grad[:, :n_features] += alpha * w
0.81 logistic.py(349):     if fit_intercept:
0.81 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
0.81 logistic.py(351):     return loss, grad.ravel(), p
0.81 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
0.81 logistic.py(339):     n_classes = Y.shape[1]
0.81 logistic.py(340):     n_features = X.shape[1]
0.81 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
0.81 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
0.81 logistic.py(343):                     dtype=X.dtype)
0.81 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
0.81 logistic.py(282):     n_classes = Y.shape[1]
0.81 logistic.py(283):     n_features = X.shape[1]
0.81 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
0.81 logistic.py(285):     w = w.reshape(n_classes, -1)
0.81 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
0.81 logistic.py(287):     if fit_intercept:
0.81 logistic.py(288):         intercept = w[:, -1]
0.81 logistic.py(289):         w = w[:, :-1]
0.81 logistic.py(292):     p = safe_sparse_dot(X, w.T)
0.81 logistic.py(293):     p += intercept
0.81 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
0.81 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
0.81 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
0.81 logistic.py(297):     p = np.exp(p, p)
0.81 logistic.py(298):     return loss, p, w
0.81 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
0.81 logistic.py(346):     diff = sample_weight * (p - Y)
0.81 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
0.81 logistic.py(348):     grad[:, :n_features] += alpha * w
0.81 logistic.py(349):     if fit_intercept:
0.81 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
0.81 logistic.py(351):     return loss, grad.ravel(), p
0.81 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
0.81 logistic.py(339):     n_classes = Y.shape[1]
0.81 logistic.py(340):     n_features = X.shape[1]
0.81 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
0.81 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
0.81 logistic.py(343):                     dtype=X.dtype)
0.82 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
0.82 logistic.py(282):     n_classes = Y.shape[1]
0.82 logistic.py(283):     n_features = X.shape[1]
0.82 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
0.82 logistic.py(285):     w = w.reshape(n_classes, -1)
0.82 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
0.82 logistic.py(287):     if fit_intercept:
0.82 logistic.py(288):         intercept = w[:, -1]
0.82 logistic.py(289):         w = w[:, :-1]
0.82 logistic.py(292):     p = safe_sparse_dot(X, w.T)
0.82 logistic.py(293):     p += intercept
0.82 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
0.82 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
0.82 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
0.82 logistic.py(297):     p = np.exp(p, p)
0.82 logistic.py(298):     return loss, p, w
0.82 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
0.82 logistic.py(346):     diff = sample_weight * (p - Y)
0.82 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
0.82 logistic.py(348):     grad[:, :n_features] += alpha * w
0.82 logistic.py(349):     if fit_intercept:
0.82 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
0.82 logistic.py(351):     return loss, grad.ravel(), p
0.82 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
0.82 logistic.py(339):     n_classes = Y.shape[1]
0.82 logistic.py(340):     n_features = X.shape[1]
0.82 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
0.82 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
0.82 logistic.py(343):                     dtype=X.dtype)
0.82 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
0.82 logistic.py(282):     n_classes = Y.shape[1]
0.82 logistic.py(283):     n_features = X.shape[1]
0.82 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
0.82 logistic.py(285):     w = w.reshape(n_classes, -1)
0.82 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
0.82 logistic.py(287):     if fit_intercept:
0.82 logistic.py(288):         intercept = w[:, -1]
0.82 logistic.py(289):         w = w[:, :-1]
0.82 logistic.py(292):     p = safe_sparse_dot(X, w.T)
0.82 logistic.py(293):     p += intercept
0.82 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
0.82 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
0.82 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
0.82 logistic.py(297):     p = np.exp(p, p)
0.82 logistic.py(298):     return loss, p, w
0.82 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
0.82 logistic.py(346):     diff = sample_weight * (p - Y)
0.82 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
0.82 logistic.py(348):     grad[:, :n_features] += alpha * w
0.82 logistic.py(349):     if fit_intercept:
0.82 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
0.82 logistic.py(351):     return loss, grad.ravel(), p
0.82 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
0.82 logistic.py(339):     n_classes = Y.shape[1]
0.82 logistic.py(340):     n_features = X.shape[1]
0.82 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
0.82 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
0.82 logistic.py(343):                     dtype=X.dtype)
0.82 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
0.82 logistic.py(282):     n_classes = Y.shape[1]
0.82 logistic.py(283):     n_features = X.shape[1]
0.82 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
0.82 logistic.py(285):     w = w.reshape(n_classes, -1)
0.82 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
0.82 logistic.py(287):     if fit_intercept:
0.82 logistic.py(288):         intercept = w[:, -1]
0.82 logistic.py(289):         w = w[:, :-1]
0.82 logistic.py(292):     p = safe_sparse_dot(X, w.T)
0.82 logistic.py(293):     p += intercept
0.82 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
0.82 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
0.82 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
0.82 logistic.py(297):     p = np.exp(p, p)
0.82 logistic.py(298):     return loss, p, w
0.82 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
0.82 logistic.py(346):     diff = sample_weight * (p - Y)
0.82 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
0.82 logistic.py(348):     grad[:, :n_features] += alpha * w
0.82 logistic.py(349):     if fit_intercept:
0.82 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
0.82 logistic.py(351):     return loss, grad.ravel(), p
0.82 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
0.82 logistic.py(339):     n_classes = Y.shape[1]
0.82 logistic.py(340):     n_features = X.shape[1]
0.82 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
0.82 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
0.82 logistic.py(343):                     dtype=X.dtype)
0.82 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
0.82 logistic.py(282):     n_classes = Y.shape[1]
0.82 logistic.py(283):     n_features = X.shape[1]
0.82 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
0.82 logistic.py(285):     w = w.reshape(n_classes, -1)
0.82 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
0.82 logistic.py(287):     if fit_intercept:
0.82 logistic.py(288):         intercept = w[:, -1]
0.82 logistic.py(289):         w = w[:, :-1]
0.82 logistic.py(292):     p = safe_sparse_dot(X, w.T)
0.82 logistic.py(293):     p += intercept
0.82 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
0.82 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
0.82 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
0.82 logistic.py(297):     p = np.exp(p, p)
0.82 logistic.py(298):     return loss, p, w
0.82 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
0.82 logistic.py(346):     diff = sample_weight * (p - Y)
0.82 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
0.82 logistic.py(348):     grad[:, :n_features] += alpha * w
0.82 logistic.py(349):     if fit_intercept:
0.82 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
0.82 logistic.py(351):     return loss, grad.ravel(), p
0.82 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
0.82 logistic.py(339):     n_classes = Y.shape[1]
0.82 logistic.py(340):     n_features = X.shape[1]
0.82 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
0.82 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
0.82 logistic.py(343):                     dtype=X.dtype)
0.82 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
0.82 logistic.py(282):     n_classes = Y.shape[1]
0.82 logistic.py(283):     n_features = X.shape[1]
0.82 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
0.82 logistic.py(285):     w = w.reshape(n_classes, -1)
0.82 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
0.82 logistic.py(287):     if fit_intercept:
0.82 logistic.py(288):         intercept = w[:, -1]
0.82 logistic.py(289):         w = w[:, :-1]
0.82 logistic.py(292):     p = safe_sparse_dot(X, w.T)
0.82 logistic.py(293):     p += intercept
0.82 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
0.82 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
0.82 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
0.82 logistic.py(297):     p = np.exp(p, p)
0.82 logistic.py(298):     return loss, p, w
0.82 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
0.82 logistic.py(346):     diff = sample_weight * (p - Y)
0.82 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
0.82 logistic.py(348):     grad[:, :n_features] += alpha * w
0.82 logistic.py(349):     if fit_intercept:
0.82 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
0.82 logistic.py(351):     return loss, grad.ravel(), p
0.82 logistic.py(718):             if info["warnflag"] == 1:
0.82 logistic.py(723):             n_iter_i = min(info['nit'], max_iter)
0.82 logistic.py(760):         if multi_class == 'multinomial':
0.82 logistic.py(761):             multi_w0 = np.reshape(w0, (classes.size, -1))
0.82 logistic.py(762):             if classes.size == 2:
0.82 logistic.py(764):             coefs.append(multi_w0)
0.82 logistic.py(768):         n_iter[i] = n_iter_i
0.82 logistic.py(712):     for i, C in enumerate(Cs):
0.82 logistic.py(713):         if solver == 'lbfgs':
0.82 logistic.py(714):             w0, loss, info = optimize.fmin_l_bfgs_b(
0.82 logistic.py(715):                 func, w0, fprime=None,
0.82 logistic.py(716):                 args=(X, target, 1. / C, sample_weight),
0.82 logistic.py(717):                 iprint=(verbose > 0) - 1, pgtol=tol, maxiter=max_iter)
0.82 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
0.82 logistic.py(339):     n_classes = Y.shape[1]
0.82 logistic.py(340):     n_features = X.shape[1]
0.82 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
0.82 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
0.82 logistic.py(343):                     dtype=X.dtype)
0.82 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
0.82 logistic.py(282):     n_classes = Y.shape[1]
0.82 logistic.py(283):     n_features = X.shape[1]
0.82 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
0.82 logistic.py(285):     w = w.reshape(n_classes, -1)
0.82 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
0.82 logistic.py(287):     if fit_intercept:
0.82 logistic.py(288):         intercept = w[:, -1]
0.82 logistic.py(289):         w = w[:, :-1]
0.82 logistic.py(292):     p = safe_sparse_dot(X, w.T)
0.82 logistic.py(293):     p += intercept
0.82 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
0.82 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
0.82 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
0.82 logistic.py(297):     p = np.exp(p, p)
0.82 logistic.py(298):     return loss, p, w
0.82 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
0.82 logistic.py(346):     diff = sample_weight * (p - Y)
0.82 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
0.82 logistic.py(348):     grad[:, :n_features] += alpha * w
0.82 logistic.py(349):     if fit_intercept:
0.82 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
0.82 logistic.py(351):     return loss, grad.ravel(), p
0.82 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
0.82 logistic.py(339):     n_classes = Y.shape[1]
0.82 logistic.py(340):     n_features = X.shape[1]
0.82 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
0.82 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
0.82 logistic.py(343):                     dtype=X.dtype)
0.82 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
0.82 logistic.py(282):     n_classes = Y.shape[1]
0.82 logistic.py(283):     n_features = X.shape[1]
0.82 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
0.82 logistic.py(285):     w = w.reshape(n_classes, -1)
0.82 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
0.82 logistic.py(287):     if fit_intercept:
0.82 logistic.py(288):         intercept = w[:, -1]
0.82 logistic.py(289):         w = w[:, :-1]
0.82 logistic.py(292):     p = safe_sparse_dot(X, w.T)
0.82 logistic.py(293):     p += intercept
0.82 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
0.82 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
0.82 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
0.82 logistic.py(297):     p = np.exp(p, p)
0.82 logistic.py(298):     return loss, p, w
0.82 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
0.82 logistic.py(346):     diff = sample_weight * (p - Y)
0.82 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
0.82 logistic.py(348):     grad[:, :n_features] += alpha * w
0.82 logistic.py(349):     if fit_intercept:
0.82 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
0.82 logistic.py(351):     return loss, grad.ravel(), p
0.82 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
0.82 logistic.py(339):     n_classes = Y.shape[1]
0.82 logistic.py(340):     n_features = X.shape[1]
0.82 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
0.82 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
0.82 logistic.py(343):                     dtype=X.dtype)
0.82 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
0.82 logistic.py(282):     n_classes = Y.shape[1]
0.82 logistic.py(283):     n_features = X.shape[1]
0.82 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
0.82 logistic.py(285):     w = w.reshape(n_classes, -1)
0.82 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
0.82 logistic.py(287):     if fit_intercept:
0.82 logistic.py(288):         intercept = w[:, -1]
0.82 logistic.py(289):         w = w[:, :-1]
0.82 logistic.py(292):     p = safe_sparse_dot(X, w.T)
0.82 logistic.py(293):     p += intercept
0.82 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
0.82 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
0.82 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
0.82 logistic.py(297):     p = np.exp(p, p)
0.82 logistic.py(298):     return loss, p, w
0.82 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
0.82 logistic.py(346):     diff = sample_weight * (p - Y)
0.82 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
0.82 logistic.py(348):     grad[:, :n_features] += alpha * w
0.82 logistic.py(349):     if fit_intercept:
0.82 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
0.82 logistic.py(351):     return loss, grad.ravel(), p
0.82 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
0.82 logistic.py(339):     n_classes = Y.shape[1]
0.82 logistic.py(340):     n_features = X.shape[1]
0.82 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
0.82 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
0.82 logistic.py(343):                     dtype=X.dtype)
0.82 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
0.82 logistic.py(282):     n_classes = Y.shape[1]
0.82 logistic.py(283):     n_features = X.shape[1]
0.82 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
0.82 logistic.py(285):     w = w.reshape(n_classes, -1)
0.82 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
0.82 logistic.py(287):     if fit_intercept:
0.82 logistic.py(288):         intercept = w[:, -1]
0.82 logistic.py(289):         w = w[:, :-1]
0.82 logistic.py(292):     p = safe_sparse_dot(X, w.T)
0.82 logistic.py(293):     p += intercept
0.82 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
0.82 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
0.82 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
0.82 logistic.py(297):     p = np.exp(p, p)
0.82 logistic.py(298):     return loss, p, w
0.82 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
0.82 logistic.py(346):     diff = sample_weight * (p - Y)
0.82 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
0.82 logistic.py(348):     grad[:, :n_features] += alpha * w
0.82 logistic.py(349):     if fit_intercept:
0.82 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
0.82 logistic.py(351):     return loss, grad.ravel(), p
0.82 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
0.82 logistic.py(339):     n_classes = Y.shape[1]
0.82 logistic.py(340):     n_features = X.shape[1]
0.82 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
0.82 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
0.82 logistic.py(343):                     dtype=X.dtype)
0.82 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
0.82 logistic.py(282):     n_classes = Y.shape[1]
0.82 logistic.py(283):     n_features = X.shape[1]
0.82 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
0.82 logistic.py(285):     w = w.reshape(n_classes, -1)
0.82 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
0.82 logistic.py(287):     if fit_intercept:
0.82 logistic.py(288):         intercept = w[:, -1]
0.82 logistic.py(289):         w = w[:, :-1]
0.82 logistic.py(292):     p = safe_sparse_dot(X, w.T)
0.82 logistic.py(293):     p += intercept
0.82 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
0.82 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
0.82 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
0.82 logistic.py(297):     p = np.exp(p, p)
0.82 logistic.py(298):     return loss, p, w
0.82 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
0.82 logistic.py(346):     diff = sample_weight * (p - Y)
0.82 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
0.82 logistic.py(348):     grad[:, :n_features] += alpha * w
0.82 logistic.py(349):     if fit_intercept:
0.82 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
0.82 logistic.py(351):     return loss, grad.ravel(), p
0.82 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
0.82 logistic.py(339):     n_classes = Y.shape[1]
0.82 logistic.py(340):     n_features = X.shape[1]
0.82 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
0.82 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
0.82 logistic.py(343):                     dtype=X.dtype)
0.82 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
0.82 logistic.py(282):     n_classes = Y.shape[1]
0.82 logistic.py(283):     n_features = X.shape[1]
0.82 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
0.82 logistic.py(285):     w = w.reshape(n_classes, -1)
0.82 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
0.82 logistic.py(287):     if fit_intercept:
0.82 logistic.py(288):         intercept = w[:, -1]
0.82 logistic.py(289):         w = w[:, :-1]
0.82 logistic.py(292):     p = safe_sparse_dot(X, w.T)
0.82 logistic.py(293):     p += intercept
0.82 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
0.82 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
0.82 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
0.82 logistic.py(297):     p = np.exp(p, p)
0.82 logistic.py(298):     return loss, p, w
0.82 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
0.82 logistic.py(346):     diff = sample_weight * (p - Y)
0.82 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
0.82 logistic.py(348):     grad[:, :n_features] += alpha * w
0.82 logistic.py(349):     if fit_intercept:
0.82 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
0.82 logistic.py(351):     return loss, grad.ravel(), p
0.82 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
0.82 logistic.py(339):     n_classes = Y.shape[1]
0.82 logistic.py(340):     n_features = X.shape[1]
0.82 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
0.82 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
0.82 logistic.py(343):                     dtype=X.dtype)
0.82 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
0.82 logistic.py(282):     n_classes = Y.shape[1]
0.82 logistic.py(283):     n_features = X.shape[1]
0.82 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
0.82 logistic.py(285):     w = w.reshape(n_classes, -1)
0.82 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
0.82 logistic.py(287):     if fit_intercept:
0.82 logistic.py(288):         intercept = w[:, -1]
0.82 logistic.py(289):         w = w[:, :-1]
0.82 logistic.py(292):     p = safe_sparse_dot(X, w.T)
0.82 logistic.py(293):     p += intercept
0.82 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
0.82 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
0.82 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
0.82 logistic.py(297):     p = np.exp(p, p)
0.82 logistic.py(298):     return loss, p, w
0.82 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
0.82 logistic.py(346):     diff = sample_weight * (p - Y)
0.82 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
0.82 logistic.py(348):     grad[:, :n_features] += alpha * w
0.82 logistic.py(349):     if fit_intercept:
0.82 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
0.82 logistic.py(351):     return loss, grad.ravel(), p
0.82 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
0.82 logistic.py(339):     n_classes = Y.shape[1]
0.82 logistic.py(340):     n_features = X.shape[1]
0.82 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
0.82 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
0.82 logistic.py(343):                     dtype=X.dtype)
0.82 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
0.82 logistic.py(282):     n_classes = Y.shape[1]
0.82 logistic.py(283):     n_features = X.shape[1]
0.82 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
0.82 logistic.py(285):     w = w.reshape(n_classes, -1)
0.82 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
0.82 logistic.py(287):     if fit_intercept:
0.82 logistic.py(288):         intercept = w[:, -1]
0.82 logistic.py(289):         w = w[:, :-1]
0.82 logistic.py(292):     p = safe_sparse_dot(X, w.T)
0.82 logistic.py(293):     p += intercept
0.82 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
0.82 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
0.82 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
0.82 logistic.py(297):     p = np.exp(p, p)
0.82 logistic.py(298):     return loss, p, w
0.82 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
0.82 logistic.py(346):     diff = sample_weight * (p - Y)
0.82 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
0.82 logistic.py(348):     grad[:, :n_features] += alpha * w
0.82 logistic.py(349):     if fit_intercept:
0.82 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
0.82 logistic.py(351):     return loss, grad.ravel(), p
0.82 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
0.82 logistic.py(339):     n_classes = Y.shape[1]
0.82 logistic.py(340):     n_features = X.shape[1]
0.82 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
0.82 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
0.82 logistic.py(343):                     dtype=X.dtype)
0.82 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
0.82 logistic.py(282):     n_classes = Y.shape[1]
0.82 logistic.py(283):     n_features = X.shape[1]
0.82 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
0.82 logistic.py(285):     w = w.reshape(n_classes, -1)
0.82 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
0.82 logistic.py(287):     if fit_intercept:
0.82 logistic.py(288):         intercept = w[:, -1]
0.82 logistic.py(289):         w = w[:, :-1]
0.82 logistic.py(292):     p = safe_sparse_dot(X, w.T)
0.82 logistic.py(293):     p += intercept
0.82 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
0.82 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
0.82 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
0.82 logistic.py(297):     p = np.exp(p, p)
0.82 logistic.py(298):     return loss, p, w
0.82 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
0.82 logistic.py(346):     diff = sample_weight * (p - Y)
0.82 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
0.82 logistic.py(348):     grad[:, :n_features] += alpha * w
0.82 logistic.py(349):     if fit_intercept:
0.82 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
0.82 logistic.py(351):     return loss, grad.ravel(), p
0.82 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
0.82 logistic.py(339):     n_classes = Y.shape[1]
0.82 logistic.py(340):     n_features = X.shape[1]
0.82 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
0.82 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
0.82 logistic.py(343):                     dtype=X.dtype)
0.82 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
0.82 logistic.py(282):     n_classes = Y.shape[1]
0.82 logistic.py(283):     n_features = X.shape[1]
0.82 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
0.82 logistic.py(285):     w = w.reshape(n_classes, -1)
0.82 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
0.82 logistic.py(287):     if fit_intercept:
0.82 logistic.py(288):         intercept = w[:, -1]
0.82 logistic.py(289):         w = w[:, :-1]
0.82 logistic.py(292):     p = safe_sparse_dot(X, w.T)
0.82 logistic.py(293):     p += intercept
0.82 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
0.82 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
0.82 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
0.82 logistic.py(297):     p = np.exp(p, p)
0.82 logistic.py(298):     return loss, p, w
0.82 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
0.82 logistic.py(346):     diff = sample_weight * (p - Y)
0.82 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
0.82 logistic.py(348):     grad[:, :n_features] += alpha * w
0.82 logistic.py(349):     if fit_intercept:
0.82 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
0.82 logistic.py(351):     return loss, grad.ravel(), p
0.82 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
0.82 logistic.py(339):     n_classes = Y.shape[1]
0.82 logistic.py(340):     n_features = X.shape[1]
0.82 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
0.82 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
0.82 logistic.py(343):                     dtype=X.dtype)
0.82 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
0.82 logistic.py(282):     n_classes = Y.shape[1]
0.82 logistic.py(283):     n_features = X.shape[1]
0.82 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
0.82 logistic.py(285):     w = w.reshape(n_classes, -1)
0.82 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
0.82 logistic.py(287):     if fit_intercept:
0.82 logistic.py(288):         intercept = w[:, -1]
0.82 logistic.py(289):         w = w[:, :-1]
0.82 logistic.py(292):     p = safe_sparse_dot(X, w.T)
0.82 logistic.py(293):     p += intercept
0.82 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
0.82 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
0.82 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
0.82 logistic.py(297):     p = np.exp(p, p)
0.82 logistic.py(298):     return loss, p, w
0.82 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
0.83 logistic.py(346):     diff = sample_weight * (p - Y)
0.83 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
0.83 logistic.py(348):     grad[:, :n_features] += alpha * w
0.83 logistic.py(349):     if fit_intercept:
0.83 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
0.83 logistic.py(351):     return loss, grad.ravel(), p
0.83 logistic.py(718):             if info["warnflag"] == 1:
0.83 logistic.py(723):             n_iter_i = min(info['nit'], max_iter)
0.83 logistic.py(760):         if multi_class == 'multinomial':
0.83 logistic.py(761):             multi_w0 = np.reshape(w0, (classes.size, -1))
0.83 logistic.py(762):             if classes.size == 2:
0.83 logistic.py(764):             coefs.append(multi_w0)
0.83 logistic.py(768):         n_iter[i] = n_iter_i
0.83 logistic.py(712):     for i, C in enumerate(Cs):
0.83 logistic.py(713):         if solver == 'lbfgs':
0.83 logistic.py(714):             w0, loss, info = optimize.fmin_l_bfgs_b(
0.83 logistic.py(715):                 func, w0, fprime=None,
0.83 logistic.py(716):                 args=(X, target, 1. / C, sample_weight),
0.83 logistic.py(717):                 iprint=(verbose > 0) - 1, pgtol=tol, maxiter=max_iter)
0.83 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
0.83 logistic.py(339):     n_classes = Y.shape[1]
0.83 logistic.py(340):     n_features = X.shape[1]
0.83 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
0.83 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
0.83 logistic.py(343):                     dtype=X.dtype)
0.83 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
0.83 logistic.py(282):     n_classes = Y.shape[1]
0.83 logistic.py(283):     n_features = X.shape[1]
0.83 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
0.83 logistic.py(285):     w = w.reshape(n_classes, -1)
0.83 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
0.83 logistic.py(287):     if fit_intercept:
0.83 logistic.py(288):         intercept = w[:, -1]
0.83 logistic.py(289):         w = w[:, :-1]
0.83 logistic.py(292):     p = safe_sparse_dot(X, w.T)
0.83 logistic.py(293):     p += intercept
0.83 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
0.83 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
0.83 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
0.83 logistic.py(297):     p = np.exp(p, p)
0.83 logistic.py(298):     return loss, p, w
0.83 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
0.83 logistic.py(346):     diff = sample_weight * (p - Y)
0.83 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
0.83 logistic.py(348):     grad[:, :n_features] += alpha * w
0.83 logistic.py(349):     if fit_intercept:
0.83 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
0.83 logistic.py(351):     return loss, grad.ravel(), p
0.83 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
0.83 logistic.py(339):     n_classes = Y.shape[1]
0.83 logistic.py(340):     n_features = X.shape[1]
0.83 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
0.83 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
0.83 logistic.py(343):                     dtype=X.dtype)
0.83 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
0.83 logistic.py(282):     n_classes = Y.shape[1]
0.83 logistic.py(283):     n_features = X.shape[1]
0.83 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
0.83 logistic.py(285):     w = w.reshape(n_classes, -1)
0.83 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
0.83 logistic.py(287):     if fit_intercept:
0.83 logistic.py(288):         intercept = w[:, -1]
0.83 logistic.py(289):         w = w[:, :-1]
0.83 logistic.py(292):     p = safe_sparse_dot(X, w.T)
0.83 logistic.py(293):     p += intercept
0.83 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
0.83 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
0.83 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
0.83 logistic.py(297):     p = np.exp(p, p)
0.83 logistic.py(298):     return loss, p, w
0.83 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
0.83 logistic.py(346):     diff = sample_weight * (p - Y)
0.83 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
0.83 logistic.py(348):     grad[:, :n_features] += alpha * w
0.83 logistic.py(349):     if fit_intercept:
0.83 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
0.83 logistic.py(351):     return loss, grad.ravel(), p
0.83 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
0.83 logistic.py(339):     n_classes = Y.shape[1]
0.83 logistic.py(340):     n_features = X.shape[1]
0.83 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
0.83 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
0.83 logistic.py(343):                     dtype=X.dtype)
0.83 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
0.83 logistic.py(282):     n_classes = Y.shape[1]
0.83 logistic.py(283):     n_features = X.shape[1]
0.83 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
0.83 logistic.py(285):     w = w.reshape(n_classes, -1)
0.83 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
0.83 logistic.py(287):     if fit_intercept:
0.83 logistic.py(288):         intercept = w[:, -1]
0.83 logistic.py(289):         w = w[:, :-1]
0.83 logistic.py(292):     p = safe_sparse_dot(X, w.T)
0.83 logistic.py(293):     p += intercept
0.83 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
0.83 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
0.83 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
0.83 logistic.py(297):     p = np.exp(p, p)
0.83 logistic.py(298):     return loss, p, w
0.83 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
0.83 logistic.py(346):     diff = sample_weight * (p - Y)
0.83 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
0.83 logistic.py(348):     grad[:, :n_features] += alpha * w
0.83 logistic.py(349):     if fit_intercept:
0.83 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
0.83 logistic.py(351):     return loss, grad.ravel(), p
0.83 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
0.83 logistic.py(339):     n_classes = Y.shape[1]
0.83 logistic.py(340):     n_features = X.shape[1]
0.83 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
0.83 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
0.83 logistic.py(343):                     dtype=X.dtype)
0.83 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
0.83 logistic.py(282):     n_classes = Y.shape[1]
0.83 logistic.py(283):     n_features = X.shape[1]
0.83 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
0.83 logistic.py(285):     w = w.reshape(n_classes, -1)
0.83 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
0.83 logistic.py(287):     if fit_intercept:
0.83 logistic.py(288):         intercept = w[:, -1]
0.83 logistic.py(289):         w = w[:, :-1]
0.83 logistic.py(292):     p = safe_sparse_dot(X, w.T)
0.83 logistic.py(293):     p += intercept
0.83 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
0.83 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
0.83 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
0.83 logistic.py(297):     p = np.exp(p, p)
0.83 logistic.py(298):     return loss, p, w
0.83 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
0.83 logistic.py(346):     diff = sample_weight * (p - Y)
0.83 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
0.83 logistic.py(348):     grad[:, :n_features] += alpha * w
0.83 logistic.py(349):     if fit_intercept:
0.83 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
0.83 logistic.py(351):     return loss, grad.ravel(), p
0.83 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
0.83 logistic.py(339):     n_classes = Y.shape[1]
0.83 logistic.py(340):     n_features = X.shape[1]
0.83 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
0.83 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
0.83 logistic.py(343):                     dtype=X.dtype)
0.83 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
0.83 logistic.py(282):     n_classes = Y.shape[1]
0.83 logistic.py(283):     n_features = X.shape[1]
0.83 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
0.83 logistic.py(285):     w = w.reshape(n_classes, -1)
0.83 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
0.83 logistic.py(287):     if fit_intercept:
0.83 logistic.py(288):         intercept = w[:, -1]
0.83 logistic.py(289):         w = w[:, :-1]
0.83 logistic.py(292):     p = safe_sparse_dot(X, w.T)
0.83 logistic.py(293):     p += intercept
0.83 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
0.83 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
0.83 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
0.83 logistic.py(297):     p = np.exp(p, p)
0.83 logistic.py(298):     return loss, p, w
0.83 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
0.83 logistic.py(346):     diff = sample_weight * (p - Y)
0.83 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
0.83 logistic.py(348):     grad[:, :n_features] += alpha * w
0.83 logistic.py(349):     if fit_intercept:
0.83 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
0.83 logistic.py(351):     return loss, grad.ravel(), p
0.83 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
0.83 logistic.py(339):     n_classes = Y.shape[1]
0.83 logistic.py(340):     n_features = X.shape[1]
0.83 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
0.83 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
0.83 logistic.py(343):                     dtype=X.dtype)
0.83 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
0.83 logistic.py(282):     n_classes = Y.shape[1]
0.83 logistic.py(283):     n_features = X.shape[1]
0.83 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
0.84 logistic.py(285):     w = w.reshape(n_classes, -1)
0.84 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
0.84 logistic.py(287):     if fit_intercept:
0.84 logistic.py(288):         intercept = w[:, -1]
0.84 logistic.py(289):         w = w[:, :-1]
0.84 logistic.py(292):     p = safe_sparse_dot(X, w.T)
0.84 logistic.py(293):     p += intercept
0.84 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
0.84 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
0.84 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
0.84 logistic.py(297):     p = np.exp(p, p)
0.84 logistic.py(298):     return loss, p, w
0.84 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
0.84 logistic.py(346):     diff = sample_weight * (p - Y)
0.84 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
0.84 logistic.py(348):     grad[:, :n_features] += alpha * w
0.84 logistic.py(349):     if fit_intercept:
0.84 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
0.84 logistic.py(351):     return loss, grad.ravel(), p
0.84 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
0.84 logistic.py(339):     n_classes = Y.shape[1]
0.84 logistic.py(340):     n_features = X.shape[1]
0.84 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
0.84 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
0.84 logistic.py(343):                     dtype=X.dtype)
0.84 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
0.84 logistic.py(282):     n_classes = Y.shape[1]
0.84 logistic.py(283):     n_features = X.shape[1]
0.84 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
0.84 logistic.py(285):     w = w.reshape(n_classes, -1)
0.84 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
0.84 logistic.py(287):     if fit_intercept:
0.84 logistic.py(288):         intercept = w[:, -1]
0.84 logistic.py(289):         w = w[:, :-1]
0.84 logistic.py(292):     p = safe_sparse_dot(X, w.T)
0.84 logistic.py(293):     p += intercept
0.84 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
0.84 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
0.84 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
0.84 logistic.py(297):     p = np.exp(p, p)
0.84 logistic.py(298):     return loss, p, w
0.84 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
0.84 logistic.py(346):     diff = sample_weight * (p - Y)
0.84 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
0.84 logistic.py(348):     grad[:, :n_features] += alpha * w
0.84 logistic.py(349):     if fit_intercept:
0.84 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
0.84 logistic.py(351):     return loss, grad.ravel(), p
0.84 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
0.84 logistic.py(339):     n_classes = Y.shape[1]
0.84 logistic.py(340):     n_features = X.shape[1]
0.84 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
0.84 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
0.84 logistic.py(343):                     dtype=X.dtype)
0.84 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
0.84 logistic.py(282):     n_classes = Y.shape[1]
0.84 logistic.py(283):     n_features = X.shape[1]
0.84 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
0.84 logistic.py(285):     w = w.reshape(n_classes, -1)
0.84 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
0.84 logistic.py(287):     if fit_intercept:
0.84 logistic.py(288):         intercept = w[:, -1]
0.84 logistic.py(289):         w = w[:, :-1]
0.84 logistic.py(292):     p = safe_sparse_dot(X, w.T)
0.84 logistic.py(293):     p += intercept
0.84 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
0.84 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
0.84 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
0.84 logistic.py(297):     p = np.exp(p, p)
0.84 logistic.py(298):     return loss, p, w
0.84 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
0.84 logistic.py(346):     diff = sample_weight * (p - Y)
0.84 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
0.84 logistic.py(348):     grad[:, :n_features] += alpha * w
0.84 logistic.py(349):     if fit_intercept:
0.84 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
0.84 logistic.py(351):     return loss, grad.ravel(), p
0.84 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
0.84 logistic.py(339):     n_classes = Y.shape[1]
0.84 logistic.py(340):     n_features = X.shape[1]
0.84 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
0.84 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
0.84 logistic.py(343):                     dtype=X.dtype)
0.84 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
0.84 logistic.py(282):     n_classes = Y.shape[1]
0.84 logistic.py(283):     n_features = X.shape[1]
0.84 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
0.84 logistic.py(285):     w = w.reshape(n_classes, -1)
0.84 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
0.84 logistic.py(287):     if fit_intercept:
0.84 logistic.py(288):         intercept = w[:, -1]
0.84 logistic.py(289):         w = w[:, :-1]
0.84 logistic.py(292):     p = safe_sparse_dot(X, w.T)
0.84 logistic.py(293):     p += intercept
0.84 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
0.84 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
0.84 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
0.84 logistic.py(297):     p = np.exp(p, p)
0.84 logistic.py(298):     return loss, p, w
0.84 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
0.84 logistic.py(346):     diff = sample_weight * (p - Y)
0.84 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
0.84 logistic.py(348):     grad[:, :n_features] += alpha * w
0.84 logistic.py(349):     if fit_intercept:
0.84 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
0.84 logistic.py(351):     return loss, grad.ravel(), p
0.84 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
0.84 logistic.py(339):     n_classes = Y.shape[1]
0.84 logistic.py(340):     n_features = X.shape[1]
0.84 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
0.84 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
0.84 logistic.py(343):                     dtype=X.dtype)
0.84 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
0.84 logistic.py(282):     n_classes = Y.shape[1]
0.84 logistic.py(283):     n_features = X.shape[1]
0.84 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
0.84 logistic.py(285):     w = w.reshape(n_classes, -1)
0.84 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
0.84 logistic.py(287):     if fit_intercept:
0.84 logistic.py(288):         intercept = w[:, -1]
0.84 logistic.py(289):         w = w[:, :-1]
0.84 logistic.py(292):     p = safe_sparse_dot(X, w.T)
0.84 logistic.py(293):     p += intercept
0.84 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
0.84 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
0.84 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
0.84 logistic.py(297):     p = np.exp(p, p)
0.84 logistic.py(298):     return loss, p, w
0.84 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
0.84 logistic.py(346):     diff = sample_weight * (p - Y)
0.84 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
0.84 logistic.py(348):     grad[:, :n_features] += alpha * w
0.84 logistic.py(349):     if fit_intercept:
0.84 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
0.84 logistic.py(351):     return loss, grad.ravel(), p
0.84 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
0.84 logistic.py(339):     n_classes = Y.shape[1]
0.84 logistic.py(340):     n_features = X.shape[1]
0.84 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
0.84 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
0.84 logistic.py(343):                     dtype=X.dtype)
0.84 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
0.84 logistic.py(282):     n_classes = Y.shape[1]
0.84 logistic.py(283):     n_features = X.shape[1]
0.84 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
0.84 logistic.py(285):     w = w.reshape(n_classes, -1)
0.84 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
0.84 logistic.py(287):     if fit_intercept:
0.84 logistic.py(288):         intercept = w[:, -1]
0.84 logistic.py(289):         w = w[:, :-1]
0.84 logistic.py(292):     p = safe_sparse_dot(X, w.T)
0.84 logistic.py(293):     p += intercept
0.84 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
0.84 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
0.84 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
0.84 logistic.py(297):     p = np.exp(p, p)
0.84 logistic.py(298):     return loss, p, w
0.84 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
0.84 logistic.py(346):     diff = sample_weight * (p - Y)
0.84 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
0.84 logistic.py(348):     grad[:, :n_features] += alpha * w
0.84 logistic.py(349):     if fit_intercept:
0.84 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
0.84 logistic.py(351):     return loss, grad.ravel(), p
0.84 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
0.84 logistic.py(339):     n_classes = Y.shape[1]
0.84 logistic.py(340):     n_features = X.shape[1]
0.84 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
0.84 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
0.84 logistic.py(343):                     dtype=X.dtype)
0.84 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
0.84 logistic.py(282):     n_classes = Y.shape[1]
0.84 logistic.py(283):     n_features = X.shape[1]
0.84 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
0.84 logistic.py(285):     w = w.reshape(n_classes, -1)
0.84 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
0.84 logistic.py(287):     if fit_intercept:
0.84 logistic.py(288):         intercept = w[:, -1]
0.84 logistic.py(289):         w = w[:, :-1]
0.84 logistic.py(292):     p = safe_sparse_dot(X, w.T)
0.84 logistic.py(293):     p += intercept
0.84 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
0.84 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
0.84 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
0.84 logistic.py(297):     p = np.exp(p, p)
0.84 logistic.py(298):     return loss, p, w
0.84 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
0.84 logistic.py(346):     diff = sample_weight * (p - Y)
0.84 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
0.84 logistic.py(348):     grad[:, :n_features] += alpha * w
0.84 logistic.py(349):     if fit_intercept:
0.84 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
0.84 logistic.py(351):     return loss, grad.ravel(), p
0.84 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
0.84 logistic.py(339):     n_classes = Y.shape[1]
0.84 logistic.py(340):     n_features = X.shape[1]
0.84 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
0.84 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
0.84 logistic.py(343):                     dtype=X.dtype)
0.84 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
0.84 logistic.py(282):     n_classes = Y.shape[1]
0.84 logistic.py(283):     n_features = X.shape[1]
0.84 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
0.84 logistic.py(285):     w = w.reshape(n_classes, -1)
0.84 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
0.84 logistic.py(287):     if fit_intercept:
0.84 logistic.py(288):         intercept = w[:, -1]
0.84 logistic.py(289):         w = w[:, :-1]
0.84 logistic.py(292):     p = safe_sparse_dot(X, w.T)
0.84 logistic.py(293):     p += intercept
0.84 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
0.84 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
0.84 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
0.84 logistic.py(297):     p = np.exp(p, p)
0.84 logistic.py(298):     return loss, p, w
0.84 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
0.84 logistic.py(346):     diff = sample_weight * (p - Y)
0.84 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
0.84 logistic.py(348):     grad[:, :n_features] += alpha * w
0.84 logistic.py(349):     if fit_intercept:
0.84 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
0.84 logistic.py(351):     return loss, grad.ravel(), p
0.84 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
0.84 logistic.py(339):     n_classes = Y.shape[1]
0.84 logistic.py(340):     n_features = X.shape[1]
0.84 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
0.84 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
0.84 logistic.py(343):                     dtype=X.dtype)
0.84 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
0.84 logistic.py(282):     n_classes = Y.shape[1]
0.84 logistic.py(283):     n_features = X.shape[1]
0.84 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
0.84 logistic.py(285):     w = w.reshape(n_classes, -1)
0.84 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
0.84 logistic.py(287):     if fit_intercept:
0.84 logistic.py(288):         intercept = w[:, -1]
0.84 logistic.py(289):         w = w[:, :-1]
0.84 logistic.py(292):     p = safe_sparse_dot(X, w.T)
0.84 logistic.py(293):     p += intercept
0.84 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
0.84 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
0.84 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
0.84 logistic.py(297):     p = np.exp(p, p)
0.84 logistic.py(298):     return loss, p, w
0.84 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
0.84 logistic.py(346):     diff = sample_weight * (p - Y)
0.84 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
0.84 logistic.py(348):     grad[:, :n_features] += alpha * w
0.84 logistic.py(349):     if fit_intercept:
0.84 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
0.84 logistic.py(351):     return loss, grad.ravel(), p
0.84 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
0.84 logistic.py(339):     n_classes = Y.shape[1]
0.84 logistic.py(340):     n_features = X.shape[1]
0.84 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
0.84 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
0.84 logistic.py(343):                     dtype=X.dtype)
0.84 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
0.84 logistic.py(282):     n_classes = Y.shape[1]
0.84 logistic.py(283):     n_features = X.shape[1]
0.84 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
0.84 logistic.py(285):     w = w.reshape(n_classes, -1)
0.84 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
0.84 logistic.py(287):     if fit_intercept:
0.84 logistic.py(288):         intercept = w[:, -1]
0.84 logistic.py(289):         w = w[:, :-1]
0.84 logistic.py(292):     p = safe_sparse_dot(X, w.T)
0.84 logistic.py(293):     p += intercept
0.84 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
0.84 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
0.84 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
0.84 logistic.py(297):     p = np.exp(p, p)
0.84 logistic.py(298):     return loss, p, w
0.84 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
0.84 logistic.py(346):     diff = sample_weight * (p - Y)
0.84 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
0.84 logistic.py(348):     grad[:, :n_features] += alpha * w
0.84 logistic.py(349):     if fit_intercept:
0.84 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
0.84 logistic.py(351):     return loss, grad.ravel(), p
0.84 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
0.84 logistic.py(339):     n_classes = Y.shape[1]
0.84 logistic.py(340):     n_features = X.shape[1]
0.84 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
0.84 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
0.84 logistic.py(343):                     dtype=X.dtype)
0.84 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
0.84 logistic.py(282):     n_classes = Y.shape[1]
0.84 logistic.py(283):     n_features = X.shape[1]
0.84 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
0.84 logistic.py(285):     w = w.reshape(n_classes, -1)
0.84 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
0.84 logistic.py(287):     if fit_intercept:
0.84 logistic.py(288):         intercept = w[:, -1]
0.84 logistic.py(289):         w = w[:, :-1]
0.84 logistic.py(292):     p = safe_sparse_dot(X, w.T)
0.84 logistic.py(293):     p += intercept
0.84 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
0.84 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
0.84 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
0.84 logistic.py(297):     p = np.exp(p, p)
0.84 logistic.py(298):     return loss, p, w
0.84 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
0.84 logistic.py(346):     diff = sample_weight * (p - Y)
0.84 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
0.84 logistic.py(348):     grad[:, :n_features] += alpha * w
0.84 logistic.py(349):     if fit_intercept:
0.84 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
0.84 logistic.py(351):     return loss, grad.ravel(), p
0.84 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
0.84 logistic.py(339):     n_classes = Y.shape[1]
0.84 logistic.py(340):     n_features = X.shape[1]
0.84 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
0.84 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
0.84 logistic.py(343):                     dtype=X.dtype)
0.84 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
0.84 logistic.py(282):     n_classes = Y.shape[1]
0.84 logistic.py(283):     n_features = X.shape[1]
0.84 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
0.84 logistic.py(285):     w = w.reshape(n_classes, -1)
0.84 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
0.84 logistic.py(287):     if fit_intercept:
0.84 logistic.py(288):         intercept = w[:, -1]
0.84 logistic.py(289):         w = w[:, :-1]
0.84 logistic.py(292):     p = safe_sparse_dot(X, w.T)
0.84 logistic.py(293):     p += intercept
0.84 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
0.85 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
0.85 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
0.85 logistic.py(297):     p = np.exp(p, p)
0.85 logistic.py(298):     return loss, p, w
0.85 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
0.85 logistic.py(346):     diff = sample_weight * (p - Y)
0.85 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
0.85 logistic.py(348):     grad[:, :n_features] += alpha * w
0.85 logistic.py(349):     if fit_intercept:
0.85 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
0.85 logistic.py(351):     return loss, grad.ravel(), p
0.85 logistic.py(718):             if info["warnflag"] == 1:
0.85 logistic.py(723):             n_iter_i = min(info['nit'], max_iter)
0.85 logistic.py(760):         if multi_class == 'multinomial':
0.85 logistic.py(761):             multi_w0 = np.reshape(w0, (classes.size, -1))
0.85 logistic.py(762):             if classes.size == 2:
0.85 logistic.py(764):             coefs.append(multi_w0)
0.85 logistic.py(768):         n_iter[i] = n_iter_i
0.85 logistic.py(712):     for i, C in enumerate(Cs):
0.85 logistic.py(713):         if solver == 'lbfgs':
0.85 logistic.py(714):             w0, loss, info = optimize.fmin_l_bfgs_b(
0.85 logistic.py(715):                 func, w0, fprime=None,
0.85 logistic.py(716):                 args=(X, target, 1. / C, sample_weight),
0.85 logistic.py(717):                 iprint=(verbose > 0) - 1, pgtol=tol, maxiter=max_iter)
0.85 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
0.85 logistic.py(339):     n_classes = Y.shape[1]
0.85 logistic.py(340):     n_features = X.shape[1]
0.85 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
0.85 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
0.85 logistic.py(343):                     dtype=X.dtype)
0.85 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
0.85 logistic.py(282):     n_classes = Y.shape[1]
0.85 logistic.py(283):     n_features = X.shape[1]
0.85 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
0.85 logistic.py(285):     w = w.reshape(n_classes, -1)
0.85 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
0.85 logistic.py(287):     if fit_intercept:
0.85 logistic.py(288):         intercept = w[:, -1]
0.85 logistic.py(289):         w = w[:, :-1]
0.85 logistic.py(292):     p = safe_sparse_dot(X, w.T)
0.85 logistic.py(293):     p += intercept
0.85 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
0.85 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
0.85 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
0.85 logistic.py(297):     p = np.exp(p, p)
0.85 logistic.py(298):     return loss, p, w
0.85 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
0.85 logistic.py(346):     diff = sample_weight * (p - Y)
0.85 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
0.85 logistic.py(348):     grad[:, :n_features] += alpha * w
0.85 logistic.py(349):     if fit_intercept:
0.85 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
0.85 logistic.py(351):     return loss, grad.ravel(), p
0.85 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
0.85 logistic.py(339):     n_classes = Y.shape[1]
0.85 logistic.py(340):     n_features = X.shape[1]
0.85 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
0.85 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
0.85 logistic.py(343):                     dtype=X.dtype)
0.85 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
0.85 logistic.py(282):     n_classes = Y.shape[1]
0.85 logistic.py(283):     n_features = X.shape[1]
0.85 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
0.85 logistic.py(285):     w = w.reshape(n_classes, -1)
0.85 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
0.85 logistic.py(287):     if fit_intercept:
0.85 logistic.py(288):         intercept = w[:, -1]
0.85 logistic.py(289):         w = w[:, :-1]
0.85 logistic.py(292):     p = safe_sparse_dot(X, w.T)
0.85 logistic.py(293):     p += intercept
0.85 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
0.85 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
0.85 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
0.85 logistic.py(297):     p = np.exp(p, p)
0.85 logistic.py(298):     return loss, p, w
0.85 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
0.85 logistic.py(346):     diff = sample_weight * (p - Y)
0.85 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
0.85 logistic.py(348):     grad[:, :n_features] += alpha * w
0.85 logistic.py(349):     if fit_intercept:
0.85 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
0.85 logistic.py(351):     return loss, grad.ravel(), p
0.85 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
0.85 logistic.py(339):     n_classes = Y.shape[1]
0.85 logistic.py(340):     n_features = X.shape[1]
0.85 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
0.85 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
0.85 logistic.py(343):                     dtype=X.dtype)
0.85 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
0.85 logistic.py(282):     n_classes = Y.shape[1]
0.85 logistic.py(283):     n_features = X.shape[1]
0.85 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
0.85 logistic.py(285):     w = w.reshape(n_classes, -1)
0.85 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
0.85 logistic.py(287):     if fit_intercept:
0.85 logistic.py(288):         intercept = w[:, -1]
0.85 logistic.py(289):         w = w[:, :-1]
0.85 logistic.py(292):     p = safe_sparse_dot(X, w.T)
0.85 logistic.py(293):     p += intercept
0.85 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
0.85 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
0.85 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
0.85 logistic.py(297):     p = np.exp(p, p)
0.85 logistic.py(298):     return loss, p, w
0.85 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
0.85 logistic.py(346):     diff = sample_weight * (p - Y)
0.85 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
0.85 logistic.py(348):     grad[:, :n_features] += alpha * w
0.85 logistic.py(349):     if fit_intercept:
0.85 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
0.85 logistic.py(351):     return loss, grad.ravel(), p
0.85 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
0.85 logistic.py(339):     n_classes = Y.shape[1]
0.85 logistic.py(340):     n_features = X.shape[1]
0.85 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
0.85 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
0.85 logistic.py(343):                     dtype=X.dtype)
0.85 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
0.85 logistic.py(282):     n_classes = Y.shape[1]
0.85 logistic.py(283):     n_features = X.shape[1]
0.85 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
0.85 logistic.py(285):     w = w.reshape(n_classes, -1)
0.85 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
0.85 logistic.py(287):     if fit_intercept:
0.85 logistic.py(288):         intercept = w[:, -1]
0.85 logistic.py(289):         w = w[:, :-1]
0.85 logistic.py(292):     p = safe_sparse_dot(X, w.T)
0.85 logistic.py(293):     p += intercept
0.85 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
0.85 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
0.85 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
0.85 logistic.py(297):     p = np.exp(p, p)
0.85 logistic.py(298):     return loss, p, w
0.85 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
0.85 logistic.py(346):     diff = sample_weight * (p - Y)
0.85 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
0.85 logistic.py(348):     grad[:, :n_features] += alpha * w
0.85 logistic.py(349):     if fit_intercept:
0.85 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
0.85 logistic.py(351):     return loss, grad.ravel(), p
0.85 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
0.85 logistic.py(339):     n_classes = Y.shape[1]
0.85 logistic.py(340):     n_features = X.shape[1]
0.85 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
0.85 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
0.85 logistic.py(343):                     dtype=X.dtype)
0.85 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
0.85 logistic.py(282):     n_classes = Y.shape[1]
0.85 logistic.py(283):     n_features = X.shape[1]
0.85 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
0.85 logistic.py(285):     w = w.reshape(n_classes, -1)
0.85 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
0.85 logistic.py(287):     if fit_intercept:
0.85 logistic.py(288):         intercept = w[:, -1]
0.85 logistic.py(289):         w = w[:, :-1]
0.85 logistic.py(292):     p = safe_sparse_dot(X, w.T)
0.85 logistic.py(293):     p += intercept
0.85 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
0.85 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
0.85 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
0.85 logistic.py(297):     p = np.exp(p, p)
0.85 logistic.py(298):     return loss, p, w
0.85 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
0.85 logistic.py(346):     diff = sample_weight * (p - Y)
0.85 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
0.85 logistic.py(348):     grad[:, :n_features] += alpha * w
0.85 logistic.py(349):     if fit_intercept:
0.85 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
0.85 logistic.py(351):     return loss, grad.ravel(), p
0.85 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
0.85 logistic.py(339):     n_classes = Y.shape[1]
0.85 logistic.py(340):     n_features = X.shape[1]
0.85 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
0.85 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
0.85 logistic.py(343):                     dtype=X.dtype)
0.85 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
0.85 logistic.py(282):     n_classes = Y.shape[1]
0.85 logistic.py(283):     n_features = X.shape[1]
0.85 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
0.85 logistic.py(285):     w = w.reshape(n_classes, -1)
0.85 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
0.85 logistic.py(287):     if fit_intercept:
0.85 logistic.py(288):         intercept = w[:, -1]
0.85 logistic.py(289):         w = w[:, :-1]
0.85 logistic.py(292):     p = safe_sparse_dot(X, w.T)
0.85 logistic.py(293):     p += intercept
0.85 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
0.85 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
0.85 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
0.85 logistic.py(297):     p = np.exp(p, p)
0.85 logistic.py(298):     return loss, p, w
0.85 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
0.85 logistic.py(346):     diff = sample_weight * (p - Y)
0.85 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
0.85 logistic.py(348):     grad[:, :n_features] += alpha * w
0.85 logistic.py(349):     if fit_intercept:
0.85 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
0.85 logistic.py(351):     return loss, grad.ravel(), p
0.85 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
0.85 logistic.py(339):     n_classes = Y.shape[1]
0.85 logistic.py(340):     n_features = X.shape[1]
0.85 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
0.85 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
0.85 logistic.py(343):                     dtype=X.dtype)
0.85 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
0.85 logistic.py(282):     n_classes = Y.shape[1]
0.85 logistic.py(283):     n_features = X.shape[1]
0.85 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
0.85 logistic.py(285):     w = w.reshape(n_classes, -1)
0.85 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
0.85 logistic.py(287):     if fit_intercept:
0.85 logistic.py(288):         intercept = w[:, -1]
0.85 logistic.py(289):         w = w[:, :-1]
0.85 logistic.py(292):     p = safe_sparse_dot(X, w.T)
0.85 logistic.py(293):     p += intercept
0.85 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
0.85 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
0.85 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
0.85 logistic.py(297):     p = np.exp(p, p)
0.85 logistic.py(298):     return loss, p, w
0.85 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
0.85 logistic.py(346):     diff = sample_weight * (p - Y)
0.85 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
0.85 logistic.py(348):     grad[:, :n_features] += alpha * w
0.85 logistic.py(349):     if fit_intercept:
0.85 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
0.85 logistic.py(351):     return loss, grad.ravel(), p
0.85 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
0.85 logistic.py(339):     n_classes = Y.shape[1]
0.85 logistic.py(340):     n_features = X.shape[1]
0.85 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
0.85 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
0.85 logistic.py(343):                     dtype=X.dtype)
0.85 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
0.85 logistic.py(282):     n_classes = Y.shape[1]
0.85 logistic.py(283):     n_features = X.shape[1]
0.85 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
0.85 logistic.py(285):     w = w.reshape(n_classes, -1)
0.85 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
0.85 logistic.py(287):     if fit_intercept:
0.85 logistic.py(288):         intercept = w[:, -1]
0.85 logistic.py(289):         w = w[:, :-1]
0.85 logistic.py(292):     p = safe_sparse_dot(X, w.T)
0.85 logistic.py(293):     p += intercept
0.85 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
0.85 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
0.85 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
0.85 logistic.py(297):     p = np.exp(p, p)
0.85 logistic.py(298):     return loss, p, w
0.85 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
0.85 logistic.py(346):     diff = sample_weight * (p - Y)
0.85 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
0.85 logistic.py(348):     grad[:, :n_features] += alpha * w
0.85 logistic.py(349):     if fit_intercept:
0.85 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
0.85 logistic.py(351):     return loss, grad.ravel(), p
0.85 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
0.85 logistic.py(339):     n_classes = Y.shape[1]
0.85 logistic.py(340):     n_features = X.shape[1]
0.85 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
0.85 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
0.85 logistic.py(343):                     dtype=X.dtype)
0.85 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
0.85 logistic.py(282):     n_classes = Y.shape[1]
0.85 logistic.py(283):     n_features = X.shape[1]
0.85 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
0.85 logistic.py(285):     w = w.reshape(n_classes, -1)
0.85 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
0.85 logistic.py(287):     if fit_intercept:
0.85 logistic.py(288):         intercept = w[:, -1]
0.85 logistic.py(289):         w = w[:, :-1]
0.85 logistic.py(292):     p = safe_sparse_dot(X, w.T)
0.85 logistic.py(293):     p += intercept
0.85 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
0.85 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
0.85 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
0.85 logistic.py(297):     p = np.exp(p, p)
0.85 logistic.py(298):     return loss, p, w
0.85 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
0.85 logistic.py(346):     diff = sample_weight * (p - Y)
0.85 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
0.85 logistic.py(348):     grad[:, :n_features] += alpha * w
0.85 logistic.py(349):     if fit_intercept:
0.85 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
0.85 logistic.py(351):     return loss, grad.ravel(), p
0.85 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
0.85 logistic.py(339):     n_classes = Y.shape[1]
0.85 logistic.py(340):     n_features = X.shape[1]
0.85 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
0.85 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
0.85 logistic.py(343):                     dtype=X.dtype)
0.85 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
0.85 logistic.py(282):     n_classes = Y.shape[1]
0.85 logistic.py(283):     n_features = X.shape[1]
0.85 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
0.85 logistic.py(285):     w = w.reshape(n_classes, -1)
0.85 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
0.85 logistic.py(287):     if fit_intercept:
0.85 logistic.py(288):         intercept = w[:, -1]
0.85 logistic.py(289):         w = w[:, :-1]
0.85 logistic.py(292):     p = safe_sparse_dot(X, w.T)
0.85 logistic.py(293):     p += intercept
0.85 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
0.85 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
0.85 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
0.85 logistic.py(297):     p = np.exp(p, p)
0.85 logistic.py(298):     return loss, p, w
0.85 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
0.85 logistic.py(346):     diff = sample_weight * (p - Y)
0.85 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
0.85 logistic.py(348):     grad[:, :n_features] += alpha * w
0.85 logistic.py(349):     if fit_intercept:
0.85 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
0.85 logistic.py(351):     return loss, grad.ravel(), p
0.85 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
0.85 logistic.py(339):     n_classes = Y.shape[1]
0.85 logistic.py(340):     n_features = X.shape[1]
0.85 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
0.85 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
0.85 logistic.py(343):                     dtype=X.dtype)
0.85 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
0.85 logistic.py(282):     n_classes = Y.shape[1]
0.85 logistic.py(283):     n_features = X.shape[1]
0.85 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
0.85 logistic.py(285):     w = w.reshape(n_classes, -1)
0.85 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
0.85 logistic.py(287):     if fit_intercept:
0.85 logistic.py(288):         intercept = w[:, -1]
0.85 logistic.py(289):         w = w[:, :-1]
0.85 logistic.py(292):     p = safe_sparse_dot(X, w.T)
0.85 logistic.py(293):     p += intercept
0.85 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
0.85 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
0.85 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
0.85 logistic.py(297):     p = np.exp(p, p)
0.85 logistic.py(298):     return loss, p, w
0.85 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
0.85 logistic.py(346):     diff = sample_weight * (p - Y)
0.85 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
0.85 logistic.py(348):     grad[:, :n_features] += alpha * w
0.85 logistic.py(349):     if fit_intercept:
0.85 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
0.85 logistic.py(351):     return loss, grad.ravel(), p
0.85 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
0.85 logistic.py(339):     n_classes = Y.shape[1]
0.85 logistic.py(340):     n_features = X.shape[1]
0.85 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
0.85 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
0.85 logistic.py(343):                     dtype=X.dtype)
0.85 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
0.85 logistic.py(282):     n_classes = Y.shape[1]
0.85 logistic.py(283):     n_features = X.shape[1]
0.85 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
0.85 logistic.py(285):     w = w.reshape(n_classes, -1)
0.85 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
0.85 logistic.py(287):     if fit_intercept:
0.85 logistic.py(288):         intercept = w[:, -1]
0.85 logistic.py(289):         w = w[:, :-1]
0.85 logistic.py(292):     p = safe_sparse_dot(X, w.T)
0.85 logistic.py(293):     p += intercept
0.85 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
0.85 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
0.85 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
0.85 logistic.py(297):     p = np.exp(p, p)
0.85 logistic.py(298):     return loss, p, w
0.85 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
0.85 logistic.py(346):     diff = sample_weight * (p - Y)
0.85 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
0.85 logistic.py(348):     grad[:, :n_features] += alpha * w
0.85 logistic.py(349):     if fit_intercept:
0.85 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
0.85 logistic.py(351):     return loss, grad.ravel(), p
0.85 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
0.85 logistic.py(339):     n_classes = Y.shape[1]
0.85 logistic.py(340):     n_features = X.shape[1]
0.85 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
0.85 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
0.85 logistic.py(343):                     dtype=X.dtype)
0.85 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
0.85 logistic.py(282):     n_classes = Y.shape[1]
0.85 logistic.py(283):     n_features = X.shape[1]
0.85 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
0.85 logistic.py(285):     w = w.reshape(n_classes, -1)
0.85 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
0.85 logistic.py(287):     if fit_intercept:
0.85 logistic.py(288):         intercept = w[:, -1]
0.85 logistic.py(289):         w = w[:, :-1]
0.85 logistic.py(292):     p = safe_sparse_dot(X, w.T)
0.85 logistic.py(293):     p += intercept
0.85 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
0.85 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
0.85 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
0.85 logistic.py(297):     p = np.exp(p, p)
0.85 logistic.py(298):     return loss, p, w
0.85 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
0.85 logistic.py(346):     diff = sample_weight * (p - Y)
0.85 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
0.85 logistic.py(348):     grad[:, :n_features] += alpha * w
0.85 logistic.py(349):     if fit_intercept:
0.85 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
0.85 logistic.py(351):     return loss, grad.ravel(), p
0.85 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
0.85 logistic.py(339):     n_classes = Y.shape[1]
0.85 logistic.py(340):     n_features = X.shape[1]
0.85 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
0.85 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
0.85 logistic.py(343):                     dtype=X.dtype)
0.85 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
0.85 logistic.py(282):     n_classes = Y.shape[1]
0.85 logistic.py(283):     n_features = X.shape[1]
0.85 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
0.85 logistic.py(285):     w = w.reshape(n_classes, -1)
0.85 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
0.85 logistic.py(287):     if fit_intercept:
0.85 logistic.py(288):         intercept = w[:, -1]
0.85 logistic.py(289):         w = w[:, :-1]
0.85 logistic.py(292):     p = safe_sparse_dot(X, w.T)
0.85 logistic.py(293):     p += intercept
0.85 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
0.85 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
0.85 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
0.85 logistic.py(297):     p = np.exp(p, p)
0.85 logistic.py(298):     return loss, p, w
0.85 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
0.85 logistic.py(346):     diff = sample_weight * (p - Y)
0.85 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
0.85 logistic.py(348):     grad[:, :n_features] += alpha * w
0.85 logistic.py(349):     if fit_intercept:
0.85 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
0.85 logistic.py(351):     return loss, grad.ravel(), p
0.85 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
0.85 logistic.py(339):     n_classes = Y.shape[1]
0.85 logistic.py(340):     n_features = X.shape[1]
0.85 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
0.85 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
0.85 logistic.py(343):                     dtype=X.dtype)
0.85 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
0.85 logistic.py(282):     n_classes = Y.shape[1]
0.85 logistic.py(283):     n_features = X.shape[1]
0.85 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
0.85 logistic.py(285):     w = w.reshape(n_classes, -1)
0.85 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
0.85 logistic.py(287):     if fit_intercept:
0.85 logistic.py(288):         intercept = w[:, -1]
0.85 logistic.py(289):         w = w[:, :-1]
0.85 logistic.py(292):     p = safe_sparse_dot(X, w.T)
0.85 logistic.py(293):     p += intercept
0.85 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
0.85 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
0.85 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
0.85 logistic.py(297):     p = np.exp(p, p)
0.85 logistic.py(298):     return loss, p, w
0.85 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
0.85 logistic.py(346):     diff = sample_weight * (p - Y)
0.85 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
0.85 logistic.py(348):     grad[:, :n_features] += alpha * w
0.85 logistic.py(349):     if fit_intercept:
0.85 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
0.85 logistic.py(351):     return loss, grad.ravel(), p
0.86 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
0.86 logistic.py(339):     n_classes = Y.shape[1]
0.86 logistic.py(340):     n_features = X.shape[1]
0.86 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
0.86 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
0.86 logistic.py(343):                     dtype=X.dtype)
0.86 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
0.86 logistic.py(282):     n_classes = Y.shape[1]
0.86 logistic.py(283):     n_features = X.shape[1]
0.86 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
0.86 logistic.py(285):     w = w.reshape(n_classes, -1)
0.86 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
0.86 logistic.py(287):     if fit_intercept:
0.86 logistic.py(288):         intercept = w[:, -1]
0.86 logistic.py(289):         w = w[:, :-1]
0.86 logistic.py(292):     p = safe_sparse_dot(X, w.T)
0.86 logistic.py(293):     p += intercept
0.86 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
0.86 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
0.86 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
0.86 logistic.py(297):     p = np.exp(p, p)
0.86 logistic.py(298):     return loss, p, w
0.86 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
0.86 logistic.py(346):     diff = sample_weight * (p - Y)
0.86 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
0.86 logistic.py(348):     grad[:, :n_features] += alpha * w
0.86 logistic.py(349):     if fit_intercept:
0.86 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
0.86 logistic.py(351):     return loss, grad.ravel(), p
0.86 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
0.86 logistic.py(339):     n_classes = Y.shape[1]
0.86 logistic.py(340):     n_features = X.shape[1]
0.86 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
0.86 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
0.86 logistic.py(343):                     dtype=X.dtype)
0.86 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
0.86 logistic.py(282):     n_classes = Y.shape[1]
0.86 logistic.py(283):     n_features = X.shape[1]
0.86 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
0.86 logistic.py(285):     w = w.reshape(n_classes, -1)
0.86 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
0.86 logistic.py(287):     if fit_intercept:
0.86 logistic.py(288):         intercept = w[:, -1]
0.86 logistic.py(289):         w = w[:, :-1]
0.86 logistic.py(292):     p = safe_sparse_dot(X, w.T)
0.86 logistic.py(293):     p += intercept
0.86 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
0.86 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
0.86 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
0.86 logistic.py(297):     p = np.exp(p, p)
0.86 logistic.py(298):     return loss, p, w
0.86 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
0.86 logistic.py(346):     diff = sample_weight * (p - Y)
0.86 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
0.86 logistic.py(348):     grad[:, :n_features] += alpha * w
0.86 logistic.py(349):     if fit_intercept:
0.86 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
0.86 logistic.py(351):     return loss, grad.ravel(), p
0.86 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
0.86 logistic.py(339):     n_classes = Y.shape[1]
0.86 logistic.py(340):     n_features = X.shape[1]
0.86 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
0.86 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
0.86 logistic.py(343):                     dtype=X.dtype)
0.86 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
0.86 logistic.py(282):     n_classes = Y.shape[1]
0.86 logistic.py(283):     n_features = X.shape[1]
0.86 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
0.86 logistic.py(285):     w = w.reshape(n_classes, -1)
0.86 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
0.86 logistic.py(287):     if fit_intercept:
0.86 logistic.py(288):         intercept = w[:, -1]
0.86 logistic.py(289):         w = w[:, :-1]
0.86 logistic.py(292):     p = safe_sparse_dot(X, w.T)
0.86 logistic.py(293):     p += intercept
0.86 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
0.86 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
0.86 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
0.86 logistic.py(297):     p = np.exp(p, p)
0.86 logistic.py(298):     return loss, p, w
0.86 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
0.86 logistic.py(346):     diff = sample_weight * (p - Y)
0.86 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
0.86 logistic.py(348):     grad[:, :n_features] += alpha * w
0.86 logistic.py(349):     if fit_intercept:
0.86 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
0.86 logistic.py(351):     return loss, grad.ravel(), p
0.86 logistic.py(718):             if info["warnflag"] == 1:
0.86 logistic.py(723):             n_iter_i = min(info['nit'], max_iter)
0.86 logistic.py(760):         if multi_class == 'multinomial':
0.86 logistic.py(761):             multi_w0 = np.reshape(w0, (classes.size, -1))
0.86 logistic.py(762):             if classes.size == 2:
0.86 logistic.py(764):             coefs.append(multi_w0)
0.86 logistic.py(768):         n_iter[i] = n_iter_i
0.86 logistic.py(712):     for i, C in enumerate(Cs):
0.86 logistic.py(713):         if solver == 'lbfgs':
0.86 logistic.py(714):             w0, loss, info = optimize.fmin_l_bfgs_b(
0.86 logistic.py(715):                 func, w0, fprime=None,
0.86 logistic.py(716):                 args=(X, target, 1. / C, sample_weight),
0.86 logistic.py(717):                 iprint=(verbose > 0) - 1, pgtol=tol, maxiter=max_iter)
0.86 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
0.86 logistic.py(339):     n_classes = Y.shape[1]
0.86 logistic.py(340):     n_features = X.shape[1]
0.86 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
0.86 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
0.86 logistic.py(343):                     dtype=X.dtype)
0.86 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
0.86 logistic.py(282):     n_classes = Y.shape[1]
0.86 logistic.py(283):     n_features = X.shape[1]
0.86 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
0.86 logistic.py(285):     w = w.reshape(n_classes, -1)
0.86 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
0.86 logistic.py(287):     if fit_intercept:
0.86 logistic.py(288):         intercept = w[:, -1]
0.86 logistic.py(289):         w = w[:, :-1]
0.86 logistic.py(292):     p = safe_sparse_dot(X, w.T)
0.86 logistic.py(293):     p += intercept
0.86 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
0.86 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
0.86 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
0.86 logistic.py(297):     p = np.exp(p, p)
0.86 logistic.py(298):     return loss, p, w
0.86 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
0.86 logistic.py(346):     diff = sample_weight * (p - Y)
0.86 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
0.86 logistic.py(348):     grad[:, :n_features] += alpha * w
0.86 logistic.py(349):     if fit_intercept:
0.86 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
0.86 logistic.py(351):     return loss, grad.ravel(), p
0.86 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
0.86 logistic.py(339):     n_classes = Y.shape[1]
0.86 logistic.py(340):     n_features = X.shape[1]
0.86 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
0.86 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
0.86 logistic.py(343):                     dtype=X.dtype)
0.86 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
0.86 logistic.py(282):     n_classes = Y.shape[1]
0.86 logistic.py(283):     n_features = X.shape[1]
0.86 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
0.86 logistic.py(285):     w = w.reshape(n_classes, -1)
0.86 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
0.86 logistic.py(287):     if fit_intercept:
0.86 logistic.py(288):         intercept = w[:, -1]
0.86 logistic.py(289):         w = w[:, :-1]
0.86 logistic.py(292):     p = safe_sparse_dot(X, w.T)
0.86 logistic.py(293):     p += intercept
0.86 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
0.86 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
0.86 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
0.86 logistic.py(297):     p = np.exp(p, p)
0.86 logistic.py(298):     return loss, p, w
0.86 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
0.86 logistic.py(346):     diff = sample_weight * (p - Y)
0.86 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
0.86 logistic.py(348):     grad[:, :n_features] += alpha * w
0.86 logistic.py(349):     if fit_intercept:
0.86 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
0.86 logistic.py(351):     return loss, grad.ravel(), p
0.86 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
0.86 logistic.py(339):     n_classes = Y.shape[1]
0.86 logistic.py(340):     n_features = X.shape[1]
0.86 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
0.86 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
0.86 logistic.py(343):                     dtype=X.dtype)
0.86 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
0.86 logistic.py(282):     n_classes = Y.shape[1]
0.86 logistic.py(283):     n_features = X.shape[1]
0.86 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
0.86 logistic.py(285):     w = w.reshape(n_classes, -1)
0.86 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
0.86 logistic.py(287):     if fit_intercept:
0.86 logistic.py(288):         intercept = w[:, -1]
0.86 logistic.py(289):         w = w[:, :-1]
0.86 logistic.py(292):     p = safe_sparse_dot(X, w.T)
0.86 logistic.py(293):     p += intercept
0.86 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
0.86 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
0.86 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
0.86 logistic.py(297):     p = np.exp(p, p)
0.86 logistic.py(298):     return loss, p, w
0.86 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
0.86 logistic.py(346):     diff = sample_weight * (p - Y)
0.86 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
0.86 logistic.py(348):     grad[:, :n_features] += alpha * w
0.86 logistic.py(349):     if fit_intercept:
0.86 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
0.86 logistic.py(351):     return loss, grad.ravel(), p
0.86 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
0.86 logistic.py(339):     n_classes = Y.shape[1]
0.86 logistic.py(340):     n_features = X.shape[1]
0.86 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
0.86 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
0.86 logistic.py(343):                     dtype=X.dtype)
0.86 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
0.86 logistic.py(282):     n_classes = Y.shape[1]
0.86 logistic.py(283):     n_features = X.shape[1]
0.86 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
0.86 logistic.py(285):     w = w.reshape(n_classes, -1)
0.86 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
0.86 logistic.py(287):     if fit_intercept:
0.86 logistic.py(288):         intercept = w[:, -1]
0.86 logistic.py(289):         w = w[:, :-1]
0.86 logistic.py(292):     p = safe_sparse_dot(X, w.T)
0.86 logistic.py(293):     p += intercept
0.86 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
0.86 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
0.86 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
0.86 logistic.py(297):     p = np.exp(p, p)
0.86 logistic.py(298):     return loss, p, w
0.86 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
0.86 logistic.py(346):     diff = sample_weight * (p - Y)
0.86 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
0.86 logistic.py(348):     grad[:, :n_features] += alpha * w
0.86 logistic.py(349):     if fit_intercept:
0.86 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
0.86 logistic.py(351):     return loss, grad.ravel(), p
0.86 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
0.86 logistic.py(339):     n_classes = Y.shape[1]
0.86 logistic.py(340):     n_features = X.shape[1]
0.86 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
0.86 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
0.86 logistic.py(343):                     dtype=X.dtype)
0.86 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
0.86 logistic.py(282):     n_classes = Y.shape[1]
0.86 logistic.py(283):     n_features = X.shape[1]
0.86 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
0.86 logistic.py(285):     w = w.reshape(n_classes, -1)
0.86 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
0.86 logistic.py(287):     if fit_intercept:
0.86 logistic.py(288):         intercept = w[:, -1]
0.86 logistic.py(289):         w = w[:, :-1]
0.86 logistic.py(292):     p = safe_sparse_dot(X, w.T)
0.86 logistic.py(293):     p += intercept
0.86 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
0.87 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
0.87 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
0.87 logistic.py(297):     p = np.exp(p, p)
0.87 logistic.py(298):     return loss, p, w
0.87 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
0.87 logistic.py(346):     diff = sample_weight * (p - Y)
0.87 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
0.87 logistic.py(348):     grad[:, :n_features] += alpha * w
0.87 logistic.py(349):     if fit_intercept:
0.87 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
0.87 logistic.py(351):     return loss, grad.ravel(), p
0.87 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
0.87 logistic.py(339):     n_classes = Y.shape[1]
0.87 logistic.py(340):     n_features = X.shape[1]
0.87 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
0.87 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
0.87 logistic.py(343):                     dtype=X.dtype)
0.87 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
0.87 logistic.py(282):     n_classes = Y.shape[1]
0.87 logistic.py(283):     n_features = X.shape[1]
0.87 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
0.87 logistic.py(285):     w = w.reshape(n_classes, -1)
0.87 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
0.87 logistic.py(287):     if fit_intercept:
0.87 logistic.py(288):         intercept = w[:, -1]
0.87 logistic.py(289):         w = w[:, :-1]
0.87 logistic.py(292):     p = safe_sparse_dot(X, w.T)
0.87 logistic.py(293):     p += intercept
0.87 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
0.87 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
0.87 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
0.87 logistic.py(297):     p = np.exp(p, p)
0.87 logistic.py(298):     return loss, p, w
0.87 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
0.87 logistic.py(346):     diff = sample_weight * (p - Y)
0.87 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
0.87 logistic.py(348):     grad[:, :n_features] += alpha * w
0.87 logistic.py(349):     if fit_intercept:
0.87 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
0.87 logistic.py(351):     return loss, grad.ravel(), p
0.87 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
0.87 logistic.py(339):     n_classes = Y.shape[1]
0.87 logistic.py(340):     n_features = X.shape[1]
0.87 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
0.87 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
0.87 logistic.py(343):                     dtype=X.dtype)
0.87 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
0.87 logistic.py(282):     n_classes = Y.shape[1]
0.87 logistic.py(283):     n_features = X.shape[1]
0.87 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
0.87 logistic.py(285):     w = w.reshape(n_classes, -1)
0.87 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
0.87 logistic.py(287):     if fit_intercept:
0.87 logistic.py(288):         intercept = w[:, -1]
0.87 logistic.py(289):         w = w[:, :-1]
0.87 logistic.py(292):     p = safe_sparse_dot(X, w.T)
0.87 logistic.py(293):     p += intercept
0.87 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
0.87 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
0.87 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
0.87 logistic.py(297):     p = np.exp(p, p)
0.87 logistic.py(298):     return loss, p, w
0.87 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
0.87 logistic.py(346):     diff = sample_weight * (p - Y)
0.87 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
0.87 logistic.py(348):     grad[:, :n_features] += alpha * w
0.87 logistic.py(349):     if fit_intercept:
0.87 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
0.87 logistic.py(351):     return loss, grad.ravel(), p
0.87 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
0.87 logistic.py(339):     n_classes = Y.shape[1]
0.87 logistic.py(340):     n_features = X.shape[1]
0.87 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
0.87 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
0.87 logistic.py(343):                     dtype=X.dtype)
0.87 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
0.87 logistic.py(282):     n_classes = Y.shape[1]
0.87 logistic.py(283):     n_features = X.shape[1]
0.87 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
0.87 logistic.py(285):     w = w.reshape(n_classes, -1)
0.87 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
0.87 logistic.py(287):     if fit_intercept:
0.87 logistic.py(288):         intercept = w[:, -1]
0.87 logistic.py(289):         w = w[:, :-1]
0.87 logistic.py(292):     p = safe_sparse_dot(X, w.T)
0.87 logistic.py(293):     p += intercept
0.87 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
0.87 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
0.87 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
0.87 logistic.py(297):     p = np.exp(p, p)
0.87 logistic.py(298):     return loss, p, w
0.87 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
0.87 logistic.py(346):     diff = sample_weight * (p - Y)
0.87 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
0.87 logistic.py(348):     grad[:, :n_features] += alpha * w
0.87 logistic.py(349):     if fit_intercept:
0.87 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
0.87 logistic.py(351):     return loss, grad.ravel(), p
0.87 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
0.87 logistic.py(339):     n_classes = Y.shape[1]
0.87 logistic.py(340):     n_features = X.shape[1]
0.87 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
0.87 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
0.87 logistic.py(343):                     dtype=X.dtype)
0.87 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
0.87 logistic.py(282):     n_classes = Y.shape[1]
0.87 logistic.py(283):     n_features = X.shape[1]
0.87 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
0.87 logistic.py(285):     w = w.reshape(n_classes, -1)
0.87 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
0.87 logistic.py(287):     if fit_intercept:
0.87 logistic.py(288):         intercept = w[:, -1]
0.87 logistic.py(289):         w = w[:, :-1]
0.87 logistic.py(292):     p = safe_sparse_dot(X, w.T)
0.87 logistic.py(293):     p += intercept
0.87 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
0.87 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
0.87 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
0.87 logistic.py(297):     p = np.exp(p, p)
0.87 logistic.py(298):     return loss, p, w
0.87 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
0.87 logistic.py(346):     diff = sample_weight * (p - Y)
0.87 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
0.87 logistic.py(348):     grad[:, :n_features] += alpha * w
0.87 logistic.py(349):     if fit_intercept:
0.87 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
0.87 logistic.py(351):     return loss, grad.ravel(), p
0.87 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
0.87 logistic.py(339):     n_classes = Y.shape[1]
0.87 logistic.py(340):     n_features = X.shape[1]
0.87 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
0.87 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
0.87 logistic.py(343):                     dtype=X.dtype)
0.87 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
0.87 logistic.py(282):     n_classes = Y.shape[1]
0.87 logistic.py(283):     n_features = X.shape[1]
0.87 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
0.87 logistic.py(285):     w = w.reshape(n_classes, -1)
0.87 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
0.87 logistic.py(287):     if fit_intercept:
0.87 logistic.py(288):         intercept = w[:, -1]
0.87 logistic.py(289):         w = w[:, :-1]
0.87 logistic.py(292):     p = safe_sparse_dot(X, w.T)
0.87 logistic.py(293):     p += intercept
0.87 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
0.87 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
0.87 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
0.87 logistic.py(297):     p = np.exp(p, p)
0.87 logistic.py(298):     return loss, p, w
0.87 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
0.87 logistic.py(346):     diff = sample_weight * (p - Y)
0.87 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
0.87 logistic.py(348):     grad[:, :n_features] += alpha * w
0.87 logistic.py(349):     if fit_intercept:
0.87 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
0.87 logistic.py(351):     return loss, grad.ravel(), p
0.87 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
0.87 logistic.py(339):     n_classes = Y.shape[1]
0.87 logistic.py(340):     n_features = X.shape[1]
0.87 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
0.87 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
0.87 logistic.py(343):                     dtype=X.dtype)
0.87 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
0.87 logistic.py(282):     n_classes = Y.shape[1]
0.87 logistic.py(283):     n_features = X.shape[1]
0.87 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
0.87 logistic.py(285):     w = w.reshape(n_classes, -1)
0.87 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
0.87 logistic.py(287):     if fit_intercept:
0.87 logistic.py(288):         intercept = w[:, -1]
0.87 logistic.py(289):         w = w[:, :-1]
0.87 logistic.py(292):     p = safe_sparse_dot(X, w.T)
0.87 logistic.py(293):     p += intercept
0.87 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
0.87 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
0.87 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
0.87 logistic.py(297):     p = np.exp(p, p)
0.87 logistic.py(298):     return loss, p, w
0.87 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
0.87 logistic.py(346):     diff = sample_weight * (p - Y)
0.87 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
0.87 logistic.py(348):     grad[:, :n_features] += alpha * w
0.87 logistic.py(349):     if fit_intercept:
0.87 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
0.87 logistic.py(351):     return loss, grad.ravel(), p
0.87 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
0.87 logistic.py(339):     n_classes = Y.shape[1]
0.87 logistic.py(340):     n_features = X.shape[1]
0.87 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
0.87 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
0.87 logistic.py(343):                     dtype=X.dtype)
0.87 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
0.87 logistic.py(282):     n_classes = Y.shape[1]
0.87 logistic.py(283):     n_features = X.shape[1]
0.87 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
0.87 logistic.py(285):     w = w.reshape(n_classes, -1)
0.87 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
0.87 logistic.py(287):     if fit_intercept:
0.87 logistic.py(288):         intercept = w[:, -1]
0.87 logistic.py(289):         w = w[:, :-1]
0.87 logistic.py(292):     p = safe_sparse_dot(X, w.T)
0.87 logistic.py(293):     p += intercept
0.87 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
0.87 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
0.87 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
0.87 logistic.py(297):     p = np.exp(p, p)
0.87 logistic.py(298):     return loss, p, w
0.87 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
0.87 logistic.py(346):     diff = sample_weight * (p - Y)
0.87 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
0.87 logistic.py(348):     grad[:, :n_features] += alpha * w
0.87 logistic.py(349):     if fit_intercept:
0.87 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
0.87 logistic.py(351):     return loss, grad.ravel(), p
0.87 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
0.87 logistic.py(339):     n_classes = Y.shape[1]
0.87 logistic.py(340):     n_features = X.shape[1]
0.87 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
0.87 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
0.87 logistic.py(343):                     dtype=X.dtype)
0.87 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
0.87 logistic.py(282):     n_classes = Y.shape[1]
0.87 logistic.py(283):     n_features = X.shape[1]
0.87 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
0.87 logistic.py(285):     w = w.reshape(n_classes, -1)
0.87 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
0.87 logistic.py(287):     if fit_intercept:
0.87 logistic.py(288):         intercept = w[:, -1]
0.87 logistic.py(289):         w = w[:, :-1]
0.87 logistic.py(292):     p = safe_sparse_dot(X, w.T)
0.87 logistic.py(293):     p += intercept
0.87 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
0.87 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
0.87 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
0.87 logistic.py(297):     p = np.exp(p, p)
0.87 logistic.py(298):     return loss, p, w
0.87 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
0.87 logistic.py(346):     diff = sample_weight * (p - Y)
0.87 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
0.87 logistic.py(348):     grad[:, :n_features] += alpha * w
0.87 logistic.py(349):     if fit_intercept:
0.87 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
0.87 logistic.py(351):     return loss, grad.ravel(), p
0.87 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
0.87 logistic.py(339):     n_classes = Y.shape[1]
0.87 logistic.py(340):     n_features = X.shape[1]
0.87 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
0.87 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
0.87 logistic.py(343):                     dtype=X.dtype)
0.87 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
0.87 logistic.py(282):     n_classes = Y.shape[1]
0.87 logistic.py(283):     n_features = X.shape[1]
0.87 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
0.87 logistic.py(285):     w = w.reshape(n_classes, -1)
0.87 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
0.87 logistic.py(287):     if fit_intercept:
0.87 logistic.py(288):         intercept = w[:, -1]
0.87 logistic.py(289):         w = w[:, :-1]
0.87 logistic.py(292):     p = safe_sparse_dot(X, w.T)
0.87 logistic.py(293):     p += intercept
0.87 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
0.87 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
0.87 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
0.87 logistic.py(297):     p = np.exp(p, p)
0.87 logistic.py(298):     return loss, p, w
0.87 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
0.87 logistic.py(346):     diff = sample_weight * (p - Y)
0.87 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
0.87 logistic.py(348):     grad[:, :n_features] += alpha * w
0.87 logistic.py(349):     if fit_intercept:
0.87 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
0.87 logistic.py(351):     return loss, grad.ravel(), p
0.87 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
0.87 logistic.py(339):     n_classes = Y.shape[1]
0.87 logistic.py(340):     n_features = X.shape[1]
0.87 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
0.87 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
0.87 logistic.py(343):                     dtype=X.dtype)
0.87 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
0.87 logistic.py(282):     n_classes = Y.shape[1]
0.87 logistic.py(283):     n_features = X.shape[1]
0.87 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
0.87 logistic.py(285):     w = w.reshape(n_classes, -1)
0.87 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
0.87 logistic.py(287):     if fit_intercept:
0.87 logistic.py(288):         intercept = w[:, -1]
0.87 logistic.py(289):         w = w[:, :-1]
0.87 logistic.py(292):     p = safe_sparse_dot(X, w.T)
0.87 logistic.py(293):     p += intercept
0.87 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
0.87 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
0.87 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
0.87 logistic.py(297):     p = np.exp(p, p)
0.87 logistic.py(298):     return loss, p, w
0.87 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
0.87 logistic.py(346):     diff = sample_weight * (p - Y)
0.87 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
0.87 logistic.py(348):     grad[:, :n_features] += alpha * w
0.87 logistic.py(349):     if fit_intercept:
0.87 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
0.87 logistic.py(351):     return loss, grad.ravel(), p
0.87 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
0.87 logistic.py(339):     n_classes = Y.shape[1]
0.87 logistic.py(340):     n_features = X.shape[1]
0.87 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
0.87 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
0.87 logistic.py(343):                     dtype=X.dtype)
0.87 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
0.87 logistic.py(282):     n_classes = Y.shape[1]
0.87 logistic.py(283):     n_features = X.shape[1]
0.87 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
0.87 logistic.py(285):     w = w.reshape(n_classes, -1)
0.87 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
0.87 logistic.py(287):     if fit_intercept:
0.87 logistic.py(288):         intercept = w[:, -1]
0.87 logistic.py(289):         w = w[:, :-1]
0.87 logistic.py(292):     p = safe_sparse_dot(X, w.T)
0.87 logistic.py(293):     p += intercept
0.87 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
0.87 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
0.87 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
0.87 logistic.py(297):     p = np.exp(p, p)
0.87 logistic.py(298):     return loss, p, w
0.87 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
0.87 logistic.py(346):     diff = sample_weight * (p - Y)
0.87 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
0.87 logistic.py(348):     grad[:, :n_features] += alpha * w
0.87 logistic.py(349):     if fit_intercept:
0.87 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
0.87 logistic.py(351):     return loss, grad.ravel(), p
0.87 logistic.py(718):             if info["warnflag"] == 1:
0.87 logistic.py(723):             n_iter_i = min(info['nit'], max_iter)
0.87 logistic.py(760):         if multi_class == 'multinomial':
0.87 logistic.py(761):             multi_w0 = np.reshape(w0, (classes.size, -1))
0.87 logistic.py(762):             if classes.size == 2:
0.87 logistic.py(764):             coefs.append(multi_w0)
0.87 logistic.py(768):         n_iter[i] = n_iter_i
0.87 logistic.py(712):     for i, C in enumerate(Cs):
0.87 logistic.py(713):         if solver == 'lbfgs':
0.87 logistic.py(714):             w0, loss, info = optimize.fmin_l_bfgs_b(
0.87 logistic.py(715):                 func, w0, fprime=None,
0.87 logistic.py(716):                 args=(X, target, 1. / C, sample_weight),
0.87 logistic.py(717):                 iprint=(verbose > 0) - 1, pgtol=tol, maxiter=max_iter)
0.87 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
0.87 logistic.py(339):     n_classes = Y.shape[1]
0.87 logistic.py(340):     n_features = X.shape[1]
0.87 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
0.87 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
0.87 logistic.py(343):                     dtype=X.dtype)
0.87 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
0.87 logistic.py(282):     n_classes = Y.shape[1]
0.87 logistic.py(283):     n_features = X.shape[1]
0.87 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
0.87 logistic.py(285):     w = w.reshape(n_classes, -1)
0.87 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
0.87 logistic.py(287):     if fit_intercept:
0.87 logistic.py(288):         intercept = w[:, -1]
0.87 logistic.py(289):         w = w[:, :-1]
0.87 logistic.py(292):     p = safe_sparse_dot(X, w.T)
0.87 logistic.py(293):     p += intercept
0.87 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
0.87 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
0.87 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
0.87 logistic.py(297):     p = np.exp(p, p)
0.87 logistic.py(298):     return loss, p, w
0.87 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
0.87 logistic.py(346):     diff = sample_weight * (p - Y)
0.87 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
0.87 logistic.py(348):     grad[:, :n_features] += alpha * w
0.87 logistic.py(349):     if fit_intercept:
0.87 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
0.87 logistic.py(351):     return loss, grad.ravel(), p
0.87 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
0.87 logistic.py(339):     n_classes = Y.shape[1]
0.87 logistic.py(340):     n_features = X.shape[1]
0.87 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
0.87 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
0.87 logistic.py(343):                     dtype=X.dtype)
0.87 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
0.87 logistic.py(282):     n_classes = Y.shape[1]
0.87 logistic.py(283):     n_features = X.shape[1]
0.87 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
0.87 logistic.py(285):     w = w.reshape(n_classes, -1)
0.87 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
0.87 logistic.py(287):     if fit_intercept:
0.87 logistic.py(288):         intercept = w[:, -1]
0.87 logistic.py(289):         w = w[:, :-1]
0.87 logistic.py(292):     p = safe_sparse_dot(X, w.T)
0.87 logistic.py(293):     p += intercept
0.87 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
0.87 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
0.87 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
0.87 logistic.py(297):     p = np.exp(p, p)
0.87 logistic.py(298):     return loss, p, w
0.87 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
0.87 logistic.py(346):     diff = sample_weight * (p - Y)
0.87 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
0.87 logistic.py(348):     grad[:, :n_features] += alpha * w
0.87 logistic.py(349):     if fit_intercept:
0.87 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
0.87 logistic.py(351):     return loss, grad.ravel(), p
0.87 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
0.87 logistic.py(339):     n_classes = Y.shape[1]
0.87 logistic.py(340):     n_features = X.shape[1]
0.87 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
0.87 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
0.88 logistic.py(343):                     dtype=X.dtype)
0.88 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
0.88 logistic.py(282):     n_classes = Y.shape[1]
0.88 logistic.py(283):     n_features = X.shape[1]
0.88 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
0.88 logistic.py(285):     w = w.reshape(n_classes, -1)
0.88 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
0.88 logistic.py(287):     if fit_intercept:
0.88 logistic.py(288):         intercept = w[:, -1]
0.88 logistic.py(289):         w = w[:, :-1]
0.88 logistic.py(292):     p = safe_sparse_dot(X, w.T)
0.88 logistic.py(293):     p += intercept
0.88 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
0.88 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
0.88 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
0.88 logistic.py(297):     p = np.exp(p, p)
0.88 logistic.py(298):     return loss, p, w
0.88 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
0.88 logistic.py(346):     diff = sample_weight * (p - Y)
0.88 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
0.88 logistic.py(348):     grad[:, :n_features] += alpha * w
0.88 logistic.py(349):     if fit_intercept:
0.88 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
0.88 logistic.py(351):     return loss, grad.ravel(), p
0.88 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
0.88 logistic.py(339):     n_classes = Y.shape[1]
0.88 logistic.py(340):     n_features = X.shape[1]
0.88 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
0.88 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
0.88 logistic.py(343):                     dtype=X.dtype)
0.88 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
0.88 logistic.py(282):     n_classes = Y.shape[1]
0.88 logistic.py(283):     n_features = X.shape[1]
0.88 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
0.88 logistic.py(285):     w = w.reshape(n_classes, -1)
0.88 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
0.88 logistic.py(287):     if fit_intercept:
0.88 logistic.py(288):         intercept = w[:, -1]
0.88 logistic.py(289):         w = w[:, :-1]
0.88 logistic.py(292):     p = safe_sparse_dot(X, w.T)
0.88 logistic.py(293):     p += intercept
0.88 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
0.88 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
0.88 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
0.88 logistic.py(297):     p = np.exp(p, p)
0.88 logistic.py(298):     return loss, p, w
0.88 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
0.88 logistic.py(346):     diff = sample_weight * (p - Y)
0.88 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
0.88 logistic.py(348):     grad[:, :n_features] += alpha * w
0.88 logistic.py(349):     if fit_intercept:
0.88 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
0.88 logistic.py(351):     return loss, grad.ravel(), p
0.88 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
0.88 logistic.py(339):     n_classes = Y.shape[1]
0.88 logistic.py(340):     n_features = X.shape[1]
0.88 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
0.88 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
0.88 logistic.py(343):                     dtype=X.dtype)
0.88 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
0.88 logistic.py(282):     n_classes = Y.shape[1]
0.88 logistic.py(283):     n_features = X.shape[1]
0.88 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
0.88 logistic.py(285):     w = w.reshape(n_classes, -1)
0.88 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
0.88 logistic.py(287):     if fit_intercept:
0.88 logistic.py(288):         intercept = w[:, -1]
0.88 logistic.py(289):         w = w[:, :-1]
0.88 logistic.py(292):     p = safe_sparse_dot(X, w.T)
0.88 logistic.py(293):     p += intercept
0.88 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
0.88 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
0.88 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
0.88 logistic.py(297):     p = np.exp(p, p)
0.88 logistic.py(298):     return loss, p, w
0.88 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
0.88 logistic.py(346):     diff = sample_weight * (p - Y)
0.88 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
0.88 logistic.py(348):     grad[:, :n_features] += alpha * w
0.88 logistic.py(349):     if fit_intercept:
0.88 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
0.88 logistic.py(351):     return loss, grad.ravel(), p
0.88 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
0.88 logistic.py(339):     n_classes = Y.shape[1]
0.88 logistic.py(340):     n_features = X.shape[1]
0.88 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
0.88 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
0.88 logistic.py(343):                     dtype=X.dtype)
0.88 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
0.88 logistic.py(282):     n_classes = Y.shape[1]
0.88 logistic.py(283):     n_features = X.shape[1]
0.88 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
0.88 logistic.py(285):     w = w.reshape(n_classes, -1)
0.88 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
0.88 logistic.py(287):     if fit_intercept:
0.88 logistic.py(288):         intercept = w[:, -1]
0.88 logistic.py(289):         w = w[:, :-1]
0.88 logistic.py(292):     p = safe_sparse_dot(X, w.T)
0.88 logistic.py(293):     p += intercept
0.88 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
0.88 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
0.88 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
0.88 logistic.py(297):     p = np.exp(p, p)
0.88 logistic.py(298):     return loss, p, w
0.88 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
0.88 logistic.py(346):     diff = sample_weight * (p - Y)
0.88 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
0.88 logistic.py(348):     grad[:, :n_features] += alpha * w
0.88 logistic.py(349):     if fit_intercept:
0.88 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
0.88 logistic.py(351):     return loss, grad.ravel(), p
0.88 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
0.88 logistic.py(339):     n_classes = Y.shape[1]
0.88 logistic.py(340):     n_features = X.shape[1]
0.88 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
0.88 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
0.88 logistic.py(343):                     dtype=X.dtype)
0.88 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
0.88 logistic.py(282):     n_classes = Y.shape[1]
0.88 logistic.py(283):     n_features = X.shape[1]
0.88 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
0.88 logistic.py(285):     w = w.reshape(n_classes, -1)
0.88 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
0.88 logistic.py(287):     if fit_intercept:
0.88 logistic.py(288):         intercept = w[:, -1]
0.88 logistic.py(289):         w = w[:, :-1]
0.88 logistic.py(292):     p = safe_sparse_dot(X, w.T)
0.88 logistic.py(293):     p += intercept
0.88 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
0.88 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
0.88 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
0.88 logistic.py(297):     p = np.exp(p, p)
0.88 logistic.py(298):     return loss, p, w
0.88 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
0.88 logistic.py(346):     diff = sample_weight * (p - Y)
0.88 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
0.88 logistic.py(348):     grad[:, :n_features] += alpha * w
0.88 logistic.py(349):     if fit_intercept:
0.88 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
0.88 logistic.py(351):     return loss, grad.ravel(), p
0.88 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
0.88 logistic.py(339):     n_classes = Y.shape[1]
0.88 logistic.py(340):     n_features = X.shape[1]
0.88 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
0.88 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
0.88 logistic.py(343):                     dtype=X.dtype)
0.88 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
0.88 logistic.py(282):     n_classes = Y.shape[1]
0.88 logistic.py(283):     n_features = X.shape[1]
0.88 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
0.88 logistic.py(285):     w = w.reshape(n_classes, -1)
0.88 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
0.88 logistic.py(287):     if fit_intercept:
0.88 logistic.py(288):         intercept = w[:, -1]
0.88 logistic.py(289):         w = w[:, :-1]
0.88 logistic.py(292):     p = safe_sparse_dot(X, w.T)
0.88 logistic.py(293):     p += intercept
0.88 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
0.88 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
0.88 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
0.88 logistic.py(297):     p = np.exp(p, p)
0.88 logistic.py(298):     return loss, p, w
0.88 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
0.88 logistic.py(346):     diff = sample_weight * (p - Y)
0.88 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
0.88 logistic.py(348):     grad[:, :n_features] += alpha * w
0.88 logistic.py(349):     if fit_intercept:
0.88 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
0.88 logistic.py(351):     return loss, grad.ravel(), p
0.88 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
0.88 logistic.py(339):     n_classes = Y.shape[1]
0.88 logistic.py(340):     n_features = X.shape[1]
0.88 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
0.88 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
0.88 logistic.py(343):                     dtype=X.dtype)
0.88 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
0.88 logistic.py(282):     n_classes = Y.shape[1]
0.88 logistic.py(283):     n_features = X.shape[1]
0.88 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
0.88 logistic.py(285):     w = w.reshape(n_classes, -1)
0.88 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
0.88 logistic.py(287):     if fit_intercept:
0.88 logistic.py(288):         intercept = w[:, -1]
0.88 logistic.py(289):         w = w[:, :-1]
0.88 logistic.py(292):     p = safe_sparse_dot(X, w.T)
0.88 logistic.py(293):     p += intercept
0.88 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
0.88 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
0.88 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
0.88 logistic.py(297):     p = np.exp(p, p)
0.88 logistic.py(298):     return loss, p, w
0.88 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
0.88 logistic.py(346):     diff = sample_weight * (p - Y)
0.88 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
0.88 logistic.py(348):     grad[:, :n_features] += alpha * w
0.88 logistic.py(349):     if fit_intercept:
0.88 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
0.88 logistic.py(351):     return loss, grad.ravel(), p
0.88 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
0.88 logistic.py(339):     n_classes = Y.shape[1]
0.88 logistic.py(340):     n_features = X.shape[1]
0.88 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
0.88 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
0.88 logistic.py(343):                     dtype=X.dtype)
0.88 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
0.88 logistic.py(282):     n_classes = Y.shape[1]
0.88 logistic.py(283):     n_features = X.shape[1]
0.88 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
0.88 logistic.py(285):     w = w.reshape(n_classes, -1)
0.88 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
0.88 logistic.py(287):     if fit_intercept:
0.88 logistic.py(288):         intercept = w[:, -1]
0.88 logistic.py(289):         w = w[:, :-1]
0.88 logistic.py(292):     p = safe_sparse_dot(X, w.T)
0.88 logistic.py(293):     p += intercept
0.88 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
0.88 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
0.88 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
0.88 logistic.py(297):     p = np.exp(p, p)
0.88 logistic.py(298):     return loss, p, w
0.88 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
0.88 logistic.py(346):     diff = sample_weight * (p - Y)
0.88 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
0.88 logistic.py(348):     grad[:, :n_features] += alpha * w
0.88 logistic.py(349):     if fit_intercept:
0.88 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
0.88 logistic.py(351):     return loss, grad.ravel(), p
0.88 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
0.88 logistic.py(339):     n_classes = Y.shape[1]
0.88 logistic.py(340):     n_features = X.shape[1]
0.88 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
0.88 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
0.88 logistic.py(343):                     dtype=X.dtype)
0.88 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
0.88 logistic.py(282):     n_classes = Y.shape[1]
0.88 logistic.py(283):     n_features = X.shape[1]
0.88 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
0.88 logistic.py(285):     w = w.reshape(n_classes, -1)
0.88 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
0.88 logistic.py(287):     if fit_intercept:
0.88 logistic.py(288):         intercept = w[:, -1]
0.88 logistic.py(289):         w = w[:, :-1]
0.88 logistic.py(292):     p = safe_sparse_dot(X, w.T)
0.88 logistic.py(293):     p += intercept
0.88 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
0.88 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
0.88 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
0.88 logistic.py(297):     p = np.exp(p, p)
0.88 logistic.py(298):     return loss, p, w
0.88 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
0.88 logistic.py(346):     diff = sample_weight * (p - Y)
0.88 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
0.88 logistic.py(348):     grad[:, :n_features] += alpha * w
0.88 logistic.py(349):     if fit_intercept:
0.88 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
0.88 logistic.py(351):     return loss, grad.ravel(), p
0.88 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
0.88 logistic.py(339):     n_classes = Y.shape[1]
0.88 logistic.py(340):     n_features = X.shape[1]
0.88 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
0.88 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
0.88 logistic.py(343):                     dtype=X.dtype)
0.88 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
0.88 logistic.py(282):     n_classes = Y.shape[1]
0.88 logistic.py(283):     n_features = X.shape[1]
0.88 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
0.88 logistic.py(285):     w = w.reshape(n_classes, -1)
0.88 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
0.88 logistic.py(287):     if fit_intercept:
0.88 logistic.py(288):         intercept = w[:, -1]
0.88 logistic.py(289):         w = w[:, :-1]
0.88 logistic.py(292):     p = safe_sparse_dot(X, w.T)
0.88 logistic.py(293):     p += intercept
0.88 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
0.88 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
0.88 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
0.88 logistic.py(297):     p = np.exp(p, p)
0.88 logistic.py(298):     return loss, p, w
0.88 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
0.88 logistic.py(346):     diff = sample_weight * (p - Y)
0.88 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
0.88 logistic.py(348):     grad[:, :n_features] += alpha * w
0.88 logistic.py(349):     if fit_intercept:
0.88 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
0.88 logistic.py(351):     return loss, grad.ravel(), p
0.88 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
0.88 logistic.py(339):     n_classes = Y.shape[1]
0.88 logistic.py(340):     n_features = X.shape[1]
0.88 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
0.88 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
0.88 logistic.py(343):                     dtype=X.dtype)
0.88 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
0.88 logistic.py(282):     n_classes = Y.shape[1]
0.88 logistic.py(283):     n_features = X.shape[1]
0.88 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
0.88 logistic.py(285):     w = w.reshape(n_classes, -1)
0.88 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
0.88 logistic.py(287):     if fit_intercept:
0.88 logistic.py(288):         intercept = w[:, -1]
0.88 logistic.py(289):         w = w[:, :-1]
0.88 logistic.py(292):     p = safe_sparse_dot(X, w.T)
0.88 logistic.py(293):     p += intercept
0.88 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
0.88 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
0.88 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
0.88 logistic.py(297):     p = np.exp(p, p)
0.88 logistic.py(298):     return loss, p, w
0.88 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
0.88 logistic.py(346):     diff = sample_weight * (p - Y)
0.88 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
0.88 logistic.py(348):     grad[:, :n_features] += alpha * w
0.88 logistic.py(349):     if fit_intercept:
0.88 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
0.88 logistic.py(351):     return loss, grad.ravel(), p
0.88 logistic.py(718):             if info["warnflag"] == 1:
0.88 logistic.py(723):             n_iter_i = min(info['nit'], max_iter)
0.88 logistic.py(760):         if multi_class == 'multinomial':
0.88 logistic.py(761):             multi_w0 = np.reshape(w0, (classes.size, -1))
0.88 logistic.py(762):             if classes.size == 2:
0.88 logistic.py(764):             coefs.append(multi_w0)
0.88 logistic.py(768):         n_iter[i] = n_iter_i
0.88 logistic.py(712):     for i, C in enumerate(Cs):
0.88 logistic.py(713):         if solver == 'lbfgs':
0.88 logistic.py(714):             w0, loss, info = optimize.fmin_l_bfgs_b(
0.88 logistic.py(715):                 func, w0, fprime=None,
0.88 logistic.py(716):                 args=(X, target, 1. / C, sample_weight),
0.88 logistic.py(717):                 iprint=(verbose > 0) - 1, pgtol=tol, maxiter=max_iter)
0.88 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
0.88 logistic.py(339):     n_classes = Y.shape[1]
0.88 logistic.py(340):     n_features = X.shape[1]
0.88 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
0.88 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
0.88 logistic.py(343):                     dtype=X.dtype)
0.88 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
0.88 logistic.py(282):     n_classes = Y.shape[1]
0.88 logistic.py(283):     n_features = X.shape[1]
0.88 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
0.88 logistic.py(285):     w = w.reshape(n_classes, -1)
0.88 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
0.88 logistic.py(287):     if fit_intercept:
0.88 logistic.py(288):         intercept = w[:, -1]
0.88 logistic.py(289):         w = w[:, :-1]
0.88 logistic.py(292):     p = safe_sparse_dot(X, w.T)
0.88 logistic.py(293):     p += intercept
0.88 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
0.88 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
0.88 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
0.88 logistic.py(297):     p = np.exp(p, p)
0.88 logistic.py(298):     return loss, p, w
0.88 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
0.88 logistic.py(346):     diff = sample_weight * (p - Y)
0.88 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
0.88 logistic.py(348):     grad[:, :n_features] += alpha * w
0.88 logistic.py(349):     if fit_intercept:
0.88 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
0.88 logistic.py(351):     return loss, grad.ravel(), p
0.88 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
0.88 logistic.py(339):     n_classes = Y.shape[1]
0.88 logistic.py(340):     n_features = X.shape[1]
0.88 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
0.88 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
0.88 logistic.py(343):                     dtype=X.dtype)
0.88 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
0.88 logistic.py(282):     n_classes = Y.shape[1]
0.88 logistic.py(283):     n_features = X.shape[1]
0.88 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
0.88 logistic.py(285):     w = w.reshape(n_classes, -1)
0.88 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
0.88 logistic.py(287):     if fit_intercept:
0.88 logistic.py(288):         intercept = w[:, -1]
0.88 logistic.py(289):         w = w[:, :-1]
0.88 logistic.py(292):     p = safe_sparse_dot(X, w.T)
0.88 logistic.py(293):     p += intercept
0.88 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
0.88 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
0.88 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
0.88 logistic.py(297):     p = np.exp(p, p)
0.88 logistic.py(298):     return loss, p, w
0.88 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
0.88 logistic.py(346):     diff = sample_weight * (p - Y)
0.88 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
0.88 logistic.py(348):     grad[:, :n_features] += alpha * w
0.88 logistic.py(349):     if fit_intercept:
0.88 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
0.88 logistic.py(351):     return loss, grad.ravel(), p
0.88 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
0.88 logistic.py(339):     n_classes = Y.shape[1]
0.88 logistic.py(340):     n_features = X.shape[1]
0.88 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
0.88 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
0.88 logistic.py(343):                     dtype=X.dtype)
0.88 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
0.88 logistic.py(282):     n_classes = Y.shape[1]
0.88 logistic.py(283):     n_features = X.shape[1]
0.88 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
0.88 logistic.py(285):     w = w.reshape(n_classes, -1)
0.88 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
0.88 logistic.py(287):     if fit_intercept:
0.88 logistic.py(288):         intercept = w[:, -1]
0.88 logistic.py(289):         w = w[:, :-1]
0.88 logistic.py(292):     p = safe_sparse_dot(X, w.T)
0.88 logistic.py(293):     p += intercept
0.88 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
0.88 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
0.88 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
0.88 logistic.py(297):     p = np.exp(p, p)
0.88 logistic.py(298):     return loss, p, w
0.88 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
0.88 logistic.py(346):     diff = sample_weight * (p - Y)
0.88 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
0.88 logistic.py(348):     grad[:, :n_features] += alpha * w
0.88 logistic.py(349):     if fit_intercept:
0.88 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
0.88 logistic.py(351):     return loss, grad.ravel(), p
0.88 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
0.88 logistic.py(339):     n_classes = Y.shape[1]
0.88 logistic.py(340):     n_features = X.shape[1]
0.88 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
0.88 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
0.88 logistic.py(343):                     dtype=X.dtype)
0.88 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
0.88 logistic.py(282):     n_classes = Y.shape[1]
0.88 logistic.py(283):     n_features = X.shape[1]
0.88 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
0.88 logistic.py(285):     w = w.reshape(n_classes, -1)
0.88 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
0.88 logistic.py(287):     if fit_intercept:
0.88 logistic.py(288):         intercept = w[:, -1]
0.88 logistic.py(289):         w = w[:, :-1]
0.88 logistic.py(292):     p = safe_sparse_dot(X, w.T)
0.88 logistic.py(293):     p += intercept
0.88 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
0.88 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
0.88 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
0.88 logistic.py(297):     p = np.exp(p, p)
0.88 logistic.py(298):     return loss, p, w
0.88 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
0.88 logistic.py(346):     diff = sample_weight * (p - Y)
0.88 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
0.88 logistic.py(348):     grad[:, :n_features] += alpha * w
0.88 logistic.py(349):     if fit_intercept:
0.88 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
0.88 logistic.py(351):     return loss, grad.ravel(), p
0.88 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
0.88 logistic.py(339):     n_classes = Y.shape[1]
0.88 logistic.py(340):     n_features = X.shape[1]
0.88 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
0.88 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
0.88 logistic.py(343):                     dtype=X.dtype)
0.88 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
0.88 logistic.py(282):     n_classes = Y.shape[1]
0.88 logistic.py(283):     n_features = X.shape[1]
0.88 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
0.88 logistic.py(285):     w = w.reshape(n_classes, -1)
0.88 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
0.88 logistic.py(287):     if fit_intercept:
0.88 logistic.py(288):         intercept = w[:, -1]
0.88 logistic.py(289):         w = w[:, :-1]
0.89 logistic.py(292):     p = safe_sparse_dot(X, w.T)
0.89 logistic.py(293):     p += intercept
0.89 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
0.89 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
0.89 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
0.89 logistic.py(297):     p = np.exp(p, p)
0.89 logistic.py(298):     return loss, p, w
0.89 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
0.89 logistic.py(346):     diff = sample_weight * (p - Y)
0.89 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
0.89 logistic.py(348):     grad[:, :n_features] += alpha * w
0.89 logistic.py(349):     if fit_intercept:
0.89 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
0.89 logistic.py(351):     return loss, grad.ravel(), p
0.89 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
0.89 logistic.py(339):     n_classes = Y.shape[1]
0.89 logistic.py(340):     n_features = X.shape[1]
0.89 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
0.89 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
0.89 logistic.py(343):                     dtype=X.dtype)
0.89 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
0.89 logistic.py(282):     n_classes = Y.shape[1]
0.89 logistic.py(283):     n_features = X.shape[1]
0.89 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
0.89 logistic.py(285):     w = w.reshape(n_classes, -1)
0.89 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
0.89 logistic.py(287):     if fit_intercept:
0.89 logistic.py(288):         intercept = w[:, -1]
0.89 logistic.py(289):         w = w[:, :-1]
0.89 logistic.py(292):     p = safe_sparse_dot(X, w.T)
0.89 logistic.py(293):     p += intercept
0.89 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
0.89 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
0.89 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
0.89 logistic.py(297):     p = np.exp(p, p)
0.89 logistic.py(298):     return loss, p, w
0.89 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
0.89 logistic.py(346):     diff = sample_weight * (p - Y)
0.89 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
0.89 logistic.py(348):     grad[:, :n_features] += alpha * w
0.89 logistic.py(349):     if fit_intercept:
0.89 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
0.89 logistic.py(351):     return loss, grad.ravel(), p
0.89 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
0.89 logistic.py(339):     n_classes = Y.shape[1]
0.89 logistic.py(340):     n_features = X.shape[1]
0.89 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
0.89 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
0.89 logistic.py(343):                     dtype=X.dtype)
0.89 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
0.89 logistic.py(282):     n_classes = Y.shape[1]
0.89 logistic.py(283):     n_features = X.shape[1]
0.89 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
0.89 logistic.py(285):     w = w.reshape(n_classes, -1)
0.89 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
0.89 logistic.py(287):     if fit_intercept:
0.89 logistic.py(288):         intercept = w[:, -1]
0.89 logistic.py(289):         w = w[:, :-1]
0.89 logistic.py(292):     p = safe_sparse_dot(X, w.T)
0.89 logistic.py(293):     p += intercept
0.89 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
0.89 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
0.89 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
0.89 logistic.py(297):     p = np.exp(p, p)
0.89 logistic.py(298):     return loss, p, w
0.89 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
0.89 logistic.py(346):     diff = sample_weight * (p - Y)
0.89 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
0.89 logistic.py(348):     grad[:, :n_features] += alpha * w
0.89 logistic.py(349):     if fit_intercept:
0.89 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
0.89 logistic.py(351):     return loss, grad.ravel(), p
0.89 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
0.89 logistic.py(339):     n_classes = Y.shape[1]
0.89 logistic.py(340):     n_features = X.shape[1]
0.89 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
0.89 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
0.89 logistic.py(343):                     dtype=X.dtype)
0.89 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
0.89 logistic.py(282):     n_classes = Y.shape[1]
0.89 logistic.py(283):     n_features = X.shape[1]
0.89 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
0.89 logistic.py(285):     w = w.reshape(n_classes, -1)
0.89 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
0.89 logistic.py(287):     if fit_intercept:
0.89 logistic.py(288):         intercept = w[:, -1]
0.89 logistic.py(289):         w = w[:, :-1]
0.89 logistic.py(292):     p = safe_sparse_dot(X, w.T)
0.89 logistic.py(293):     p += intercept
0.89 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
0.89 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
0.89 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
0.89 logistic.py(297):     p = np.exp(p, p)
0.89 logistic.py(298):     return loss, p, w
0.89 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
0.89 logistic.py(346):     diff = sample_weight * (p - Y)
0.89 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
0.89 logistic.py(348):     grad[:, :n_features] += alpha * w
0.89 logistic.py(349):     if fit_intercept:
0.89 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
0.89 logistic.py(351):     return loss, grad.ravel(), p
0.89 logistic.py(718):             if info["warnflag"] == 1:
0.89 logistic.py(723):             n_iter_i = min(info['nit'], max_iter)
0.89 logistic.py(760):         if multi_class == 'multinomial':
0.89 logistic.py(761):             multi_w0 = np.reshape(w0, (classes.size, -1))
0.89 logistic.py(762):             if classes.size == 2:
0.89 logistic.py(764):             coefs.append(multi_w0)
0.89 logistic.py(768):         n_iter[i] = n_iter_i
0.89 logistic.py(712):     for i, C in enumerate(Cs):
0.89 logistic.py(713):         if solver == 'lbfgs':
0.89 logistic.py(714):             w0, loss, info = optimize.fmin_l_bfgs_b(
0.89 logistic.py(715):                 func, w0, fprime=None,
0.89 logistic.py(716):                 args=(X, target, 1. / C, sample_weight),
0.89 logistic.py(717):                 iprint=(verbose > 0) - 1, pgtol=tol, maxiter=max_iter)
0.89 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
0.89 logistic.py(339):     n_classes = Y.shape[1]
0.89 logistic.py(340):     n_features = X.shape[1]
0.89 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
0.89 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
0.89 logistic.py(343):                     dtype=X.dtype)
0.89 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
0.89 logistic.py(282):     n_classes = Y.shape[1]
0.89 logistic.py(283):     n_features = X.shape[1]
0.89 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
0.89 logistic.py(285):     w = w.reshape(n_classes, -1)
0.89 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
0.89 logistic.py(287):     if fit_intercept:
0.89 logistic.py(288):         intercept = w[:, -1]
0.89 logistic.py(289):         w = w[:, :-1]
0.89 logistic.py(292):     p = safe_sparse_dot(X, w.T)
0.89 logistic.py(293):     p += intercept
0.89 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
0.89 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
0.89 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
0.89 logistic.py(297):     p = np.exp(p, p)
0.89 logistic.py(298):     return loss, p, w
0.89 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
0.89 logistic.py(346):     diff = sample_weight * (p - Y)
0.89 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
0.89 logistic.py(348):     grad[:, :n_features] += alpha * w
0.89 logistic.py(349):     if fit_intercept:
0.89 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
0.89 logistic.py(351):     return loss, grad.ravel(), p
0.89 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
0.89 logistic.py(339):     n_classes = Y.shape[1]
0.89 logistic.py(340):     n_features = X.shape[1]
0.89 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
0.89 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
0.89 logistic.py(343):                     dtype=X.dtype)
0.89 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
0.89 logistic.py(282):     n_classes = Y.shape[1]
0.89 logistic.py(283):     n_features = X.shape[1]
0.89 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
0.89 logistic.py(285):     w = w.reshape(n_classes, -1)
0.89 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
0.89 logistic.py(287):     if fit_intercept:
0.89 logistic.py(288):         intercept = w[:, -1]
0.89 logistic.py(289):         w = w[:, :-1]
0.89 logistic.py(292):     p = safe_sparse_dot(X, w.T)
0.89 logistic.py(293):     p += intercept
0.89 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
0.89 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
0.89 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
0.89 logistic.py(297):     p = np.exp(p, p)
0.89 logistic.py(298):     return loss, p, w
0.89 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
0.89 logistic.py(346):     diff = sample_weight * (p - Y)
0.89 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
0.89 logistic.py(348):     grad[:, :n_features] += alpha * w
0.89 logistic.py(349):     if fit_intercept:
0.89 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
0.89 logistic.py(351):     return loss, grad.ravel(), p
0.89 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
0.89 logistic.py(339):     n_classes = Y.shape[1]
0.89 logistic.py(340):     n_features = X.shape[1]
0.89 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
0.89 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
0.89 logistic.py(343):                     dtype=X.dtype)
0.89 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
0.89 logistic.py(282):     n_classes = Y.shape[1]
0.89 logistic.py(283):     n_features = X.shape[1]
0.89 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
0.89 logistic.py(285):     w = w.reshape(n_classes, -1)
0.89 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
0.89 logistic.py(287):     if fit_intercept:
0.89 logistic.py(288):         intercept = w[:, -1]
0.89 logistic.py(289):         w = w[:, :-1]
0.89 logistic.py(292):     p = safe_sparse_dot(X, w.T)
0.89 logistic.py(293):     p += intercept
0.89 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
0.89 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
0.89 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
0.89 logistic.py(297):     p = np.exp(p, p)
0.89 logistic.py(298):     return loss, p, w
0.89 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
0.89 logistic.py(346):     diff = sample_weight * (p - Y)
0.89 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
0.89 logistic.py(348):     grad[:, :n_features] += alpha * w
0.89 logistic.py(349):     if fit_intercept:
0.89 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
0.89 logistic.py(351):     return loss, grad.ravel(), p
0.89 logistic.py(718):             if info["warnflag"] == 1:
0.89 logistic.py(723):             n_iter_i = min(info['nit'], max_iter)
0.89 logistic.py(760):         if multi_class == 'multinomial':
0.89 logistic.py(761):             multi_w0 = np.reshape(w0, (classes.size, -1))
0.89 logistic.py(762):             if classes.size == 2:
0.89 logistic.py(764):             coefs.append(multi_w0)
0.89 logistic.py(768):         n_iter[i] = n_iter_i
0.89 logistic.py(712):     for i, C in enumerate(Cs):
0.89 logistic.py(770):     return coefs, np.array(Cs), n_iter
0.89 logistic.py(925):     log_reg = LogisticRegression(multi_class=multi_class)
0.89 logistic.py(1171):         self.penalty = penalty
0.89 logistic.py(1172):         self.dual = dual
0.89 logistic.py(1173):         self.tol = tol
0.89 logistic.py(1174):         self.C = C
0.89 logistic.py(1175):         self.fit_intercept = fit_intercept
0.89 logistic.py(1176):         self.intercept_scaling = intercept_scaling
0.89 logistic.py(1177):         self.class_weight = class_weight
0.89 logistic.py(1178):         self.random_state = random_state
0.89 logistic.py(1179):         self.solver = solver
0.89 logistic.py(1180):         self.max_iter = max_iter
0.89 logistic.py(1181):         self.multi_class = multi_class
0.89 logistic.py(1182):         self.verbose = verbose
0.89 logistic.py(1183):         self.warm_start = warm_start
0.89 logistic.py(1184):         self.n_jobs = n_jobs
0.89 logistic.py(928):     if multi_class == 'ovr':
0.89 logistic.py(930):     elif multi_class == 'multinomial':
0.89 logistic.py(931):         log_reg.classes_ = np.unique(y_train)
0.89 logistic.py(936):     if pos_class is not None:
0.89 logistic.py(941):     scores = list()
0.89 logistic.py(943):     if isinstance(scoring, six.string_types):
0.89 logistic.py(945):     for w in coefs:
0.89 logistic.py(946):         if multi_class == 'ovr':
0.89 logistic.py(948):         if fit_intercept:
0.89 logistic.py(949):             log_reg.coef_ = w[:, :-1]
0.89 logistic.py(950):             log_reg.intercept_ = w[:, -1]
0.89 logistic.py(955):         if scoring is None:
0.89 logistic.py(956):             scores.append(log_reg.score(X_test, y_test))
0.89 logistic.py(945):     for w in coefs:
0.89 logistic.py(946):         if multi_class == 'ovr':
0.89 logistic.py(948):         if fit_intercept:
0.89 logistic.py(949):             log_reg.coef_ = w[:, :-1]
0.89 logistic.py(950):             log_reg.intercept_ = w[:, -1]
0.89 logistic.py(955):         if scoring is None:
0.89 logistic.py(956):             scores.append(log_reg.score(X_test, y_test))
0.89 logistic.py(945):     for w in coefs:
0.89 logistic.py(946):         if multi_class == 'ovr':
0.89 logistic.py(948):         if fit_intercept:
0.89 logistic.py(949):             log_reg.coef_ = w[:, :-1]
0.89 logistic.py(950):             log_reg.intercept_ = w[:, -1]
0.89 logistic.py(955):         if scoring is None:
0.89 logistic.py(956):             scores.append(log_reg.score(X_test, y_test))
0.89 logistic.py(945):     for w in coefs:
0.89 logistic.py(946):         if multi_class == 'ovr':
0.89 logistic.py(948):         if fit_intercept:
0.89 logistic.py(949):             log_reg.coef_ = w[:, :-1]
0.89 logistic.py(950):             log_reg.intercept_ = w[:, -1]
0.89 logistic.py(955):         if scoring is None:
0.89 logistic.py(956):             scores.append(log_reg.score(X_test, y_test))
0.89 logistic.py(945):     for w in coefs:
0.89 logistic.py(946):         if multi_class == 'ovr':
0.89 logistic.py(948):         if fit_intercept:
0.89 logistic.py(949):             log_reg.coef_ = w[:, :-1]
0.89 logistic.py(950):             log_reg.intercept_ = w[:, -1]
0.89 logistic.py(955):         if scoring is None:
0.89 logistic.py(956):             scores.append(log_reg.score(X_test, y_test))
0.89 logistic.py(945):     for w in coefs:
0.89 logistic.py(946):         if multi_class == 'ovr':
0.89 logistic.py(948):         if fit_intercept:
0.89 logistic.py(949):             log_reg.coef_ = w[:, :-1]
0.89 logistic.py(950):             log_reg.intercept_ = w[:, -1]
0.89 logistic.py(955):         if scoring is None:
0.89 logistic.py(956):             scores.append(log_reg.score(X_test, y_test))
0.89 logistic.py(945):     for w in coefs:
0.89 logistic.py(946):         if multi_class == 'ovr':
0.89 logistic.py(948):         if fit_intercept:
0.89 logistic.py(949):             log_reg.coef_ = w[:, :-1]
0.89 logistic.py(950):             log_reg.intercept_ = w[:, -1]
0.89 logistic.py(955):         if scoring is None:
0.89 logistic.py(956):             scores.append(log_reg.score(X_test, y_test))
0.89 logistic.py(945):     for w in coefs:
0.89 logistic.py(946):         if multi_class == 'ovr':
0.89 logistic.py(948):         if fit_intercept:
0.89 logistic.py(949):             log_reg.coef_ = w[:, :-1]
0.89 logistic.py(950):             log_reg.intercept_ = w[:, -1]
0.89 logistic.py(955):         if scoring is None:
0.89 logistic.py(956):             scores.append(log_reg.score(X_test, y_test))
0.89 logistic.py(945):     for w in coefs:
0.89 logistic.py(946):         if multi_class == 'ovr':
0.89 logistic.py(948):         if fit_intercept:
0.89 logistic.py(949):             log_reg.coef_ = w[:, :-1]
0.89 logistic.py(950):             log_reg.intercept_ = w[:, -1]
0.89 logistic.py(955):         if scoring is None:
0.89 logistic.py(956):             scores.append(log_reg.score(X_test, y_test))
0.89 logistic.py(945):     for w in coefs:
0.89 logistic.py(946):         if multi_class == 'ovr':
0.89 logistic.py(948):         if fit_intercept:
0.89 logistic.py(949):             log_reg.coef_ = w[:, :-1]
0.89 logistic.py(950):             log_reg.intercept_ = w[:, -1]
0.89 logistic.py(955):         if scoring is None:
0.89 logistic.py(956):             scores.append(log_reg.score(X_test, y_test))
0.89 logistic.py(945):     for w in coefs:
0.89 logistic.py(959):     return coefs, Cs, np.array(scores), n_iter
0.89 logistic.py(1703):             for train, test in folds)
0.89 logistic.py(903):     _check_solver_option(solver, multi_class, penalty, dual)
0.89 logistic.py(428):     if solver not in ['liblinear', 'newton-cg', 'lbfgs', 'sag', 'saga']:
0.89 logistic.py(433):     if multi_class not in ['multinomial', 'ovr']:
0.89 logistic.py(437):     if multi_class == 'multinomial' and solver == 'liblinear':
0.89 logistic.py(441):     if solver not in ['liblinear', 'saga']:
0.89 logistic.py(442):         if penalty != 'l2':
0.89 logistic.py(445):     if solver != 'liblinear':
0.89 logistic.py(446):         if dual:
0.89 logistic.py(905):     X_train = X[train]
0.89 logistic.py(906):     X_test = X[test]
0.89 logistic.py(907):     y_train = y[train]
0.89 logistic.py(908):     y_test = y[test]
0.89 logistic.py(910):     if sample_weight is not None:
0.89 logistic.py(916):     coefs, Cs, n_iter = logistic_regression_path(
0.89 logistic.py(917):         X_train, y_train, Cs=Cs, fit_intercept=fit_intercept,
0.89 logistic.py(918):         solver=solver, max_iter=max_iter, class_weight=class_weight,
0.89 logistic.py(919):         pos_class=pos_class, multi_class=multi_class,
0.89 logistic.py(920):         tol=tol, verbose=verbose, dual=dual, penalty=penalty,
0.89 logistic.py(921):         intercept_scaling=intercept_scaling, random_state=random_state,
0.89 logistic.py(922):         check_input=False, max_squared_sum=max_squared_sum,
0.89 logistic.py(923):         sample_weight=sample_weight)
0.89 logistic.py(591):     if isinstance(Cs, numbers.Integral):
0.89 logistic.py(592):         Cs = np.logspace(-4, 4, Cs)
0.89 logistic.py(594):     _check_solver_option(solver, multi_class, penalty, dual)
0.89 logistic.py(428):     if solver not in ['liblinear', 'newton-cg', 'lbfgs', 'sag', 'saga']:
0.89 logistic.py(433):     if multi_class not in ['multinomial', 'ovr']:
0.89 logistic.py(437):     if multi_class == 'multinomial' and solver == 'liblinear':
0.89 logistic.py(441):     if solver not in ['liblinear', 'saga']:
0.89 logistic.py(442):         if penalty != 'l2':
0.89 logistic.py(445):     if solver != 'liblinear':
0.89 logistic.py(446):         if dual:
0.89 logistic.py(597):     if check_input:
0.89 logistic.py(602):     _, n_features = X.shape
0.89 logistic.py(603):     classes = np.unique(y)
0.89 logistic.py(604):     random_state = check_random_state(random_state)
0.89 logistic.py(606):     if pos_class is None and multi_class != 'multinomial':
0.89 logistic.py(615):     if sample_weight is not None:
0.89 logistic.py(619):         sample_weight = np.ones(X.shape[0], dtype=X.dtype)
0.89 logistic.py(624):     le = LabelEncoder()
0.89 logistic.py(625):     if isinstance(class_weight, dict) or multi_class == 'multinomial':
0.89 logistic.py(626):         class_weight_ = compute_class_weight(class_weight, classes, y)
0.89 logistic.py(627):         sample_weight *= class_weight_[le.fit_transform(y)]
0.89 logistic.py(631):     if multi_class == 'ovr':
0.89 logistic.py(645):         if solver not in ['sag', 'saga']:
0.89 logistic.py(646):             lbin = LabelBinarizer()
0.89 logistic.py(647):             Y_multi = lbin.fit_transform(y)
0.89 logistic.py(648):             if Y_multi.shape[1] == 1:
0.89 logistic.py(655):         w0 = np.zeros((classes.size, n_features + int(fit_intercept)),
0.89 logistic.py(656):                       order='F', dtype=X.dtype)
0.89 logistic.py(658):     if coef is not None:
0.89 logistic.py(688):     if multi_class == 'multinomial':
0.89 logistic.py(690):         if solver in ['lbfgs', 'newton-cg']:
0.89 logistic.py(691):             w0 = w0.ravel()
0.89 logistic.py(692):         target = Y_multi
0.89 logistic.py(693):         if solver == 'lbfgs':
0.89 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
0.89 logistic.py(699):         warm_start_sag = {'coef': w0.T}
0.89 logistic.py(710):     coefs = list()
0.89 logistic.py(711):     n_iter = np.zeros(len(Cs), dtype=np.int32)
0.89 logistic.py(712):     for i, C in enumerate(Cs):
0.89 logistic.py(713):         if solver == 'lbfgs':
0.89 logistic.py(714):             w0, loss, info = optimize.fmin_l_bfgs_b(
0.89 logistic.py(715):                 func, w0, fprime=None,
0.89 logistic.py(716):                 args=(X, target, 1. / C, sample_weight),
0.89 logistic.py(717):                 iprint=(verbose > 0) - 1, pgtol=tol, maxiter=max_iter)
0.89 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
0.89 logistic.py(339):     n_classes = Y.shape[1]
0.89 logistic.py(340):     n_features = X.shape[1]
0.89 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
0.89 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
0.89 logistic.py(343):                     dtype=X.dtype)
0.89 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
0.89 logistic.py(282):     n_classes = Y.shape[1]
0.89 logistic.py(283):     n_features = X.shape[1]
0.89 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
0.89 logistic.py(285):     w = w.reshape(n_classes, -1)
0.89 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
0.89 logistic.py(287):     if fit_intercept:
0.89 logistic.py(288):         intercept = w[:, -1]
0.89 logistic.py(289):         w = w[:, :-1]
0.89 logistic.py(292):     p = safe_sparse_dot(X, w.T)
0.89 logistic.py(293):     p += intercept
0.89 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
0.89 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
0.89 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
0.89 logistic.py(297):     p = np.exp(p, p)
0.89 logistic.py(298):     return loss, p, w
0.89 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
0.89 logistic.py(346):     diff = sample_weight * (p - Y)
0.90 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
0.90 logistic.py(348):     grad[:, :n_features] += alpha * w
0.90 logistic.py(349):     if fit_intercept:
0.90 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
0.90 logistic.py(351):     return loss, grad.ravel(), p
0.90 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
0.90 logistic.py(339):     n_classes = Y.shape[1]
0.90 logistic.py(340):     n_features = X.shape[1]
0.90 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
0.90 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
0.90 logistic.py(343):                     dtype=X.dtype)
0.90 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
0.90 logistic.py(282):     n_classes = Y.shape[1]
0.90 logistic.py(283):     n_features = X.shape[1]
0.90 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
0.90 logistic.py(285):     w = w.reshape(n_classes, -1)
0.90 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
0.90 logistic.py(287):     if fit_intercept:
0.90 logistic.py(288):         intercept = w[:, -1]
0.90 logistic.py(289):         w = w[:, :-1]
0.90 logistic.py(292):     p = safe_sparse_dot(X, w.T)
0.90 logistic.py(293):     p += intercept
0.90 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
0.90 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
0.90 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
0.90 logistic.py(297):     p = np.exp(p, p)
0.90 logistic.py(298):     return loss, p, w
0.90 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
0.90 logistic.py(346):     diff = sample_weight * (p - Y)
0.90 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
0.90 logistic.py(348):     grad[:, :n_features] += alpha * w
0.90 logistic.py(349):     if fit_intercept:
0.90 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
0.90 logistic.py(351):     return loss, grad.ravel(), p
0.90 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
0.90 logistic.py(339):     n_classes = Y.shape[1]
0.90 logistic.py(340):     n_features = X.shape[1]
0.90 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
0.90 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
0.90 logistic.py(343):                     dtype=X.dtype)
0.90 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
0.90 logistic.py(282):     n_classes = Y.shape[1]
0.90 logistic.py(283):     n_features = X.shape[1]
0.90 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
0.90 logistic.py(285):     w = w.reshape(n_classes, -1)
0.90 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
0.90 logistic.py(287):     if fit_intercept:
0.90 logistic.py(288):         intercept = w[:, -1]
0.90 logistic.py(289):         w = w[:, :-1]
0.90 logistic.py(292):     p = safe_sparse_dot(X, w.T)
0.90 logistic.py(293):     p += intercept
0.90 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
0.90 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
0.90 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
0.90 logistic.py(297):     p = np.exp(p, p)
0.90 logistic.py(298):     return loss, p, w
0.90 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
0.90 logistic.py(346):     diff = sample_weight * (p - Y)
0.90 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
0.90 logistic.py(348):     grad[:, :n_features] += alpha * w
0.90 logistic.py(349):     if fit_intercept:
0.90 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
0.90 logistic.py(351):     return loss, grad.ravel(), p
0.90 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
0.90 logistic.py(339):     n_classes = Y.shape[1]
0.90 logistic.py(340):     n_features = X.shape[1]
0.90 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
0.90 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
0.90 logistic.py(343):                     dtype=X.dtype)
0.90 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
0.90 logistic.py(282):     n_classes = Y.shape[1]
0.90 logistic.py(283):     n_features = X.shape[1]
0.90 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
0.90 logistic.py(285):     w = w.reshape(n_classes, -1)
0.90 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
0.90 logistic.py(287):     if fit_intercept:
0.90 logistic.py(288):         intercept = w[:, -1]
0.90 logistic.py(289):         w = w[:, :-1]
0.90 logistic.py(292):     p = safe_sparse_dot(X, w.T)
0.90 logistic.py(293):     p += intercept
0.90 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
0.90 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
0.90 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
0.90 logistic.py(297):     p = np.exp(p, p)
0.90 logistic.py(298):     return loss, p, w
0.90 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
0.90 logistic.py(346):     diff = sample_weight * (p - Y)
0.90 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
0.90 logistic.py(348):     grad[:, :n_features] += alpha * w
0.90 logistic.py(349):     if fit_intercept:
0.90 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
0.90 logistic.py(351):     return loss, grad.ravel(), p
0.90 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
0.90 logistic.py(339):     n_classes = Y.shape[1]
0.90 logistic.py(340):     n_features = X.shape[1]
0.90 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
0.90 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
0.90 logistic.py(343):                     dtype=X.dtype)
0.90 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
0.90 logistic.py(282):     n_classes = Y.shape[1]
0.90 logistic.py(283):     n_features = X.shape[1]
0.90 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
0.90 logistic.py(285):     w = w.reshape(n_classes, -1)
0.90 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
0.90 logistic.py(287):     if fit_intercept:
0.90 logistic.py(288):         intercept = w[:, -1]
0.90 logistic.py(289):         w = w[:, :-1]
0.90 logistic.py(292):     p = safe_sparse_dot(X, w.T)
0.90 logistic.py(293):     p += intercept
0.90 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
0.90 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
0.90 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
0.90 logistic.py(297):     p = np.exp(p, p)
0.90 logistic.py(298):     return loss, p, w
0.90 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
0.90 logistic.py(346):     diff = sample_weight * (p - Y)
0.90 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
0.90 logistic.py(348):     grad[:, :n_features] += alpha * w
0.90 logistic.py(349):     if fit_intercept:
0.90 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
0.90 logistic.py(351):     return loss, grad.ravel(), p
0.90 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
0.90 logistic.py(339):     n_classes = Y.shape[1]
0.90 logistic.py(340):     n_features = X.shape[1]
0.90 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
0.90 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
0.90 logistic.py(343):                     dtype=X.dtype)
0.90 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
0.90 logistic.py(282):     n_classes = Y.shape[1]
0.90 logistic.py(283):     n_features = X.shape[1]
0.90 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
0.90 logistic.py(285):     w = w.reshape(n_classes, -1)
0.90 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
0.90 logistic.py(287):     if fit_intercept:
0.90 logistic.py(288):         intercept = w[:, -1]
0.90 logistic.py(289):         w = w[:, :-1]
0.90 logistic.py(292):     p = safe_sparse_dot(X, w.T)
0.90 logistic.py(293):     p += intercept
0.90 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
0.90 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
0.90 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
0.90 logistic.py(297):     p = np.exp(p, p)
0.90 logistic.py(298):     return loss, p, w
0.90 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
0.90 logistic.py(346):     diff = sample_weight * (p - Y)
0.90 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
0.90 logistic.py(348):     grad[:, :n_features] += alpha * w
0.90 logistic.py(349):     if fit_intercept:
0.90 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
0.90 logistic.py(351):     return loss, grad.ravel(), p
0.90 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
0.90 logistic.py(339):     n_classes = Y.shape[1]
0.90 logistic.py(340):     n_features = X.shape[1]
0.90 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
0.90 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
0.90 logistic.py(343):                     dtype=X.dtype)
0.90 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
0.90 logistic.py(282):     n_classes = Y.shape[1]
0.90 logistic.py(283):     n_features = X.shape[1]
0.90 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
0.90 logistic.py(285):     w = w.reshape(n_classes, -1)
0.90 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
0.90 logistic.py(287):     if fit_intercept:
0.90 logistic.py(288):         intercept = w[:, -1]
0.90 logistic.py(289):         w = w[:, :-1]
0.90 logistic.py(292):     p = safe_sparse_dot(X, w.T)
0.90 logistic.py(293):     p += intercept
0.90 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
0.90 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
0.90 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
0.90 logistic.py(297):     p = np.exp(p, p)
0.90 logistic.py(298):     return loss, p, w
0.90 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
0.90 logistic.py(346):     diff = sample_weight * (p - Y)
0.90 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
0.90 logistic.py(348):     grad[:, :n_features] += alpha * w
0.90 logistic.py(349):     if fit_intercept:
0.90 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
0.90 logistic.py(351):     return loss, grad.ravel(), p
0.90 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
0.90 logistic.py(339):     n_classes = Y.shape[1]
0.90 logistic.py(340):     n_features = X.shape[1]
0.90 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
0.90 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
0.90 logistic.py(343):                     dtype=X.dtype)
0.90 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
0.90 logistic.py(282):     n_classes = Y.shape[1]
0.90 logistic.py(283):     n_features = X.shape[1]
0.90 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
0.90 logistic.py(285):     w = w.reshape(n_classes, -1)
0.90 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
0.90 logistic.py(287):     if fit_intercept:
0.90 logistic.py(288):         intercept = w[:, -1]
0.90 logistic.py(289):         w = w[:, :-1]
0.90 logistic.py(292):     p = safe_sparse_dot(X, w.T)
0.90 logistic.py(293):     p += intercept
0.90 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
0.90 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
0.90 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
0.90 logistic.py(297):     p = np.exp(p, p)
0.90 logistic.py(298):     return loss, p, w
0.90 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
0.90 logistic.py(346):     diff = sample_weight * (p - Y)
0.90 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
0.90 logistic.py(348):     grad[:, :n_features] += alpha * w
0.90 logistic.py(349):     if fit_intercept:
0.90 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
0.90 logistic.py(351):     return loss, grad.ravel(), p
0.90 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
0.90 logistic.py(339):     n_classes = Y.shape[1]
0.90 logistic.py(340):     n_features = X.shape[1]
0.90 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
0.90 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
0.90 logistic.py(343):                     dtype=X.dtype)
0.90 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
0.90 logistic.py(282):     n_classes = Y.shape[1]
0.90 logistic.py(283):     n_features = X.shape[1]
0.90 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
0.90 logistic.py(285):     w = w.reshape(n_classes, -1)
0.90 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
0.90 logistic.py(287):     if fit_intercept:
0.90 logistic.py(288):         intercept = w[:, -1]
0.90 logistic.py(289):         w = w[:, :-1]
0.90 logistic.py(292):     p = safe_sparse_dot(X, w.T)
0.90 logistic.py(293):     p += intercept
0.90 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
0.90 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
0.90 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
0.90 logistic.py(297):     p = np.exp(p, p)
0.90 logistic.py(298):     return loss, p, w
0.90 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
0.90 logistic.py(346):     diff = sample_weight * (p - Y)
0.90 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
0.90 logistic.py(348):     grad[:, :n_features] += alpha * w
0.90 logistic.py(349):     if fit_intercept:
0.90 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
0.90 logistic.py(351):     return loss, grad.ravel(), p
0.90 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
0.90 logistic.py(339):     n_classes = Y.shape[1]
0.90 logistic.py(340):     n_features = X.shape[1]
0.90 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
0.90 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
0.90 logistic.py(343):                     dtype=X.dtype)
0.90 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
0.90 logistic.py(282):     n_classes = Y.shape[1]
0.90 logistic.py(283):     n_features = X.shape[1]
0.90 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
0.90 logistic.py(285):     w = w.reshape(n_classes, -1)
0.90 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
0.90 logistic.py(287):     if fit_intercept:
0.90 logistic.py(288):         intercept = w[:, -1]
0.90 logistic.py(289):         w = w[:, :-1]
0.90 logistic.py(292):     p = safe_sparse_dot(X, w.T)
0.90 logistic.py(293):     p += intercept
0.90 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
0.90 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
0.90 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
0.90 logistic.py(297):     p = np.exp(p, p)
0.90 logistic.py(298):     return loss, p, w
0.90 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
0.90 logistic.py(346):     diff = sample_weight * (p - Y)
0.90 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
0.90 logistic.py(348):     grad[:, :n_features] += alpha * w
0.90 logistic.py(349):     if fit_intercept:
0.90 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
0.90 logistic.py(351):     return loss, grad.ravel(), p
0.90 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
0.90 logistic.py(339):     n_classes = Y.shape[1]
0.90 logistic.py(340):     n_features = X.shape[1]
0.90 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
0.90 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
0.90 logistic.py(343):                     dtype=X.dtype)
0.90 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
0.90 logistic.py(282):     n_classes = Y.shape[1]
0.90 logistic.py(283):     n_features = X.shape[1]
0.90 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
0.90 logistic.py(285):     w = w.reshape(n_classes, -1)
0.90 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
0.90 logistic.py(287):     if fit_intercept:
0.90 logistic.py(288):         intercept = w[:, -1]
0.90 logistic.py(289):         w = w[:, :-1]
0.90 logistic.py(292):     p = safe_sparse_dot(X, w.T)
0.90 logistic.py(293):     p += intercept
0.90 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
0.90 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
0.90 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
0.90 logistic.py(297):     p = np.exp(p, p)
0.90 logistic.py(298):     return loss, p, w
0.90 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
0.90 logistic.py(346):     diff = sample_weight * (p - Y)
0.90 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
0.90 logistic.py(348):     grad[:, :n_features] += alpha * w
0.90 logistic.py(349):     if fit_intercept:
0.90 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
0.90 logistic.py(351):     return loss, grad.ravel(), p
0.90 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
0.90 logistic.py(339):     n_classes = Y.shape[1]
0.90 logistic.py(340):     n_features = X.shape[1]
0.90 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
0.90 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
0.90 logistic.py(343):                     dtype=X.dtype)
0.90 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
0.90 logistic.py(282):     n_classes = Y.shape[1]
0.90 logistic.py(283):     n_features = X.shape[1]
0.90 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
0.90 logistic.py(285):     w = w.reshape(n_classes, -1)
0.90 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
0.90 logistic.py(287):     if fit_intercept:
0.90 logistic.py(288):         intercept = w[:, -1]
0.90 logistic.py(289):         w = w[:, :-1]
0.90 logistic.py(292):     p = safe_sparse_dot(X, w.T)
0.90 logistic.py(293):     p += intercept
0.90 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
0.90 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
0.90 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
0.90 logistic.py(297):     p = np.exp(p, p)
0.90 logistic.py(298):     return loss, p, w
0.90 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
0.90 logistic.py(346):     diff = sample_weight * (p - Y)
0.90 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
0.90 logistic.py(348):     grad[:, :n_features] += alpha * w
0.90 logistic.py(349):     if fit_intercept:
0.90 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
0.90 logistic.py(351):     return loss, grad.ravel(), p
0.90 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
0.90 logistic.py(339):     n_classes = Y.shape[1]
0.90 logistic.py(340):     n_features = X.shape[1]
0.90 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
0.90 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
0.90 logistic.py(343):                     dtype=X.dtype)
0.90 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
0.90 logistic.py(282):     n_classes = Y.shape[1]
0.90 logistic.py(283):     n_features = X.shape[1]
0.90 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
0.90 logistic.py(285):     w = w.reshape(n_classes, -1)
0.90 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
0.90 logistic.py(287):     if fit_intercept:
0.90 logistic.py(288):         intercept = w[:, -1]
0.90 logistic.py(289):         w = w[:, :-1]
0.90 logistic.py(292):     p = safe_sparse_dot(X, w.T)
0.90 logistic.py(293):     p += intercept
0.90 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
0.90 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
0.90 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
0.90 logistic.py(297):     p = np.exp(p, p)
0.90 logistic.py(298):     return loss, p, w
0.90 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
0.90 logistic.py(346):     diff = sample_weight * (p - Y)
0.90 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
0.90 logistic.py(348):     grad[:, :n_features] += alpha * w
0.90 logistic.py(349):     if fit_intercept:
0.90 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
0.90 logistic.py(351):     return loss, grad.ravel(), p
0.90 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
0.90 logistic.py(339):     n_classes = Y.shape[1]
0.90 logistic.py(340):     n_features = X.shape[1]
0.90 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
0.90 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
0.90 logistic.py(343):                     dtype=X.dtype)
0.90 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
0.90 logistic.py(282):     n_classes = Y.shape[1]
0.90 logistic.py(283):     n_features = X.shape[1]
0.90 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
0.90 logistic.py(285):     w = w.reshape(n_classes, -1)
0.90 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
0.90 logistic.py(287):     if fit_intercept:
0.90 logistic.py(288):         intercept = w[:, -1]
0.90 logistic.py(289):         w = w[:, :-1]
0.90 logistic.py(292):     p = safe_sparse_dot(X, w.T)
0.90 logistic.py(293):     p += intercept
0.90 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
0.90 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
0.90 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
0.90 logistic.py(297):     p = np.exp(p, p)
0.90 logistic.py(298):     return loss, p, w
0.90 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
0.90 logistic.py(346):     diff = sample_weight * (p - Y)
0.90 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
0.90 logistic.py(348):     grad[:, :n_features] += alpha * w
0.90 logistic.py(349):     if fit_intercept:
0.90 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
0.90 logistic.py(351):     return loss, grad.ravel(), p
0.90 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
0.90 logistic.py(339):     n_classes = Y.shape[1]
0.90 logistic.py(340):     n_features = X.shape[1]
0.90 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
0.90 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
0.90 logistic.py(343):                     dtype=X.dtype)
0.90 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
0.90 logistic.py(282):     n_classes = Y.shape[1]
0.90 logistic.py(283):     n_features = X.shape[1]
0.90 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
0.90 logistic.py(285):     w = w.reshape(n_classes, -1)
0.90 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
0.90 logistic.py(287):     if fit_intercept:
0.90 logistic.py(288):         intercept = w[:, -1]
0.90 logistic.py(289):         w = w[:, :-1]
0.90 logistic.py(292):     p = safe_sparse_dot(X, w.T)
0.90 logistic.py(293):     p += intercept
0.90 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
0.90 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
0.90 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
0.90 logistic.py(297):     p = np.exp(p, p)
0.90 logistic.py(298):     return loss, p, w
0.90 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
0.90 logistic.py(346):     diff = sample_weight * (p - Y)
0.90 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
0.90 logistic.py(348):     grad[:, :n_features] += alpha * w
0.90 logistic.py(349):     if fit_intercept:
0.90 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
0.90 logistic.py(351):     return loss, grad.ravel(), p
0.90 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
0.90 logistic.py(339):     n_classes = Y.shape[1]
0.90 logistic.py(340):     n_features = X.shape[1]
0.90 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
0.90 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
0.90 logistic.py(343):                     dtype=X.dtype)
0.90 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
0.90 logistic.py(282):     n_classes = Y.shape[1]
0.90 logistic.py(283):     n_features = X.shape[1]
0.90 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
0.90 logistic.py(285):     w = w.reshape(n_classes, -1)
0.90 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
0.90 logistic.py(287):     if fit_intercept:
0.90 logistic.py(288):         intercept = w[:, -1]
0.90 logistic.py(289):         w = w[:, :-1]
0.90 logistic.py(292):     p = safe_sparse_dot(X, w.T)
0.90 logistic.py(293):     p += intercept
0.90 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
0.90 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
0.90 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
0.90 logistic.py(297):     p = np.exp(p, p)
0.90 logistic.py(298):     return loss, p, w
0.90 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
0.90 logistic.py(346):     diff = sample_weight * (p - Y)
0.90 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
0.90 logistic.py(348):     grad[:, :n_features] += alpha * w
0.90 logistic.py(349):     if fit_intercept:
0.90 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
0.90 logistic.py(351):     return loss, grad.ravel(), p
0.90 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
0.90 logistic.py(339):     n_classes = Y.shape[1]
0.90 logistic.py(340):     n_features = X.shape[1]
0.90 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
0.90 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
0.90 logistic.py(343):                     dtype=X.dtype)
0.90 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
0.90 logistic.py(282):     n_classes = Y.shape[1]
0.90 logistic.py(283):     n_features = X.shape[1]
0.90 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
0.90 logistic.py(285):     w = w.reshape(n_classes, -1)
0.90 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
0.90 logistic.py(287):     if fit_intercept:
0.90 logistic.py(288):         intercept = w[:, -1]
0.90 logistic.py(289):         w = w[:, :-1]
0.90 logistic.py(292):     p = safe_sparse_dot(X, w.T)
0.90 logistic.py(293):     p += intercept
0.90 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
0.90 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
0.90 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
0.90 logistic.py(297):     p = np.exp(p, p)
0.90 logistic.py(298):     return loss, p, w
0.90 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
0.90 logistic.py(346):     diff = sample_weight * (p - Y)
0.90 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
0.90 logistic.py(348):     grad[:, :n_features] += alpha * w
0.90 logistic.py(349):     if fit_intercept:
0.90 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
0.90 logistic.py(351):     return loss, grad.ravel(), p
0.90 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
0.90 logistic.py(339):     n_classes = Y.shape[1]
0.90 logistic.py(340):     n_features = X.shape[1]
0.90 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
0.90 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
0.90 logistic.py(343):                     dtype=X.dtype)
0.90 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
0.90 logistic.py(282):     n_classes = Y.shape[1]
0.90 logistic.py(283):     n_features = X.shape[1]
0.90 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
0.90 logistic.py(285):     w = w.reshape(n_classes, -1)
0.90 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
0.90 logistic.py(287):     if fit_intercept:
0.90 logistic.py(288):         intercept = w[:, -1]
0.90 logistic.py(289):         w = w[:, :-1]
0.90 logistic.py(292):     p = safe_sparse_dot(X, w.T)
0.90 logistic.py(293):     p += intercept
0.90 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
0.91 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
0.91 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
0.91 logistic.py(297):     p = np.exp(p, p)
0.91 logistic.py(298):     return loss, p, w
0.91 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
0.91 logistic.py(346):     diff = sample_weight * (p - Y)
0.91 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
0.91 logistic.py(348):     grad[:, :n_features] += alpha * w
0.91 logistic.py(349):     if fit_intercept:
0.91 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
0.91 logistic.py(351):     return loss, grad.ravel(), p
0.91 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
0.91 logistic.py(339):     n_classes = Y.shape[1]
0.91 logistic.py(340):     n_features = X.shape[1]
0.91 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
0.91 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
0.91 logistic.py(343):                     dtype=X.dtype)
0.91 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
0.91 logistic.py(282):     n_classes = Y.shape[1]
0.91 logistic.py(283):     n_features = X.shape[1]
0.91 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
0.91 logistic.py(285):     w = w.reshape(n_classes, -1)
0.91 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
0.91 logistic.py(287):     if fit_intercept:
0.91 logistic.py(288):         intercept = w[:, -1]
0.91 logistic.py(289):         w = w[:, :-1]
0.91 logistic.py(292):     p = safe_sparse_dot(X, w.T)
0.91 logistic.py(293):     p += intercept
0.91 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
0.91 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
0.91 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
0.91 logistic.py(297):     p = np.exp(p, p)
0.91 logistic.py(298):     return loss, p, w
0.91 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
0.91 logistic.py(346):     diff = sample_weight * (p - Y)
0.91 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
0.91 logistic.py(348):     grad[:, :n_features] += alpha * w
0.91 logistic.py(349):     if fit_intercept:
0.91 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
0.91 logistic.py(351):     return loss, grad.ravel(), p
0.91 logistic.py(718):             if info["warnflag"] == 1:
0.91 logistic.py(723):             n_iter_i = min(info['nit'], max_iter)
0.91 logistic.py(760):         if multi_class == 'multinomial':
0.91 logistic.py(761):             multi_w0 = np.reshape(w0, (classes.size, -1))
0.91 logistic.py(762):             if classes.size == 2:
0.91 logistic.py(764):             coefs.append(multi_w0)
0.91 logistic.py(768):         n_iter[i] = n_iter_i
0.91 logistic.py(712):     for i, C in enumerate(Cs):
0.91 logistic.py(713):         if solver == 'lbfgs':
0.91 logistic.py(714):             w0, loss, info = optimize.fmin_l_bfgs_b(
0.91 logistic.py(715):                 func, w0, fprime=None,
0.91 logistic.py(716):                 args=(X, target, 1. / C, sample_weight),
0.91 logistic.py(717):                 iprint=(verbose > 0) - 1, pgtol=tol, maxiter=max_iter)
0.91 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
0.91 logistic.py(339):     n_classes = Y.shape[1]
0.91 logistic.py(340):     n_features = X.shape[1]
0.91 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
0.91 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
0.91 logistic.py(343):                     dtype=X.dtype)
0.91 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
0.91 logistic.py(282):     n_classes = Y.shape[1]
0.91 logistic.py(283):     n_features = X.shape[1]
0.91 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
0.91 logistic.py(285):     w = w.reshape(n_classes, -1)
0.91 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
0.91 logistic.py(287):     if fit_intercept:
0.91 logistic.py(288):         intercept = w[:, -1]
0.91 logistic.py(289):         w = w[:, :-1]
0.91 logistic.py(292):     p = safe_sparse_dot(X, w.T)
0.91 logistic.py(293):     p += intercept
0.91 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
0.91 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
0.91 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
0.91 logistic.py(297):     p = np.exp(p, p)
0.91 logistic.py(298):     return loss, p, w
0.91 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
0.91 logistic.py(346):     diff = sample_weight * (p - Y)
0.91 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
0.91 logistic.py(348):     grad[:, :n_features] += alpha * w
0.91 logistic.py(349):     if fit_intercept:
0.91 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
0.91 logistic.py(351):     return loss, grad.ravel(), p
0.91 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
0.91 logistic.py(339):     n_classes = Y.shape[1]
0.91 logistic.py(340):     n_features = X.shape[1]
0.91 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
0.91 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
0.91 logistic.py(343):                     dtype=X.dtype)
0.91 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
0.91 logistic.py(282):     n_classes = Y.shape[1]
0.91 logistic.py(283):     n_features = X.shape[1]
0.91 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
0.91 logistic.py(285):     w = w.reshape(n_classes, -1)
0.91 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
0.91 logistic.py(287):     if fit_intercept:
0.91 logistic.py(288):         intercept = w[:, -1]
0.91 logistic.py(289):         w = w[:, :-1]
0.91 logistic.py(292):     p = safe_sparse_dot(X, w.T)
0.91 logistic.py(293):     p += intercept
0.91 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
0.91 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
0.91 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
0.91 logistic.py(297):     p = np.exp(p, p)
0.91 logistic.py(298):     return loss, p, w
0.91 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
0.91 logistic.py(346):     diff = sample_weight * (p - Y)
0.91 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
0.91 logistic.py(348):     grad[:, :n_features] += alpha * w
0.91 logistic.py(349):     if fit_intercept:
0.91 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
0.91 logistic.py(351):     return loss, grad.ravel(), p
0.91 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
0.91 logistic.py(339):     n_classes = Y.shape[1]
0.91 logistic.py(340):     n_features = X.shape[1]
0.91 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
0.91 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
0.91 logistic.py(343):                     dtype=X.dtype)
0.91 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
0.91 logistic.py(282):     n_classes = Y.shape[1]
0.91 logistic.py(283):     n_features = X.shape[1]
0.91 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
0.91 logistic.py(285):     w = w.reshape(n_classes, -1)
0.91 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
0.91 logistic.py(287):     if fit_intercept:
0.91 logistic.py(288):         intercept = w[:, -1]
0.91 logistic.py(289):         w = w[:, :-1]
0.91 logistic.py(292):     p = safe_sparse_dot(X, w.T)
0.91 logistic.py(293):     p += intercept
0.91 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
0.91 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
0.91 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
0.91 logistic.py(297):     p = np.exp(p, p)
0.91 logistic.py(298):     return loss, p, w
0.91 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
0.91 logistic.py(346):     diff = sample_weight * (p - Y)
0.91 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
0.91 logistic.py(348):     grad[:, :n_features] += alpha * w
0.91 logistic.py(349):     if fit_intercept:
0.91 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
0.91 logistic.py(351):     return loss, grad.ravel(), p
0.91 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
0.91 logistic.py(339):     n_classes = Y.shape[1]
0.91 logistic.py(340):     n_features = X.shape[1]
0.91 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
0.91 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
0.91 logistic.py(343):                     dtype=X.dtype)
0.91 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
0.91 logistic.py(282):     n_classes = Y.shape[1]
0.91 logistic.py(283):     n_features = X.shape[1]
0.91 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
0.91 logistic.py(285):     w = w.reshape(n_classes, -1)
0.91 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
0.91 logistic.py(287):     if fit_intercept:
0.91 logistic.py(288):         intercept = w[:, -1]
0.91 logistic.py(289):         w = w[:, :-1]
0.91 logistic.py(292):     p = safe_sparse_dot(X, w.T)
0.91 logistic.py(293):     p += intercept
0.91 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
0.91 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
0.91 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
0.91 logistic.py(297):     p = np.exp(p, p)
0.91 logistic.py(298):     return loss, p, w
0.91 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
0.91 logistic.py(346):     diff = sample_weight * (p - Y)
0.91 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
0.91 logistic.py(348):     grad[:, :n_features] += alpha * w
0.91 logistic.py(349):     if fit_intercept:
0.91 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
0.91 logistic.py(351):     return loss, grad.ravel(), p
0.91 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
0.91 logistic.py(339):     n_classes = Y.shape[1]
0.91 logistic.py(340):     n_features = X.shape[1]
0.91 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
0.91 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
0.91 logistic.py(343):                     dtype=X.dtype)
0.91 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
0.91 logistic.py(282):     n_classes = Y.shape[1]
0.91 logistic.py(283):     n_features = X.shape[1]
0.91 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
0.91 logistic.py(285):     w = w.reshape(n_classes, -1)
0.91 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
0.91 logistic.py(287):     if fit_intercept:
0.91 logistic.py(288):         intercept = w[:, -1]
0.91 logistic.py(289):         w = w[:, :-1]
0.91 logistic.py(292):     p = safe_sparse_dot(X, w.T)
0.91 logistic.py(293):     p += intercept
0.91 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
0.91 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
0.91 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
0.91 logistic.py(297):     p = np.exp(p, p)
0.91 logistic.py(298):     return loss, p, w
0.91 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
0.91 logistic.py(346):     diff = sample_weight * (p - Y)
0.91 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
0.91 logistic.py(348):     grad[:, :n_features] += alpha * w
0.91 logistic.py(349):     if fit_intercept:
0.91 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
0.91 logistic.py(351):     return loss, grad.ravel(), p
0.91 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
0.91 logistic.py(339):     n_classes = Y.shape[1]
0.91 logistic.py(340):     n_features = X.shape[1]
0.91 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
0.91 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
0.91 logistic.py(343):                     dtype=X.dtype)
0.91 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
0.91 logistic.py(282):     n_classes = Y.shape[1]
0.91 logistic.py(283):     n_features = X.shape[1]
0.91 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
0.91 logistic.py(285):     w = w.reshape(n_classes, -1)
0.91 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
0.91 logistic.py(287):     if fit_intercept:
0.91 logistic.py(288):         intercept = w[:, -1]
0.91 logistic.py(289):         w = w[:, :-1]
0.91 logistic.py(292):     p = safe_sparse_dot(X, w.T)
0.91 logistic.py(293):     p += intercept
0.91 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
0.91 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
0.91 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
0.91 logistic.py(297):     p = np.exp(p, p)
0.91 logistic.py(298):     return loss, p, w
0.91 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
0.91 logistic.py(346):     diff = sample_weight * (p - Y)
0.91 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
0.91 logistic.py(348):     grad[:, :n_features] += alpha * w
0.91 logistic.py(349):     if fit_intercept:
0.91 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
0.91 logistic.py(351):     return loss, grad.ravel(), p
0.91 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
0.91 logistic.py(339):     n_classes = Y.shape[1]
0.91 logistic.py(340):     n_features = X.shape[1]
0.91 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
0.91 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
0.91 logistic.py(343):                     dtype=X.dtype)
0.91 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
0.91 logistic.py(282):     n_classes = Y.shape[1]
0.91 logistic.py(283):     n_features = X.shape[1]
0.91 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
0.91 logistic.py(285):     w = w.reshape(n_classes, -1)
0.91 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
0.91 logistic.py(287):     if fit_intercept:
0.91 logistic.py(288):         intercept = w[:, -1]
0.91 logistic.py(289):         w = w[:, :-1]
0.91 logistic.py(292):     p = safe_sparse_dot(X, w.T)
0.91 logistic.py(293):     p += intercept
0.91 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
0.91 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
0.91 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
0.91 logistic.py(297):     p = np.exp(p, p)
0.91 logistic.py(298):     return loss, p, w
0.91 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
0.91 logistic.py(346):     diff = sample_weight * (p - Y)
0.91 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
0.91 logistic.py(348):     grad[:, :n_features] += alpha * w
0.91 logistic.py(349):     if fit_intercept:
0.91 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
0.91 logistic.py(351):     return loss, grad.ravel(), p
0.91 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
0.91 logistic.py(339):     n_classes = Y.shape[1]
0.91 logistic.py(340):     n_features = X.shape[1]
0.91 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
0.91 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
0.91 logistic.py(343):                     dtype=X.dtype)
0.91 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
0.91 logistic.py(282):     n_classes = Y.shape[1]
0.91 logistic.py(283):     n_features = X.shape[1]
0.91 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
0.91 logistic.py(285):     w = w.reshape(n_classes, -1)
0.91 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
0.91 logistic.py(287):     if fit_intercept:
0.91 logistic.py(288):         intercept = w[:, -1]
0.91 logistic.py(289):         w = w[:, :-1]
0.91 logistic.py(292):     p = safe_sparse_dot(X, w.T)
0.91 logistic.py(293):     p += intercept
0.91 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
0.91 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
0.91 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
0.91 logistic.py(297):     p = np.exp(p, p)
0.91 logistic.py(298):     return loss, p, w
0.91 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
0.91 logistic.py(346):     diff = sample_weight * (p - Y)
0.91 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
0.91 logistic.py(348):     grad[:, :n_features] += alpha * w
0.91 logistic.py(349):     if fit_intercept:
0.91 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
0.91 logistic.py(351):     return loss, grad.ravel(), p
0.91 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
0.91 logistic.py(339):     n_classes = Y.shape[1]
0.91 logistic.py(340):     n_features = X.shape[1]
0.91 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
0.91 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
0.91 logistic.py(343):                     dtype=X.dtype)
0.91 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
0.91 logistic.py(282):     n_classes = Y.shape[1]
0.91 logistic.py(283):     n_features = X.shape[1]
0.91 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
0.91 logistic.py(285):     w = w.reshape(n_classes, -1)
0.91 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
0.91 logistic.py(287):     if fit_intercept:
0.91 logistic.py(288):         intercept = w[:, -1]
0.91 logistic.py(289):         w = w[:, :-1]
0.91 logistic.py(292):     p = safe_sparse_dot(X, w.T)
0.91 logistic.py(293):     p += intercept
0.91 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
0.91 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
0.91 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
0.91 logistic.py(297):     p = np.exp(p, p)
0.91 logistic.py(298):     return loss, p, w
0.91 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
0.91 logistic.py(346):     diff = sample_weight * (p - Y)
0.91 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
0.91 logistic.py(348):     grad[:, :n_features] += alpha * w
0.91 logistic.py(349):     if fit_intercept:
0.91 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
0.91 logistic.py(351):     return loss, grad.ravel(), p
0.91 logistic.py(718):             if info["warnflag"] == 1:
0.91 logistic.py(723):             n_iter_i = min(info['nit'], max_iter)
0.91 logistic.py(760):         if multi_class == 'multinomial':
0.91 logistic.py(761):             multi_w0 = np.reshape(w0, (classes.size, -1))
0.91 logistic.py(762):             if classes.size == 2:
0.91 logistic.py(764):             coefs.append(multi_w0)
0.91 logistic.py(768):         n_iter[i] = n_iter_i
0.91 logistic.py(712):     for i, C in enumerate(Cs):
0.91 logistic.py(713):         if solver == 'lbfgs':
0.91 logistic.py(714):             w0, loss, info = optimize.fmin_l_bfgs_b(
0.91 logistic.py(715):                 func, w0, fprime=None,
0.91 logistic.py(716):                 args=(X, target, 1. / C, sample_weight),
0.91 logistic.py(717):                 iprint=(verbose > 0) - 1, pgtol=tol, maxiter=max_iter)
0.91 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
0.91 logistic.py(339):     n_classes = Y.shape[1]
0.91 logistic.py(340):     n_features = X.shape[1]
0.91 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
0.91 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
0.91 logistic.py(343):                     dtype=X.dtype)
0.91 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
0.91 logistic.py(282):     n_classes = Y.shape[1]
0.91 logistic.py(283):     n_features = X.shape[1]
0.91 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
0.91 logistic.py(285):     w = w.reshape(n_classes, -1)
0.91 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
0.91 logistic.py(287):     if fit_intercept:
0.91 logistic.py(288):         intercept = w[:, -1]
0.91 logistic.py(289):         w = w[:, :-1]
0.91 logistic.py(292):     p = safe_sparse_dot(X, w.T)
0.91 logistic.py(293):     p += intercept
0.91 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
0.91 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
0.91 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
0.91 logistic.py(297):     p = np.exp(p, p)
0.91 logistic.py(298):     return loss, p, w
0.91 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
0.91 logistic.py(346):     diff = sample_weight * (p - Y)
0.91 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
0.91 logistic.py(348):     grad[:, :n_features] += alpha * w
0.91 logistic.py(349):     if fit_intercept:
0.91 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
0.91 logistic.py(351):     return loss, grad.ravel(), p
0.91 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
0.91 logistic.py(339):     n_classes = Y.shape[1]
0.91 logistic.py(340):     n_features = X.shape[1]
0.91 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
0.91 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
0.91 logistic.py(343):                     dtype=X.dtype)
0.91 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
0.91 logistic.py(282):     n_classes = Y.shape[1]
0.91 logistic.py(283):     n_features = X.shape[1]
0.91 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
0.91 logistic.py(285):     w = w.reshape(n_classes, -1)
0.91 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
0.91 logistic.py(287):     if fit_intercept:
0.91 logistic.py(288):         intercept = w[:, -1]
0.91 logistic.py(289):         w = w[:, :-1]
0.91 logistic.py(292):     p = safe_sparse_dot(X, w.T)
0.91 logistic.py(293):     p += intercept
0.91 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
0.91 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
0.91 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
0.91 logistic.py(297):     p = np.exp(p, p)
0.91 logistic.py(298):     return loss, p, w
0.91 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
0.91 logistic.py(346):     diff = sample_weight * (p - Y)
0.91 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
0.91 logistic.py(348):     grad[:, :n_features] += alpha * w
0.91 logistic.py(349):     if fit_intercept:
0.91 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
0.91 logistic.py(351):     return loss, grad.ravel(), p
0.91 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
0.91 logistic.py(339):     n_classes = Y.shape[1]
0.91 logistic.py(340):     n_features = X.shape[1]
0.91 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
0.91 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
0.91 logistic.py(343):                     dtype=X.dtype)
0.91 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
0.91 logistic.py(282):     n_classes = Y.shape[1]
0.91 logistic.py(283):     n_features = X.shape[1]
0.91 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
0.91 logistic.py(285):     w = w.reshape(n_classes, -1)
0.91 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
0.91 logistic.py(287):     if fit_intercept:
0.91 logistic.py(288):         intercept = w[:, -1]
0.91 logistic.py(289):         w = w[:, :-1]
0.91 logistic.py(292):     p = safe_sparse_dot(X, w.T)
0.91 logistic.py(293):     p += intercept
0.91 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
0.92 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
0.92 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
0.92 logistic.py(297):     p = np.exp(p, p)
0.92 logistic.py(298):     return loss, p, w
0.92 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
0.92 logistic.py(346):     diff = sample_weight * (p - Y)
0.92 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
0.92 logistic.py(348):     grad[:, :n_features] += alpha * w
0.92 logistic.py(349):     if fit_intercept:
0.92 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
0.92 logistic.py(351):     return loss, grad.ravel(), p
0.92 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
0.92 logistic.py(339):     n_classes = Y.shape[1]
0.92 logistic.py(340):     n_features = X.shape[1]
0.92 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
0.92 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
0.92 logistic.py(343):                     dtype=X.dtype)
0.92 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
0.92 logistic.py(282):     n_classes = Y.shape[1]
0.92 logistic.py(283):     n_features = X.shape[1]
0.92 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
0.92 logistic.py(285):     w = w.reshape(n_classes, -1)
0.92 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
0.92 logistic.py(287):     if fit_intercept:
0.92 logistic.py(288):         intercept = w[:, -1]
0.92 logistic.py(289):         w = w[:, :-1]
0.92 logistic.py(292):     p = safe_sparse_dot(X, w.T)
0.92 logistic.py(293):     p += intercept
0.92 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
0.92 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
0.92 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
0.92 logistic.py(297):     p = np.exp(p, p)
0.92 logistic.py(298):     return loss, p, w
0.92 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
0.92 logistic.py(346):     diff = sample_weight * (p - Y)
0.92 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
0.92 logistic.py(348):     grad[:, :n_features] += alpha * w
0.92 logistic.py(349):     if fit_intercept:
0.92 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
0.92 logistic.py(351):     return loss, grad.ravel(), p
0.92 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
0.92 logistic.py(339):     n_classes = Y.shape[1]
0.92 logistic.py(340):     n_features = X.shape[1]
0.92 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
0.92 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
0.92 logistic.py(343):                     dtype=X.dtype)
0.92 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
0.92 logistic.py(282):     n_classes = Y.shape[1]
0.92 logistic.py(283):     n_features = X.shape[1]
0.92 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
0.92 logistic.py(285):     w = w.reshape(n_classes, -1)
0.92 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
0.92 logistic.py(287):     if fit_intercept:
0.92 logistic.py(288):         intercept = w[:, -1]
0.92 logistic.py(289):         w = w[:, :-1]
0.92 logistic.py(292):     p = safe_sparse_dot(X, w.T)
0.92 logistic.py(293):     p += intercept
0.92 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
0.92 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
0.92 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
0.92 logistic.py(297):     p = np.exp(p, p)
0.92 logistic.py(298):     return loss, p, w
0.92 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
0.92 logistic.py(346):     diff = sample_weight * (p - Y)
0.92 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
0.92 logistic.py(348):     grad[:, :n_features] += alpha * w
0.92 logistic.py(349):     if fit_intercept:
0.92 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
0.92 logistic.py(351):     return loss, grad.ravel(), p
0.92 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
0.92 logistic.py(339):     n_classes = Y.shape[1]
0.92 logistic.py(340):     n_features = X.shape[1]
0.92 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
0.92 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
0.92 logistic.py(343):                     dtype=X.dtype)
0.92 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
0.92 logistic.py(282):     n_classes = Y.shape[1]
0.92 logistic.py(283):     n_features = X.shape[1]
0.92 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
0.92 logistic.py(285):     w = w.reshape(n_classes, -1)
0.92 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
0.92 logistic.py(287):     if fit_intercept:
0.92 logistic.py(288):         intercept = w[:, -1]
0.92 logistic.py(289):         w = w[:, :-1]
0.92 logistic.py(292):     p = safe_sparse_dot(X, w.T)
0.92 logistic.py(293):     p += intercept
0.92 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
0.92 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
0.92 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
0.92 logistic.py(297):     p = np.exp(p, p)
0.92 logistic.py(298):     return loss, p, w
0.92 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
0.92 logistic.py(346):     diff = sample_weight * (p - Y)
0.92 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
0.92 logistic.py(348):     grad[:, :n_features] += alpha * w
0.92 logistic.py(349):     if fit_intercept:
0.92 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
0.92 logistic.py(351):     return loss, grad.ravel(), p
0.92 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
0.92 logistic.py(339):     n_classes = Y.shape[1]
0.92 logistic.py(340):     n_features = X.shape[1]
0.92 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
0.92 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
0.92 logistic.py(343):                     dtype=X.dtype)
0.92 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
0.92 logistic.py(282):     n_classes = Y.shape[1]
0.92 logistic.py(283):     n_features = X.shape[1]
0.92 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
0.92 logistic.py(285):     w = w.reshape(n_classes, -1)
0.92 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
0.92 logistic.py(287):     if fit_intercept:
0.92 logistic.py(288):         intercept = w[:, -1]
0.92 logistic.py(289):         w = w[:, :-1]
0.92 logistic.py(292):     p = safe_sparse_dot(X, w.T)
0.92 logistic.py(293):     p += intercept
0.92 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
0.92 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
0.92 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
0.92 logistic.py(297):     p = np.exp(p, p)
0.92 logistic.py(298):     return loss, p, w
0.92 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
0.92 logistic.py(346):     diff = sample_weight * (p - Y)
0.92 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
0.92 logistic.py(348):     grad[:, :n_features] += alpha * w
0.92 logistic.py(349):     if fit_intercept:
0.92 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
0.92 logistic.py(351):     return loss, grad.ravel(), p
0.92 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
0.92 logistic.py(339):     n_classes = Y.shape[1]
0.92 logistic.py(340):     n_features = X.shape[1]
0.92 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
0.92 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
0.92 logistic.py(343):                     dtype=X.dtype)
0.92 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
0.92 logistic.py(282):     n_classes = Y.shape[1]
0.92 logistic.py(283):     n_features = X.shape[1]
0.92 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
0.92 logistic.py(285):     w = w.reshape(n_classes, -1)
0.92 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
0.92 logistic.py(287):     if fit_intercept:
0.92 logistic.py(288):         intercept = w[:, -1]
0.92 logistic.py(289):         w = w[:, :-1]
0.92 logistic.py(292):     p = safe_sparse_dot(X, w.T)
0.92 logistic.py(293):     p += intercept
0.92 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
0.92 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
0.92 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
0.92 logistic.py(297):     p = np.exp(p, p)
0.92 logistic.py(298):     return loss, p, w
0.92 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
0.92 logistic.py(346):     diff = sample_weight * (p - Y)
0.92 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
0.92 logistic.py(348):     grad[:, :n_features] += alpha * w
0.92 logistic.py(349):     if fit_intercept:
0.92 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
0.92 logistic.py(351):     return loss, grad.ravel(), p
0.92 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
0.92 logistic.py(339):     n_classes = Y.shape[1]
0.92 logistic.py(340):     n_features = X.shape[1]
0.92 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
0.92 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
0.92 logistic.py(343):                     dtype=X.dtype)
0.92 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
0.92 logistic.py(282):     n_classes = Y.shape[1]
0.92 logistic.py(283):     n_features = X.shape[1]
0.92 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
0.92 logistic.py(285):     w = w.reshape(n_classes, -1)
0.92 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
0.92 logistic.py(287):     if fit_intercept:
0.92 logistic.py(288):         intercept = w[:, -1]
0.92 logistic.py(289):         w = w[:, :-1]
0.92 logistic.py(292):     p = safe_sparse_dot(X, w.T)
0.92 logistic.py(293):     p += intercept
0.92 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
0.92 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
0.92 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
0.92 logistic.py(297):     p = np.exp(p, p)
0.92 logistic.py(298):     return loss, p, w
0.92 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
0.92 logistic.py(346):     diff = sample_weight * (p - Y)
0.92 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
0.92 logistic.py(348):     grad[:, :n_features] += alpha * w
0.92 logistic.py(349):     if fit_intercept:
0.92 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
0.92 logistic.py(351):     return loss, grad.ravel(), p
0.92 logistic.py(718):             if info["warnflag"] == 1:
0.92 logistic.py(723):             n_iter_i = min(info['nit'], max_iter)
0.92 logistic.py(760):         if multi_class == 'multinomial':
0.92 logistic.py(761):             multi_w0 = np.reshape(w0, (classes.size, -1))
0.92 logistic.py(762):             if classes.size == 2:
0.92 logistic.py(764):             coefs.append(multi_w0)
0.92 logistic.py(768):         n_iter[i] = n_iter_i
0.92 logistic.py(712):     for i, C in enumerate(Cs):
0.92 logistic.py(713):         if solver == 'lbfgs':
0.92 logistic.py(714):             w0, loss, info = optimize.fmin_l_bfgs_b(
0.92 logistic.py(715):                 func, w0, fprime=None,
0.92 logistic.py(716):                 args=(X, target, 1. / C, sample_weight),
0.92 logistic.py(717):                 iprint=(verbose > 0) - 1, pgtol=tol, maxiter=max_iter)
0.92 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
0.92 logistic.py(339):     n_classes = Y.shape[1]
0.92 logistic.py(340):     n_features = X.shape[1]
0.92 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
0.92 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
0.92 logistic.py(343):                     dtype=X.dtype)
0.92 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
0.92 logistic.py(282):     n_classes = Y.shape[1]
0.92 logistic.py(283):     n_features = X.shape[1]
0.92 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
0.92 logistic.py(285):     w = w.reshape(n_classes, -1)
0.92 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
0.92 logistic.py(287):     if fit_intercept:
0.92 logistic.py(288):         intercept = w[:, -1]
0.92 logistic.py(289):         w = w[:, :-1]
0.92 logistic.py(292):     p = safe_sparse_dot(X, w.T)
0.92 logistic.py(293):     p += intercept
0.92 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
0.92 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
0.92 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
0.92 logistic.py(297):     p = np.exp(p, p)
0.92 logistic.py(298):     return loss, p, w
0.92 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
0.92 logistic.py(346):     diff = sample_weight * (p - Y)
0.92 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
0.92 logistic.py(348):     grad[:, :n_features] += alpha * w
0.92 logistic.py(349):     if fit_intercept:
0.92 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
0.92 logistic.py(351):     return loss, grad.ravel(), p
0.92 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
0.92 logistic.py(339):     n_classes = Y.shape[1]
0.92 logistic.py(340):     n_features = X.shape[1]
0.92 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
0.92 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
0.92 logistic.py(343):                     dtype=X.dtype)
0.92 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
0.92 logistic.py(282):     n_classes = Y.shape[1]
0.92 logistic.py(283):     n_features = X.shape[1]
0.92 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
0.92 logistic.py(285):     w = w.reshape(n_classes, -1)
0.92 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
0.92 logistic.py(287):     if fit_intercept:
0.92 logistic.py(288):         intercept = w[:, -1]
0.92 logistic.py(289):         w = w[:, :-1]
0.92 logistic.py(292):     p = safe_sparse_dot(X, w.T)
0.92 logistic.py(293):     p += intercept
0.92 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
0.92 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
0.92 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
0.92 logistic.py(297):     p = np.exp(p, p)
0.92 logistic.py(298):     return loss, p, w
0.92 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
0.92 logistic.py(346):     diff = sample_weight * (p - Y)
0.92 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
0.92 logistic.py(348):     grad[:, :n_features] += alpha * w
0.92 logistic.py(349):     if fit_intercept:
0.92 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
0.92 logistic.py(351):     return loss, grad.ravel(), p
0.92 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
0.92 logistic.py(339):     n_classes = Y.shape[1]
0.92 logistic.py(340):     n_features = X.shape[1]
0.92 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
0.92 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
0.92 logistic.py(343):                     dtype=X.dtype)
0.92 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
0.92 logistic.py(282):     n_classes = Y.shape[1]
0.92 logistic.py(283):     n_features = X.shape[1]
0.92 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
0.92 logistic.py(285):     w = w.reshape(n_classes, -1)
0.92 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
0.92 logistic.py(287):     if fit_intercept:
0.92 logistic.py(288):         intercept = w[:, -1]
0.92 logistic.py(289):         w = w[:, :-1]
0.92 logistic.py(292):     p = safe_sparse_dot(X, w.T)
0.92 logistic.py(293):     p += intercept
0.92 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
0.92 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
0.92 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
0.92 logistic.py(297):     p = np.exp(p, p)
0.92 logistic.py(298):     return loss, p, w
0.92 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
0.92 logistic.py(346):     diff = sample_weight * (p - Y)
0.92 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
0.92 logistic.py(348):     grad[:, :n_features] += alpha * w
0.92 logistic.py(349):     if fit_intercept:
0.92 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
0.92 logistic.py(351):     return loss, grad.ravel(), p
0.92 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
0.92 logistic.py(339):     n_classes = Y.shape[1]
0.92 logistic.py(340):     n_features = X.shape[1]
0.92 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
0.92 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
0.92 logistic.py(343):                     dtype=X.dtype)
0.92 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
0.92 logistic.py(282):     n_classes = Y.shape[1]
0.92 logistic.py(283):     n_features = X.shape[1]
0.92 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
0.92 logistic.py(285):     w = w.reshape(n_classes, -1)
0.92 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
0.92 logistic.py(287):     if fit_intercept:
0.92 logistic.py(288):         intercept = w[:, -1]
0.92 logistic.py(289):         w = w[:, :-1]
0.92 logistic.py(292):     p = safe_sparse_dot(X, w.T)
0.92 logistic.py(293):     p += intercept
0.92 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
0.92 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
0.92 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
0.92 logistic.py(297):     p = np.exp(p, p)
0.92 logistic.py(298):     return loss, p, w
0.92 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
0.92 logistic.py(346):     diff = sample_weight * (p - Y)
0.92 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
0.92 logistic.py(348):     grad[:, :n_features] += alpha * w
0.92 logistic.py(349):     if fit_intercept:
0.92 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
0.92 logistic.py(351):     return loss, grad.ravel(), p
0.92 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
0.92 logistic.py(339):     n_classes = Y.shape[1]
0.92 logistic.py(340):     n_features = X.shape[1]
0.92 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
0.92 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
0.92 logistic.py(343):                     dtype=X.dtype)
0.92 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
0.92 logistic.py(282):     n_classes = Y.shape[1]
0.92 logistic.py(283):     n_features = X.shape[1]
0.92 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
0.92 logistic.py(285):     w = w.reshape(n_classes, -1)
0.92 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
0.92 logistic.py(287):     if fit_intercept:
0.92 logistic.py(288):         intercept = w[:, -1]
0.92 logistic.py(289):         w = w[:, :-1]
0.92 logistic.py(292):     p = safe_sparse_dot(X, w.T)
0.92 logistic.py(293):     p += intercept
0.92 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
0.92 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
0.92 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
0.92 logistic.py(297):     p = np.exp(p, p)
0.92 logistic.py(298):     return loss, p, w
0.92 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
0.92 logistic.py(346):     diff = sample_weight * (p - Y)
0.92 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
0.92 logistic.py(348):     grad[:, :n_features] += alpha * w
0.92 logistic.py(349):     if fit_intercept:
0.92 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
0.92 logistic.py(351):     return loss, grad.ravel(), p
0.92 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
0.92 logistic.py(339):     n_classes = Y.shape[1]
0.92 logistic.py(340):     n_features = X.shape[1]
0.92 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
0.92 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
0.92 logistic.py(343):                     dtype=X.dtype)
0.92 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
0.92 logistic.py(282):     n_classes = Y.shape[1]
0.92 logistic.py(283):     n_features = X.shape[1]
0.92 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
0.92 logistic.py(285):     w = w.reshape(n_classes, -1)
0.92 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
0.92 logistic.py(287):     if fit_intercept:
0.92 logistic.py(288):         intercept = w[:, -1]
0.92 logistic.py(289):         w = w[:, :-1]
0.92 logistic.py(292):     p = safe_sparse_dot(X, w.T)
0.92 logistic.py(293):     p += intercept
0.92 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
0.92 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
0.92 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
0.92 logistic.py(297):     p = np.exp(p, p)
0.92 logistic.py(298):     return loss, p, w
0.92 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
0.92 logistic.py(346):     diff = sample_weight * (p - Y)
0.92 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
0.92 logistic.py(348):     grad[:, :n_features] += alpha * w
0.92 logistic.py(349):     if fit_intercept:
0.92 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
0.92 logistic.py(351):     return loss, grad.ravel(), p
0.92 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
0.92 logistic.py(339):     n_classes = Y.shape[1]
0.92 logistic.py(340):     n_features = X.shape[1]
0.92 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
0.92 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
0.92 logistic.py(343):                     dtype=X.dtype)
0.92 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
0.92 logistic.py(282):     n_classes = Y.shape[1]
0.92 logistic.py(283):     n_features = X.shape[1]
0.92 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
0.92 logistic.py(285):     w = w.reshape(n_classes, -1)
0.92 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
0.92 logistic.py(287):     if fit_intercept:
0.92 logistic.py(288):         intercept = w[:, -1]
0.92 logistic.py(289):         w = w[:, :-1]
0.92 logistic.py(292):     p = safe_sparse_dot(X, w.T)
0.92 logistic.py(293):     p += intercept
0.92 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
0.92 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
0.92 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
0.92 logistic.py(297):     p = np.exp(p, p)
0.92 logistic.py(298):     return loss, p, w
0.92 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
0.92 logistic.py(346):     diff = sample_weight * (p - Y)
0.92 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
0.92 logistic.py(348):     grad[:, :n_features] += alpha * w
0.92 logistic.py(349):     if fit_intercept:
0.92 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
0.92 logistic.py(351):     return loss, grad.ravel(), p
0.92 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
0.92 logistic.py(339):     n_classes = Y.shape[1]
0.92 logistic.py(340):     n_features = X.shape[1]
0.92 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
0.92 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
0.92 logistic.py(343):                     dtype=X.dtype)
0.92 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
0.92 logistic.py(282):     n_classes = Y.shape[1]
0.92 logistic.py(283):     n_features = X.shape[1]
0.92 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
0.92 logistic.py(285):     w = w.reshape(n_classes, -1)
0.92 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
0.92 logistic.py(287):     if fit_intercept:
0.92 logistic.py(288):         intercept = w[:, -1]
0.92 logistic.py(289):         w = w[:, :-1]
0.92 logistic.py(292):     p = safe_sparse_dot(X, w.T)
0.92 logistic.py(293):     p += intercept
0.92 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
0.92 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
0.92 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
0.92 logistic.py(297):     p = np.exp(p, p)
0.92 logistic.py(298):     return loss, p, w
0.92 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
0.92 logistic.py(346):     diff = sample_weight * (p - Y)
0.92 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
0.92 logistic.py(348):     grad[:, :n_features] += alpha * w
0.92 logistic.py(349):     if fit_intercept:
0.92 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
0.92 logistic.py(351):     return loss, grad.ravel(), p
0.92 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
0.92 logistic.py(339):     n_classes = Y.shape[1]
0.92 logistic.py(340):     n_features = X.shape[1]
0.92 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
0.92 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
0.92 logistic.py(343):                     dtype=X.dtype)
0.92 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
0.92 logistic.py(282):     n_classes = Y.shape[1]
0.92 logistic.py(283):     n_features = X.shape[1]
0.92 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
0.92 logistic.py(285):     w = w.reshape(n_classes, -1)
0.92 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
0.92 logistic.py(287):     if fit_intercept:
0.92 logistic.py(288):         intercept = w[:, -1]
0.92 logistic.py(289):         w = w[:, :-1]
0.92 logistic.py(292):     p = safe_sparse_dot(X, w.T)
0.92 logistic.py(293):     p += intercept
0.92 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
0.92 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
0.92 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
0.92 logistic.py(297):     p = np.exp(p, p)
0.92 logistic.py(298):     return loss, p, w
0.92 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
0.92 logistic.py(346):     diff = sample_weight * (p - Y)
0.92 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
0.92 logistic.py(348):     grad[:, :n_features] += alpha * w
0.92 logistic.py(349):     if fit_intercept:
0.92 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
0.92 logistic.py(351):     return loss, grad.ravel(), p
0.92 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
0.92 logistic.py(339):     n_classes = Y.shape[1]
0.92 logistic.py(340):     n_features = X.shape[1]
0.92 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
0.92 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
0.92 logistic.py(343):                     dtype=X.dtype)
0.92 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
0.92 logistic.py(282):     n_classes = Y.shape[1]
0.92 logistic.py(283):     n_features = X.shape[1]
0.92 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
0.92 logistic.py(285):     w = w.reshape(n_classes, -1)
0.92 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
0.93 logistic.py(287):     if fit_intercept:
0.93 logistic.py(288):         intercept = w[:, -1]
0.93 logistic.py(289):         w = w[:, :-1]
0.93 logistic.py(292):     p = safe_sparse_dot(X, w.T)
0.93 logistic.py(293):     p += intercept
0.93 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
0.93 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
0.93 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
0.93 logistic.py(297):     p = np.exp(p, p)
0.93 logistic.py(298):     return loss, p, w
0.93 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
0.93 logistic.py(346):     diff = sample_weight * (p - Y)
0.93 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
0.93 logistic.py(348):     grad[:, :n_features] += alpha * w
0.93 logistic.py(349):     if fit_intercept:
0.93 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
0.93 logistic.py(351):     return loss, grad.ravel(), p
0.93 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
0.93 logistic.py(339):     n_classes = Y.shape[1]
0.93 logistic.py(340):     n_features = X.shape[1]
0.93 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
0.93 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
0.93 logistic.py(343):                     dtype=X.dtype)
0.93 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
0.93 logistic.py(282):     n_classes = Y.shape[1]
0.93 logistic.py(283):     n_features = X.shape[1]
0.93 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
0.93 logistic.py(285):     w = w.reshape(n_classes, -1)
0.93 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
0.93 logistic.py(287):     if fit_intercept:
0.93 logistic.py(288):         intercept = w[:, -1]
0.93 logistic.py(289):         w = w[:, :-1]
0.93 logistic.py(292):     p = safe_sparse_dot(X, w.T)
0.93 logistic.py(293):     p += intercept
0.93 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
0.93 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
0.93 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
0.93 logistic.py(297):     p = np.exp(p, p)
0.93 logistic.py(298):     return loss, p, w
0.93 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
0.93 logistic.py(346):     diff = sample_weight * (p - Y)
0.93 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
0.93 logistic.py(348):     grad[:, :n_features] += alpha * w
0.93 logistic.py(349):     if fit_intercept:
0.93 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
0.93 logistic.py(351):     return loss, grad.ravel(), p
0.93 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
0.93 logistic.py(339):     n_classes = Y.shape[1]
0.93 logistic.py(340):     n_features = X.shape[1]
0.93 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
0.93 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
0.93 logistic.py(343):                     dtype=X.dtype)
0.93 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
0.93 logistic.py(282):     n_classes = Y.shape[1]
0.93 logistic.py(283):     n_features = X.shape[1]
0.93 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
0.93 logistic.py(285):     w = w.reshape(n_classes, -1)
0.93 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
0.93 logistic.py(287):     if fit_intercept:
0.93 logistic.py(288):         intercept = w[:, -1]
0.93 logistic.py(289):         w = w[:, :-1]
0.93 logistic.py(292):     p = safe_sparse_dot(X, w.T)
0.93 logistic.py(293):     p += intercept
0.93 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
0.93 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
0.93 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
0.93 logistic.py(297):     p = np.exp(p, p)
0.93 logistic.py(298):     return loss, p, w
0.93 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
0.93 logistic.py(346):     diff = sample_weight * (p - Y)
0.93 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
0.93 logistic.py(348):     grad[:, :n_features] += alpha * w
0.93 logistic.py(349):     if fit_intercept:
0.93 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
0.93 logistic.py(351):     return loss, grad.ravel(), p
0.93 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
0.93 logistic.py(339):     n_classes = Y.shape[1]
0.93 logistic.py(340):     n_features = X.shape[1]
0.93 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
0.93 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
0.93 logistic.py(343):                     dtype=X.dtype)
0.93 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
0.93 logistic.py(282):     n_classes = Y.shape[1]
0.93 logistic.py(283):     n_features = X.shape[1]
0.93 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
0.93 logistic.py(285):     w = w.reshape(n_classes, -1)
0.93 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
0.93 logistic.py(287):     if fit_intercept:
0.93 logistic.py(288):         intercept = w[:, -1]
0.93 logistic.py(289):         w = w[:, :-1]
0.93 logistic.py(292):     p = safe_sparse_dot(X, w.T)
0.93 logistic.py(293):     p += intercept
0.93 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
0.93 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
0.93 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
0.93 logistic.py(297):     p = np.exp(p, p)
0.93 logistic.py(298):     return loss, p, w
0.93 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
0.93 logistic.py(346):     diff = sample_weight * (p - Y)
0.93 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
0.93 logistic.py(348):     grad[:, :n_features] += alpha * w
0.93 logistic.py(349):     if fit_intercept:
0.93 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
0.93 logistic.py(351):     return loss, grad.ravel(), p
0.93 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
0.93 logistic.py(339):     n_classes = Y.shape[1]
0.93 logistic.py(340):     n_features = X.shape[1]
0.93 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
0.93 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
0.93 logistic.py(343):                     dtype=X.dtype)
0.93 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
0.93 logistic.py(282):     n_classes = Y.shape[1]
0.93 logistic.py(283):     n_features = X.shape[1]
0.93 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
0.93 logistic.py(285):     w = w.reshape(n_classes, -1)
0.93 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
0.93 logistic.py(287):     if fit_intercept:
0.93 logistic.py(288):         intercept = w[:, -1]
0.93 logistic.py(289):         w = w[:, :-1]
0.93 logistic.py(292):     p = safe_sparse_dot(X, w.T)
0.93 logistic.py(293):     p += intercept
0.93 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
0.93 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
0.93 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
0.93 logistic.py(297):     p = np.exp(p, p)
0.93 logistic.py(298):     return loss, p, w
0.93 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
0.93 logistic.py(346):     diff = sample_weight * (p - Y)
0.93 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
0.93 logistic.py(348):     grad[:, :n_features] += alpha * w
0.93 logistic.py(349):     if fit_intercept:
0.93 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
0.93 logistic.py(351):     return loss, grad.ravel(), p
0.93 logistic.py(718):             if info["warnflag"] == 1:
0.93 logistic.py(723):             n_iter_i = min(info['nit'], max_iter)
0.93 logistic.py(760):         if multi_class == 'multinomial':
0.93 logistic.py(761):             multi_w0 = np.reshape(w0, (classes.size, -1))
0.93 logistic.py(762):             if classes.size == 2:
0.93 logistic.py(764):             coefs.append(multi_w0)
0.93 logistic.py(768):         n_iter[i] = n_iter_i
0.93 logistic.py(712):     for i, C in enumerate(Cs):
0.93 logistic.py(713):         if solver == 'lbfgs':
0.93 logistic.py(714):             w0, loss, info = optimize.fmin_l_bfgs_b(
0.93 logistic.py(715):                 func, w0, fprime=None,
0.93 logistic.py(716):                 args=(X, target, 1. / C, sample_weight),
0.93 logistic.py(717):                 iprint=(verbose > 0) - 1, pgtol=tol, maxiter=max_iter)
0.93 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
0.93 logistic.py(339):     n_classes = Y.shape[1]
0.93 logistic.py(340):     n_features = X.shape[1]
0.93 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
0.93 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
0.93 logistic.py(343):                     dtype=X.dtype)
0.93 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
0.93 logistic.py(282):     n_classes = Y.shape[1]
0.93 logistic.py(283):     n_features = X.shape[1]
0.93 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
0.93 logistic.py(285):     w = w.reshape(n_classes, -1)
0.93 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
0.93 logistic.py(287):     if fit_intercept:
0.93 logistic.py(288):         intercept = w[:, -1]
0.93 logistic.py(289):         w = w[:, :-1]
0.93 logistic.py(292):     p = safe_sparse_dot(X, w.T)
0.93 logistic.py(293):     p += intercept
0.93 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
0.93 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
0.93 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
0.93 logistic.py(297):     p = np.exp(p, p)
0.93 logistic.py(298):     return loss, p, w
0.93 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
0.93 logistic.py(346):     diff = sample_weight * (p - Y)
0.93 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
0.93 logistic.py(348):     grad[:, :n_features] += alpha * w
0.93 logistic.py(349):     if fit_intercept:
0.93 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
0.93 logistic.py(351):     return loss, grad.ravel(), p
0.93 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
0.93 logistic.py(339):     n_classes = Y.shape[1]
0.93 logistic.py(340):     n_features = X.shape[1]
0.93 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
0.93 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
0.93 logistic.py(343):                     dtype=X.dtype)
0.93 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
0.93 logistic.py(282):     n_classes = Y.shape[1]
0.93 logistic.py(283):     n_features = X.shape[1]
0.93 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
0.93 logistic.py(285):     w = w.reshape(n_classes, -1)
0.93 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
0.93 logistic.py(287):     if fit_intercept:
0.93 logistic.py(288):         intercept = w[:, -1]
0.93 logistic.py(289):         w = w[:, :-1]
0.93 logistic.py(292):     p = safe_sparse_dot(X, w.T)
0.93 logistic.py(293):     p += intercept
0.93 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
0.93 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
0.93 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
0.93 logistic.py(297):     p = np.exp(p, p)
0.93 logistic.py(298):     return loss, p, w
0.93 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
0.93 logistic.py(346):     diff = sample_weight * (p - Y)
0.93 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
0.93 logistic.py(348):     grad[:, :n_features] += alpha * w
0.93 logistic.py(349):     if fit_intercept:
0.93 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
0.93 logistic.py(351):     return loss, grad.ravel(), p
0.93 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
0.93 logistic.py(339):     n_classes = Y.shape[1]
0.93 logistic.py(340):     n_features = X.shape[1]
0.93 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
0.93 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
0.93 logistic.py(343):                     dtype=X.dtype)
0.93 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
0.93 logistic.py(282):     n_classes = Y.shape[1]
0.93 logistic.py(283):     n_features = X.shape[1]
0.93 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
0.93 logistic.py(285):     w = w.reshape(n_classes, -1)
0.93 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
0.93 logistic.py(287):     if fit_intercept:
0.93 logistic.py(288):         intercept = w[:, -1]
0.93 logistic.py(289):         w = w[:, :-1]
0.93 logistic.py(292):     p = safe_sparse_dot(X, w.T)
0.93 logistic.py(293):     p += intercept
0.93 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
0.93 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
0.93 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
0.93 logistic.py(297):     p = np.exp(p, p)
0.93 logistic.py(298):     return loss, p, w
0.93 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
0.93 logistic.py(346):     diff = sample_weight * (p - Y)
0.93 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
0.93 logistic.py(348):     grad[:, :n_features] += alpha * w
0.93 logistic.py(349):     if fit_intercept:
0.93 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
0.93 logistic.py(351):     return loss, grad.ravel(), p
0.93 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
0.93 logistic.py(339):     n_classes = Y.shape[1]
0.93 logistic.py(340):     n_features = X.shape[1]
0.93 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
0.93 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
0.93 logistic.py(343):                     dtype=X.dtype)
0.93 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
0.93 logistic.py(282):     n_classes = Y.shape[1]
0.93 logistic.py(283):     n_features = X.shape[1]
0.93 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
0.93 logistic.py(285):     w = w.reshape(n_classes, -1)
0.93 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
0.93 logistic.py(287):     if fit_intercept:
0.93 logistic.py(288):         intercept = w[:, -1]
0.93 logistic.py(289):         w = w[:, :-1]
0.93 logistic.py(292):     p = safe_sparse_dot(X, w.T)
0.93 logistic.py(293):     p += intercept
0.93 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
0.93 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
0.93 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
0.93 logistic.py(297):     p = np.exp(p, p)
0.93 logistic.py(298):     return loss, p, w
0.93 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
0.93 logistic.py(346):     diff = sample_weight * (p - Y)
0.93 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
0.93 logistic.py(348):     grad[:, :n_features] += alpha * w
0.93 logistic.py(349):     if fit_intercept:
0.93 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
0.93 logistic.py(351):     return loss, grad.ravel(), p
0.93 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
0.93 logistic.py(339):     n_classes = Y.shape[1]
0.93 logistic.py(340):     n_features = X.shape[1]
0.93 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
0.93 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
0.93 logistic.py(343):                     dtype=X.dtype)
0.93 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
0.93 logistic.py(282):     n_classes = Y.shape[1]
0.93 logistic.py(283):     n_features = X.shape[1]
0.93 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
0.93 logistic.py(285):     w = w.reshape(n_classes, -1)
0.93 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
0.93 logistic.py(287):     if fit_intercept:
0.93 logistic.py(288):         intercept = w[:, -1]
0.93 logistic.py(289):         w = w[:, :-1]
0.93 logistic.py(292):     p = safe_sparse_dot(X, w.T)
0.93 logistic.py(293):     p += intercept
0.93 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
0.93 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
0.93 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
0.93 logistic.py(297):     p = np.exp(p, p)
0.93 logistic.py(298):     return loss, p, w
0.93 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
0.93 logistic.py(346):     diff = sample_weight * (p - Y)
0.93 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
0.93 logistic.py(348):     grad[:, :n_features] += alpha * w
0.93 logistic.py(349):     if fit_intercept:
0.93 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
0.93 logistic.py(351):     return loss, grad.ravel(), p
0.93 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
0.93 logistic.py(339):     n_classes = Y.shape[1]
0.93 logistic.py(340):     n_features = X.shape[1]
0.93 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
0.93 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
0.93 logistic.py(343):                     dtype=X.dtype)
0.93 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
0.93 logistic.py(282):     n_classes = Y.shape[1]
0.93 logistic.py(283):     n_features = X.shape[1]
0.93 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
0.93 logistic.py(285):     w = w.reshape(n_classes, -1)
0.93 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
0.93 logistic.py(287):     if fit_intercept:
0.93 logistic.py(288):         intercept = w[:, -1]
0.93 logistic.py(289):         w = w[:, :-1]
0.93 logistic.py(292):     p = safe_sparse_dot(X, w.T)
0.93 logistic.py(293):     p += intercept
0.93 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
0.93 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
0.93 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
0.93 logistic.py(297):     p = np.exp(p, p)
0.93 logistic.py(298):     return loss, p, w
0.93 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
0.93 logistic.py(346):     diff = sample_weight * (p - Y)
0.93 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
0.93 logistic.py(348):     grad[:, :n_features] += alpha * w
0.93 logistic.py(349):     if fit_intercept:
0.93 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
0.93 logistic.py(351):     return loss, grad.ravel(), p
0.93 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
0.93 logistic.py(339):     n_classes = Y.shape[1]
0.93 logistic.py(340):     n_features = X.shape[1]
0.93 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
0.93 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
0.93 logistic.py(343):                     dtype=X.dtype)
0.93 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
0.93 logistic.py(282):     n_classes = Y.shape[1]
0.93 logistic.py(283):     n_features = X.shape[1]
0.93 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
0.93 logistic.py(285):     w = w.reshape(n_classes, -1)
0.93 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
0.93 logistic.py(287):     if fit_intercept:
0.93 logistic.py(288):         intercept = w[:, -1]
0.93 logistic.py(289):         w = w[:, :-1]
0.93 logistic.py(292):     p = safe_sparse_dot(X, w.T)
0.93 logistic.py(293):     p += intercept
0.93 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
0.93 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
0.93 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
0.93 logistic.py(297):     p = np.exp(p, p)
0.93 logistic.py(298):     return loss, p, w
0.93 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
0.93 logistic.py(346):     diff = sample_weight * (p - Y)
0.93 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
0.93 logistic.py(348):     grad[:, :n_features] += alpha * w
0.93 logistic.py(349):     if fit_intercept:
0.93 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
0.93 logistic.py(351):     return loss, grad.ravel(), p
0.93 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
0.93 logistic.py(339):     n_classes = Y.shape[1]
0.93 logistic.py(340):     n_features = X.shape[1]
0.93 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
0.93 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
0.93 logistic.py(343):                     dtype=X.dtype)
0.93 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
0.93 logistic.py(282):     n_classes = Y.shape[1]
0.93 logistic.py(283):     n_features = X.shape[1]
0.93 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
0.93 logistic.py(285):     w = w.reshape(n_classes, -1)
0.93 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
0.93 logistic.py(287):     if fit_intercept:
0.93 logistic.py(288):         intercept = w[:, -1]
0.93 logistic.py(289):         w = w[:, :-1]
0.93 logistic.py(292):     p = safe_sparse_dot(X, w.T)
0.93 logistic.py(293):     p += intercept
0.93 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
0.93 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
0.93 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
0.93 logistic.py(297):     p = np.exp(p, p)
0.93 logistic.py(298):     return loss, p, w
0.93 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
0.93 logistic.py(346):     diff = sample_weight * (p - Y)
0.93 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
0.93 logistic.py(348):     grad[:, :n_features] += alpha * w
0.93 logistic.py(349):     if fit_intercept:
0.93 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
0.93 logistic.py(351):     return loss, grad.ravel(), p
0.93 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
0.93 logistic.py(339):     n_classes = Y.shape[1]
0.93 logistic.py(340):     n_features = X.shape[1]
0.93 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
0.93 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
0.93 logistic.py(343):                     dtype=X.dtype)
0.93 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
0.93 logistic.py(282):     n_classes = Y.shape[1]
0.93 logistic.py(283):     n_features = X.shape[1]
0.93 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
0.93 logistic.py(285):     w = w.reshape(n_classes, -1)
0.93 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
0.93 logistic.py(287):     if fit_intercept:
0.93 logistic.py(288):         intercept = w[:, -1]
0.93 logistic.py(289):         w = w[:, :-1]
0.93 logistic.py(292):     p = safe_sparse_dot(X, w.T)
0.93 logistic.py(293):     p += intercept
0.93 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
0.93 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
0.93 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
0.93 logistic.py(297):     p = np.exp(p, p)
0.93 logistic.py(298):     return loss, p, w
0.93 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
0.93 logistic.py(346):     diff = sample_weight * (p - Y)
0.93 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
0.93 logistic.py(348):     grad[:, :n_features] += alpha * w
0.93 logistic.py(349):     if fit_intercept:
0.93 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
0.93 logistic.py(351):     return loss, grad.ravel(), p
0.93 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
0.93 logistic.py(339):     n_classes = Y.shape[1]
0.93 logistic.py(340):     n_features = X.shape[1]
0.93 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
0.93 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
0.93 logistic.py(343):                     dtype=X.dtype)
0.93 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
0.93 logistic.py(282):     n_classes = Y.shape[1]
0.93 logistic.py(283):     n_features = X.shape[1]
0.93 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
0.93 logistic.py(285):     w = w.reshape(n_classes, -1)
0.93 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
0.93 logistic.py(287):     if fit_intercept:
0.93 logistic.py(288):         intercept = w[:, -1]
0.93 logistic.py(289):         w = w[:, :-1]
0.93 logistic.py(292):     p = safe_sparse_dot(X, w.T)
0.93 logistic.py(293):     p += intercept
0.93 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
0.94 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
0.94 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
0.94 logistic.py(297):     p = np.exp(p, p)
0.94 logistic.py(298):     return loss, p, w
0.94 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
0.94 logistic.py(346):     diff = sample_weight * (p - Y)
0.94 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
0.94 logistic.py(348):     grad[:, :n_features] += alpha * w
0.94 logistic.py(349):     if fit_intercept:
0.94 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
0.94 logistic.py(351):     return loss, grad.ravel(), p
0.94 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
0.94 logistic.py(339):     n_classes = Y.shape[1]
0.94 logistic.py(340):     n_features = X.shape[1]
0.94 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
0.94 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
0.94 logistic.py(343):                     dtype=X.dtype)
0.94 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
0.94 logistic.py(282):     n_classes = Y.shape[1]
0.94 logistic.py(283):     n_features = X.shape[1]
0.94 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
0.94 logistic.py(285):     w = w.reshape(n_classes, -1)
0.94 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
0.94 logistic.py(287):     if fit_intercept:
0.94 logistic.py(288):         intercept = w[:, -1]
0.94 logistic.py(289):         w = w[:, :-1]
0.94 logistic.py(292):     p = safe_sparse_dot(X, w.T)
0.94 logistic.py(293):     p += intercept
0.94 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
0.94 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
0.94 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
0.94 logistic.py(297):     p = np.exp(p, p)
0.94 logistic.py(298):     return loss, p, w
0.94 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
0.94 logistic.py(346):     diff = sample_weight * (p - Y)
0.94 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
0.94 logistic.py(348):     grad[:, :n_features] += alpha * w
0.94 logistic.py(349):     if fit_intercept:
0.94 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
0.94 logistic.py(351):     return loss, grad.ravel(), p
0.94 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
0.94 logistic.py(339):     n_classes = Y.shape[1]
0.94 logistic.py(340):     n_features = X.shape[1]
0.94 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
0.94 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
0.94 logistic.py(343):                     dtype=X.dtype)
0.94 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
0.94 logistic.py(282):     n_classes = Y.shape[1]
0.94 logistic.py(283):     n_features = X.shape[1]
0.94 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
0.94 logistic.py(285):     w = w.reshape(n_classes, -1)
0.94 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
0.94 logistic.py(287):     if fit_intercept:
0.94 logistic.py(288):         intercept = w[:, -1]
0.94 logistic.py(289):         w = w[:, :-1]
0.94 logistic.py(292):     p = safe_sparse_dot(X, w.T)
0.94 logistic.py(293):     p += intercept
0.94 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
0.94 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
0.94 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
0.94 logistic.py(297):     p = np.exp(p, p)
0.94 logistic.py(298):     return loss, p, w
0.94 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
0.94 logistic.py(346):     diff = sample_weight * (p - Y)
0.94 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
0.94 logistic.py(348):     grad[:, :n_features] += alpha * w
0.94 logistic.py(349):     if fit_intercept:
0.94 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
0.94 logistic.py(351):     return loss, grad.ravel(), p
0.94 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
0.94 logistic.py(339):     n_classes = Y.shape[1]
0.94 logistic.py(340):     n_features = X.shape[1]
0.94 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
0.94 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
0.94 logistic.py(343):                     dtype=X.dtype)
0.94 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
0.94 logistic.py(282):     n_classes = Y.shape[1]
0.94 logistic.py(283):     n_features = X.shape[1]
0.94 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
0.94 logistic.py(285):     w = w.reshape(n_classes, -1)
0.94 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
0.94 logistic.py(287):     if fit_intercept:
0.94 logistic.py(288):         intercept = w[:, -1]
0.94 logistic.py(289):         w = w[:, :-1]
0.94 logistic.py(292):     p = safe_sparse_dot(X, w.T)
0.94 logistic.py(293):     p += intercept
0.94 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
0.94 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
0.94 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
0.94 logistic.py(297):     p = np.exp(p, p)
0.94 logistic.py(298):     return loss, p, w
0.94 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
0.94 logistic.py(346):     diff = sample_weight * (p - Y)
0.94 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
0.94 logistic.py(348):     grad[:, :n_features] += alpha * w
0.94 logistic.py(349):     if fit_intercept:
0.94 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
0.94 logistic.py(351):     return loss, grad.ravel(), p
0.94 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
0.94 logistic.py(339):     n_classes = Y.shape[1]
0.94 logistic.py(340):     n_features = X.shape[1]
0.94 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
0.94 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
0.94 logistic.py(343):                     dtype=X.dtype)
0.94 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
0.94 logistic.py(282):     n_classes = Y.shape[1]
0.94 logistic.py(283):     n_features = X.shape[1]
0.94 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
0.94 logistic.py(285):     w = w.reshape(n_classes, -1)
0.94 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
0.94 logistic.py(287):     if fit_intercept:
0.94 logistic.py(288):         intercept = w[:, -1]
0.94 logistic.py(289):         w = w[:, :-1]
0.94 logistic.py(292):     p = safe_sparse_dot(X, w.T)
0.94 logistic.py(293):     p += intercept
0.94 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
0.94 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
0.94 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
0.94 logistic.py(297):     p = np.exp(p, p)
0.94 logistic.py(298):     return loss, p, w
0.94 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
0.94 logistic.py(346):     diff = sample_weight * (p - Y)
0.94 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
0.94 logistic.py(348):     grad[:, :n_features] += alpha * w
0.94 logistic.py(349):     if fit_intercept:
0.94 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
0.94 logistic.py(351):     return loss, grad.ravel(), p
0.94 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
0.94 logistic.py(339):     n_classes = Y.shape[1]
0.94 logistic.py(340):     n_features = X.shape[1]
0.94 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
0.94 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
0.94 logistic.py(343):                     dtype=X.dtype)
0.94 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
0.94 logistic.py(282):     n_classes = Y.shape[1]
0.94 logistic.py(283):     n_features = X.shape[1]
0.94 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
0.94 logistic.py(285):     w = w.reshape(n_classes, -1)
0.94 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
0.94 logistic.py(287):     if fit_intercept:
0.94 logistic.py(288):         intercept = w[:, -1]
0.94 logistic.py(289):         w = w[:, :-1]
0.94 logistic.py(292):     p = safe_sparse_dot(X, w.T)
0.94 logistic.py(293):     p += intercept
0.94 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
0.94 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
0.94 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
0.94 logistic.py(297):     p = np.exp(p, p)
0.94 logistic.py(298):     return loss, p, w
0.94 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
0.94 logistic.py(346):     diff = sample_weight * (p - Y)
0.94 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
0.94 logistic.py(348):     grad[:, :n_features] += alpha * w
0.94 logistic.py(349):     if fit_intercept:
0.94 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
0.94 logistic.py(351):     return loss, grad.ravel(), p
0.94 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
0.94 logistic.py(339):     n_classes = Y.shape[1]
0.94 logistic.py(340):     n_features = X.shape[1]
0.94 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
0.94 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
0.94 logistic.py(343):                     dtype=X.dtype)
0.94 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
0.94 logistic.py(282):     n_classes = Y.shape[1]
0.94 logistic.py(283):     n_features = X.shape[1]
0.94 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
0.94 logistic.py(285):     w = w.reshape(n_classes, -1)
0.94 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
0.94 logistic.py(287):     if fit_intercept:
0.94 logistic.py(288):         intercept = w[:, -1]
0.94 logistic.py(289):         w = w[:, :-1]
0.94 logistic.py(292):     p = safe_sparse_dot(X, w.T)
0.94 logistic.py(293):     p += intercept
0.94 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
0.94 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
0.94 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
0.94 logistic.py(297):     p = np.exp(p, p)
0.94 logistic.py(298):     return loss, p, w
0.94 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
0.94 logistic.py(346):     diff = sample_weight * (p - Y)
0.94 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
0.94 logistic.py(348):     grad[:, :n_features] += alpha * w
0.94 logistic.py(349):     if fit_intercept:
0.94 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
0.94 logistic.py(351):     return loss, grad.ravel(), p
0.94 logistic.py(718):             if info["warnflag"] == 1:
0.94 logistic.py(723):             n_iter_i = min(info['nit'], max_iter)
0.94 logistic.py(760):         if multi_class == 'multinomial':
0.94 logistic.py(761):             multi_w0 = np.reshape(w0, (classes.size, -1))
0.94 logistic.py(762):             if classes.size == 2:
0.94 logistic.py(764):             coefs.append(multi_w0)
0.94 logistic.py(768):         n_iter[i] = n_iter_i
0.94 logistic.py(712):     for i, C in enumerate(Cs):
0.94 logistic.py(713):         if solver == 'lbfgs':
0.94 logistic.py(714):             w0, loss, info = optimize.fmin_l_bfgs_b(
0.94 logistic.py(715):                 func, w0, fprime=None,
0.94 logistic.py(716):                 args=(X, target, 1. / C, sample_weight),
0.94 logistic.py(717):                 iprint=(verbose > 0) - 1, pgtol=tol, maxiter=max_iter)
0.94 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
0.94 logistic.py(339):     n_classes = Y.shape[1]
0.94 logistic.py(340):     n_features = X.shape[1]
0.94 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
0.94 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
0.94 logistic.py(343):                     dtype=X.dtype)
0.94 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
0.94 logistic.py(282):     n_classes = Y.shape[1]
0.94 logistic.py(283):     n_features = X.shape[1]
0.94 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
0.94 logistic.py(285):     w = w.reshape(n_classes, -1)
0.94 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
0.94 logistic.py(287):     if fit_intercept:
0.94 logistic.py(288):         intercept = w[:, -1]
0.94 logistic.py(289):         w = w[:, :-1]
0.94 logistic.py(292):     p = safe_sparse_dot(X, w.T)
0.94 logistic.py(293):     p += intercept
0.94 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
0.94 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
0.94 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
0.94 logistic.py(297):     p = np.exp(p, p)
0.94 logistic.py(298):     return loss, p, w
0.94 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
0.94 logistic.py(346):     diff = sample_weight * (p - Y)
0.94 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
0.94 logistic.py(348):     grad[:, :n_features] += alpha * w
0.94 logistic.py(349):     if fit_intercept:
0.94 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
0.94 logistic.py(351):     return loss, grad.ravel(), p
0.94 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
0.94 logistic.py(339):     n_classes = Y.shape[1]
0.94 logistic.py(340):     n_features = X.shape[1]
0.94 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
0.94 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
0.94 logistic.py(343):                     dtype=X.dtype)
0.94 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
0.94 logistic.py(282):     n_classes = Y.shape[1]
0.94 logistic.py(283):     n_features = X.shape[1]
0.94 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
0.94 logistic.py(285):     w = w.reshape(n_classes, -1)
0.94 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
0.94 logistic.py(287):     if fit_intercept:
0.94 logistic.py(288):         intercept = w[:, -1]
0.94 logistic.py(289):         w = w[:, :-1]
0.94 logistic.py(292):     p = safe_sparse_dot(X, w.T)
0.94 logistic.py(293):     p += intercept
0.94 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
0.94 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
0.94 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
0.94 logistic.py(297):     p = np.exp(p, p)
0.94 logistic.py(298):     return loss, p, w
0.94 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
0.94 logistic.py(346):     diff = sample_weight * (p - Y)
0.94 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
0.94 logistic.py(348):     grad[:, :n_features] += alpha * w
0.94 logistic.py(349):     if fit_intercept:
0.94 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
0.94 logistic.py(351):     return loss, grad.ravel(), p
0.94 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
0.94 logistic.py(339):     n_classes = Y.shape[1]
0.94 logistic.py(340):     n_features = X.shape[1]
0.94 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
0.94 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
0.94 logistic.py(343):                     dtype=X.dtype)
0.94 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
0.94 logistic.py(282):     n_classes = Y.shape[1]
0.94 logistic.py(283):     n_features = X.shape[1]
0.94 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
0.94 logistic.py(285):     w = w.reshape(n_classes, -1)
0.94 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
0.94 logistic.py(287):     if fit_intercept:
0.94 logistic.py(288):         intercept = w[:, -1]
0.94 logistic.py(289):         w = w[:, :-1]
0.94 logistic.py(292):     p = safe_sparse_dot(X, w.T)
0.94 logistic.py(293):     p += intercept
0.94 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
0.94 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
0.94 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
0.94 logistic.py(297):     p = np.exp(p, p)
0.94 logistic.py(298):     return loss, p, w
0.94 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
0.94 logistic.py(346):     diff = sample_weight * (p - Y)
0.94 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
0.94 logistic.py(348):     grad[:, :n_features] += alpha * w
0.94 logistic.py(349):     if fit_intercept:
0.94 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
0.94 logistic.py(351):     return loss, grad.ravel(), p
0.94 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
0.94 logistic.py(339):     n_classes = Y.shape[1]
0.94 logistic.py(340):     n_features = X.shape[1]
0.94 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
0.94 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
0.94 logistic.py(343):                     dtype=X.dtype)
0.94 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
0.94 logistic.py(282):     n_classes = Y.shape[1]
0.94 logistic.py(283):     n_features = X.shape[1]
0.94 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
0.94 logistic.py(285):     w = w.reshape(n_classes, -1)
0.94 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
0.94 logistic.py(287):     if fit_intercept:
0.94 logistic.py(288):         intercept = w[:, -1]
0.94 logistic.py(289):         w = w[:, :-1]
0.94 logistic.py(292):     p = safe_sparse_dot(X, w.T)
0.94 logistic.py(293):     p += intercept
0.94 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
0.94 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
0.94 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
0.94 logistic.py(297):     p = np.exp(p, p)
0.94 logistic.py(298):     return loss, p, w
0.94 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
0.94 logistic.py(346):     diff = sample_weight * (p - Y)
0.94 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
0.94 logistic.py(348):     grad[:, :n_features] += alpha * w
0.94 logistic.py(349):     if fit_intercept:
0.94 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
0.94 logistic.py(351):     return loss, grad.ravel(), p
0.94 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
0.94 logistic.py(339):     n_classes = Y.shape[1]
0.94 logistic.py(340):     n_features = X.shape[1]
0.94 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
0.94 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
0.94 logistic.py(343):                     dtype=X.dtype)
0.94 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
0.94 logistic.py(282):     n_classes = Y.shape[1]
0.94 logistic.py(283):     n_features = X.shape[1]
0.94 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
0.94 logistic.py(285):     w = w.reshape(n_classes, -1)
0.94 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
0.94 logistic.py(287):     if fit_intercept:
0.94 logistic.py(288):         intercept = w[:, -1]
0.94 logistic.py(289):         w = w[:, :-1]
0.94 logistic.py(292):     p = safe_sparse_dot(X, w.T)
0.94 logistic.py(293):     p += intercept
0.94 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
0.94 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
0.94 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
0.94 logistic.py(297):     p = np.exp(p, p)
0.94 logistic.py(298):     return loss, p, w
0.94 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
0.94 logistic.py(346):     diff = sample_weight * (p - Y)
0.94 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
0.94 logistic.py(348):     grad[:, :n_features] += alpha * w
0.94 logistic.py(349):     if fit_intercept:
0.94 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
0.94 logistic.py(351):     return loss, grad.ravel(), p
0.94 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
0.94 logistic.py(339):     n_classes = Y.shape[1]
0.94 logistic.py(340):     n_features = X.shape[1]
0.94 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
0.94 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
0.94 logistic.py(343):                     dtype=X.dtype)
0.94 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
0.94 logistic.py(282):     n_classes = Y.shape[1]
0.94 logistic.py(283):     n_features = X.shape[1]
0.94 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
0.94 logistic.py(285):     w = w.reshape(n_classes, -1)
0.94 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
0.94 logistic.py(287):     if fit_intercept:
0.94 logistic.py(288):         intercept = w[:, -1]
0.94 logistic.py(289):         w = w[:, :-1]
0.94 logistic.py(292):     p = safe_sparse_dot(X, w.T)
0.94 logistic.py(293):     p += intercept
0.94 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
0.94 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
0.94 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
0.94 logistic.py(297):     p = np.exp(p, p)
0.94 logistic.py(298):     return loss, p, w
0.94 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
0.94 logistic.py(346):     diff = sample_weight * (p - Y)
0.94 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
0.94 logistic.py(348):     grad[:, :n_features] += alpha * w
0.94 logistic.py(349):     if fit_intercept:
0.94 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
0.94 logistic.py(351):     return loss, grad.ravel(), p
0.94 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
0.94 logistic.py(339):     n_classes = Y.shape[1]
0.94 logistic.py(340):     n_features = X.shape[1]
0.94 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
0.94 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
0.94 logistic.py(343):                     dtype=X.dtype)
0.94 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
0.94 logistic.py(282):     n_classes = Y.shape[1]
0.94 logistic.py(283):     n_features = X.shape[1]
0.94 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
0.94 logistic.py(285):     w = w.reshape(n_classes, -1)
0.94 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
0.94 logistic.py(287):     if fit_intercept:
0.94 logistic.py(288):         intercept = w[:, -1]
0.94 logistic.py(289):         w = w[:, :-1]
0.94 logistic.py(292):     p = safe_sparse_dot(X, w.T)
0.94 logistic.py(293):     p += intercept
0.94 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
0.94 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
0.94 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
0.94 logistic.py(297):     p = np.exp(p, p)
0.94 logistic.py(298):     return loss, p, w
0.94 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
0.94 logistic.py(346):     diff = sample_weight * (p - Y)
0.94 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
0.94 logistic.py(348):     grad[:, :n_features] += alpha * w
0.94 logistic.py(349):     if fit_intercept:
0.94 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
0.94 logistic.py(351):     return loss, grad.ravel(), p
0.94 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
0.94 logistic.py(339):     n_classes = Y.shape[1]
0.94 logistic.py(340):     n_features = X.shape[1]
0.94 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
0.94 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
0.94 logistic.py(343):                     dtype=X.dtype)
0.94 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
0.94 logistic.py(282):     n_classes = Y.shape[1]
0.94 logistic.py(283):     n_features = X.shape[1]
0.94 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
0.94 logistic.py(285):     w = w.reshape(n_classes, -1)
0.94 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
0.94 logistic.py(287):     if fit_intercept:
0.94 logistic.py(288):         intercept = w[:, -1]
0.94 logistic.py(289):         w = w[:, :-1]
0.94 logistic.py(292):     p = safe_sparse_dot(X, w.T)
0.94 logistic.py(293):     p += intercept
0.94 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
0.94 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
0.94 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
0.95 logistic.py(297):     p = np.exp(p, p)
0.95 logistic.py(298):     return loss, p, w
0.95 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
0.95 logistic.py(346):     diff = sample_weight * (p - Y)
0.95 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
0.95 logistic.py(348):     grad[:, :n_features] += alpha * w
0.95 logistic.py(349):     if fit_intercept:
0.95 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
0.95 logistic.py(351):     return loss, grad.ravel(), p
0.95 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
0.95 logistic.py(339):     n_classes = Y.shape[1]
0.95 logistic.py(340):     n_features = X.shape[1]
0.95 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
0.95 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
0.95 logistic.py(343):                     dtype=X.dtype)
0.95 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
0.95 logistic.py(282):     n_classes = Y.shape[1]
0.95 logistic.py(283):     n_features = X.shape[1]
0.95 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
0.95 logistic.py(285):     w = w.reshape(n_classes, -1)
0.95 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
0.95 logistic.py(287):     if fit_intercept:
0.95 logistic.py(288):         intercept = w[:, -1]
0.95 logistic.py(289):         w = w[:, :-1]
0.95 logistic.py(292):     p = safe_sparse_dot(X, w.T)
0.95 logistic.py(293):     p += intercept
0.95 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
0.95 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
0.95 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
0.95 logistic.py(297):     p = np.exp(p, p)
0.95 logistic.py(298):     return loss, p, w
0.95 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
0.95 logistic.py(346):     diff = sample_weight * (p - Y)
0.95 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
0.95 logistic.py(348):     grad[:, :n_features] += alpha * w
0.95 logistic.py(349):     if fit_intercept:
0.95 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
0.95 logistic.py(351):     return loss, grad.ravel(), p
0.95 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
0.95 logistic.py(339):     n_classes = Y.shape[1]
0.95 logistic.py(340):     n_features = X.shape[1]
0.95 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
0.95 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
0.95 logistic.py(343):                     dtype=X.dtype)
0.95 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
0.95 logistic.py(282):     n_classes = Y.shape[1]
0.95 logistic.py(283):     n_features = X.shape[1]
0.95 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
0.95 logistic.py(285):     w = w.reshape(n_classes, -1)
0.95 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
0.95 logistic.py(287):     if fit_intercept:
0.95 logistic.py(288):         intercept = w[:, -1]
0.95 logistic.py(289):         w = w[:, :-1]
0.95 logistic.py(292):     p = safe_sparse_dot(X, w.T)
0.95 logistic.py(293):     p += intercept
0.95 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
0.95 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
0.95 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
0.95 logistic.py(297):     p = np.exp(p, p)
0.95 logistic.py(298):     return loss, p, w
0.95 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
0.95 logistic.py(346):     diff = sample_weight * (p - Y)
0.95 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
0.95 logistic.py(348):     grad[:, :n_features] += alpha * w
0.95 logistic.py(349):     if fit_intercept:
0.95 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
0.95 logistic.py(351):     return loss, grad.ravel(), p
0.95 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
0.95 logistic.py(339):     n_classes = Y.shape[1]
0.95 logistic.py(340):     n_features = X.shape[1]
0.95 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
0.95 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
0.95 logistic.py(343):                     dtype=X.dtype)
0.95 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
0.95 logistic.py(282):     n_classes = Y.shape[1]
0.95 logistic.py(283):     n_features = X.shape[1]
0.95 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
0.95 logistic.py(285):     w = w.reshape(n_classes, -1)
0.95 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
0.95 logistic.py(287):     if fit_intercept:
0.95 logistic.py(288):         intercept = w[:, -1]
0.95 logistic.py(289):         w = w[:, :-1]
0.95 logistic.py(292):     p = safe_sparse_dot(X, w.T)
0.95 logistic.py(293):     p += intercept
0.95 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
0.95 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
0.95 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
0.95 logistic.py(297):     p = np.exp(p, p)
0.95 logistic.py(298):     return loss, p, w
0.95 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
0.95 logistic.py(346):     diff = sample_weight * (p - Y)
0.95 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
0.95 logistic.py(348):     grad[:, :n_features] += alpha * w
0.95 logistic.py(349):     if fit_intercept:
0.95 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
0.95 logistic.py(351):     return loss, grad.ravel(), p
0.95 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
0.95 logistic.py(339):     n_classes = Y.shape[1]
0.95 logistic.py(340):     n_features = X.shape[1]
0.95 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
0.95 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
0.95 logistic.py(343):                     dtype=X.dtype)
0.95 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
0.95 logistic.py(282):     n_classes = Y.shape[1]
0.95 logistic.py(283):     n_features = X.shape[1]
0.95 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
0.95 logistic.py(285):     w = w.reshape(n_classes, -1)
0.95 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
0.95 logistic.py(287):     if fit_intercept:
0.95 logistic.py(288):         intercept = w[:, -1]
0.95 logistic.py(289):         w = w[:, :-1]
0.95 logistic.py(292):     p = safe_sparse_dot(X, w.T)
0.95 logistic.py(293):     p += intercept
0.95 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
0.95 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
0.95 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
0.95 logistic.py(297):     p = np.exp(p, p)
0.95 logistic.py(298):     return loss, p, w
0.95 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
0.95 logistic.py(346):     diff = sample_weight * (p - Y)
0.95 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
0.95 logistic.py(348):     grad[:, :n_features] += alpha * w
0.95 logistic.py(349):     if fit_intercept:
0.95 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
0.95 logistic.py(351):     return loss, grad.ravel(), p
0.95 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
0.95 logistic.py(339):     n_classes = Y.shape[1]
0.95 logistic.py(340):     n_features = X.shape[1]
0.95 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
0.95 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
0.95 logistic.py(343):                     dtype=X.dtype)
0.95 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
0.95 logistic.py(282):     n_classes = Y.shape[1]
0.95 logistic.py(283):     n_features = X.shape[1]
0.95 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
0.95 logistic.py(285):     w = w.reshape(n_classes, -1)
0.95 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
0.95 logistic.py(287):     if fit_intercept:
0.95 logistic.py(288):         intercept = w[:, -1]
0.95 logistic.py(289):         w = w[:, :-1]
0.95 logistic.py(292):     p = safe_sparse_dot(X, w.T)
0.95 logistic.py(293):     p += intercept
0.95 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
0.95 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
0.95 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
0.95 logistic.py(297):     p = np.exp(p, p)
0.95 logistic.py(298):     return loss, p, w
0.95 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
0.95 logistic.py(346):     diff = sample_weight * (p - Y)
0.95 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
0.95 logistic.py(348):     grad[:, :n_features] += alpha * w
0.95 logistic.py(349):     if fit_intercept:
0.95 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
0.95 logistic.py(351):     return loss, grad.ravel(), p
0.95 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
0.95 logistic.py(339):     n_classes = Y.shape[1]
0.95 logistic.py(340):     n_features = X.shape[1]
0.95 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
0.95 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
0.95 logistic.py(343):                     dtype=X.dtype)
0.95 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
0.95 logistic.py(282):     n_classes = Y.shape[1]
0.95 logistic.py(283):     n_features = X.shape[1]
0.95 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
0.95 logistic.py(285):     w = w.reshape(n_classes, -1)
0.95 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
0.95 logistic.py(287):     if fit_intercept:
0.95 logistic.py(288):         intercept = w[:, -1]
0.95 logistic.py(289):         w = w[:, :-1]
0.95 logistic.py(292):     p = safe_sparse_dot(X, w.T)
0.95 logistic.py(293):     p += intercept
0.95 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
0.95 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
0.95 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
0.95 logistic.py(297):     p = np.exp(p, p)
0.95 logistic.py(298):     return loss, p, w
0.95 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
0.95 logistic.py(346):     diff = sample_weight * (p - Y)
0.95 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
0.95 logistic.py(348):     grad[:, :n_features] += alpha * w
0.95 logistic.py(349):     if fit_intercept:
0.95 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
0.95 logistic.py(351):     return loss, grad.ravel(), p
0.95 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
0.95 logistic.py(339):     n_classes = Y.shape[1]
0.95 logistic.py(340):     n_features = X.shape[1]
0.95 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
0.95 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
0.95 logistic.py(343):                     dtype=X.dtype)
0.95 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
0.95 logistic.py(282):     n_classes = Y.shape[1]
0.95 logistic.py(283):     n_features = X.shape[1]
0.95 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
0.95 logistic.py(285):     w = w.reshape(n_classes, -1)
0.95 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
0.95 logistic.py(287):     if fit_intercept:
0.95 logistic.py(288):         intercept = w[:, -1]
0.95 logistic.py(289):         w = w[:, :-1]
0.95 logistic.py(292):     p = safe_sparse_dot(X, w.T)
0.95 logistic.py(293):     p += intercept
0.95 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
0.95 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
0.95 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
0.95 logistic.py(297):     p = np.exp(p, p)
0.95 logistic.py(298):     return loss, p, w
0.95 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
0.95 logistic.py(346):     diff = sample_weight * (p - Y)
0.95 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
0.95 logistic.py(348):     grad[:, :n_features] += alpha * w
0.95 logistic.py(349):     if fit_intercept:
0.95 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
0.95 logistic.py(351):     return loss, grad.ravel(), p
0.95 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
0.95 logistic.py(339):     n_classes = Y.shape[1]
0.95 logistic.py(340):     n_features = X.shape[1]
0.95 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
0.95 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
0.95 logistic.py(343):                     dtype=X.dtype)
0.95 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
0.95 logistic.py(282):     n_classes = Y.shape[1]
0.95 logistic.py(283):     n_features = X.shape[1]
0.95 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
0.95 logistic.py(285):     w = w.reshape(n_classes, -1)
0.95 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
0.95 logistic.py(287):     if fit_intercept:
0.95 logistic.py(288):         intercept = w[:, -1]
0.95 logistic.py(289):         w = w[:, :-1]
0.95 logistic.py(292):     p = safe_sparse_dot(X, w.T)
0.95 logistic.py(293):     p += intercept
0.95 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
0.95 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
0.95 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
0.95 logistic.py(297):     p = np.exp(p, p)
0.95 logistic.py(298):     return loss, p, w
0.95 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
0.95 logistic.py(346):     diff = sample_weight * (p - Y)
0.95 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
0.95 logistic.py(348):     grad[:, :n_features] += alpha * w
0.95 logistic.py(349):     if fit_intercept:
0.95 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
0.95 logistic.py(351):     return loss, grad.ravel(), p
0.95 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
0.95 logistic.py(339):     n_classes = Y.shape[1]
0.95 logistic.py(340):     n_features = X.shape[1]
0.95 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
0.95 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
0.95 logistic.py(343):                     dtype=X.dtype)
0.95 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
0.95 logistic.py(282):     n_classes = Y.shape[1]
0.95 logistic.py(283):     n_features = X.shape[1]
0.95 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
0.95 logistic.py(285):     w = w.reshape(n_classes, -1)
0.95 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
0.95 logistic.py(287):     if fit_intercept:
0.95 logistic.py(288):         intercept = w[:, -1]
0.95 logistic.py(289):         w = w[:, :-1]
0.95 logistic.py(292):     p = safe_sparse_dot(X, w.T)
0.95 logistic.py(293):     p += intercept
0.95 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
0.95 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
0.95 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
0.95 logistic.py(297):     p = np.exp(p, p)
0.95 logistic.py(298):     return loss, p, w
0.95 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
0.95 logistic.py(346):     diff = sample_weight * (p - Y)
0.95 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
0.95 logistic.py(348):     grad[:, :n_features] += alpha * w
0.95 logistic.py(349):     if fit_intercept:
0.95 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
0.95 logistic.py(351):     return loss, grad.ravel(), p
0.95 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
0.95 logistic.py(339):     n_classes = Y.shape[1]
0.95 logistic.py(340):     n_features = X.shape[1]
0.95 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
0.95 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
0.95 logistic.py(343):                     dtype=X.dtype)
0.95 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
0.95 logistic.py(282):     n_classes = Y.shape[1]
0.95 logistic.py(283):     n_features = X.shape[1]
0.95 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
0.95 logistic.py(285):     w = w.reshape(n_classes, -1)
0.95 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
0.95 logistic.py(287):     if fit_intercept:
0.95 logistic.py(288):         intercept = w[:, -1]
0.95 logistic.py(289):         w = w[:, :-1]
0.95 logistic.py(292):     p = safe_sparse_dot(X, w.T)
0.95 logistic.py(293):     p += intercept
0.95 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
0.95 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
0.95 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
0.95 logistic.py(297):     p = np.exp(p, p)
0.95 logistic.py(298):     return loss, p, w
0.95 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
0.95 logistic.py(346):     diff = sample_weight * (p - Y)
0.95 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
0.95 logistic.py(348):     grad[:, :n_features] += alpha * w
0.95 logistic.py(349):     if fit_intercept:
0.95 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
0.95 logistic.py(351):     return loss, grad.ravel(), p
0.95 logistic.py(718):             if info["warnflag"] == 1:
0.95 logistic.py(723):             n_iter_i = min(info['nit'], max_iter)
0.95 logistic.py(760):         if multi_class == 'multinomial':
0.95 logistic.py(761):             multi_w0 = np.reshape(w0, (classes.size, -1))
0.95 logistic.py(762):             if classes.size == 2:
0.95 logistic.py(764):             coefs.append(multi_w0)
0.95 logistic.py(768):         n_iter[i] = n_iter_i
0.95 logistic.py(712):     for i, C in enumerate(Cs):
0.95 logistic.py(713):         if solver == 'lbfgs':
0.95 logistic.py(714):             w0, loss, info = optimize.fmin_l_bfgs_b(
0.95 logistic.py(715):                 func, w0, fprime=None,
0.95 logistic.py(716):                 args=(X, target, 1. / C, sample_weight),
0.95 logistic.py(717):                 iprint=(verbose > 0) - 1, pgtol=tol, maxiter=max_iter)
0.95 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
0.95 logistic.py(339):     n_classes = Y.shape[1]
0.95 logistic.py(340):     n_features = X.shape[1]
0.95 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
0.95 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
0.95 logistic.py(343):                     dtype=X.dtype)
0.95 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
0.95 logistic.py(282):     n_classes = Y.shape[1]
0.95 logistic.py(283):     n_features = X.shape[1]
0.95 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
0.95 logistic.py(285):     w = w.reshape(n_classes, -1)
0.95 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
0.95 logistic.py(287):     if fit_intercept:
0.95 logistic.py(288):         intercept = w[:, -1]
0.95 logistic.py(289):         w = w[:, :-1]
0.95 logistic.py(292):     p = safe_sparse_dot(X, w.T)
0.95 logistic.py(293):     p += intercept
0.95 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
0.95 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
0.95 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
0.95 logistic.py(297):     p = np.exp(p, p)
0.95 logistic.py(298):     return loss, p, w
0.95 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
0.95 logistic.py(346):     diff = sample_weight * (p - Y)
0.95 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
0.95 logistic.py(348):     grad[:, :n_features] += alpha * w
0.95 logistic.py(349):     if fit_intercept:
0.95 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
0.95 logistic.py(351):     return loss, grad.ravel(), p
0.95 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
0.95 logistic.py(339):     n_classes = Y.shape[1]
0.95 logistic.py(340):     n_features = X.shape[1]
0.95 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
0.95 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
0.95 logistic.py(343):                     dtype=X.dtype)
0.95 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
0.95 logistic.py(282):     n_classes = Y.shape[1]
0.95 logistic.py(283):     n_features = X.shape[1]
0.95 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
0.95 logistic.py(285):     w = w.reshape(n_classes, -1)
0.95 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
0.95 logistic.py(287):     if fit_intercept:
0.95 logistic.py(288):         intercept = w[:, -1]
0.95 logistic.py(289):         w = w[:, :-1]
0.95 logistic.py(292):     p = safe_sparse_dot(X, w.T)
0.95 logistic.py(293):     p += intercept
0.95 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
0.95 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
0.95 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
0.95 logistic.py(297):     p = np.exp(p, p)
0.95 logistic.py(298):     return loss, p, w
0.95 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
0.95 logistic.py(346):     diff = sample_weight * (p - Y)
0.95 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
0.95 logistic.py(348):     grad[:, :n_features] += alpha * w
0.95 logistic.py(349):     if fit_intercept:
0.95 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
0.95 logistic.py(351):     return loss, grad.ravel(), p
0.95 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
0.95 logistic.py(339):     n_classes = Y.shape[1]
0.95 logistic.py(340):     n_features = X.shape[1]
0.95 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
0.95 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
0.95 logistic.py(343):                     dtype=X.dtype)
0.95 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
0.95 logistic.py(282):     n_classes = Y.shape[1]
0.95 logistic.py(283):     n_features = X.shape[1]
0.95 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
0.95 logistic.py(285):     w = w.reshape(n_classes, -1)
0.95 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
0.95 logistic.py(287):     if fit_intercept:
0.95 logistic.py(288):         intercept = w[:, -1]
0.95 logistic.py(289):         w = w[:, :-1]
0.95 logistic.py(292):     p = safe_sparse_dot(X, w.T)
0.95 logistic.py(293):     p += intercept
0.95 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
0.95 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
0.95 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
0.95 logistic.py(297):     p = np.exp(p, p)
0.95 logistic.py(298):     return loss, p, w
0.95 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
0.95 logistic.py(346):     diff = sample_weight * (p - Y)
0.95 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
0.95 logistic.py(348):     grad[:, :n_features] += alpha * w
0.95 logistic.py(349):     if fit_intercept:
0.95 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
0.95 logistic.py(351):     return loss, grad.ravel(), p
0.95 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
0.95 logistic.py(339):     n_classes = Y.shape[1]
0.95 logistic.py(340):     n_features = X.shape[1]
0.95 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
0.95 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
0.95 logistic.py(343):                     dtype=X.dtype)
0.95 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
0.95 logistic.py(282):     n_classes = Y.shape[1]
0.95 logistic.py(283):     n_features = X.shape[1]
0.95 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
0.95 logistic.py(285):     w = w.reshape(n_classes, -1)
0.95 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
0.95 logistic.py(287):     if fit_intercept:
0.95 logistic.py(288):         intercept = w[:, -1]
0.95 logistic.py(289):         w = w[:, :-1]
0.95 logistic.py(292):     p = safe_sparse_dot(X, w.T)
0.95 logistic.py(293):     p += intercept
0.95 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
0.95 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
0.95 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
0.95 logistic.py(297):     p = np.exp(p, p)
0.95 logistic.py(298):     return loss, p, w
0.95 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
0.95 logistic.py(346):     diff = sample_weight * (p - Y)
0.95 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
0.95 logistic.py(348):     grad[:, :n_features] += alpha * w
0.95 logistic.py(349):     if fit_intercept:
0.95 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
0.95 logistic.py(351):     return loss, grad.ravel(), p
0.95 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
0.95 logistic.py(339):     n_classes = Y.shape[1]
0.95 logistic.py(340):     n_features = X.shape[1]
0.95 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
0.95 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
0.95 logistic.py(343):                     dtype=X.dtype)
0.95 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
0.95 logistic.py(282):     n_classes = Y.shape[1]
0.95 logistic.py(283):     n_features = X.shape[1]
0.95 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
0.95 logistic.py(285):     w = w.reshape(n_classes, -1)
0.95 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
0.95 logistic.py(287):     if fit_intercept:
0.95 logistic.py(288):         intercept = w[:, -1]
0.95 logistic.py(289):         w = w[:, :-1]
0.95 logistic.py(292):     p = safe_sparse_dot(X, w.T)
0.95 logistic.py(293):     p += intercept
0.95 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
0.95 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
0.95 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
0.95 logistic.py(297):     p = np.exp(p, p)
0.95 logistic.py(298):     return loss, p, w
0.95 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
0.95 logistic.py(346):     diff = sample_weight * (p - Y)
0.95 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
0.95 logistic.py(348):     grad[:, :n_features] += alpha * w
0.95 logistic.py(349):     if fit_intercept:
0.95 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
0.95 logistic.py(351):     return loss, grad.ravel(), p
0.95 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
0.95 logistic.py(339):     n_classes = Y.shape[1]
0.95 logistic.py(340):     n_features = X.shape[1]
0.95 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
0.95 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
0.95 logistic.py(343):                     dtype=X.dtype)
0.96 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
0.96 logistic.py(282):     n_classes = Y.shape[1]
0.96 logistic.py(283):     n_features = X.shape[1]
0.96 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
0.96 logistic.py(285):     w = w.reshape(n_classes, -1)
0.96 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
0.96 logistic.py(287):     if fit_intercept:
0.96 logistic.py(288):         intercept = w[:, -1]
0.96 logistic.py(289):         w = w[:, :-1]
0.96 logistic.py(292):     p = safe_sparse_dot(X, w.T)
0.96 logistic.py(293):     p += intercept
0.96 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
0.96 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
0.96 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
0.96 logistic.py(297):     p = np.exp(p, p)
0.96 logistic.py(298):     return loss, p, w
0.96 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
0.96 logistic.py(346):     diff = sample_weight * (p - Y)
0.96 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
0.96 logistic.py(348):     grad[:, :n_features] += alpha * w
0.96 logistic.py(349):     if fit_intercept:
0.96 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
0.96 logistic.py(351):     return loss, grad.ravel(), p
0.96 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
0.96 logistic.py(339):     n_classes = Y.shape[1]
0.96 logistic.py(340):     n_features = X.shape[1]
0.96 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
0.96 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
0.96 logistic.py(343):                     dtype=X.dtype)
0.96 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
0.96 logistic.py(282):     n_classes = Y.shape[1]
0.96 logistic.py(283):     n_features = X.shape[1]
0.96 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
0.96 logistic.py(285):     w = w.reshape(n_classes, -1)
0.96 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
0.96 logistic.py(287):     if fit_intercept:
0.96 logistic.py(288):         intercept = w[:, -1]
0.96 logistic.py(289):         w = w[:, :-1]
0.96 logistic.py(292):     p = safe_sparse_dot(X, w.T)
0.96 logistic.py(293):     p += intercept
0.96 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
0.96 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
0.96 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
0.96 logistic.py(297):     p = np.exp(p, p)
0.96 logistic.py(298):     return loss, p, w
0.96 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
0.96 logistic.py(346):     diff = sample_weight * (p - Y)
0.96 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
0.96 logistic.py(348):     grad[:, :n_features] += alpha * w
0.96 logistic.py(349):     if fit_intercept:
0.96 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
0.96 logistic.py(351):     return loss, grad.ravel(), p
0.96 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
0.96 logistic.py(339):     n_classes = Y.shape[1]
0.96 logistic.py(340):     n_features = X.shape[1]
0.96 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
0.96 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
0.96 logistic.py(343):                     dtype=X.dtype)
0.96 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
0.96 logistic.py(282):     n_classes = Y.shape[1]
0.96 logistic.py(283):     n_features = X.shape[1]
0.96 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
0.96 logistic.py(285):     w = w.reshape(n_classes, -1)
0.96 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
0.96 logistic.py(287):     if fit_intercept:
0.96 logistic.py(288):         intercept = w[:, -1]
0.96 logistic.py(289):         w = w[:, :-1]
0.96 logistic.py(292):     p = safe_sparse_dot(X, w.T)
0.96 logistic.py(293):     p += intercept
0.96 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
0.96 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
0.96 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
0.96 logistic.py(297):     p = np.exp(p, p)
0.96 logistic.py(298):     return loss, p, w
0.96 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
0.96 logistic.py(346):     diff = sample_weight * (p - Y)
0.96 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
0.96 logistic.py(348):     grad[:, :n_features] += alpha * w
0.96 logistic.py(349):     if fit_intercept:
0.96 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
0.96 logistic.py(351):     return loss, grad.ravel(), p
0.96 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
0.96 logistic.py(339):     n_classes = Y.shape[1]
0.96 logistic.py(340):     n_features = X.shape[1]
0.96 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
0.96 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
0.96 logistic.py(343):                     dtype=X.dtype)
0.96 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
0.96 logistic.py(282):     n_classes = Y.shape[1]
0.96 logistic.py(283):     n_features = X.shape[1]
0.96 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
0.96 logistic.py(285):     w = w.reshape(n_classes, -1)
0.96 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
0.96 logistic.py(287):     if fit_intercept:
0.96 logistic.py(288):         intercept = w[:, -1]
0.96 logistic.py(289):         w = w[:, :-1]
0.96 logistic.py(292):     p = safe_sparse_dot(X, w.T)
0.96 logistic.py(293):     p += intercept
0.96 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
0.96 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
0.96 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
0.96 logistic.py(297):     p = np.exp(p, p)
0.96 logistic.py(298):     return loss, p, w
0.96 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
0.96 logistic.py(346):     diff = sample_weight * (p - Y)
0.96 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
0.96 logistic.py(348):     grad[:, :n_features] += alpha * w
0.96 logistic.py(349):     if fit_intercept:
0.96 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
0.96 logistic.py(351):     return loss, grad.ravel(), p
0.96 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
0.96 logistic.py(339):     n_classes = Y.shape[1]
0.96 logistic.py(340):     n_features = X.shape[1]
0.96 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
0.96 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
0.96 logistic.py(343):                     dtype=X.dtype)
0.96 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
0.96 logistic.py(282):     n_classes = Y.shape[1]
0.96 logistic.py(283):     n_features = X.shape[1]
0.96 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
0.96 logistic.py(285):     w = w.reshape(n_classes, -1)
0.96 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
0.96 logistic.py(287):     if fit_intercept:
0.96 logistic.py(288):         intercept = w[:, -1]
0.96 logistic.py(289):         w = w[:, :-1]
0.96 logistic.py(292):     p = safe_sparse_dot(X, w.T)
0.96 logistic.py(293):     p += intercept
0.96 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
0.96 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
0.96 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
0.96 logistic.py(297):     p = np.exp(p, p)
0.96 logistic.py(298):     return loss, p, w
0.96 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
0.96 logistic.py(346):     diff = sample_weight * (p - Y)
0.96 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
0.96 logistic.py(348):     grad[:, :n_features] += alpha * w
0.96 logistic.py(349):     if fit_intercept:
0.96 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
0.96 logistic.py(351):     return loss, grad.ravel(), p
0.96 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
0.96 logistic.py(339):     n_classes = Y.shape[1]
0.96 logistic.py(340):     n_features = X.shape[1]
0.96 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
0.96 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
0.96 logistic.py(343):                     dtype=X.dtype)
0.96 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
0.96 logistic.py(282):     n_classes = Y.shape[1]
0.96 logistic.py(283):     n_features = X.shape[1]
0.96 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
0.96 logistic.py(285):     w = w.reshape(n_classes, -1)
0.96 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
0.96 logistic.py(287):     if fit_intercept:
0.96 logistic.py(288):         intercept = w[:, -1]
0.96 logistic.py(289):         w = w[:, :-1]
0.96 logistic.py(292):     p = safe_sparse_dot(X, w.T)
0.96 logistic.py(293):     p += intercept
0.96 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
0.96 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
0.96 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
0.96 logistic.py(297):     p = np.exp(p, p)
0.96 logistic.py(298):     return loss, p, w
0.96 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
0.96 logistic.py(346):     diff = sample_weight * (p - Y)
0.96 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
0.96 logistic.py(348):     grad[:, :n_features] += alpha * w
0.96 logistic.py(349):     if fit_intercept:
0.96 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
0.96 logistic.py(351):     return loss, grad.ravel(), p
0.96 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
0.96 logistic.py(339):     n_classes = Y.shape[1]
0.96 logistic.py(340):     n_features = X.shape[1]
0.96 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
0.96 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
0.96 logistic.py(343):                     dtype=X.dtype)
0.96 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
0.96 logistic.py(282):     n_classes = Y.shape[1]
0.96 logistic.py(283):     n_features = X.shape[1]
0.96 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
0.96 logistic.py(285):     w = w.reshape(n_classes, -1)
0.96 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
0.96 logistic.py(287):     if fit_intercept:
0.96 logistic.py(288):         intercept = w[:, -1]
0.96 logistic.py(289):         w = w[:, :-1]
0.96 logistic.py(292):     p = safe_sparse_dot(X, w.T)
0.96 logistic.py(293):     p += intercept
0.96 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
0.96 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
0.96 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
0.96 logistic.py(297):     p = np.exp(p, p)
0.96 logistic.py(298):     return loss, p, w
0.96 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
0.96 logistic.py(346):     diff = sample_weight * (p - Y)
0.96 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
0.96 logistic.py(348):     grad[:, :n_features] += alpha * w
0.96 logistic.py(349):     if fit_intercept:
0.96 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
0.96 logistic.py(351):     return loss, grad.ravel(), p
0.96 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
0.96 logistic.py(339):     n_classes = Y.shape[1]
0.96 logistic.py(340):     n_features = X.shape[1]
0.96 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
0.96 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
0.96 logistic.py(343):                     dtype=X.dtype)
0.96 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
0.96 logistic.py(282):     n_classes = Y.shape[1]
0.96 logistic.py(283):     n_features = X.shape[1]
0.96 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
0.96 logistic.py(285):     w = w.reshape(n_classes, -1)
0.96 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
0.96 logistic.py(287):     if fit_intercept:
0.96 logistic.py(288):         intercept = w[:, -1]
0.96 logistic.py(289):         w = w[:, :-1]
0.96 logistic.py(292):     p = safe_sparse_dot(X, w.T)
0.96 logistic.py(293):     p += intercept
0.96 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
0.96 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
0.96 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
0.96 logistic.py(297):     p = np.exp(p, p)
0.96 logistic.py(298):     return loss, p, w
0.96 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
0.96 logistic.py(346):     diff = sample_weight * (p - Y)
0.96 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
0.96 logistic.py(348):     grad[:, :n_features] += alpha * w
0.96 logistic.py(349):     if fit_intercept:
0.96 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
0.96 logistic.py(351):     return loss, grad.ravel(), p
0.96 logistic.py(718):             if info["warnflag"] == 1:
0.96 logistic.py(723):             n_iter_i = min(info['nit'], max_iter)
0.96 logistic.py(760):         if multi_class == 'multinomial':
0.96 logistic.py(761):             multi_w0 = np.reshape(w0, (classes.size, -1))
0.96 logistic.py(762):             if classes.size == 2:
0.96 logistic.py(764):             coefs.append(multi_w0)
0.96 logistic.py(768):         n_iter[i] = n_iter_i
0.96 logistic.py(712):     for i, C in enumerate(Cs):
0.96 logistic.py(713):         if solver == 'lbfgs':
0.96 logistic.py(714):             w0, loss, info = optimize.fmin_l_bfgs_b(
0.96 logistic.py(715):                 func, w0, fprime=None,
0.96 logistic.py(716):                 args=(X, target, 1. / C, sample_weight),
0.96 logistic.py(717):                 iprint=(verbose > 0) - 1, pgtol=tol, maxiter=max_iter)
0.96 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
0.96 logistic.py(339):     n_classes = Y.shape[1]
0.96 logistic.py(340):     n_features = X.shape[1]
0.96 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
0.96 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
0.96 logistic.py(343):                     dtype=X.dtype)
0.96 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
0.96 logistic.py(282):     n_classes = Y.shape[1]
0.96 logistic.py(283):     n_features = X.shape[1]
0.96 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
0.96 logistic.py(285):     w = w.reshape(n_classes, -1)
0.96 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
0.96 logistic.py(287):     if fit_intercept:
0.96 logistic.py(288):         intercept = w[:, -1]
0.96 logistic.py(289):         w = w[:, :-1]
0.96 logistic.py(292):     p = safe_sparse_dot(X, w.T)
0.96 logistic.py(293):     p += intercept
0.96 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
0.96 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
0.96 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
0.96 logistic.py(297):     p = np.exp(p, p)
0.96 logistic.py(298):     return loss, p, w
0.96 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
0.96 logistic.py(346):     diff = sample_weight * (p - Y)
0.96 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
0.96 logistic.py(348):     grad[:, :n_features] += alpha * w
0.96 logistic.py(349):     if fit_intercept:
0.96 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
0.96 logistic.py(351):     return loss, grad.ravel(), p
0.96 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
0.96 logistic.py(339):     n_classes = Y.shape[1]
0.96 logistic.py(340):     n_features = X.shape[1]
0.96 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
0.96 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
0.96 logistic.py(343):                     dtype=X.dtype)
0.96 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
0.96 logistic.py(282):     n_classes = Y.shape[1]
0.96 logistic.py(283):     n_features = X.shape[1]
0.96 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
0.96 logistic.py(285):     w = w.reshape(n_classes, -1)
0.96 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
0.96 logistic.py(287):     if fit_intercept:
0.96 logistic.py(288):         intercept = w[:, -1]
0.96 logistic.py(289):         w = w[:, :-1]
0.96 logistic.py(292):     p = safe_sparse_dot(X, w.T)
0.96 logistic.py(293):     p += intercept
0.96 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
0.96 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
0.96 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
0.96 logistic.py(297):     p = np.exp(p, p)
0.96 logistic.py(298):     return loss, p, w
0.96 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
0.96 logistic.py(346):     diff = sample_weight * (p - Y)
0.96 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
0.96 logistic.py(348):     grad[:, :n_features] += alpha * w
0.96 logistic.py(349):     if fit_intercept:
0.96 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
0.96 logistic.py(351):     return loss, grad.ravel(), p
0.96 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
0.96 logistic.py(339):     n_classes = Y.shape[1]
0.96 logistic.py(340):     n_features = X.shape[1]
0.96 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
0.96 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
0.96 logistic.py(343):                     dtype=X.dtype)
0.96 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
0.96 logistic.py(282):     n_classes = Y.shape[1]
0.96 logistic.py(283):     n_features = X.shape[1]
0.96 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
0.96 logistic.py(285):     w = w.reshape(n_classes, -1)
0.96 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
0.96 logistic.py(287):     if fit_intercept:
0.96 logistic.py(288):         intercept = w[:, -1]
0.96 logistic.py(289):         w = w[:, :-1]
0.96 logistic.py(292):     p = safe_sparse_dot(X, w.T)
0.96 logistic.py(293):     p += intercept
0.96 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
0.96 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
0.96 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
0.96 logistic.py(297):     p = np.exp(p, p)
0.96 logistic.py(298):     return loss, p, w
0.96 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
0.96 logistic.py(346):     diff = sample_weight * (p - Y)
0.96 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
0.96 logistic.py(348):     grad[:, :n_features] += alpha * w
0.96 logistic.py(349):     if fit_intercept:
0.96 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
0.96 logistic.py(351):     return loss, grad.ravel(), p
0.96 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
0.96 logistic.py(339):     n_classes = Y.shape[1]
0.96 logistic.py(340):     n_features = X.shape[1]
0.96 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
0.96 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
0.96 logistic.py(343):                     dtype=X.dtype)
0.96 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
0.96 logistic.py(282):     n_classes = Y.shape[1]
0.96 logistic.py(283):     n_features = X.shape[1]
0.96 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
0.96 logistic.py(285):     w = w.reshape(n_classes, -1)
0.96 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
0.96 logistic.py(287):     if fit_intercept:
0.96 logistic.py(288):         intercept = w[:, -1]
0.96 logistic.py(289):         w = w[:, :-1]
0.96 logistic.py(292):     p = safe_sparse_dot(X, w.T)
0.96 logistic.py(293):     p += intercept
0.96 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
0.96 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
0.96 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
0.96 logistic.py(297):     p = np.exp(p, p)
0.96 logistic.py(298):     return loss, p, w
0.96 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
0.96 logistic.py(346):     diff = sample_weight * (p - Y)
0.96 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
0.96 logistic.py(348):     grad[:, :n_features] += alpha * w
0.96 logistic.py(349):     if fit_intercept:
0.96 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
0.96 logistic.py(351):     return loss, grad.ravel(), p
0.96 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
0.96 logistic.py(339):     n_classes = Y.shape[1]
0.96 logistic.py(340):     n_features = X.shape[1]
0.96 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
0.96 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
0.96 logistic.py(343):                     dtype=X.dtype)
0.96 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
0.96 logistic.py(282):     n_classes = Y.shape[1]
0.96 logistic.py(283):     n_features = X.shape[1]
0.96 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
0.96 logistic.py(285):     w = w.reshape(n_classes, -1)
0.96 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
0.96 logistic.py(287):     if fit_intercept:
0.96 logistic.py(288):         intercept = w[:, -1]
0.96 logistic.py(289):         w = w[:, :-1]
0.96 logistic.py(292):     p = safe_sparse_dot(X, w.T)
0.96 logistic.py(293):     p += intercept
0.96 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
0.96 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
0.96 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
0.96 logistic.py(297):     p = np.exp(p, p)
0.96 logistic.py(298):     return loss, p, w
0.96 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
0.96 logistic.py(346):     diff = sample_weight * (p - Y)
0.96 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
0.96 logistic.py(348):     grad[:, :n_features] += alpha * w
0.96 logistic.py(349):     if fit_intercept:
0.96 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
0.96 logistic.py(351):     return loss, grad.ravel(), p
0.96 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
0.96 logistic.py(339):     n_classes = Y.shape[1]
0.96 logistic.py(340):     n_features = X.shape[1]
0.96 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
0.96 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
0.96 logistic.py(343):                     dtype=X.dtype)
0.96 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
0.96 logistic.py(282):     n_classes = Y.shape[1]
0.96 logistic.py(283):     n_features = X.shape[1]
0.96 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
0.96 logistic.py(285):     w = w.reshape(n_classes, -1)
0.96 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
0.96 logistic.py(287):     if fit_intercept:
0.96 logistic.py(288):         intercept = w[:, -1]
0.96 logistic.py(289):         w = w[:, :-1]
0.96 logistic.py(292):     p = safe_sparse_dot(X, w.T)
0.96 logistic.py(293):     p += intercept
0.96 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
0.96 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
0.96 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
0.96 logistic.py(297):     p = np.exp(p, p)
0.96 logistic.py(298):     return loss, p, w
0.96 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
0.96 logistic.py(346):     diff = sample_weight * (p - Y)
0.96 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
0.96 logistic.py(348):     grad[:, :n_features] += alpha * w
0.96 logistic.py(349):     if fit_intercept:
0.96 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
0.96 logistic.py(351):     return loss, grad.ravel(), p
0.96 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
0.96 logistic.py(339):     n_classes = Y.shape[1]
0.96 logistic.py(340):     n_features = X.shape[1]
0.96 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
0.96 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
0.96 logistic.py(343):                     dtype=X.dtype)
0.96 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
0.96 logistic.py(282):     n_classes = Y.shape[1]
0.96 logistic.py(283):     n_features = X.shape[1]
0.96 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
0.96 logistic.py(285):     w = w.reshape(n_classes, -1)
0.96 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
0.96 logistic.py(287):     if fit_intercept:
0.96 logistic.py(288):         intercept = w[:, -1]
0.96 logistic.py(289):         w = w[:, :-1]
0.96 logistic.py(292):     p = safe_sparse_dot(X, w.T)
0.96 logistic.py(293):     p += intercept
0.96 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
0.96 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
0.97 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
0.97 logistic.py(297):     p = np.exp(p, p)
0.97 logistic.py(298):     return loss, p, w
0.97 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
0.97 logistic.py(346):     diff = sample_weight * (p - Y)
0.97 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
0.97 logistic.py(348):     grad[:, :n_features] += alpha * w
0.97 logistic.py(349):     if fit_intercept:
0.97 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
0.97 logistic.py(351):     return loss, grad.ravel(), p
0.97 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
0.97 logistic.py(339):     n_classes = Y.shape[1]
0.97 logistic.py(340):     n_features = X.shape[1]
0.97 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
0.97 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
0.97 logistic.py(343):                     dtype=X.dtype)
0.97 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
0.97 logistic.py(282):     n_classes = Y.shape[1]
0.97 logistic.py(283):     n_features = X.shape[1]
0.97 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
0.97 logistic.py(285):     w = w.reshape(n_classes, -1)
0.97 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
0.97 logistic.py(287):     if fit_intercept:
0.97 logistic.py(288):         intercept = w[:, -1]
0.97 logistic.py(289):         w = w[:, :-1]
0.97 logistic.py(292):     p = safe_sparse_dot(X, w.T)
0.97 logistic.py(293):     p += intercept
0.97 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
0.97 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
0.97 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
0.97 logistic.py(297):     p = np.exp(p, p)
0.97 logistic.py(298):     return loss, p, w
0.97 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
0.97 logistic.py(346):     diff = sample_weight * (p - Y)
0.97 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
0.97 logistic.py(348):     grad[:, :n_features] += alpha * w
0.97 logistic.py(349):     if fit_intercept:
0.97 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
0.97 logistic.py(351):     return loss, grad.ravel(), p
0.97 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
0.97 logistic.py(339):     n_classes = Y.shape[1]
0.97 logistic.py(340):     n_features = X.shape[1]
0.97 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
0.97 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
0.97 logistic.py(343):                     dtype=X.dtype)
0.97 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
0.97 logistic.py(282):     n_classes = Y.shape[1]
0.97 logistic.py(283):     n_features = X.shape[1]
0.97 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
0.97 logistic.py(285):     w = w.reshape(n_classes, -1)
0.97 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
0.97 logistic.py(287):     if fit_intercept:
0.97 logistic.py(288):         intercept = w[:, -1]
0.97 logistic.py(289):         w = w[:, :-1]
0.97 logistic.py(292):     p = safe_sparse_dot(X, w.T)
0.97 logistic.py(293):     p += intercept
0.97 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
0.97 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
0.97 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
0.97 logistic.py(297):     p = np.exp(p, p)
0.97 logistic.py(298):     return loss, p, w
0.97 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
0.97 logistic.py(346):     diff = sample_weight * (p - Y)
0.97 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
0.97 logistic.py(348):     grad[:, :n_features] += alpha * w
0.97 logistic.py(349):     if fit_intercept:
0.97 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
0.97 logistic.py(351):     return loss, grad.ravel(), p
0.97 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
0.97 logistic.py(339):     n_classes = Y.shape[1]
0.97 logistic.py(340):     n_features = X.shape[1]
0.97 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
0.97 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
0.97 logistic.py(343):                     dtype=X.dtype)
0.97 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
0.97 logistic.py(282):     n_classes = Y.shape[1]
0.97 logistic.py(283):     n_features = X.shape[1]
0.97 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
0.97 logistic.py(285):     w = w.reshape(n_classes, -1)
0.97 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
0.97 logistic.py(287):     if fit_intercept:
0.97 logistic.py(288):         intercept = w[:, -1]
0.97 logistic.py(289):         w = w[:, :-1]
0.97 logistic.py(292):     p = safe_sparse_dot(X, w.T)
0.97 logistic.py(293):     p += intercept
0.97 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
0.97 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
0.97 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
0.97 logistic.py(297):     p = np.exp(p, p)
0.97 logistic.py(298):     return loss, p, w
0.97 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
0.97 logistic.py(346):     diff = sample_weight * (p - Y)
0.97 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
0.97 logistic.py(348):     grad[:, :n_features] += alpha * w
0.97 logistic.py(349):     if fit_intercept:
0.97 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
0.97 logistic.py(351):     return loss, grad.ravel(), p
0.97 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
0.97 logistic.py(339):     n_classes = Y.shape[1]
0.97 logistic.py(340):     n_features = X.shape[1]
0.97 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
0.97 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
0.97 logistic.py(343):                     dtype=X.dtype)
0.97 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
0.97 logistic.py(282):     n_classes = Y.shape[1]
0.97 logistic.py(283):     n_features = X.shape[1]
0.97 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
0.97 logistic.py(285):     w = w.reshape(n_classes, -1)
0.97 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
0.97 logistic.py(287):     if fit_intercept:
0.97 logistic.py(288):         intercept = w[:, -1]
0.97 logistic.py(289):         w = w[:, :-1]
0.97 logistic.py(292):     p = safe_sparse_dot(X, w.T)
0.97 logistic.py(293):     p += intercept
0.97 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
0.97 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
0.97 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
0.97 logistic.py(297):     p = np.exp(p, p)
0.97 logistic.py(298):     return loss, p, w
0.97 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
0.97 logistic.py(346):     diff = sample_weight * (p - Y)
0.97 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
0.97 logistic.py(348):     grad[:, :n_features] += alpha * w
0.97 logistic.py(349):     if fit_intercept:
0.97 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
0.97 logistic.py(351):     return loss, grad.ravel(), p
0.97 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
0.97 logistic.py(339):     n_classes = Y.shape[1]
0.97 logistic.py(340):     n_features = X.shape[1]
0.97 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
0.97 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
0.97 logistic.py(343):                     dtype=X.dtype)
0.97 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
0.97 logistic.py(282):     n_classes = Y.shape[1]
0.97 logistic.py(283):     n_features = X.shape[1]
0.97 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
0.97 logistic.py(285):     w = w.reshape(n_classes, -1)
0.97 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
0.97 logistic.py(287):     if fit_intercept:
0.97 logistic.py(288):         intercept = w[:, -1]
0.97 logistic.py(289):         w = w[:, :-1]
0.97 logistic.py(292):     p = safe_sparse_dot(X, w.T)
0.97 logistic.py(293):     p += intercept
0.97 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
0.97 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
0.97 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
0.97 logistic.py(297):     p = np.exp(p, p)
0.97 logistic.py(298):     return loss, p, w
0.97 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
0.97 logistic.py(346):     diff = sample_weight * (p - Y)
0.97 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
0.97 logistic.py(348):     grad[:, :n_features] += alpha * w
0.97 logistic.py(349):     if fit_intercept:
0.97 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
0.97 logistic.py(351):     return loss, grad.ravel(), p
0.97 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
0.97 logistic.py(339):     n_classes = Y.shape[1]
0.97 logistic.py(340):     n_features = X.shape[1]
0.97 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
0.97 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
0.97 logistic.py(343):                     dtype=X.dtype)
0.97 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
0.97 logistic.py(282):     n_classes = Y.shape[1]
0.97 logistic.py(283):     n_features = X.shape[1]
0.97 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
0.97 logistic.py(285):     w = w.reshape(n_classes, -1)
0.97 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
0.97 logistic.py(287):     if fit_intercept:
0.97 logistic.py(288):         intercept = w[:, -1]
0.97 logistic.py(289):         w = w[:, :-1]
0.97 logistic.py(292):     p = safe_sparse_dot(X, w.T)
0.97 logistic.py(293):     p += intercept
0.97 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
0.97 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
0.97 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
0.97 logistic.py(297):     p = np.exp(p, p)
0.97 logistic.py(298):     return loss, p, w
0.97 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
0.97 logistic.py(346):     diff = sample_weight * (p - Y)
0.97 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
0.97 logistic.py(348):     grad[:, :n_features] += alpha * w
0.97 logistic.py(349):     if fit_intercept:
0.97 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
0.97 logistic.py(351):     return loss, grad.ravel(), p
0.97 logistic.py(718):             if info["warnflag"] == 1:
0.97 logistic.py(723):             n_iter_i = min(info['nit'], max_iter)
0.97 logistic.py(760):         if multi_class == 'multinomial':
0.97 logistic.py(761):             multi_w0 = np.reshape(w0, (classes.size, -1))
0.97 logistic.py(762):             if classes.size == 2:
0.97 logistic.py(764):             coefs.append(multi_w0)
0.97 logistic.py(768):         n_iter[i] = n_iter_i
0.97 logistic.py(712):     for i, C in enumerate(Cs):
0.97 logistic.py(713):         if solver == 'lbfgs':
0.97 logistic.py(714):             w0, loss, info = optimize.fmin_l_bfgs_b(
0.97 logistic.py(715):                 func, w0, fprime=None,
0.97 logistic.py(716):                 args=(X, target, 1. / C, sample_weight),
0.97 logistic.py(717):                 iprint=(verbose > 0) - 1, pgtol=tol, maxiter=max_iter)
0.97 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
0.97 logistic.py(339):     n_classes = Y.shape[1]
0.97 logistic.py(340):     n_features = X.shape[1]
0.97 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
0.97 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
0.97 logistic.py(343):                     dtype=X.dtype)
0.97 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
0.97 logistic.py(282):     n_classes = Y.shape[1]
0.97 logistic.py(283):     n_features = X.shape[1]
0.97 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
0.97 logistic.py(285):     w = w.reshape(n_classes, -1)
0.97 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
0.97 logistic.py(287):     if fit_intercept:
0.97 logistic.py(288):         intercept = w[:, -1]
0.97 logistic.py(289):         w = w[:, :-1]
0.97 logistic.py(292):     p = safe_sparse_dot(X, w.T)
0.97 logistic.py(293):     p += intercept
0.97 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
0.97 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
0.97 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
0.97 logistic.py(297):     p = np.exp(p, p)
0.97 logistic.py(298):     return loss, p, w
0.97 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
0.97 logistic.py(346):     diff = sample_weight * (p - Y)
0.97 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
0.97 logistic.py(348):     grad[:, :n_features] += alpha * w
0.97 logistic.py(349):     if fit_intercept:
0.97 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
0.97 logistic.py(351):     return loss, grad.ravel(), p
0.97 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
0.97 logistic.py(339):     n_classes = Y.shape[1]
0.97 logistic.py(340):     n_features = X.shape[1]
0.97 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
0.97 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
0.97 logistic.py(343):                     dtype=X.dtype)
0.97 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
0.97 logistic.py(282):     n_classes = Y.shape[1]
0.97 logistic.py(283):     n_features = X.shape[1]
0.97 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
0.97 logistic.py(285):     w = w.reshape(n_classes, -1)
0.97 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
0.97 logistic.py(287):     if fit_intercept:
0.97 logistic.py(288):         intercept = w[:, -1]
0.97 logistic.py(289):         w = w[:, :-1]
0.97 logistic.py(292):     p = safe_sparse_dot(X, w.T)
0.97 logistic.py(293):     p += intercept
0.97 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
0.97 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
0.97 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
0.97 logistic.py(297):     p = np.exp(p, p)
0.97 logistic.py(298):     return loss, p, w
0.97 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
0.97 logistic.py(346):     diff = sample_weight * (p - Y)
0.97 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
0.97 logistic.py(348):     grad[:, :n_features] += alpha * w
0.97 logistic.py(349):     if fit_intercept:
0.97 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
0.97 logistic.py(351):     return loss, grad.ravel(), p
0.97 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
0.97 logistic.py(339):     n_classes = Y.shape[1]
0.97 logistic.py(340):     n_features = X.shape[1]
0.97 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
0.97 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
0.97 logistic.py(343):                     dtype=X.dtype)
0.97 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
0.97 logistic.py(282):     n_classes = Y.shape[1]
0.97 logistic.py(283):     n_features = X.shape[1]
0.97 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
0.97 logistic.py(285):     w = w.reshape(n_classes, -1)
0.97 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
0.97 logistic.py(287):     if fit_intercept:
0.97 logistic.py(288):         intercept = w[:, -1]
0.97 logistic.py(289):         w = w[:, :-1]
0.97 logistic.py(292):     p = safe_sparse_dot(X, w.T)
0.97 logistic.py(293):     p += intercept
0.97 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
0.97 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
0.97 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
0.97 logistic.py(297):     p = np.exp(p, p)
0.97 logistic.py(298):     return loss, p, w
0.97 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
0.97 logistic.py(346):     diff = sample_weight * (p - Y)
0.97 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
0.97 logistic.py(348):     grad[:, :n_features] += alpha * w
0.97 logistic.py(349):     if fit_intercept:
0.97 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
0.97 logistic.py(351):     return loss, grad.ravel(), p
0.97 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
0.97 logistic.py(339):     n_classes = Y.shape[1]
0.97 logistic.py(340):     n_features = X.shape[1]
0.97 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
0.97 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
0.97 logistic.py(343):                     dtype=X.dtype)
0.97 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
0.97 logistic.py(282):     n_classes = Y.shape[1]
0.97 logistic.py(283):     n_features = X.shape[1]
0.97 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
0.97 logistic.py(285):     w = w.reshape(n_classes, -1)
0.97 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
0.97 logistic.py(287):     if fit_intercept:
0.97 logistic.py(288):         intercept = w[:, -1]
0.97 logistic.py(289):         w = w[:, :-1]
0.97 logistic.py(292):     p = safe_sparse_dot(X, w.T)
0.97 logistic.py(293):     p += intercept
0.97 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
0.97 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
0.97 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
0.97 logistic.py(297):     p = np.exp(p, p)
0.97 logistic.py(298):     return loss, p, w
0.97 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
0.97 logistic.py(346):     diff = sample_weight * (p - Y)
0.97 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
0.97 logistic.py(348):     grad[:, :n_features] += alpha * w
0.97 logistic.py(349):     if fit_intercept:
0.97 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
0.97 logistic.py(351):     return loss, grad.ravel(), p
0.97 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
0.97 logistic.py(339):     n_classes = Y.shape[1]
0.97 logistic.py(340):     n_features = X.shape[1]
0.97 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
0.97 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
0.97 logistic.py(343):                     dtype=X.dtype)
0.97 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
0.97 logistic.py(282):     n_classes = Y.shape[1]
0.97 logistic.py(283):     n_features = X.shape[1]
0.97 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
0.97 logistic.py(285):     w = w.reshape(n_classes, -1)
0.97 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
0.97 logistic.py(287):     if fit_intercept:
0.97 logistic.py(288):         intercept = w[:, -1]
0.97 logistic.py(289):         w = w[:, :-1]
0.97 logistic.py(292):     p = safe_sparse_dot(X, w.T)
0.97 logistic.py(293):     p += intercept
0.97 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
0.97 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
0.97 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
0.97 logistic.py(297):     p = np.exp(p, p)
0.97 logistic.py(298):     return loss, p, w
0.97 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
0.97 logistic.py(346):     diff = sample_weight * (p - Y)
0.97 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
0.97 logistic.py(348):     grad[:, :n_features] += alpha * w
0.97 logistic.py(349):     if fit_intercept:
0.97 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
0.97 logistic.py(351):     return loss, grad.ravel(), p
0.97 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
0.97 logistic.py(339):     n_classes = Y.shape[1]
0.97 logistic.py(340):     n_features = X.shape[1]
0.97 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
0.97 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
0.97 logistic.py(343):                     dtype=X.dtype)
0.97 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
0.97 logistic.py(282):     n_classes = Y.shape[1]
0.97 logistic.py(283):     n_features = X.shape[1]
0.97 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
0.97 logistic.py(285):     w = w.reshape(n_classes, -1)
0.97 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
0.97 logistic.py(287):     if fit_intercept:
0.97 logistic.py(288):         intercept = w[:, -1]
0.97 logistic.py(289):         w = w[:, :-1]
0.97 logistic.py(292):     p = safe_sparse_dot(X, w.T)
0.97 logistic.py(293):     p += intercept
0.97 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
0.97 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
0.97 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
0.97 logistic.py(297):     p = np.exp(p, p)
0.97 logistic.py(298):     return loss, p, w
0.97 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
0.97 logistic.py(346):     diff = sample_weight * (p - Y)
0.97 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
0.97 logistic.py(348):     grad[:, :n_features] += alpha * w
0.97 logistic.py(349):     if fit_intercept:
0.97 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
0.97 logistic.py(351):     return loss, grad.ravel(), p
0.97 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
0.97 logistic.py(339):     n_classes = Y.shape[1]
0.97 logistic.py(340):     n_features = X.shape[1]
0.97 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
0.97 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
0.97 logistic.py(343):                     dtype=X.dtype)
0.97 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
0.97 logistic.py(282):     n_classes = Y.shape[1]
0.97 logistic.py(283):     n_features = X.shape[1]
0.97 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
0.97 logistic.py(285):     w = w.reshape(n_classes, -1)
0.97 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
0.97 logistic.py(287):     if fit_intercept:
0.97 logistic.py(288):         intercept = w[:, -1]
0.97 logistic.py(289):         w = w[:, :-1]
0.97 logistic.py(292):     p = safe_sparse_dot(X, w.T)
0.97 logistic.py(293):     p += intercept
0.97 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
0.97 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
0.97 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
0.97 logistic.py(297):     p = np.exp(p, p)
0.97 logistic.py(298):     return loss, p, w
0.97 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
0.97 logistic.py(346):     diff = sample_weight * (p - Y)
0.97 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
0.97 logistic.py(348):     grad[:, :n_features] += alpha * w
0.97 logistic.py(349):     if fit_intercept:
0.97 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
0.97 logistic.py(351):     return loss, grad.ravel(), p
0.97 logistic.py(718):             if info["warnflag"] == 1:
0.97 logistic.py(723):             n_iter_i = min(info['nit'], max_iter)
0.97 logistic.py(760):         if multi_class == 'multinomial':
0.97 logistic.py(761):             multi_w0 = np.reshape(w0, (classes.size, -1))
0.97 logistic.py(762):             if classes.size == 2:
0.97 logistic.py(764):             coefs.append(multi_w0)
0.97 logistic.py(768):         n_iter[i] = n_iter_i
0.97 logistic.py(712):     for i, C in enumerate(Cs):
0.97 logistic.py(713):         if solver == 'lbfgs':
0.97 logistic.py(714):             w0, loss, info = optimize.fmin_l_bfgs_b(
0.97 logistic.py(715):                 func, w0, fprime=None,
0.97 logistic.py(716):                 args=(X, target, 1. / C, sample_weight),
0.97 logistic.py(717):                 iprint=(verbose > 0) - 1, pgtol=tol, maxiter=max_iter)
0.97 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
0.97 logistic.py(339):     n_classes = Y.shape[1]
0.97 logistic.py(340):     n_features = X.shape[1]
0.97 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
0.97 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
0.97 logistic.py(343):                     dtype=X.dtype)
0.97 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
0.97 logistic.py(282):     n_classes = Y.shape[1]
0.97 logistic.py(283):     n_features = X.shape[1]
0.97 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
0.97 logistic.py(285):     w = w.reshape(n_classes, -1)
0.97 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
0.97 logistic.py(287):     if fit_intercept:
0.97 logistic.py(288):         intercept = w[:, -1]
0.97 logistic.py(289):         w = w[:, :-1]
0.97 logistic.py(292):     p = safe_sparse_dot(X, w.T)
0.97 logistic.py(293):     p += intercept
0.97 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
0.97 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
0.97 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
0.97 logistic.py(297):     p = np.exp(p, p)
0.97 logistic.py(298):     return loss, p, w
0.97 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
0.97 logistic.py(346):     diff = sample_weight * (p - Y)
0.97 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
0.98 logistic.py(348):     grad[:, :n_features] += alpha * w
0.98 logistic.py(349):     if fit_intercept:
0.98 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
0.98 logistic.py(351):     return loss, grad.ravel(), p
0.98 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
0.98 logistic.py(339):     n_classes = Y.shape[1]
0.98 logistic.py(340):     n_features = X.shape[1]
0.98 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
0.98 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
0.98 logistic.py(343):                     dtype=X.dtype)
0.98 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
0.98 logistic.py(282):     n_classes = Y.shape[1]
0.98 logistic.py(283):     n_features = X.shape[1]
0.98 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
0.98 logistic.py(285):     w = w.reshape(n_classes, -1)
0.98 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
0.98 logistic.py(287):     if fit_intercept:
0.98 logistic.py(288):         intercept = w[:, -1]
0.98 logistic.py(289):         w = w[:, :-1]
0.98 logistic.py(292):     p = safe_sparse_dot(X, w.T)
0.98 logistic.py(293):     p += intercept
0.98 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
0.98 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
0.98 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
0.98 logistic.py(297):     p = np.exp(p, p)
0.98 logistic.py(298):     return loss, p, w
0.98 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
0.98 logistic.py(346):     diff = sample_weight * (p - Y)
0.98 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
0.98 logistic.py(348):     grad[:, :n_features] += alpha * w
0.98 logistic.py(349):     if fit_intercept:
0.98 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
0.98 logistic.py(351):     return loss, grad.ravel(), p
0.98 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
0.98 logistic.py(339):     n_classes = Y.shape[1]
0.98 logistic.py(340):     n_features = X.shape[1]
0.98 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
0.98 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
0.98 logistic.py(343):                     dtype=X.dtype)
0.98 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
0.98 logistic.py(282):     n_classes = Y.shape[1]
0.98 logistic.py(283):     n_features = X.shape[1]
0.98 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
0.98 logistic.py(285):     w = w.reshape(n_classes, -1)
0.98 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
0.98 logistic.py(287):     if fit_intercept:
0.98 logistic.py(288):         intercept = w[:, -1]
0.98 logistic.py(289):         w = w[:, :-1]
0.98 logistic.py(292):     p = safe_sparse_dot(X, w.T)
0.98 logistic.py(293):     p += intercept
0.98 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
0.98 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
0.98 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
0.98 logistic.py(297):     p = np.exp(p, p)
0.98 logistic.py(298):     return loss, p, w
0.98 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
0.98 logistic.py(346):     diff = sample_weight * (p - Y)
0.98 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
0.98 logistic.py(348):     grad[:, :n_features] += alpha * w
0.98 logistic.py(349):     if fit_intercept:
0.98 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
0.98 logistic.py(351):     return loss, grad.ravel(), p
0.98 logistic.py(718):             if info["warnflag"] == 1:
0.98 logistic.py(723):             n_iter_i = min(info['nit'], max_iter)
0.98 logistic.py(760):         if multi_class == 'multinomial':
0.98 logistic.py(761):             multi_w0 = np.reshape(w0, (classes.size, -1))
0.98 logistic.py(762):             if classes.size == 2:
0.98 logistic.py(764):             coefs.append(multi_w0)
0.98 logistic.py(768):         n_iter[i] = n_iter_i
0.98 logistic.py(712):     for i, C in enumerate(Cs):
0.98 logistic.py(770):     return coefs, np.array(Cs), n_iter
0.98 logistic.py(925):     log_reg = LogisticRegression(multi_class=multi_class)
0.98 logistic.py(1171):         self.penalty = penalty
0.98 logistic.py(1172):         self.dual = dual
0.98 logistic.py(1173):         self.tol = tol
0.98 logistic.py(1174):         self.C = C
0.98 logistic.py(1175):         self.fit_intercept = fit_intercept
0.98 logistic.py(1176):         self.intercept_scaling = intercept_scaling
0.98 logistic.py(1177):         self.class_weight = class_weight
0.98 logistic.py(1178):         self.random_state = random_state
0.98 logistic.py(1179):         self.solver = solver
0.98 logistic.py(1180):         self.max_iter = max_iter
0.98 logistic.py(1181):         self.multi_class = multi_class
0.98 logistic.py(1182):         self.verbose = verbose
0.98 logistic.py(1183):         self.warm_start = warm_start
0.98 logistic.py(1184):         self.n_jobs = n_jobs
0.98 logistic.py(928):     if multi_class == 'ovr':
0.98 logistic.py(930):     elif multi_class == 'multinomial':
0.98 logistic.py(931):         log_reg.classes_ = np.unique(y_train)
0.98 logistic.py(936):     if pos_class is not None:
0.98 logistic.py(941):     scores = list()
0.98 logistic.py(943):     if isinstance(scoring, six.string_types):
0.98 logistic.py(945):     for w in coefs:
0.98 logistic.py(946):         if multi_class == 'ovr':
0.98 logistic.py(948):         if fit_intercept:
0.98 logistic.py(949):             log_reg.coef_ = w[:, :-1]
0.98 logistic.py(950):             log_reg.intercept_ = w[:, -1]
0.98 logistic.py(955):         if scoring is None:
0.98 logistic.py(956):             scores.append(log_reg.score(X_test, y_test))
0.98 logistic.py(945):     for w in coefs:
0.98 logistic.py(946):         if multi_class == 'ovr':
0.98 logistic.py(948):         if fit_intercept:
0.98 logistic.py(949):             log_reg.coef_ = w[:, :-1]
0.98 logistic.py(950):             log_reg.intercept_ = w[:, -1]
0.98 logistic.py(955):         if scoring is None:
0.98 logistic.py(956):             scores.append(log_reg.score(X_test, y_test))
0.98 logistic.py(945):     for w in coefs:
0.98 logistic.py(946):         if multi_class == 'ovr':
0.98 logistic.py(948):         if fit_intercept:
0.98 logistic.py(949):             log_reg.coef_ = w[:, :-1]
0.98 logistic.py(950):             log_reg.intercept_ = w[:, -1]
0.98 logistic.py(955):         if scoring is None:
0.98 logistic.py(956):             scores.append(log_reg.score(X_test, y_test))
0.98 logistic.py(945):     for w in coefs:
0.98 logistic.py(946):         if multi_class == 'ovr':
0.98 logistic.py(948):         if fit_intercept:
0.98 logistic.py(949):             log_reg.coef_ = w[:, :-1]
0.98 logistic.py(950):             log_reg.intercept_ = w[:, -1]
0.98 logistic.py(955):         if scoring is None:
0.98 logistic.py(956):             scores.append(log_reg.score(X_test, y_test))
0.98 logistic.py(945):     for w in coefs:
0.98 logistic.py(946):         if multi_class == 'ovr':
0.98 logistic.py(948):         if fit_intercept:
0.98 logistic.py(949):             log_reg.coef_ = w[:, :-1]
0.98 logistic.py(950):             log_reg.intercept_ = w[:, -1]
0.98 logistic.py(955):         if scoring is None:
0.98 logistic.py(956):             scores.append(log_reg.score(X_test, y_test))
0.98 logistic.py(945):     for w in coefs:
0.98 logistic.py(946):         if multi_class == 'ovr':
0.98 logistic.py(948):         if fit_intercept:
0.98 logistic.py(949):             log_reg.coef_ = w[:, :-1]
0.98 logistic.py(950):             log_reg.intercept_ = w[:, -1]
0.98 logistic.py(955):         if scoring is None:
0.98 logistic.py(956):             scores.append(log_reg.score(X_test, y_test))
0.98 logistic.py(945):     for w in coefs:
0.98 logistic.py(946):         if multi_class == 'ovr':
0.98 logistic.py(948):         if fit_intercept:
0.98 logistic.py(949):             log_reg.coef_ = w[:, :-1]
0.98 logistic.py(950):             log_reg.intercept_ = w[:, -1]
0.98 logistic.py(955):         if scoring is None:
0.98 logistic.py(956):             scores.append(log_reg.score(X_test, y_test))
0.98 logistic.py(945):     for w in coefs:
0.98 logistic.py(946):         if multi_class == 'ovr':
0.98 logistic.py(948):         if fit_intercept:
0.98 logistic.py(949):             log_reg.coef_ = w[:, :-1]
0.98 logistic.py(950):             log_reg.intercept_ = w[:, -1]
0.98 logistic.py(955):         if scoring is None:
0.98 logistic.py(956):             scores.append(log_reg.score(X_test, y_test))
0.98 logistic.py(945):     for w in coefs:
0.98 logistic.py(946):         if multi_class == 'ovr':
0.98 logistic.py(948):         if fit_intercept:
0.98 logistic.py(949):             log_reg.coef_ = w[:, :-1]
0.98 logistic.py(950):             log_reg.intercept_ = w[:, -1]
0.98 logistic.py(955):         if scoring is None:
0.98 logistic.py(956):             scores.append(log_reg.score(X_test, y_test))
0.98 logistic.py(945):     for w in coefs:
0.98 logistic.py(946):         if multi_class == 'ovr':
0.98 logistic.py(948):         if fit_intercept:
0.98 logistic.py(949):             log_reg.coef_ = w[:, :-1]
0.98 logistic.py(950):             log_reg.intercept_ = w[:, -1]
0.98 logistic.py(955):         if scoring is None:
0.98 logistic.py(956):             scores.append(log_reg.score(X_test, y_test))
0.98 logistic.py(945):     for w in coefs:
0.98 logistic.py(959):     return coefs, Cs, np.array(scores), n_iter
0.98 logistic.py(1703):             for train, test in folds)
0.98 logistic.py(903):     _check_solver_option(solver, multi_class, penalty, dual)
0.98 logistic.py(428):     if solver not in ['liblinear', 'newton-cg', 'lbfgs', 'sag', 'saga']:
0.98 logistic.py(433):     if multi_class not in ['multinomial', 'ovr']:
0.98 logistic.py(437):     if multi_class == 'multinomial' and solver == 'liblinear':
0.98 logistic.py(441):     if solver not in ['liblinear', 'saga']:
0.98 logistic.py(442):         if penalty != 'l2':
0.98 logistic.py(445):     if solver != 'liblinear':
0.98 logistic.py(446):         if dual:
0.98 logistic.py(905):     X_train = X[train]
0.98 logistic.py(906):     X_test = X[test]
0.98 logistic.py(907):     y_train = y[train]
0.98 logistic.py(908):     y_test = y[test]
0.98 logistic.py(910):     if sample_weight is not None:
0.98 logistic.py(916):     coefs, Cs, n_iter = logistic_regression_path(
0.98 logistic.py(917):         X_train, y_train, Cs=Cs, fit_intercept=fit_intercept,
0.98 logistic.py(918):         solver=solver, max_iter=max_iter, class_weight=class_weight,
0.98 logistic.py(919):         pos_class=pos_class, multi_class=multi_class,
0.98 logistic.py(920):         tol=tol, verbose=verbose, dual=dual, penalty=penalty,
0.98 logistic.py(921):         intercept_scaling=intercept_scaling, random_state=random_state,
0.98 logistic.py(922):         check_input=False, max_squared_sum=max_squared_sum,
0.98 logistic.py(923):         sample_weight=sample_weight)
0.98 logistic.py(591):     if isinstance(Cs, numbers.Integral):
0.98 logistic.py(592):         Cs = np.logspace(-4, 4, Cs)
0.98 logistic.py(594):     _check_solver_option(solver, multi_class, penalty, dual)
0.98 logistic.py(428):     if solver not in ['liblinear', 'newton-cg', 'lbfgs', 'sag', 'saga']:
0.98 logistic.py(433):     if multi_class not in ['multinomial', 'ovr']:
0.98 logistic.py(437):     if multi_class == 'multinomial' and solver == 'liblinear':
0.98 logistic.py(441):     if solver not in ['liblinear', 'saga']:
0.98 logistic.py(442):         if penalty != 'l2':
0.98 logistic.py(445):     if solver != 'liblinear':
0.98 logistic.py(446):         if dual:
0.98 logistic.py(597):     if check_input:
0.98 logistic.py(602):     _, n_features = X.shape
0.98 logistic.py(603):     classes = np.unique(y)
0.98 logistic.py(604):     random_state = check_random_state(random_state)
0.98 logistic.py(606):     if pos_class is None and multi_class != 'multinomial':
0.98 logistic.py(615):     if sample_weight is not None:
0.98 logistic.py(619):         sample_weight = np.ones(X.shape[0], dtype=X.dtype)
0.98 logistic.py(624):     le = LabelEncoder()
0.98 logistic.py(625):     if isinstance(class_weight, dict) or multi_class == 'multinomial':
0.98 logistic.py(626):         class_weight_ = compute_class_weight(class_weight, classes, y)
0.98 logistic.py(627):         sample_weight *= class_weight_[le.fit_transform(y)]
0.98 logistic.py(631):     if multi_class == 'ovr':
0.98 logistic.py(645):         if solver not in ['sag', 'saga']:
0.98 logistic.py(646):             lbin = LabelBinarizer()
0.98 logistic.py(647):             Y_multi = lbin.fit_transform(y)
0.98 logistic.py(648):             if Y_multi.shape[1] == 1:
0.98 logistic.py(655):         w0 = np.zeros((classes.size, n_features + int(fit_intercept)),
0.98 logistic.py(656):                       order='F', dtype=X.dtype)
0.98 logistic.py(658):     if coef is not None:
0.98 logistic.py(688):     if multi_class == 'multinomial':
0.98 logistic.py(690):         if solver in ['lbfgs', 'newton-cg']:
0.98 logistic.py(691):             w0 = w0.ravel()
0.98 logistic.py(692):         target = Y_multi
0.98 logistic.py(693):         if solver == 'lbfgs':
0.98 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
0.98 logistic.py(699):         warm_start_sag = {'coef': w0.T}
0.98 logistic.py(710):     coefs = list()
0.98 logistic.py(711):     n_iter = np.zeros(len(Cs), dtype=np.int32)
0.98 logistic.py(712):     for i, C in enumerate(Cs):
0.98 logistic.py(713):         if solver == 'lbfgs':
0.98 logistic.py(714):             w0, loss, info = optimize.fmin_l_bfgs_b(
0.98 logistic.py(715):                 func, w0, fprime=None,
0.98 logistic.py(716):                 args=(X, target, 1. / C, sample_weight),
0.98 logistic.py(717):                 iprint=(verbose > 0) - 1, pgtol=tol, maxiter=max_iter)
0.98 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
0.98 logistic.py(339):     n_classes = Y.shape[1]
0.98 logistic.py(340):     n_features = X.shape[1]
0.98 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
0.98 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
0.98 logistic.py(343):                     dtype=X.dtype)
0.98 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
0.98 logistic.py(282):     n_classes = Y.shape[1]
0.98 logistic.py(283):     n_features = X.shape[1]
0.98 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
0.98 logistic.py(285):     w = w.reshape(n_classes, -1)
0.98 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
0.98 logistic.py(287):     if fit_intercept:
0.98 logistic.py(288):         intercept = w[:, -1]
0.98 logistic.py(289):         w = w[:, :-1]
0.98 logistic.py(292):     p = safe_sparse_dot(X, w.T)
0.98 logistic.py(293):     p += intercept
0.98 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
0.98 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
0.98 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
0.98 logistic.py(297):     p = np.exp(p, p)
0.98 logistic.py(298):     return loss, p, w
0.98 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
0.98 logistic.py(346):     diff = sample_weight * (p - Y)
0.98 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
0.98 logistic.py(348):     grad[:, :n_features] += alpha * w
0.98 logistic.py(349):     if fit_intercept:
0.98 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
0.98 logistic.py(351):     return loss, grad.ravel(), p
0.98 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
0.98 logistic.py(339):     n_classes = Y.shape[1]
0.98 logistic.py(340):     n_features = X.shape[1]
0.98 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
0.98 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
0.98 logistic.py(343):                     dtype=X.dtype)
0.98 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
0.98 logistic.py(282):     n_classes = Y.shape[1]
0.98 logistic.py(283):     n_features = X.shape[1]
0.98 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
0.98 logistic.py(285):     w = w.reshape(n_classes, -1)
0.98 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
0.98 logistic.py(287):     if fit_intercept:
0.98 logistic.py(288):         intercept = w[:, -1]
0.98 logistic.py(289):         w = w[:, :-1]
0.98 logistic.py(292):     p = safe_sparse_dot(X, w.T)
0.98 logistic.py(293):     p += intercept
0.98 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
0.98 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
0.98 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
0.98 logistic.py(297):     p = np.exp(p, p)
0.98 logistic.py(298):     return loss, p, w
0.98 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
0.98 logistic.py(346):     diff = sample_weight * (p - Y)
0.98 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
0.98 logistic.py(348):     grad[:, :n_features] += alpha * w
0.98 logistic.py(349):     if fit_intercept:
0.98 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
0.98 logistic.py(351):     return loss, grad.ravel(), p
0.98 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
0.98 logistic.py(339):     n_classes = Y.shape[1]
0.98 logistic.py(340):     n_features = X.shape[1]
0.98 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
0.98 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
0.98 logistic.py(343):                     dtype=X.dtype)
0.98 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
0.98 logistic.py(282):     n_classes = Y.shape[1]
0.98 logistic.py(283):     n_features = X.shape[1]
0.98 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
0.98 logistic.py(285):     w = w.reshape(n_classes, -1)
0.98 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
0.98 logistic.py(287):     if fit_intercept:
0.98 logistic.py(288):         intercept = w[:, -1]
0.98 logistic.py(289):         w = w[:, :-1]
0.98 logistic.py(292):     p = safe_sparse_dot(X, w.T)
0.98 logistic.py(293):     p += intercept
0.98 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
0.98 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
0.98 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
0.98 logistic.py(297):     p = np.exp(p, p)
0.98 logistic.py(298):     return loss, p, w
0.98 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
0.98 logistic.py(346):     diff = sample_weight * (p - Y)
0.98 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
0.98 logistic.py(348):     grad[:, :n_features] += alpha * w
0.98 logistic.py(349):     if fit_intercept:
0.98 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
0.98 logistic.py(351):     return loss, grad.ravel(), p
0.98 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
0.98 logistic.py(339):     n_classes = Y.shape[1]
0.98 logistic.py(340):     n_features = X.shape[1]
0.98 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
0.98 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
0.98 logistic.py(343):                     dtype=X.dtype)
0.98 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
0.98 logistic.py(282):     n_classes = Y.shape[1]
0.98 logistic.py(283):     n_features = X.shape[1]
0.98 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
0.98 logistic.py(285):     w = w.reshape(n_classes, -1)
0.98 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
0.98 logistic.py(287):     if fit_intercept:
0.98 logistic.py(288):         intercept = w[:, -1]
0.98 logistic.py(289):         w = w[:, :-1]
0.98 logistic.py(292):     p = safe_sparse_dot(X, w.T)
0.98 logistic.py(293):     p += intercept
0.98 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
0.99 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
0.99 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
0.99 logistic.py(297):     p = np.exp(p, p)
0.99 logistic.py(298):     return loss, p, w
0.99 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
0.99 logistic.py(346):     diff = sample_weight * (p - Y)
0.99 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
0.99 logistic.py(348):     grad[:, :n_features] += alpha * w
0.99 logistic.py(349):     if fit_intercept:
0.99 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
0.99 logistic.py(351):     return loss, grad.ravel(), p
0.99 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
0.99 logistic.py(339):     n_classes = Y.shape[1]
0.99 logistic.py(340):     n_features = X.shape[1]
0.99 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
0.99 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
0.99 logistic.py(343):                     dtype=X.dtype)
0.99 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
0.99 logistic.py(282):     n_classes = Y.shape[1]
0.99 logistic.py(283):     n_features = X.shape[1]
0.99 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
0.99 logistic.py(285):     w = w.reshape(n_classes, -1)
0.99 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
0.99 logistic.py(287):     if fit_intercept:
0.99 logistic.py(288):         intercept = w[:, -1]
0.99 logistic.py(289):         w = w[:, :-1]
0.99 logistic.py(292):     p = safe_sparse_dot(X, w.T)
0.99 logistic.py(293):     p += intercept
0.99 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
0.99 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
0.99 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
0.99 logistic.py(297):     p = np.exp(p, p)
0.99 logistic.py(298):     return loss, p, w
0.99 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
0.99 logistic.py(346):     diff = sample_weight * (p - Y)
0.99 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
0.99 logistic.py(348):     grad[:, :n_features] += alpha * w
0.99 logistic.py(349):     if fit_intercept:
0.99 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
0.99 logistic.py(351):     return loss, grad.ravel(), p
0.99 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
0.99 logistic.py(339):     n_classes = Y.shape[1]
0.99 logistic.py(340):     n_features = X.shape[1]
0.99 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
0.99 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
0.99 logistic.py(343):                     dtype=X.dtype)
0.99 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
0.99 logistic.py(282):     n_classes = Y.shape[1]
0.99 logistic.py(283):     n_features = X.shape[1]
0.99 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
0.99 logistic.py(285):     w = w.reshape(n_classes, -1)
0.99 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
0.99 logistic.py(287):     if fit_intercept:
0.99 logistic.py(288):         intercept = w[:, -1]
0.99 logistic.py(289):         w = w[:, :-1]
0.99 logistic.py(292):     p = safe_sparse_dot(X, w.T)
0.99 logistic.py(293):     p += intercept
0.99 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
0.99 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
0.99 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
0.99 logistic.py(297):     p = np.exp(p, p)
0.99 logistic.py(298):     return loss, p, w
0.99 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
0.99 logistic.py(346):     diff = sample_weight * (p - Y)
0.99 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
0.99 logistic.py(348):     grad[:, :n_features] += alpha * w
0.99 logistic.py(349):     if fit_intercept:
0.99 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
0.99 logistic.py(351):     return loss, grad.ravel(), p
0.99 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
0.99 logistic.py(339):     n_classes = Y.shape[1]
0.99 logistic.py(340):     n_features = X.shape[1]
0.99 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
0.99 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
0.99 logistic.py(343):                     dtype=X.dtype)
0.99 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
0.99 logistic.py(282):     n_classes = Y.shape[1]
0.99 logistic.py(283):     n_features = X.shape[1]
0.99 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
0.99 logistic.py(285):     w = w.reshape(n_classes, -1)
0.99 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
0.99 logistic.py(287):     if fit_intercept:
0.99 logistic.py(288):         intercept = w[:, -1]
0.99 logistic.py(289):         w = w[:, :-1]
0.99 logistic.py(292):     p = safe_sparse_dot(X, w.T)
0.99 logistic.py(293):     p += intercept
0.99 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
0.99 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
0.99 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
0.99 logistic.py(297):     p = np.exp(p, p)
0.99 logistic.py(298):     return loss, p, w
0.99 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
0.99 logistic.py(346):     diff = sample_weight * (p - Y)
0.99 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
0.99 logistic.py(348):     grad[:, :n_features] += alpha * w
0.99 logistic.py(349):     if fit_intercept:
0.99 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
0.99 logistic.py(351):     return loss, grad.ravel(), p
0.99 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
0.99 logistic.py(339):     n_classes = Y.shape[1]
0.99 logistic.py(340):     n_features = X.shape[1]
0.99 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
0.99 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
0.99 logistic.py(343):                     dtype=X.dtype)
0.99 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
0.99 logistic.py(282):     n_classes = Y.shape[1]
0.99 logistic.py(283):     n_features = X.shape[1]
0.99 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
0.99 logistic.py(285):     w = w.reshape(n_classes, -1)
0.99 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
0.99 logistic.py(287):     if fit_intercept:
0.99 logistic.py(288):         intercept = w[:, -1]
0.99 logistic.py(289):         w = w[:, :-1]
0.99 logistic.py(292):     p = safe_sparse_dot(X, w.T)
0.99 logistic.py(293):     p += intercept
0.99 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
0.99 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
0.99 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
0.99 logistic.py(297):     p = np.exp(p, p)
0.99 logistic.py(298):     return loss, p, w
0.99 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
0.99 logistic.py(346):     diff = sample_weight * (p - Y)
0.99 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
0.99 logistic.py(348):     grad[:, :n_features] += alpha * w
0.99 logistic.py(349):     if fit_intercept:
0.99 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
0.99 logistic.py(351):     return loss, grad.ravel(), p
0.99 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
0.99 logistic.py(339):     n_classes = Y.shape[1]
0.99 logistic.py(340):     n_features = X.shape[1]
0.99 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
0.99 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
0.99 logistic.py(343):                     dtype=X.dtype)
0.99 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
0.99 logistic.py(282):     n_classes = Y.shape[1]
0.99 logistic.py(283):     n_features = X.shape[1]
0.99 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
0.99 logistic.py(285):     w = w.reshape(n_classes, -1)
0.99 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
0.99 logistic.py(287):     if fit_intercept:
0.99 logistic.py(288):         intercept = w[:, -1]
0.99 logistic.py(289):         w = w[:, :-1]
0.99 logistic.py(292):     p = safe_sparse_dot(X, w.T)
0.99 logistic.py(293):     p += intercept
0.99 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
0.99 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
0.99 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
0.99 logistic.py(297):     p = np.exp(p, p)
0.99 logistic.py(298):     return loss, p, w
0.99 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
0.99 logistic.py(346):     diff = sample_weight * (p - Y)
0.99 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
0.99 logistic.py(348):     grad[:, :n_features] += alpha * w
0.99 logistic.py(349):     if fit_intercept:
0.99 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
0.99 logistic.py(351):     return loss, grad.ravel(), p
0.99 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
0.99 logistic.py(339):     n_classes = Y.shape[1]
0.99 logistic.py(340):     n_features = X.shape[1]
0.99 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
0.99 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
0.99 logistic.py(343):                     dtype=X.dtype)
0.99 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
0.99 logistic.py(282):     n_classes = Y.shape[1]
0.99 logistic.py(283):     n_features = X.shape[1]
0.99 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
0.99 logistic.py(285):     w = w.reshape(n_classes, -1)
0.99 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
0.99 logistic.py(287):     if fit_intercept:
0.99 logistic.py(288):         intercept = w[:, -1]
0.99 logistic.py(289):         w = w[:, :-1]
0.99 logistic.py(292):     p = safe_sparse_dot(X, w.T)
0.99 logistic.py(293):     p += intercept
0.99 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
0.99 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
0.99 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
0.99 logistic.py(297):     p = np.exp(p, p)
0.99 logistic.py(298):     return loss, p, w
0.99 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
0.99 logistic.py(346):     diff = sample_weight * (p - Y)
0.99 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
0.99 logistic.py(348):     grad[:, :n_features] += alpha * w
0.99 logistic.py(349):     if fit_intercept:
0.99 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
0.99 logistic.py(351):     return loss, grad.ravel(), p
0.99 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
0.99 logistic.py(339):     n_classes = Y.shape[1]
0.99 logistic.py(340):     n_features = X.shape[1]
0.99 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
0.99 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
0.99 logistic.py(343):                     dtype=X.dtype)
0.99 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
0.99 logistic.py(282):     n_classes = Y.shape[1]
0.99 logistic.py(283):     n_features = X.shape[1]
0.99 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
0.99 logistic.py(285):     w = w.reshape(n_classes, -1)
0.99 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
0.99 logistic.py(287):     if fit_intercept:
0.99 logistic.py(288):         intercept = w[:, -1]
0.99 logistic.py(289):         w = w[:, :-1]
0.99 logistic.py(292):     p = safe_sparse_dot(X, w.T)
0.99 logistic.py(293):     p += intercept
0.99 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
0.99 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
0.99 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
0.99 logistic.py(297):     p = np.exp(p, p)
0.99 logistic.py(298):     return loss, p, w
0.99 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
0.99 logistic.py(346):     diff = sample_weight * (p - Y)
0.99 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
0.99 logistic.py(348):     grad[:, :n_features] += alpha * w
0.99 logistic.py(349):     if fit_intercept:
0.99 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
0.99 logistic.py(351):     return loss, grad.ravel(), p
0.99 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
0.99 logistic.py(339):     n_classes = Y.shape[1]
0.99 logistic.py(340):     n_features = X.shape[1]
0.99 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
0.99 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
0.99 logistic.py(343):                     dtype=X.dtype)
0.99 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
0.99 logistic.py(282):     n_classes = Y.shape[1]
0.99 logistic.py(283):     n_features = X.shape[1]
0.99 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
0.99 logistic.py(285):     w = w.reshape(n_classes, -1)
0.99 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
0.99 logistic.py(287):     if fit_intercept:
0.99 logistic.py(288):         intercept = w[:, -1]
0.99 logistic.py(289):         w = w[:, :-1]
0.99 logistic.py(292):     p = safe_sparse_dot(X, w.T)
0.99 logistic.py(293):     p += intercept
0.99 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
0.99 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
0.99 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
0.99 logistic.py(297):     p = np.exp(p, p)
0.99 logistic.py(298):     return loss, p, w
0.99 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
0.99 logistic.py(346):     diff = sample_weight * (p - Y)
0.99 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
0.99 logistic.py(348):     grad[:, :n_features] += alpha * w
0.99 logistic.py(349):     if fit_intercept:
0.99 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
0.99 logistic.py(351):     return loss, grad.ravel(), p
0.99 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
0.99 logistic.py(339):     n_classes = Y.shape[1]
0.99 logistic.py(340):     n_features = X.shape[1]
0.99 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
0.99 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
0.99 logistic.py(343):                     dtype=X.dtype)
0.99 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
0.99 logistic.py(282):     n_classes = Y.shape[1]
0.99 logistic.py(283):     n_features = X.shape[1]
0.99 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
0.99 logistic.py(285):     w = w.reshape(n_classes, -1)
0.99 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
0.99 logistic.py(287):     if fit_intercept:
0.99 logistic.py(288):         intercept = w[:, -1]
0.99 logistic.py(289):         w = w[:, :-1]
0.99 logistic.py(292):     p = safe_sparse_dot(X, w.T)
0.99 logistic.py(293):     p += intercept
0.99 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
0.99 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
0.99 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
0.99 logistic.py(297):     p = np.exp(p, p)
0.99 logistic.py(298):     return loss, p, w
0.99 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
0.99 logistic.py(346):     diff = sample_weight * (p - Y)
0.99 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
0.99 logistic.py(348):     grad[:, :n_features] += alpha * w
0.99 logistic.py(349):     if fit_intercept:
0.99 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
0.99 logistic.py(351):     return loss, grad.ravel(), p
0.99 logistic.py(718):             if info["warnflag"] == 1:
0.99 logistic.py(723):             n_iter_i = min(info['nit'], max_iter)
0.99 logistic.py(760):         if multi_class == 'multinomial':
0.99 logistic.py(761):             multi_w0 = np.reshape(w0, (classes.size, -1))
0.99 logistic.py(762):             if classes.size == 2:
0.99 logistic.py(764):             coefs.append(multi_w0)
0.99 logistic.py(768):         n_iter[i] = n_iter_i
0.99 logistic.py(712):     for i, C in enumerate(Cs):
0.99 logistic.py(713):         if solver == 'lbfgs':
0.99 logistic.py(714):             w0, loss, info = optimize.fmin_l_bfgs_b(
0.99 logistic.py(715):                 func, w0, fprime=None,
0.99 logistic.py(716):                 args=(X, target, 1. / C, sample_weight),
0.99 logistic.py(717):                 iprint=(verbose > 0) - 1, pgtol=tol, maxiter=max_iter)
0.99 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
0.99 logistic.py(339):     n_classes = Y.shape[1]
0.99 logistic.py(340):     n_features = X.shape[1]
0.99 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
0.99 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
0.99 logistic.py(343):                     dtype=X.dtype)
0.99 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
0.99 logistic.py(282):     n_classes = Y.shape[1]
0.99 logistic.py(283):     n_features = X.shape[1]
0.99 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
0.99 logistic.py(285):     w = w.reshape(n_classes, -1)
0.99 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
0.99 logistic.py(287):     if fit_intercept:
0.99 logistic.py(288):         intercept = w[:, -1]
0.99 logistic.py(289):         w = w[:, :-1]
0.99 logistic.py(292):     p = safe_sparse_dot(X, w.T)
0.99 logistic.py(293):     p += intercept
0.99 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
0.99 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
0.99 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
0.99 logistic.py(297):     p = np.exp(p, p)
0.99 logistic.py(298):     return loss, p, w
0.99 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
0.99 logistic.py(346):     diff = sample_weight * (p - Y)
0.99 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
0.99 logistic.py(348):     grad[:, :n_features] += alpha * w
0.99 logistic.py(349):     if fit_intercept:
0.99 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
0.99 logistic.py(351):     return loss, grad.ravel(), p
0.99 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
0.99 logistic.py(339):     n_classes = Y.shape[1]
0.99 logistic.py(340):     n_features = X.shape[1]
0.99 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
0.99 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
0.99 logistic.py(343):                     dtype=X.dtype)
0.99 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
0.99 logistic.py(282):     n_classes = Y.shape[1]
0.99 logistic.py(283):     n_features = X.shape[1]
0.99 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
0.99 logistic.py(285):     w = w.reshape(n_classes, -1)
0.99 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
0.99 logistic.py(287):     if fit_intercept:
0.99 logistic.py(288):         intercept = w[:, -1]
0.99 logistic.py(289):         w = w[:, :-1]
0.99 logistic.py(292):     p = safe_sparse_dot(X, w.T)
0.99 logistic.py(293):     p += intercept
0.99 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
0.99 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
0.99 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
0.99 logistic.py(297):     p = np.exp(p, p)
0.99 logistic.py(298):     return loss, p, w
0.99 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
0.99 logistic.py(346):     diff = sample_weight * (p - Y)
0.99 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
0.99 logistic.py(348):     grad[:, :n_features] += alpha * w
0.99 logistic.py(349):     if fit_intercept:
0.99 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
0.99 logistic.py(351):     return loss, grad.ravel(), p
0.99 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
0.99 logistic.py(339):     n_classes = Y.shape[1]
0.99 logistic.py(340):     n_features = X.shape[1]
0.99 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
0.99 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
0.99 logistic.py(343):                     dtype=X.dtype)
0.99 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
0.99 logistic.py(282):     n_classes = Y.shape[1]
0.99 logistic.py(283):     n_features = X.shape[1]
0.99 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
0.99 logistic.py(285):     w = w.reshape(n_classes, -1)
0.99 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
0.99 logistic.py(287):     if fit_intercept:
0.99 logistic.py(288):         intercept = w[:, -1]
0.99 logistic.py(289):         w = w[:, :-1]
0.99 logistic.py(292):     p = safe_sparse_dot(X, w.T)
0.99 logistic.py(293):     p += intercept
0.99 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
0.99 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
0.99 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
0.99 logistic.py(297):     p = np.exp(p, p)
0.99 logistic.py(298):     return loss, p, w
0.99 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
0.99 logistic.py(346):     diff = sample_weight * (p - Y)
0.99 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
0.99 logistic.py(348):     grad[:, :n_features] += alpha * w
0.99 logistic.py(349):     if fit_intercept:
0.99 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
0.99 logistic.py(351):     return loss, grad.ravel(), p
0.99 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
0.99 logistic.py(339):     n_classes = Y.shape[1]
0.99 logistic.py(340):     n_features = X.shape[1]
0.99 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
0.99 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
0.99 logistic.py(343):                     dtype=X.dtype)
0.99 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
0.99 logistic.py(282):     n_classes = Y.shape[1]
0.99 logistic.py(283):     n_features = X.shape[1]
0.99 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
0.99 logistic.py(285):     w = w.reshape(n_classes, -1)
0.99 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
0.99 logistic.py(287):     if fit_intercept:
0.99 logistic.py(288):         intercept = w[:, -1]
0.99 logistic.py(289):         w = w[:, :-1]
0.99 logistic.py(292):     p = safe_sparse_dot(X, w.T)
0.99 logistic.py(293):     p += intercept
0.99 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
0.99 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
0.99 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
0.99 logistic.py(297):     p = np.exp(p, p)
0.99 logistic.py(298):     return loss, p, w
0.99 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
0.99 logistic.py(346):     diff = sample_weight * (p - Y)
0.99 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
0.99 logistic.py(348):     grad[:, :n_features] += alpha * w
0.99 logistic.py(349):     if fit_intercept:
0.99 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
0.99 logistic.py(351):     return loss, grad.ravel(), p
1.00 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
1.00 logistic.py(339):     n_classes = Y.shape[1]
1.00 logistic.py(340):     n_features = X.shape[1]
1.00 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
1.00 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
1.00 logistic.py(343):                     dtype=X.dtype)
1.00 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
1.00 logistic.py(282):     n_classes = Y.shape[1]
1.00 logistic.py(283):     n_features = X.shape[1]
1.00 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
1.00 logistic.py(285):     w = w.reshape(n_classes, -1)
1.00 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
1.00 logistic.py(287):     if fit_intercept:
1.00 logistic.py(288):         intercept = w[:, -1]
1.00 logistic.py(289):         w = w[:, :-1]
1.00 logistic.py(292):     p = safe_sparse_dot(X, w.T)
1.00 logistic.py(293):     p += intercept
1.00 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
1.00 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
1.00 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
1.00 logistic.py(297):     p = np.exp(p, p)
1.00 logistic.py(298):     return loss, p, w
1.00 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
1.00 logistic.py(346):     diff = sample_weight * (p - Y)
1.00 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
1.00 logistic.py(348):     grad[:, :n_features] += alpha * w
1.00 logistic.py(349):     if fit_intercept:
1.00 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
1.00 logistic.py(351):     return loss, grad.ravel(), p
1.00 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
1.00 logistic.py(339):     n_classes = Y.shape[1]
1.00 logistic.py(340):     n_features = X.shape[1]
1.00 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
1.00 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
1.00 logistic.py(343):                     dtype=X.dtype)
1.00 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
1.00 logistic.py(282):     n_classes = Y.shape[1]
1.00 logistic.py(283):     n_features = X.shape[1]
1.00 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
1.00 logistic.py(285):     w = w.reshape(n_classes, -1)
1.00 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
1.00 logistic.py(287):     if fit_intercept:
1.00 logistic.py(288):         intercept = w[:, -1]
1.00 logistic.py(289):         w = w[:, :-1]
1.00 logistic.py(292):     p = safe_sparse_dot(X, w.T)
1.00 logistic.py(293):     p += intercept
1.00 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
1.00 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
1.00 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
1.00 logistic.py(297):     p = np.exp(p, p)
1.00 logistic.py(298):     return loss, p, w
1.00 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
1.00 logistic.py(346):     diff = sample_weight * (p - Y)
1.00 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
1.00 logistic.py(348):     grad[:, :n_features] += alpha * w
1.00 logistic.py(349):     if fit_intercept:
1.00 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
1.00 logistic.py(351):     return loss, grad.ravel(), p
1.00 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
1.00 logistic.py(339):     n_classes = Y.shape[1]
1.00 logistic.py(340):     n_features = X.shape[1]
1.00 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
1.00 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
1.00 logistic.py(343):                     dtype=X.dtype)
1.00 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
1.00 logistic.py(282):     n_classes = Y.shape[1]
1.00 logistic.py(283):     n_features = X.shape[1]
1.00 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
1.00 logistic.py(285):     w = w.reshape(n_classes, -1)
1.00 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
1.00 logistic.py(287):     if fit_intercept:
1.00 logistic.py(288):         intercept = w[:, -1]
1.00 logistic.py(289):         w = w[:, :-1]
1.00 logistic.py(292):     p = safe_sparse_dot(X, w.T)
1.00 logistic.py(293):     p += intercept
1.00 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
1.00 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
1.00 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
1.00 logistic.py(297):     p = np.exp(p, p)
1.00 logistic.py(298):     return loss, p, w
1.00 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
1.00 logistic.py(346):     diff = sample_weight * (p - Y)
1.00 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
1.00 logistic.py(348):     grad[:, :n_features] += alpha * w
1.00 logistic.py(349):     if fit_intercept:
1.00 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
1.00 logistic.py(351):     return loss, grad.ravel(), p
1.00 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
1.00 logistic.py(339):     n_classes = Y.shape[1]
1.00 logistic.py(340):     n_features = X.shape[1]
1.00 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
1.00 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
1.00 logistic.py(343):                     dtype=X.dtype)
1.00 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
1.00 logistic.py(282):     n_classes = Y.shape[1]
1.00 logistic.py(283):     n_features = X.shape[1]
1.00 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
1.00 logistic.py(285):     w = w.reshape(n_classes, -1)
1.00 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
1.00 logistic.py(287):     if fit_intercept:
1.00 logistic.py(288):         intercept = w[:, -1]
1.00 logistic.py(289):         w = w[:, :-1]
1.00 logistic.py(292):     p = safe_sparse_dot(X, w.T)
1.00 logistic.py(293):     p += intercept
1.00 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
1.00 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
1.00 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
1.00 logistic.py(297):     p = np.exp(p, p)
1.00 logistic.py(298):     return loss, p, w
1.00 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
1.00 logistic.py(346):     diff = sample_weight * (p - Y)
1.00 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
1.00 logistic.py(348):     grad[:, :n_features] += alpha * w
1.00 logistic.py(349):     if fit_intercept:
1.00 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
1.00 logistic.py(351):     return loss, grad.ravel(), p
1.00 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
1.00 logistic.py(339):     n_classes = Y.shape[1]
1.00 logistic.py(340):     n_features = X.shape[1]
1.00 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
1.00 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
1.00 logistic.py(343):                     dtype=X.dtype)
1.00 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
1.00 logistic.py(282):     n_classes = Y.shape[1]
1.00 logistic.py(283):     n_features = X.shape[1]
1.00 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
1.00 logistic.py(285):     w = w.reshape(n_classes, -1)
1.00 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
1.00 logistic.py(287):     if fit_intercept:
1.00 logistic.py(288):         intercept = w[:, -1]
1.00 logistic.py(289):         w = w[:, :-1]
1.00 logistic.py(292):     p = safe_sparse_dot(X, w.T)
1.00 logistic.py(293):     p += intercept
1.00 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
1.00 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
1.00 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
1.00 logistic.py(297):     p = np.exp(p, p)
1.00 logistic.py(298):     return loss, p, w
1.00 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
1.00 logistic.py(346):     diff = sample_weight * (p - Y)
1.00 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
1.00 logistic.py(348):     grad[:, :n_features] += alpha * w
1.00 logistic.py(349):     if fit_intercept:
1.00 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
1.00 logistic.py(351):     return loss, grad.ravel(), p
1.00 logistic.py(718):             if info["warnflag"] == 1:
1.00 logistic.py(723):             n_iter_i = min(info['nit'], max_iter)
1.00 logistic.py(760):         if multi_class == 'multinomial':
1.00 logistic.py(761):             multi_w0 = np.reshape(w0, (classes.size, -1))
1.00 logistic.py(762):             if classes.size == 2:
1.00 logistic.py(764):             coefs.append(multi_w0)
1.00 logistic.py(768):         n_iter[i] = n_iter_i
1.00 logistic.py(712):     for i, C in enumerate(Cs):
1.00 logistic.py(713):         if solver == 'lbfgs':
1.00 logistic.py(714):             w0, loss, info = optimize.fmin_l_bfgs_b(
1.00 logistic.py(715):                 func, w0, fprime=None,
1.00 logistic.py(716):                 args=(X, target, 1. / C, sample_weight),
1.00 logistic.py(717):                 iprint=(verbose > 0) - 1, pgtol=tol, maxiter=max_iter)
1.00 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
1.00 logistic.py(339):     n_classes = Y.shape[1]
1.00 logistic.py(340):     n_features = X.shape[1]
1.00 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
1.00 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
1.00 logistic.py(343):                     dtype=X.dtype)
1.00 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
1.00 logistic.py(282):     n_classes = Y.shape[1]
1.00 logistic.py(283):     n_features = X.shape[1]
1.00 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
1.00 logistic.py(285):     w = w.reshape(n_classes, -1)
1.00 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
1.00 logistic.py(287):     if fit_intercept:
1.00 logistic.py(288):         intercept = w[:, -1]
1.00 logistic.py(289):         w = w[:, :-1]
1.00 logistic.py(292):     p = safe_sparse_dot(X, w.T)
1.00 logistic.py(293):     p += intercept
1.00 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
1.00 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
1.00 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
1.00 logistic.py(297):     p = np.exp(p, p)
1.00 logistic.py(298):     return loss, p, w
1.00 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
1.00 logistic.py(346):     diff = sample_weight * (p - Y)
1.00 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
1.00 logistic.py(348):     grad[:, :n_features] += alpha * w
1.00 logistic.py(349):     if fit_intercept:
1.00 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
1.00 logistic.py(351):     return loss, grad.ravel(), p
1.00 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
1.00 logistic.py(339):     n_classes = Y.shape[1]
1.00 logistic.py(340):     n_features = X.shape[1]
1.00 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
1.00 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
1.00 logistic.py(343):                     dtype=X.dtype)
1.00 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
1.00 logistic.py(282):     n_classes = Y.shape[1]
1.00 logistic.py(283):     n_features = X.shape[1]
1.00 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
1.00 logistic.py(285):     w = w.reshape(n_classes, -1)
1.00 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
1.00 logistic.py(287):     if fit_intercept:
1.00 logistic.py(288):         intercept = w[:, -1]
1.00 logistic.py(289):         w = w[:, :-1]
1.00 logistic.py(292):     p = safe_sparse_dot(X, w.T)
1.00 logistic.py(293):     p += intercept
1.00 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
1.00 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
1.00 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
1.00 logistic.py(297):     p = np.exp(p, p)
1.00 logistic.py(298):     return loss, p, w
1.00 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
1.00 logistic.py(346):     diff = sample_weight * (p - Y)
1.00 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
1.00 logistic.py(348):     grad[:, :n_features] += alpha * w
1.00 logistic.py(349):     if fit_intercept:
1.00 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
1.00 logistic.py(351):     return loss, grad.ravel(), p
1.00 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
1.00 logistic.py(339):     n_classes = Y.shape[1]
1.00 logistic.py(340):     n_features = X.shape[1]
1.00 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
1.00 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
1.00 logistic.py(343):                     dtype=X.dtype)
1.00 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
1.00 logistic.py(282):     n_classes = Y.shape[1]
1.00 logistic.py(283):     n_features = X.shape[1]
1.00 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
1.00 logistic.py(285):     w = w.reshape(n_classes, -1)
1.00 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
1.00 logistic.py(287):     if fit_intercept:
1.00 logistic.py(288):         intercept = w[:, -1]
1.00 logistic.py(289):         w = w[:, :-1]
1.00 logistic.py(292):     p = safe_sparse_dot(X, w.T)
1.00 logistic.py(293):     p += intercept
1.00 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
1.00 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
1.00 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
1.00 logistic.py(297):     p = np.exp(p, p)
1.00 logistic.py(298):     return loss, p, w
1.00 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
1.00 logistic.py(346):     diff = sample_weight * (p - Y)
1.00 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
1.00 logistic.py(348):     grad[:, :n_features] += alpha * w
1.00 logistic.py(349):     if fit_intercept:
1.00 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
1.00 logistic.py(351):     return loss, grad.ravel(), p
1.00 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
1.00 logistic.py(339):     n_classes = Y.shape[1]
1.00 logistic.py(340):     n_features = X.shape[1]
1.00 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
1.00 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
1.00 logistic.py(343):                     dtype=X.dtype)
1.00 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
1.00 logistic.py(282):     n_classes = Y.shape[1]
1.00 logistic.py(283):     n_features = X.shape[1]
1.00 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
1.00 logistic.py(285):     w = w.reshape(n_classes, -1)
1.00 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
1.00 logistic.py(287):     if fit_intercept:
1.00 logistic.py(288):         intercept = w[:, -1]
1.00 logistic.py(289):         w = w[:, :-1]
1.00 logistic.py(292):     p = safe_sparse_dot(X, w.T)
1.00 logistic.py(293):     p += intercept
1.00 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
1.00 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
1.00 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
1.00 logistic.py(297):     p = np.exp(p, p)
1.00 logistic.py(298):     return loss, p, w
1.00 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
1.00 logistic.py(346):     diff = sample_weight * (p - Y)
1.00 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
1.00 logistic.py(348):     grad[:, :n_features] += alpha * w
1.00 logistic.py(349):     if fit_intercept:
1.00 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
1.00 logistic.py(351):     return loss, grad.ravel(), p
1.00 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
1.00 logistic.py(339):     n_classes = Y.shape[1]
1.00 logistic.py(340):     n_features = X.shape[1]
1.00 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
1.00 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
1.00 logistic.py(343):                     dtype=X.dtype)
1.00 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
1.00 logistic.py(282):     n_classes = Y.shape[1]
1.00 logistic.py(283):     n_features = X.shape[1]
1.00 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
1.00 logistic.py(285):     w = w.reshape(n_classes, -1)
1.00 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
1.00 logistic.py(287):     if fit_intercept:
1.00 logistic.py(288):         intercept = w[:, -1]
1.00 logistic.py(289):         w = w[:, :-1]
1.00 logistic.py(292):     p = safe_sparse_dot(X, w.T)
1.00 logistic.py(293):     p += intercept
1.00 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
1.00 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
1.00 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
1.00 logistic.py(297):     p = np.exp(p, p)
1.00 logistic.py(298):     return loss, p, w
1.00 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
1.00 logistic.py(346):     diff = sample_weight * (p - Y)
1.00 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
1.00 logistic.py(348):     grad[:, :n_features] += alpha * w
1.00 logistic.py(349):     if fit_intercept:
1.00 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
1.00 logistic.py(351):     return loss, grad.ravel(), p
1.00 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
1.00 logistic.py(339):     n_classes = Y.shape[1]
1.00 logistic.py(340):     n_features = X.shape[1]
1.00 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
1.00 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
1.00 logistic.py(343):                     dtype=X.dtype)
1.00 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
1.00 logistic.py(282):     n_classes = Y.shape[1]
1.00 logistic.py(283):     n_features = X.shape[1]
1.00 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
1.00 logistic.py(285):     w = w.reshape(n_classes, -1)
1.00 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
1.00 logistic.py(287):     if fit_intercept:
1.00 logistic.py(288):         intercept = w[:, -1]
1.00 logistic.py(289):         w = w[:, :-1]
1.00 logistic.py(292):     p = safe_sparse_dot(X, w.T)
1.00 logistic.py(293):     p += intercept
1.00 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
1.00 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
1.00 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
1.00 logistic.py(297):     p = np.exp(p, p)
1.00 logistic.py(298):     return loss, p, w
1.00 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
1.00 logistic.py(346):     diff = sample_weight * (p - Y)
1.00 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
1.00 logistic.py(348):     grad[:, :n_features] += alpha * w
1.00 logistic.py(349):     if fit_intercept:
1.00 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
1.00 logistic.py(351):     return loss, grad.ravel(), p
1.00 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
1.00 logistic.py(339):     n_classes = Y.shape[1]
1.00 logistic.py(340):     n_features = X.shape[1]
1.00 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
1.00 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
1.00 logistic.py(343):                     dtype=X.dtype)
1.00 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
1.00 logistic.py(282):     n_classes = Y.shape[1]
1.00 logistic.py(283):     n_features = X.shape[1]
1.00 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
1.00 logistic.py(285):     w = w.reshape(n_classes, -1)
1.00 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
1.00 logistic.py(287):     if fit_intercept:
1.00 logistic.py(288):         intercept = w[:, -1]
1.00 logistic.py(289):         w = w[:, :-1]
1.00 logistic.py(292):     p = safe_sparse_dot(X, w.T)
1.00 logistic.py(293):     p += intercept
1.00 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
1.00 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
1.00 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
1.00 logistic.py(297):     p = np.exp(p, p)
1.00 logistic.py(298):     return loss, p, w
1.00 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
1.00 logistic.py(346):     diff = sample_weight * (p - Y)
1.00 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
1.00 logistic.py(348):     grad[:, :n_features] += alpha * w
1.00 logistic.py(349):     if fit_intercept:
1.00 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
1.00 logistic.py(351):     return loss, grad.ravel(), p
1.00 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
1.00 logistic.py(339):     n_classes = Y.shape[1]
1.00 logistic.py(340):     n_features = X.shape[1]
1.00 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
1.00 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
1.00 logistic.py(343):                     dtype=X.dtype)
1.00 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
1.00 logistic.py(282):     n_classes = Y.shape[1]
1.00 logistic.py(283):     n_features = X.shape[1]
1.00 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
1.00 logistic.py(285):     w = w.reshape(n_classes, -1)
1.00 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
1.00 logistic.py(287):     if fit_intercept:
1.00 logistic.py(288):         intercept = w[:, -1]
1.00 logistic.py(289):         w = w[:, :-1]
1.00 logistic.py(292):     p = safe_sparse_dot(X, w.T)
1.00 logistic.py(293):     p += intercept
1.00 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
1.00 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
1.00 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
1.00 logistic.py(297):     p = np.exp(p, p)
1.00 logistic.py(298):     return loss, p, w
1.00 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
1.00 logistic.py(346):     diff = sample_weight * (p - Y)
1.00 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
1.00 logistic.py(348):     grad[:, :n_features] += alpha * w
1.00 logistic.py(349):     if fit_intercept:
1.00 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
1.00 logistic.py(351):     return loss, grad.ravel(), p
1.00 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
1.00 logistic.py(339):     n_classes = Y.shape[1]
1.00 logistic.py(340):     n_features = X.shape[1]
1.00 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
1.00 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
1.00 logistic.py(343):                     dtype=X.dtype)
1.00 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
1.00 logistic.py(282):     n_classes = Y.shape[1]
1.00 logistic.py(283):     n_features = X.shape[1]
1.00 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
1.00 logistic.py(285):     w = w.reshape(n_classes, -1)
1.00 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
1.00 logistic.py(287):     if fit_intercept:
1.00 logistic.py(288):         intercept = w[:, -1]
1.00 logistic.py(289):         w = w[:, :-1]
1.00 logistic.py(292):     p = safe_sparse_dot(X, w.T)
1.00 logistic.py(293):     p += intercept
1.01 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
1.01 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
1.01 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
1.01 logistic.py(297):     p = np.exp(p, p)
1.01 logistic.py(298):     return loss, p, w
1.01 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
1.01 logistic.py(346):     diff = sample_weight * (p - Y)
1.01 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
1.01 logistic.py(348):     grad[:, :n_features] += alpha * w
1.01 logistic.py(349):     if fit_intercept:
1.01 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
1.01 logistic.py(351):     return loss, grad.ravel(), p
1.01 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
1.01 logistic.py(339):     n_classes = Y.shape[1]
1.01 logistic.py(340):     n_features = X.shape[1]
1.01 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
1.01 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
1.01 logistic.py(343):                     dtype=X.dtype)
1.01 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
1.01 logistic.py(282):     n_classes = Y.shape[1]
1.01 logistic.py(283):     n_features = X.shape[1]
1.01 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
1.01 logistic.py(285):     w = w.reshape(n_classes, -1)
1.01 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
1.01 logistic.py(287):     if fit_intercept:
1.01 logistic.py(288):         intercept = w[:, -1]
1.01 logistic.py(289):         w = w[:, :-1]
1.01 logistic.py(292):     p = safe_sparse_dot(X, w.T)
1.01 logistic.py(293):     p += intercept
1.01 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
1.01 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
1.01 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
1.01 logistic.py(297):     p = np.exp(p, p)
1.01 logistic.py(298):     return loss, p, w
1.01 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
1.01 logistic.py(346):     diff = sample_weight * (p - Y)
1.01 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
1.01 logistic.py(348):     grad[:, :n_features] += alpha * w
1.01 logistic.py(349):     if fit_intercept:
1.01 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
1.01 logistic.py(351):     return loss, grad.ravel(), p
1.01 logistic.py(718):             if info["warnflag"] == 1:
1.01 logistic.py(723):             n_iter_i = min(info['nit'], max_iter)
1.01 logistic.py(760):         if multi_class == 'multinomial':
1.01 logistic.py(761):             multi_w0 = np.reshape(w0, (classes.size, -1))
1.01 logistic.py(762):             if classes.size == 2:
1.01 logistic.py(764):             coefs.append(multi_w0)
1.01 logistic.py(768):         n_iter[i] = n_iter_i
1.01 logistic.py(712):     for i, C in enumerate(Cs):
1.01 logistic.py(713):         if solver == 'lbfgs':
1.01 logistic.py(714):             w0, loss, info = optimize.fmin_l_bfgs_b(
1.01 logistic.py(715):                 func, w0, fprime=None,
1.01 logistic.py(716):                 args=(X, target, 1. / C, sample_weight),
1.01 logistic.py(717):                 iprint=(verbose > 0) - 1, pgtol=tol, maxiter=max_iter)
1.01 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
1.01 logistic.py(339):     n_classes = Y.shape[1]
1.01 logistic.py(340):     n_features = X.shape[1]
1.01 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
1.01 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
1.01 logistic.py(343):                     dtype=X.dtype)
1.01 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
1.01 logistic.py(282):     n_classes = Y.shape[1]
1.01 logistic.py(283):     n_features = X.shape[1]
1.01 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
1.01 logistic.py(285):     w = w.reshape(n_classes, -1)
1.01 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
1.01 logistic.py(287):     if fit_intercept:
1.01 logistic.py(288):         intercept = w[:, -1]
1.01 logistic.py(289):         w = w[:, :-1]
1.01 logistic.py(292):     p = safe_sparse_dot(X, w.T)
1.01 logistic.py(293):     p += intercept
1.01 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
1.01 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
1.01 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
1.01 logistic.py(297):     p = np.exp(p, p)
1.01 logistic.py(298):     return loss, p, w
1.01 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
1.01 logistic.py(346):     diff = sample_weight * (p - Y)
1.01 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
1.01 logistic.py(348):     grad[:, :n_features] += alpha * w
1.01 logistic.py(349):     if fit_intercept:
1.01 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
1.01 logistic.py(351):     return loss, grad.ravel(), p
1.01 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
1.01 logistic.py(339):     n_classes = Y.shape[1]
1.01 logistic.py(340):     n_features = X.shape[1]
1.01 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
1.01 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
1.01 logistic.py(343):                     dtype=X.dtype)
1.01 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
1.01 logistic.py(282):     n_classes = Y.shape[1]
1.01 logistic.py(283):     n_features = X.shape[1]
1.01 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
1.01 logistic.py(285):     w = w.reshape(n_classes, -1)
1.01 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
1.01 logistic.py(287):     if fit_intercept:
1.01 logistic.py(288):         intercept = w[:, -1]
1.01 logistic.py(289):         w = w[:, :-1]
1.01 logistic.py(292):     p = safe_sparse_dot(X, w.T)
1.01 logistic.py(293):     p += intercept
1.01 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
1.01 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
1.01 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
1.01 logistic.py(297):     p = np.exp(p, p)
1.01 logistic.py(298):     return loss, p, w
1.01 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
1.01 logistic.py(346):     diff = sample_weight * (p - Y)
1.01 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
1.01 logistic.py(348):     grad[:, :n_features] += alpha * w
1.01 logistic.py(349):     if fit_intercept:
1.01 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
1.01 logistic.py(351):     return loss, grad.ravel(), p
1.01 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
1.01 logistic.py(339):     n_classes = Y.shape[1]
1.01 logistic.py(340):     n_features = X.shape[1]
1.01 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
1.01 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
1.01 logistic.py(343):                     dtype=X.dtype)
1.01 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
1.01 logistic.py(282):     n_classes = Y.shape[1]
1.01 logistic.py(283):     n_features = X.shape[1]
1.01 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
1.01 logistic.py(285):     w = w.reshape(n_classes, -1)
1.01 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
1.01 logistic.py(287):     if fit_intercept:
1.01 logistic.py(288):         intercept = w[:, -1]
1.01 logistic.py(289):         w = w[:, :-1]
1.01 logistic.py(292):     p = safe_sparse_dot(X, w.T)
1.01 logistic.py(293):     p += intercept
1.01 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
1.01 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
1.01 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
1.01 logistic.py(297):     p = np.exp(p, p)
1.01 logistic.py(298):     return loss, p, w
1.01 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
1.01 logistic.py(346):     diff = sample_weight * (p - Y)
1.01 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
1.01 logistic.py(348):     grad[:, :n_features] += alpha * w
1.01 logistic.py(349):     if fit_intercept:
1.01 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
1.01 logistic.py(351):     return loss, grad.ravel(), p
1.01 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
1.01 logistic.py(339):     n_classes = Y.shape[1]
1.01 logistic.py(340):     n_features = X.shape[1]
1.01 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
1.01 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
1.01 logistic.py(343):                     dtype=X.dtype)
1.01 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
1.01 logistic.py(282):     n_classes = Y.shape[1]
1.01 logistic.py(283):     n_features = X.shape[1]
1.01 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
1.01 logistic.py(285):     w = w.reshape(n_classes, -1)
1.01 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
1.01 logistic.py(287):     if fit_intercept:
1.01 logistic.py(288):         intercept = w[:, -1]
1.01 logistic.py(289):         w = w[:, :-1]
1.01 logistic.py(292):     p = safe_sparse_dot(X, w.T)
1.01 logistic.py(293):     p += intercept
1.01 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
1.01 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
1.01 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
1.01 logistic.py(297):     p = np.exp(p, p)
1.01 logistic.py(298):     return loss, p, w
1.01 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
1.01 logistic.py(346):     diff = sample_weight * (p - Y)
1.01 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
1.01 logistic.py(348):     grad[:, :n_features] += alpha * w
1.01 logistic.py(349):     if fit_intercept:
1.01 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
1.01 logistic.py(351):     return loss, grad.ravel(), p
1.01 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
1.01 logistic.py(339):     n_classes = Y.shape[1]
1.01 logistic.py(340):     n_features = X.shape[1]
1.01 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
1.01 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
1.01 logistic.py(343):                     dtype=X.dtype)
1.01 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
1.01 logistic.py(282):     n_classes = Y.shape[1]
1.01 logistic.py(283):     n_features = X.shape[1]
1.01 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
1.01 logistic.py(285):     w = w.reshape(n_classes, -1)
1.01 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
1.01 logistic.py(287):     if fit_intercept:
1.01 logistic.py(288):         intercept = w[:, -1]
1.01 logistic.py(289):         w = w[:, :-1]
1.01 logistic.py(292):     p = safe_sparse_dot(X, w.T)
1.01 logistic.py(293):     p += intercept
1.01 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
1.01 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
1.01 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
1.01 logistic.py(297):     p = np.exp(p, p)
1.01 logistic.py(298):     return loss, p, w
1.01 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
1.01 logistic.py(346):     diff = sample_weight * (p - Y)
1.01 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
1.01 logistic.py(348):     grad[:, :n_features] += alpha * w
1.01 logistic.py(349):     if fit_intercept:
1.01 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
1.01 logistic.py(351):     return loss, grad.ravel(), p
1.01 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
1.01 logistic.py(339):     n_classes = Y.shape[1]
1.01 logistic.py(340):     n_features = X.shape[1]
1.01 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
1.01 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
1.01 logistic.py(343):                     dtype=X.dtype)
1.01 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
1.01 logistic.py(282):     n_classes = Y.shape[1]
1.01 logistic.py(283):     n_features = X.shape[1]
1.01 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
1.01 logistic.py(285):     w = w.reshape(n_classes, -1)
1.01 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
1.01 logistic.py(287):     if fit_intercept:
1.01 logistic.py(288):         intercept = w[:, -1]
1.01 logistic.py(289):         w = w[:, :-1]
1.01 logistic.py(292):     p = safe_sparse_dot(X, w.T)
1.01 logistic.py(293):     p += intercept
1.01 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
1.01 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
1.01 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
1.01 logistic.py(297):     p = np.exp(p, p)
1.01 logistic.py(298):     return loss, p, w
1.01 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
1.01 logistic.py(346):     diff = sample_weight * (p - Y)
1.01 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
1.01 logistic.py(348):     grad[:, :n_features] += alpha * w
1.01 logistic.py(349):     if fit_intercept:
1.01 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
1.01 logistic.py(351):     return loss, grad.ravel(), p
1.01 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
1.01 logistic.py(339):     n_classes = Y.shape[1]
1.01 logistic.py(340):     n_features = X.shape[1]
1.01 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
1.01 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
1.01 logistic.py(343):                     dtype=X.dtype)
1.01 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
1.01 logistic.py(282):     n_classes = Y.shape[1]
1.01 logistic.py(283):     n_features = X.shape[1]
1.01 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
1.01 logistic.py(285):     w = w.reshape(n_classes, -1)
1.01 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
1.01 logistic.py(287):     if fit_intercept:
1.01 logistic.py(288):         intercept = w[:, -1]
1.01 logistic.py(289):         w = w[:, :-1]
1.01 logistic.py(292):     p = safe_sparse_dot(X, w.T)
1.01 logistic.py(293):     p += intercept
1.01 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
1.01 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
1.01 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
1.01 logistic.py(297):     p = np.exp(p, p)
1.01 logistic.py(298):     return loss, p, w
1.01 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
1.01 logistic.py(346):     diff = sample_weight * (p - Y)
1.01 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
1.01 logistic.py(348):     grad[:, :n_features] += alpha * w
1.01 logistic.py(349):     if fit_intercept:
1.01 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
1.01 logistic.py(351):     return loss, grad.ravel(), p
1.01 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
1.01 logistic.py(339):     n_classes = Y.shape[1]
1.01 logistic.py(340):     n_features = X.shape[1]
1.01 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
1.01 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
1.01 logistic.py(343):                     dtype=X.dtype)
1.01 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
1.01 logistic.py(282):     n_classes = Y.shape[1]
1.01 logistic.py(283):     n_features = X.shape[1]
1.01 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
1.01 logistic.py(285):     w = w.reshape(n_classes, -1)
1.01 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
1.01 logistic.py(287):     if fit_intercept:
1.01 logistic.py(288):         intercept = w[:, -1]
1.01 logistic.py(289):         w = w[:, :-1]
1.01 logistic.py(292):     p = safe_sparse_dot(X, w.T)
1.01 logistic.py(293):     p += intercept
1.01 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
1.01 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
1.01 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
1.01 logistic.py(297):     p = np.exp(p, p)
1.01 logistic.py(298):     return loss, p, w
1.01 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
1.01 logistic.py(346):     diff = sample_weight * (p - Y)
1.01 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
1.01 logistic.py(348):     grad[:, :n_features] += alpha * w
1.01 logistic.py(349):     if fit_intercept:
1.01 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
1.01 logistic.py(351):     return loss, grad.ravel(), p
1.01 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
1.01 logistic.py(339):     n_classes = Y.shape[1]
1.01 logistic.py(340):     n_features = X.shape[1]
1.01 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
1.01 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
1.01 logistic.py(343):                     dtype=X.dtype)
1.01 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
1.01 logistic.py(282):     n_classes = Y.shape[1]
1.01 logistic.py(283):     n_features = X.shape[1]
1.01 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
1.01 logistic.py(285):     w = w.reshape(n_classes, -1)
1.01 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
1.01 logistic.py(287):     if fit_intercept:
1.01 logistic.py(288):         intercept = w[:, -1]
1.01 logistic.py(289):         w = w[:, :-1]
1.01 logistic.py(292):     p = safe_sparse_dot(X, w.T)
1.01 logistic.py(293):     p += intercept
1.01 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
1.01 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
1.01 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
1.01 logistic.py(297):     p = np.exp(p, p)
1.01 logistic.py(298):     return loss, p, w
1.01 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
1.01 logistic.py(346):     diff = sample_weight * (p - Y)
1.01 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
1.01 logistic.py(348):     grad[:, :n_features] += alpha * w
1.01 logistic.py(349):     if fit_intercept:
1.01 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
1.01 logistic.py(351):     return loss, grad.ravel(), p
1.01 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
1.01 logistic.py(339):     n_classes = Y.shape[1]
1.01 logistic.py(340):     n_features = X.shape[1]
1.01 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
1.01 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
1.01 logistic.py(343):                     dtype=X.dtype)
1.01 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
1.01 logistic.py(282):     n_classes = Y.shape[1]
1.01 logistic.py(283):     n_features = X.shape[1]
1.01 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
1.01 logistic.py(285):     w = w.reshape(n_classes, -1)
1.01 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
1.01 logistic.py(287):     if fit_intercept:
1.01 logistic.py(288):         intercept = w[:, -1]
1.01 logistic.py(289):         w = w[:, :-1]
1.01 logistic.py(292):     p = safe_sparse_dot(X, w.T)
1.01 logistic.py(293):     p += intercept
1.01 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
1.01 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
1.01 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
1.01 logistic.py(297):     p = np.exp(p, p)
1.01 logistic.py(298):     return loss, p, w
1.01 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
1.01 logistic.py(346):     diff = sample_weight * (p - Y)
1.01 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
1.01 logistic.py(348):     grad[:, :n_features] += alpha * w
1.01 logistic.py(349):     if fit_intercept:
1.01 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
1.01 logistic.py(351):     return loss, grad.ravel(), p
1.01 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
1.01 logistic.py(339):     n_classes = Y.shape[1]
1.01 logistic.py(340):     n_features = X.shape[1]
1.01 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
1.01 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
1.01 logistic.py(343):                     dtype=X.dtype)
1.01 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
1.01 logistic.py(282):     n_classes = Y.shape[1]
1.01 logistic.py(283):     n_features = X.shape[1]
1.01 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
1.01 logistic.py(285):     w = w.reshape(n_classes, -1)
1.01 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
1.01 logistic.py(287):     if fit_intercept:
1.01 logistic.py(288):         intercept = w[:, -1]
1.01 logistic.py(289):         w = w[:, :-1]
1.01 logistic.py(292):     p = safe_sparse_dot(X, w.T)
1.01 logistic.py(293):     p += intercept
1.01 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
1.01 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
1.01 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
1.01 logistic.py(297):     p = np.exp(p, p)
1.01 logistic.py(298):     return loss, p, w
1.01 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
1.01 logistic.py(346):     diff = sample_weight * (p - Y)
1.01 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
1.01 logistic.py(348):     grad[:, :n_features] += alpha * w
1.01 logistic.py(349):     if fit_intercept:
1.01 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
1.01 logistic.py(351):     return loss, grad.ravel(), p
1.01 logistic.py(718):             if info["warnflag"] == 1:
1.01 logistic.py(723):             n_iter_i = min(info['nit'], max_iter)
1.01 logistic.py(760):         if multi_class == 'multinomial':
1.01 logistic.py(761):             multi_w0 = np.reshape(w0, (classes.size, -1))
1.01 logistic.py(762):             if classes.size == 2:
1.01 logistic.py(764):             coefs.append(multi_w0)
1.01 logistic.py(768):         n_iter[i] = n_iter_i
1.01 logistic.py(712):     for i, C in enumerate(Cs):
1.01 logistic.py(713):         if solver == 'lbfgs':
1.01 logistic.py(714):             w0, loss, info = optimize.fmin_l_bfgs_b(
1.01 logistic.py(715):                 func, w0, fprime=None,
1.01 logistic.py(716):                 args=(X, target, 1. / C, sample_weight),
1.01 logistic.py(717):                 iprint=(verbose > 0) - 1, pgtol=tol, maxiter=max_iter)
1.01 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
1.01 logistic.py(339):     n_classes = Y.shape[1]
1.01 logistic.py(340):     n_features = X.shape[1]
1.01 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
1.01 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
1.01 logistic.py(343):                     dtype=X.dtype)
1.01 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
1.01 logistic.py(282):     n_classes = Y.shape[1]
1.01 logistic.py(283):     n_features = X.shape[1]
1.01 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
1.01 logistic.py(285):     w = w.reshape(n_classes, -1)
1.01 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
1.01 logistic.py(287):     if fit_intercept:
1.01 logistic.py(288):         intercept = w[:, -1]
1.02 logistic.py(289):         w = w[:, :-1]
1.02 logistic.py(292):     p = safe_sparse_dot(X, w.T)
1.02 logistic.py(293):     p += intercept
1.02 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
1.02 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
1.02 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
1.02 logistic.py(297):     p = np.exp(p, p)
1.02 logistic.py(298):     return loss, p, w
1.02 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
1.02 logistic.py(346):     diff = sample_weight * (p - Y)
1.02 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
1.02 logistic.py(348):     grad[:, :n_features] += alpha * w
1.02 logistic.py(349):     if fit_intercept:
1.02 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
1.02 logistic.py(351):     return loss, grad.ravel(), p
1.02 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
1.02 logistic.py(339):     n_classes = Y.shape[1]
1.02 logistic.py(340):     n_features = X.shape[1]
1.02 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
1.02 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
1.02 logistic.py(343):                     dtype=X.dtype)
1.02 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
1.02 logistic.py(282):     n_classes = Y.shape[1]
1.02 logistic.py(283):     n_features = X.shape[1]
1.02 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
1.02 logistic.py(285):     w = w.reshape(n_classes, -1)
1.02 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
1.02 logistic.py(287):     if fit_intercept:
1.02 logistic.py(288):         intercept = w[:, -1]
1.02 logistic.py(289):         w = w[:, :-1]
1.02 logistic.py(292):     p = safe_sparse_dot(X, w.T)
1.02 logistic.py(293):     p += intercept
1.02 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
1.02 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
1.02 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
1.02 logistic.py(297):     p = np.exp(p, p)
1.02 logistic.py(298):     return loss, p, w
1.02 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
1.02 logistic.py(346):     diff = sample_weight * (p - Y)
1.02 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
1.02 logistic.py(348):     grad[:, :n_features] += alpha * w
1.02 logistic.py(349):     if fit_intercept:
1.02 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
1.02 logistic.py(351):     return loss, grad.ravel(), p
1.02 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
1.02 logistic.py(339):     n_classes = Y.shape[1]
1.02 logistic.py(340):     n_features = X.shape[1]
1.02 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
1.02 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
1.02 logistic.py(343):                     dtype=X.dtype)
1.02 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
1.02 logistic.py(282):     n_classes = Y.shape[1]
1.02 logistic.py(283):     n_features = X.shape[1]
1.02 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
1.02 logistic.py(285):     w = w.reshape(n_classes, -1)
1.02 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
1.02 logistic.py(287):     if fit_intercept:
1.02 logistic.py(288):         intercept = w[:, -1]
1.02 logistic.py(289):         w = w[:, :-1]
1.02 logistic.py(292):     p = safe_sparse_dot(X, w.T)
1.02 logistic.py(293):     p += intercept
1.02 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
1.02 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
1.02 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
1.02 logistic.py(297):     p = np.exp(p, p)
1.02 logistic.py(298):     return loss, p, w
1.02 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
1.02 logistic.py(346):     diff = sample_weight * (p - Y)
1.02 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
1.02 logistic.py(348):     grad[:, :n_features] += alpha * w
1.02 logistic.py(349):     if fit_intercept:
1.02 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
1.02 logistic.py(351):     return loss, grad.ravel(), p
1.02 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
1.02 logistic.py(339):     n_classes = Y.shape[1]
1.02 logistic.py(340):     n_features = X.shape[1]
1.02 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
1.02 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
1.02 logistic.py(343):                     dtype=X.dtype)
1.02 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
1.02 logistic.py(282):     n_classes = Y.shape[1]
1.02 logistic.py(283):     n_features = X.shape[1]
1.02 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
1.02 logistic.py(285):     w = w.reshape(n_classes, -1)
1.02 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
1.02 logistic.py(287):     if fit_intercept:
1.02 logistic.py(288):         intercept = w[:, -1]
1.02 logistic.py(289):         w = w[:, :-1]
1.02 logistic.py(292):     p = safe_sparse_dot(X, w.T)
1.02 logistic.py(293):     p += intercept
1.02 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
1.02 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
1.02 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
1.02 logistic.py(297):     p = np.exp(p, p)
1.02 logistic.py(298):     return loss, p, w
1.02 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
1.02 logistic.py(346):     diff = sample_weight * (p - Y)
1.02 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
1.02 logistic.py(348):     grad[:, :n_features] += alpha * w
1.02 logistic.py(349):     if fit_intercept:
1.02 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
1.02 logistic.py(351):     return loss, grad.ravel(), p
1.02 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
1.02 logistic.py(339):     n_classes = Y.shape[1]
1.02 logistic.py(340):     n_features = X.shape[1]
1.02 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
1.02 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
1.02 logistic.py(343):                     dtype=X.dtype)
1.02 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
1.02 logistic.py(282):     n_classes = Y.shape[1]
1.02 logistic.py(283):     n_features = X.shape[1]
1.02 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
1.02 logistic.py(285):     w = w.reshape(n_classes, -1)
1.02 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
1.02 logistic.py(287):     if fit_intercept:
1.02 logistic.py(288):         intercept = w[:, -1]
1.02 logistic.py(289):         w = w[:, :-1]
1.02 logistic.py(292):     p = safe_sparse_dot(X, w.T)
1.02 logistic.py(293):     p += intercept
1.02 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
1.02 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
1.02 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
1.02 logistic.py(297):     p = np.exp(p, p)
1.02 logistic.py(298):     return loss, p, w
1.02 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
1.02 logistic.py(346):     diff = sample_weight * (p - Y)
1.02 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
1.02 logistic.py(348):     grad[:, :n_features] += alpha * w
1.02 logistic.py(349):     if fit_intercept:
1.02 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
1.02 logistic.py(351):     return loss, grad.ravel(), p
1.02 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
1.02 logistic.py(339):     n_classes = Y.shape[1]
1.02 logistic.py(340):     n_features = X.shape[1]
1.02 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
1.02 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
1.02 logistic.py(343):                     dtype=X.dtype)
1.02 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
1.02 logistic.py(282):     n_classes = Y.shape[1]
1.02 logistic.py(283):     n_features = X.shape[1]
1.02 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
1.02 logistic.py(285):     w = w.reshape(n_classes, -1)
1.02 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
1.02 logistic.py(287):     if fit_intercept:
1.02 logistic.py(288):         intercept = w[:, -1]
1.02 logistic.py(289):         w = w[:, :-1]
1.02 logistic.py(292):     p = safe_sparse_dot(X, w.T)
1.02 logistic.py(293):     p += intercept
1.02 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
1.02 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
1.02 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
1.02 logistic.py(297):     p = np.exp(p, p)
1.02 logistic.py(298):     return loss, p, w
1.02 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
1.02 logistic.py(346):     diff = sample_weight * (p - Y)
1.02 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
1.02 logistic.py(348):     grad[:, :n_features] += alpha * w
1.02 logistic.py(349):     if fit_intercept:
1.02 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
1.02 logistic.py(351):     return loss, grad.ravel(), p
1.02 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
1.02 logistic.py(339):     n_classes = Y.shape[1]
1.02 logistic.py(340):     n_features = X.shape[1]
1.02 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
1.02 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
1.02 logistic.py(343):                     dtype=X.dtype)
1.02 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
1.02 logistic.py(282):     n_classes = Y.shape[1]
1.02 logistic.py(283):     n_features = X.shape[1]
1.02 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
1.02 logistic.py(285):     w = w.reshape(n_classes, -1)
1.02 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
1.02 logistic.py(287):     if fit_intercept:
1.02 logistic.py(288):         intercept = w[:, -1]
1.02 logistic.py(289):         w = w[:, :-1]
1.02 logistic.py(292):     p = safe_sparse_dot(X, w.T)
1.02 logistic.py(293):     p += intercept
1.02 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
1.02 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
1.02 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
1.02 logistic.py(297):     p = np.exp(p, p)
1.02 logistic.py(298):     return loss, p, w
1.02 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
1.02 logistic.py(346):     diff = sample_weight * (p - Y)
1.02 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
1.02 logistic.py(348):     grad[:, :n_features] += alpha * w
1.02 logistic.py(349):     if fit_intercept:
1.02 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
1.02 logistic.py(351):     return loss, grad.ravel(), p
1.02 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
1.02 logistic.py(339):     n_classes = Y.shape[1]
1.02 logistic.py(340):     n_features = X.shape[1]
1.02 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
1.02 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
1.02 logistic.py(343):                     dtype=X.dtype)
1.02 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
1.02 logistic.py(282):     n_classes = Y.shape[1]
1.02 logistic.py(283):     n_features = X.shape[1]
1.02 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
1.02 logistic.py(285):     w = w.reshape(n_classes, -1)
1.02 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
1.02 logistic.py(287):     if fit_intercept:
1.02 logistic.py(288):         intercept = w[:, -1]
1.02 logistic.py(289):         w = w[:, :-1]
1.02 logistic.py(292):     p = safe_sparse_dot(X, w.T)
1.02 logistic.py(293):     p += intercept
1.02 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
1.02 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
1.02 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
1.02 logistic.py(297):     p = np.exp(p, p)
1.02 logistic.py(298):     return loss, p, w
1.02 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
1.02 logistic.py(346):     diff = sample_weight * (p - Y)
1.02 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
1.02 logistic.py(348):     grad[:, :n_features] += alpha * w
1.02 logistic.py(349):     if fit_intercept:
1.02 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
1.02 logistic.py(351):     return loss, grad.ravel(), p
1.02 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
1.02 logistic.py(339):     n_classes = Y.shape[1]
1.02 logistic.py(340):     n_features = X.shape[1]
1.02 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
1.02 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
1.02 logistic.py(343):                     dtype=X.dtype)
1.02 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
1.02 logistic.py(282):     n_classes = Y.shape[1]
1.02 logistic.py(283):     n_features = X.shape[1]
1.02 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
1.02 logistic.py(285):     w = w.reshape(n_classes, -1)
1.02 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
1.02 logistic.py(287):     if fit_intercept:
1.02 logistic.py(288):         intercept = w[:, -1]
1.02 logistic.py(289):         w = w[:, :-1]
1.02 logistic.py(292):     p = safe_sparse_dot(X, w.T)
1.02 logistic.py(293):     p += intercept
1.02 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
1.02 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
1.02 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
1.02 logistic.py(297):     p = np.exp(p, p)
1.02 logistic.py(298):     return loss, p, w
1.02 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
1.02 logistic.py(346):     diff = sample_weight * (p - Y)
1.02 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
1.02 logistic.py(348):     grad[:, :n_features] += alpha * w
1.02 logistic.py(349):     if fit_intercept:
1.02 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
1.02 logistic.py(351):     return loss, grad.ravel(), p
1.02 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
1.02 logistic.py(339):     n_classes = Y.shape[1]
1.02 logistic.py(340):     n_features = X.shape[1]
1.02 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
1.02 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
1.02 logistic.py(343):                     dtype=X.dtype)
1.02 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
1.02 logistic.py(282):     n_classes = Y.shape[1]
1.02 logistic.py(283):     n_features = X.shape[1]
1.02 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
1.02 logistic.py(285):     w = w.reshape(n_classes, -1)
1.02 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
1.02 logistic.py(287):     if fit_intercept:
1.02 logistic.py(288):         intercept = w[:, -1]
1.02 logistic.py(289):         w = w[:, :-1]
1.02 logistic.py(292):     p = safe_sparse_dot(X, w.T)
1.02 logistic.py(293):     p += intercept
1.02 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
1.02 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
1.02 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
1.02 logistic.py(297):     p = np.exp(p, p)
1.02 logistic.py(298):     return loss, p, w
1.02 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
1.02 logistic.py(346):     diff = sample_weight * (p - Y)
1.02 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
1.02 logistic.py(348):     grad[:, :n_features] += alpha * w
1.02 logistic.py(349):     if fit_intercept:
1.02 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
1.02 logistic.py(351):     return loss, grad.ravel(), p
1.02 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
1.02 logistic.py(339):     n_classes = Y.shape[1]
1.02 logistic.py(340):     n_features = X.shape[1]
1.02 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
1.02 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
1.02 logistic.py(343):                     dtype=X.dtype)
1.02 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
1.02 logistic.py(282):     n_classes = Y.shape[1]
1.02 logistic.py(283):     n_features = X.shape[1]
1.02 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
1.02 logistic.py(285):     w = w.reshape(n_classes, -1)
1.02 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
1.02 logistic.py(287):     if fit_intercept:
1.02 logistic.py(288):         intercept = w[:, -1]
1.02 logistic.py(289):         w = w[:, :-1]
1.02 logistic.py(292):     p = safe_sparse_dot(X, w.T)
1.02 logistic.py(293):     p += intercept
1.02 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
1.02 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
1.02 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
1.02 logistic.py(297):     p = np.exp(p, p)
1.02 logistic.py(298):     return loss, p, w
1.02 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
1.02 logistic.py(346):     diff = sample_weight * (p - Y)
1.02 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
1.02 logistic.py(348):     grad[:, :n_features] += alpha * w
1.02 logistic.py(349):     if fit_intercept:
1.02 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
1.02 logistic.py(351):     return loss, grad.ravel(), p
1.02 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
1.02 logistic.py(339):     n_classes = Y.shape[1]
1.02 logistic.py(340):     n_features = X.shape[1]
1.02 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
1.02 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
1.02 logistic.py(343):                     dtype=X.dtype)
1.02 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
1.02 logistic.py(282):     n_classes = Y.shape[1]
1.02 logistic.py(283):     n_features = X.shape[1]
1.02 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
1.02 logistic.py(285):     w = w.reshape(n_classes, -1)
1.02 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
1.02 logistic.py(287):     if fit_intercept:
1.02 logistic.py(288):         intercept = w[:, -1]
1.02 logistic.py(289):         w = w[:, :-1]
1.02 logistic.py(292):     p = safe_sparse_dot(X, w.T)
1.02 logistic.py(293):     p += intercept
1.02 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
1.02 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
1.02 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
1.02 logistic.py(297):     p = np.exp(p, p)
1.02 logistic.py(298):     return loss, p, w
1.02 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
1.02 logistic.py(346):     diff = sample_weight * (p - Y)
1.02 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
1.02 logistic.py(348):     grad[:, :n_features] += alpha * w
1.02 logistic.py(349):     if fit_intercept:
1.02 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
1.02 logistic.py(351):     return loss, grad.ravel(), p
1.02 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
1.02 logistic.py(339):     n_classes = Y.shape[1]
1.02 logistic.py(340):     n_features = X.shape[1]
1.02 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
1.02 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
1.02 logistic.py(343):                     dtype=X.dtype)
1.02 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
1.02 logistic.py(282):     n_classes = Y.shape[1]
1.02 logistic.py(283):     n_features = X.shape[1]
1.02 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
1.02 logistic.py(285):     w = w.reshape(n_classes, -1)
1.02 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
1.02 logistic.py(287):     if fit_intercept:
1.02 logistic.py(288):         intercept = w[:, -1]
1.02 logistic.py(289):         w = w[:, :-1]
1.02 logistic.py(292):     p = safe_sparse_dot(X, w.T)
1.02 logistic.py(293):     p += intercept
1.02 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
1.02 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
1.02 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
1.02 logistic.py(297):     p = np.exp(p, p)
1.02 logistic.py(298):     return loss, p, w
1.02 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
1.02 logistic.py(346):     diff = sample_weight * (p - Y)
1.02 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
1.02 logistic.py(348):     grad[:, :n_features] += alpha * w
1.02 logistic.py(349):     if fit_intercept:
1.02 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
1.02 logistic.py(351):     return loss, grad.ravel(), p
1.02 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
1.02 logistic.py(339):     n_classes = Y.shape[1]
1.02 logistic.py(340):     n_features = X.shape[1]
1.02 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
1.02 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
1.02 logistic.py(343):                     dtype=X.dtype)
1.02 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
1.02 logistic.py(282):     n_classes = Y.shape[1]
1.02 logistic.py(283):     n_features = X.shape[1]
1.02 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
1.02 logistic.py(285):     w = w.reshape(n_classes, -1)
1.02 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
1.02 logistic.py(287):     if fit_intercept:
1.02 logistic.py(288):         intercept = w[:, -1]
1.02 logistic.py(289):         w = w[:, :-1]
1.02 logistic.py(292):     p = safe_sparse_dot(X, w.T)
1.02 logistic.py(293):     p += intercept
1.02 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
1.02 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
1.02 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
1.02 logistic.py(297):     p = np.exp(p, p)
1.02 logistic.py(298):     return loss, p, w
1.02 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
1.02 logistic.py(346):     diff = sample_weight * (p - Y)
1.02 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
1.02 logistic.py(348):     grad[:, :n_features] += alpha * w
1.02 logistic.py(349):     if fit_intercept:
1.02 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
1.02 logistic.py(351):     return loss, grad.ravel(), p
1.02 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
1.02 logistic.py(339):     n_classes = Y.shape[1]
1.02 logistic.py(340):     n_features = X.shape[1]
1.02 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
1.02 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
1.02 logistic.py(343):                     dtype=X.dtype)
1.02 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
1.02 logistic.py(282):     n_classes = Y.shape[1]
1.02 logistic.py(283):     n_features = X.shape[1]
1.02 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
1.02 logistic.py(285):     w = w.reshape(n_classes, -1)
1.02 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
1.02 logistic.py(287):     if fit_intercept:
1.02 logistic.py(288):         intercept = w[:, -1]
1.02 logistic.py(289):         w = w[:, :-1]
1.02 logistic.py(292):     p = safe_sparse_dot(X, w.T)
1.02 logistic.py(293):     p += intercept
1.02 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
1.02 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
1.02 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
1.02 logistic.py(297):     p = np.exp(p, p)
1.02 logistic.py(298):     return loss, p, w
1.02 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
1.02 logistic.py(346):     diff = sample_weight * (p - Y)
1.02 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
1.02 logistic.py(348):     grad[:, :n_features] += alpha * w
1.02 logistic.py(349):     if fit_intercept:
1.02 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
1.02 logistic.py(351):     return loss, grad.ravel(), p
1.02 logistic.py(718):             if info["warnflag"] == 1:
1.02 logistic.py(723):             n_iter_i = min(info['nit'], max_iter)
1.02 logistic.py(760):         if multi_class == 'multinomial':
1.02 logistic.py(761):             multi_w0 = np.reshape(w0, (classes.size, -1))
1.02 logistic.py(762):             if classes.size == 2:
1.02 logistic.py(764):             coefs.append(multi_w0)
1.02 logistic.py(768):         n_iter[i] = n_iter_i
1.02 logistic.py(712):     for i, C in enumerate(Cs):
1.02 logistic.py(713):         if solver == 'lbfgs':
1.02 logistic.py(714):             w0, loss, info = optimize.fmin_l_bfgs_b(
1.02 logistic.py(715):                 func, w0, fprime=None,
1.02 logistic.py(716):                 args=(X, target, 1. / C, sample_weight),
1.02 logistic.py(717):                 iprint=(verbose > 0) - 1, pgtol=tol, maxiter=max_iter)
1.02 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
1.02 logistic.py(339):     n_classes = Y.shape[1]
1.02 logistic.py(340):     n_features = X.shape[1]
1.02 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
1.02 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
1.02 logistic.py(343):                     dtype=X.dtype)
1.02 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
1.02 logistic.py(282):     n_classes = Y.shape[1]
1.02 logistic.py(283):     n_features = X.shape[1]
1.02 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
1.02 logistic.py(285):     w = w.reshape(n_classes, -1)
1.02 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
1.02 logistic.py(287):     if fit_intercept:
1.02 logistic.py(288):         intercept = w[:, -1]
1.02 logistic.py(289):         w = w[:, :-1]
1.02 logistic.py(292):     p = safe_sparse_dot(X, w.T)
1.02 logistic.py(293):     p += intercept
1.02 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
1.03 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
1.03 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
1.03 logistic.py(297):     p = np.exp(p, p)
1.03 logistic.py(298):     return loss, p, w
1.03 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
1.03 logistic.py(346):     diff = sample_weight * (p - Y)
1.03 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
1.03 logistic.py(348):     grad[:, :n_features] += alpha * w
1.03 logistic.py(349):     if fit_intercept:
1.03 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
1.03 logistic.py(351):     return loss, grad.ravel(), p
1.03 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
1.03 logistic.py(339):     n_classes = Y.shape[1]
1.03 logistic.py(340):     n_features = X.shape[1]
1.03 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
1.03 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
1.03 logistic.py(343):                     dtype=X.dtype)
1.03 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
1.03 logistic.py(282):     n_classes = Y.shape[1]
1.03 logistic.py(283):     n_features = X.shape[1]
1.03 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
1.03 logistic.py(285):     w = w.reshape(n_classes, -1)
1.03 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
1.03 logistic.py(287):     if fit_intercept:
1.03 logistic.py(288):         intercept = w[:, -1]
1.03 logistic.py(289):         w = w[:, :-1]
1.03 logistic.py(292):     p = safe_sparse_dot(X, w.T)
1.03 logistic.py(293):     p += intercept
1.03 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
1.03 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
1.03 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
1.03 logistic.py(297):     p = np.exp(p, p)
1.03 logistic.py(298):     return loss, p, w
1.03 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
1.03 logistic.py(346):     diff = sample_weight * (p - Y)
1.03 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
1.03 logistic.py(348):     grad[:, :n_features] += alpha * w
1.03 logistic.py(349):     if fit_intercept:
1.03 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
1.03 logistic.py(351):     return loss, grad.ravel(), p
1.03 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
1.03 logistic.py(339):     n_classes = Y.shape[1]
1.03 logistic.py(340):     n_features = X.shape[1]
1.03 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
1.03 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
1.03 logistic.py(343):                     dtype=X.dtype)
1.03 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
1.03 logistic.py(282):     n_classes = Y.shape[1]
1.03 logistic.py(283):     n_features = X.shape[1]
1.03 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
1.03 logistic.py(285):     w = w.reshape(n_classes, -1)
1.03 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
1.03 logistic.py(287):     if fit_intercept:
1.03 logistic.py(288):         intercept = w[:, -1]
1.03 logistic.py(289):         w = w[:, :-1]
1.03 logistic.py(292):     p = safe_sparse_dot(X, w.T)
1.03 logistic.py(293):     p += intercept
1.03 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
1.03 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
1.03 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
1.03 logistic.py(297):     p = np.exp(p, p)
1.03 logistic.py(298):     return loss, p, w
1.03 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
1.03 logistic.py(346):     diff = sample_weight * (p - Y)
1.03 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
1.03 logistic.py(348):     grad[:, :n_features] += alpha * w
1.03 logistic.py(349):     if fit_intercept:
1.03 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
1.03 logistic.py(351):     return loss, grad.ravel(), p
1.03 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
1.03 logistic.py(339):     n_classes = Y.shape[1]
1.03 logistic.py(340):     n_features = X.shape[1]
1.03 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
1.03 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
1.03 logistic.py(343):                     dtype=X.dtype)
1.03 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
1.03 logistic.py(282):     n_classes = Y.shape[1]
1.03 logistic.py(283):     n_features = X.shape[1]
1.03 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
1.03 logistic.py(285):     w = w.reshape(n_classes, -1)
1.03 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
1.03 logistic.py(287):     if fit_intercept:
1.03 logistic.py(288):         intercept = w[:, -1]
1.03 logistic.py(289):         w = w[:, :-1]
1.03 logistic.py(292):     p = safe_sparse_dot(X, w.T)
1.03 logistic.py(293):     p += intercept
1.03 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
1.03 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
1.03 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
1.03 logistic.py(297):     p = np.exp(p, p)
1.03 logistic.py(298):     return loss, p, w
1.03 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
1.03 logistic.py(346):     diff = sample_weight * (p - Y)
1.03 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
1.03 logistic.py(348):     grad[:, :n_features] += alpha * w
1.03 logistic.py(349):     if fit_intercept:
1.03 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
1.03 logistic.py(351):     return loss, grad.ravel(), p
1.03 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
1.03 logistic.py(339):     n_classes = Y.shape[1]
1.03 logistic.py(340):     n_features = X.shape[1]
1.03 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
1.03 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
1.03 logistic.py(343):                     dtype=X.dtype)
1.03 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
1.03 logistic.py(282):     n_classes = Y.shape[1]
1.03 logistic.py(283):     n_features = X.shape[1]
1.03 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
1.03 logistic.py(285):     w = w.reshape(n_classes, -1)
1.03 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
1.03 logistic.py(287):     if fit_intercept:
1.03 logistic.py(288):         intercept = w[:, -1]
1.03 logistic.py(289):         w = w[:, :-1]
1.03 logistic.py(292):     p = safe_sparse_dot(X, w.T)
1.03 logistic.py(293):     p += intercept
1.03 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
1.03 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
1.03 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
1.03 logistic.py(297):     p = np.exp(p, p)
1.03 logistic.py(298):     return loss, p, w
1.03 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
1.03 logistic.py(346):     diff = sample_weight * (p - Y)
1.03 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
1.03 logistic.py(348):     grad[:, :n_features] += alpha * w
1.03 logistic.py(349):     if fit_intercept:
1.03 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
1.03 logistic.py(351):     return loss, grad.ravel(), p
1.03 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
1.03 logistic.py(339):     n_classes = Y.shape[1]
1.03 logistic.py(340):     n_features = X.shape[1]
1.03 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
1.03 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
1.03 logistic.py(343):                     dtype=X.dtype)
1.03 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
1.03 logistic.py(282):     n_classes = Y.shape[1]
1.03 logistic.py(283):     n_features = X.shape[1]
1.03 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
1.03 logistic.py(285):     w = w.reshape(n_classes, -1)
1.03 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
1.03 logistic.py(287):     if fit_intercept:
1.03 logistic.py(288):         intercept = w[:, -1]
1.03 logistic.py(289):         w = w[:, :-1]
1.03 logistic.py(292):     p = safe_sparse_dot(X, w.T)
1.03 logistic.py(293):     p += intercept
1.03 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
1.03 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
1.03 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
1.03 logistic.py(297):     p = np.exp(p, p)
1.03 logistic.py(298):     return loss, p, w
1.03 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
1.03 logistic.py(346):     diff = sample_weight * (p - Y)
1.03 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
1.03 logistic.py(348):     grad[:, :n_features] += alpha * w
1.03 logistic.py(349):     if fit_intercept:
1.03 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
1.03 logistic.py(351):     return loss, grad.ravel(), p
1.03 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
1.03 logistic.py(339):     n_classes = Y.shape[1]
1.03 logistic.py(340):     n_features = X.shape[1]
1.03 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
1.03 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
1.03 logistic.py(343):                     dtype=X.dtype)
1.03 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
1.03 logistic.py(282):     n_classes = Y.shape[1]
1.03 logistic.py(283):     n_features = X.shape[1]
1.03 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
1.03 logistic.py(285):     w = w.reshape(n_classes, -1)
1.03 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
1.03 logistic.py(287):     if fit_intercept:
1.03 logistic.py(288):         intercept = w[:, -1]
1.03 logistic.py(289):         w = w[:, :-1]
1.03 logistic.py(292):     p = safe_sparse_dot(X, w.T)
1.03 logistic.py(293):     p += intercept
1.03 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
1.03 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
1.03 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
1.03 logistic.py(297):     p = np.exp(p, p)
1.03 logistic.py(298):     return loss, p, w
1.03 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
1.03 logistic.py(346):     diff = sample_weight * (p - Y)
1.03 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
1.03 logistic.py(348):     grad[:, :n_features] += alpha * w
1.03 logistic.py(349):     if fit_intercept:
1.03 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
1.03 logistic.py(351):     return loss, grad.ravel(), p
1.03 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
1.03 logistic.py(339):     n_classes = Y.shape[1]
1.03 logistic.py(340):     n_features = X.shape[1]
1.03 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
1.03 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
1.03 logistic.py(343):                     dtype=X.dtype)
1.03 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
1.03 logistic.py(282):     n_classes = Y.shape[1]
1.03 logistic.py(283):     n_features = X.shape[1]
1.03 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
1.03 logistic.py(285):     w = w.reshape(n_classes, -1)
1.03 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
1.03 logistic.py(287):     if fit_intercept:
1.03 logistic.py(288):         intercept = w[:, -1]
1.03 logistic.py(289):         w = w[:, :-1]
1.03 logistic.py(292):     p = safe_sparse_dot(X, w.T)
1.03 logistic.py(293):     p += intercept
1.03 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
1.03 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
1.03 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
1.03 logistic.py(297):     p = np.exp(p, p)
1.03 logistic.py(298):     return loss, p, w
1.03 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
1.03 logistic.py(346):     diff = sample_weight * (p - Y)
1.03 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
1.03 logistic.py(348):     grad[:, :n_features] += alpha * w
1.03 logistic.py(349):     if fit_intercept:
1.03 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
1.03 logistic.py(351):     return loss, grad.ravel(), p
1.03 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
1.03 logistic.py(339):     n_classes = Y.shape[1]
1.03 logistic.py(340):     n_features = X.shape[1]
1.03 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
1.03 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
1.03 logistic.py(343):                     dtype=X.dtype)
1.03 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
1.03 logistic.py(282):     n_classes = Y.shape[1]
1.03 logistic.py(283):     n_features = X.shape[1]
1.03 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
1.03 logistic.py(285):     w = w.reshape(n_classes, -1)
1.03 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
1.03 logistic.py(287):     if fit_intercept:
1.03 logistic.py(288):         intercept = w[:, -1]
1.03 logistic.py(289):         w = w[:, :-1]
1.03 logistic.py(292):     p = safe_sparse_dot(X, w.T)
1.03 logistic.py(293):     p += intercept
1.03 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
1.03 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
1.03 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
1.03 logistic.py(297):     p = np.exp(p, p)
1.03 logistic.py(298):     return loss, p, w
1.03 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
1.03 logistic.py(346):     diff = sample_weight * (p - Y)
1.03 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
1.03 logistic.py(348):     grad[:, :n_features] += alpha * w
1.03 logistic.py(349):     if fit_intercept:
1.03 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
1.03 logistic.py(351):     return loss, grad.ravel(), p
1.03 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
1.03 logistic.py(339):     n_classes = Y.shape[1]
1.03 logistic.py(340):     n_features = X.shape[1]
1.03 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
1.03 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
1.03 logistic.py(343):                     dtype=X.dtype)
1.03 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
1.03 logistic.py(282):     n_classes = Y.shape[1]
1.03 logistic.py(283):     n_features = X.shape[1]
1.03 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
1.03 logistic.py(285):     w = w.reshape(n_classes, -1)
1.03 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
1.03 logistic.py(287):     if fit_intercept:
1.03 logistic.py(288):         intercept = w[:, -1]
1.03 logistic.py(289):         w = w[:, :-1]
1.03 logistic.py(292):     p = safe_sparse_dot(X, w.T)
1.03 logistic.py(293):     p += intercept
1.03 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
1.03 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
1.03 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
1.03 logistic.py(297):     p = np.exp(p, p)
1.03 logistic.py(298):     return loss, p, w
1.03 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
1.03 logistic.py(346):     diff = sample_weight * (p - Y)
1.03 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
1.03 logistic.py(348):     grad[:, :n_features] += alpha * w
1.03 logistic.py(349):     if fit_intercept:
1.03 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
1.03 logistic.py(351):     return loss, grad.ravel(), p
1.03 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
1.03 logistic.py(339):     n_classes = Y.shape[1]
1.03 logistic.py(340):     n_features = X.shape[1]
1.03 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
1.03 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
1.03 logistic.py(343):                     dtype=X.dtype)
1.03 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
1.03 logistic.py(282):     n_classes = Y.shape[1]
1.03 logistic.py(283):     n_features = X.shape[1]
1.03 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
1.03 logistic.py(285):     w = w.reshape(n_classes, -1)
1.03 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
1.03 logistic.py(287):     if fit_intercept:
1.03 logistic.py(288):         intercept = w[:, -1]
1.03 logistic.py(289):         w = w[:, :-1]
1.03 logistic.py(292):     p = safe_sparse_dot(X, w.T)
1.03 logistic.py(293):     p += intercept
1.03 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
1.03 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
1.03 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
1.03 logistic.py(297):     p = np.exp(p, p)
1.03 logistic.py(298):     return loss, p, w
1.03 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
1.03 logistic.py(346):     diff = sample_weight * (p - Y)
1.03 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
1.03 logistic.py(348):     grad[:, :n_features] += alpha * w
1.03 logistic.py(349):     if fit_intercept:
1.03 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
1.03 logistic.py(351):     return loss, grad.ravel(), p
1.03 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
1.03 logistic.py(339):     n_classes = Y.shape[1]
1.03 logistic.py(340):     n_features = X.shape[1]
1.03 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
1.03 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
1.03 logistic.py(343):                     dtype=X.dtype)
1.03 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
1.03 logistic.py(282):     n_classes = Y.shape[1]
1.03 logistic.py(283):     n_features = X.shape[1]
1.03 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
1.03 logistic.py(285):     w = w.reshape(n_classes, -1)
1.03 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
1.03 logistic.py(287):     if fit_intercept:
1.03 logistic.py(288):         intercept = w[:, -1]
1.03 logistic.py(289):         w = w[:, :-1]
1.03 logistic.py(292):     p = safe_sparse_dot(X, w.T)
1.03 logistic.py(293):     p += intercept
1.03 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
1.03 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
1.03 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
1.03 logistic.py(297):     p = np.exp(p, p)
1.03 logistic.py(298):     return loss, p, w
1.03 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
1.03 logistic.py(346):     diff = sample_weight * (p - Y)
1.03 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
1.03 logistic.py(348):     grad[:, :n_features] += alpha * w
1.03 logistic.py(349):     if fit_intercept:
1.03 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
1.03 logistic.py(351):     return loss, grad.ravel(), p
1.03 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
1.03 logistic.py(339):     n_classes = Y.shape[1]
1.03 logistic.py(340):     n_features = X.shape[1]
1.03 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
1.03 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
1.03 logistic.py(343):                     dtype=X.dtype)
1.03 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
1.03 logistic.py(282):     n_classes = Y.shape[1]
1.03 logistic.py(283):     n_features = X.shape[1]
1.03 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
1.03 logistic.py(285):     w = w.reshape(n_classes, -1)
1.03 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
1.03 logistic.py(287):     if fit_intercept:
1.03 logistic.py(288):         intercept = w[:, -1]
1.03 logistic.py(289):         w = w[:, :-1]
1.03 logistic.py(292):     p = safe_sparse_dot(X, w.T)
1.03 logistic.py(293):     p += intercept
1.03 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
1.03 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
1.03 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
1.03 logistic.py(297):     p = np.exp(p, p)
1.03 logistic.py(298):     return loss, p, w
1.03 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
1.03 logistic.py(346):     diff = sample_weight * (p - Y)
1.03 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
1.03 logistic.py(348):     grad[:, :n_features] += alpha * w
1.03 logistic.py(349):     if fit_intercept:
1.03 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
1.03 logistic.py(351):     return loss, grad.ravel(), p
1.03 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
1.03 logistic.py(339):     n_classes = Y.shape[1]
1.03 logistic.py(340):     n_features = X.shape[1]
1.03 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
1.03 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
1.03 logistic.py(343):                     dtype=X.dtype)
1.03 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
1.03 logistic.py(282):     n_classes = Y.shape[1]
1.03 logistic.py(283):     n_features = X.shape[1]
1.03 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
1.03 logistic.py(285):     w = w.reshape(n_classes, -1)
1.03 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
1.03 logistic.py(287):     if fit_intercept:
1.03 logistic.py(288):         intercept = w[:, -1]
1.03 logistic.py(289):         w = w[:, :-1]
1.03 logistic.py(292):     p = safe_sparse_dot(X, w.T)
1.03 logistic.py(293):     p += intercept
1.03 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
1.03 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
1.03 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
1.03 logistic.py(297):     p = np.exp(p, p)
1.03 logistic.py(298):     return loss, p, w
1.03 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
1.03 logistic.py(346):     diff = sample_weight * (p - Y)
1.03 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
1.03 logistic.py(348):     grad[:, :n_features] += alpha * w
1.03 logistic.py(349):     if fit_intercept:
1.03 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
1.03 logistic.py(351):     return loss, grad.ravel(), p
1.03 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
1.03 logistic.py(339):     n_classes = Y.shape[1]
1.03 logistic.py(340):     n_features = X.shape[1]
1.03 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
1.03 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
1.03 logistic.py(343):                     dtype=X.dtype)
1.03 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
1.03 logistic.py(282):     n_classes = Y.shape[1]
1.03 logistic.py(283):     n_features = X.shape[1]
1.03 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
1.03 logistic.py(285):     w = w.reshape(n_classes, -1)
1.03 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
1.03 logistic.py(287):     if fit_intercept:
1.03 logistic.py(288):         intercept = w[:, -1]
1.03 logistic.py(289):         w = w[:, :-1]
1.03 logistic.py(292):     p = safe_sparse_dot(X, w.T)
1.03 logistic.py(293):     p += intercept
1.03 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
1.03 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
1.03 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
1.03 logistic.py(297):     p = np.exp(p, p)
1.03 logistic.py(298):     return loss, p, w
1.03 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
1.03 logistic.py(346):     diff = sample_weight * (p - Y)
1.03 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
1.03 logistic.py(348):     grad[:, :n_features] += alpha * w
1.03 logistic.py(349):     if fit_intercept:
1.03 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
1.03 logistic.py(351):     return loss, grad.ravel(), p
1.03 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
1.03 logistic.py(339):     n_classes = Y.shape[1]
1.03 logistic.py(340):     n_features = X.shape[1]
1.03 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
1.03 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
1.03 logistic.py(343):                     dtype=X.dtype)
1.03 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
1.03 logistic.py(282):     n_classes = Y.shape[1]
1.03 logistic.py(283):     n_features = X.shape[1]
1.03 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
1.03 logistic.py(285):     w = w.reshape(n_classes, -1)
1.03 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
1.03 logistic.py(287):     if fit_intercept:
1.03 logistic.py(288):         intercept = w[:, -1]
1.03 logistic.py(289):         w = w[:, :-1]
1.03 logistic.py(292):     p = safe_sparse_dot(X, w.T)
1.03 logistic.py(293):     p += intercept
1.03 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
1.03 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
1.03 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
1.03 logistic.py(297):     p = np.exp(p, p)
1.03 logistic.py(298):     return loss, p, w
1.03 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
1.03 logistic.py(346):     diff = sample_weight * (p - Y)
1.03 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
1.03 logistic.py(348):     grad[:, :n_features] += alpha * w
1.03 logistic.py(349):     if fit_intercept:
1.03 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
1.03 logistic.py(351):     return loss, grad.ravel(), p
1.03 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
1.03 logistic.py(339):     n_classes = Y.shape[1]
1.03 logistic.py(340):     n_features = X.shape[1]
1.03 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
1.03 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
1.03 logistic.py(343):                     dtype=X.dtype)
1.03 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
1.03 logistic.py(282):     n_classes = Y.shape[1]
1.03 logistic.py(283):     n_features = X.shape[1]
1.03 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
1.03 logistic.py(285):     w = w.reshape(n_classes, -1)
1.03 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
1.04 logistic.py(287):     if fit_intercept:
1.04 logistic.py(288):         intercept = w[:, -1]
1.04 logistic.py(289):         w = w[:, :-1]
1.04 logistic.py(292):     p = safe_sparse_dot(X, w.T)
1.04 logistic.py(293):     p += intercept
1.04 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
1.04 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
1.04 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
1.04 logistic.py(297):     p = np.exp(p, p)
1.04 logistic.py(298):     return loss, p, w
1.04 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
1.04 logistic.py(346):     diff = sample_weight * (p - Y)
1.04 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
1.04 logistic.py(348):     grad[:, :n_features] += alpha * w
1.04 logistic.py(349):     if fit_intercept:
1.04 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
1.04 logistic.py(351):     return loss, grad.ravel(), p
1.04 logistic.py(718):             if info["warnflag"] == 1:
1.04 logistic.py(723):             n_iter_i = min(info['nit'], max_iter)
1.04 logistic.py(760):         if multi_class == 'multinomial':
1.04 logistic.py(761):             multi_w0 = np.reshape(w0, (classes.size, -1))
1.04 logistic.py(762):             if classes.size == 2:
1.04 logistic.py(764):             coefs.append(multi_w0)
1.04 logistic.py(768):         n_iter[i] = n_iter_i
1.04 logistic.py(712):     for i, C in enumerate(Cs):
1.04 logistic.py(713):         if solver == 'lbfgs':
1.04 logistic.py(714):             w0, loss, info = optimize.fmin_l_bfgs_b(
1.04 logistic.py(715):                 func, w0, fprime=None,
1.04 logistic.py(716):                 args=(X, target, 1. / C, sample_weight),
1.04 logistic.py(717):                 iprint=(verbose > 0) - 1, pgtol=tol, maxiter=max_iter)
1.04 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
1.04 logistic.py(339):     n_classes = Y.shape[1]
1.04 logistic.py(340):     n_features = X.shape[1]
1.04 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
1.04 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
1.04 logistic.py(343):                     dtype=X.dtype)
1.04 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
1.04 logistic.py(282):     n_classes = Y.shape[1]
1.04 logistic.py(283):     n_features = X.shape[1]
1.04 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
1.04 logistic.py(285):     w = w.reshape(n_classes, -1)
1.04 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
1.04 logistic.py(287):     if fit_intercept:
1.04 logistic.py(288):         intercept = w[:, -1]
1.04 logistic.py(289):         w = w[:, :-1]
1.04 logistic.py(292):     p = safe_sparse_dot(X, w.T)
1.04 logistic.py(293):     p += intercept
1.04 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
1.04 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
1.04 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
1.04 logistic.py(297):     p = np.exp(p, p)
1.04 logistic.py(298):     return loss, p, w
1.04 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
1.04 logistic.py(346):     diff = sample_weight * (p - Y)
1.04 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
1.04 logistic.py(348):     grad[:, :n_features] += alpha * w
1.04 logistic.py(349):     if fit_intercept:
1.04 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
1.04 logistic.py(351):     return loss, grad.ravel(), p
1.04 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
1.04 logistic.py(339):     n_classes = Y.shape[1]
1.04 logistic.py(340):     n_features = X.shape[1]
1.04 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
1.04 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
1.04 logistic.py(343):                     dtype=X.dtype)
1.04 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
1.04 logistic.py(282):     n_classes = Y.shape[1]
1.04 logistic.py(283):     n_features = X.shape[1]
1.04 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
1.04 logistic.py(285):     w = w.reshape(n_classes, -1)
1.04 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
1.04 logistic.py(287):     if fit_intercept:
1.04 logistic.py(288):         intercept = w[:, -1]
1.04 logistic.py(289):         w = w[:, :-1]
1.04 logistic.py(292):     p = safe_sparse_dot(X, w.T)
1.04 logistic.py(293):     p += intercept
1.04 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
1.04 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
1.04 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
1.04 logistic.py(297):     p = np.exp(p, p)
1.04 logistic.py(298):     return loss, p, w
1.04 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
1.04 logistic.py(346):     diff = sample_weight * (p - Y)
1.04 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
1.04 logistic.py(348):     grad[:, :n_features] += alpha * w
1.04 logistic.py(349):     if fit_intercept:
1.04 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
1.04 logistic.py(351):     return loss, grad.ravel(), p
1.04 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
1.04 logistic.py(339):     n_classes = Y.shape[1]
1.04 logistic.py(340):     n_features = X.shape[1]
1.04 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
1.04 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
1.04 logistic.py(343):                     dtype=X.dtype)
1.04 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
1.04 logistic.py(282):     n_classes = Y.shape[1]
1.04 logistic.py(283):     n_features = X.shape[1]
1.04 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
1.04 logistic.py(285):     w = w.reshape(n_classes, -1)
1.04 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
1.04 logistic.py(287):     if fit_intercept:
1.04 logistic.py(288):         intercept = w[:, -1]
1.04 logistic.py(289):         w = w[:, :-1]
1.04 logistic.py(292):     p = safe_sparse_dot(X, w.T)
1.04 logistic.py(293):     p += intercept
1.04 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
1.04 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
1.04 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
1.04 logistic.py(297):     p = np.exp(p, p)
1.04 logistic.py(298):     return loss, p, w
1.04 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
1.04 logistic.py(346):     diff = sample_weight * (p - Y)
1.04 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
1.04 logistic.py(348):     grad[:, :n_features] += alpha * w
1.04 logistic.py(349):     if fit_intercept:
1.04 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
1.04 logistic.py(351):     return loss, grad.ravel(), p
1.04 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
1.04 logistic.py(339):     n_classes = Y.shape[1]
1.04 logistic.py(340):     n_features = X.shape[1]
1.04 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
1.04 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
1.04 logistic.py(343):                     dtype=X.dtype)
1.04 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
1.04 logistic.py(282):     n_classes = Y.shape[1]
1.04 logistic.py(283):     n_features = X.shape[1]
1.04 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
1.04 logistic.py(285):     w = w.reshape(n_classes, -1)
1.04 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
1.04 logistic.py(287):     if fit_intercept:
1.04 logistic.py(288):         intercept = w[:, -1]
1.04 logistic.py(289):         w = w[:, :-1]
1.04 logistic.py(292):     p = safe_sparse_dot(X, w.T)
1.04 logistic.py(293):     p += intercept
1.04 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
1.04 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
1.04 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
1.04 logistic.py(297):     p = np.exp(p, p)
1.04 logistic.py(298):     return loss, p, w
1.04 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
1.04 logistic.py(346):     diff = sample_weight * (p - Y)
1.04 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
1.04 logistic.py(348):     grad[:, :n_features] += alpha * w
1.04 logistic.py(349):     if fit_intercept:
1.04 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
1.04 logistic.py(351):     return loss, grad.ravel(), p
1.04 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
1.04 logistic.py(339):     n_classes = Y.shape[1]
1.04 logistic.py(340):     n_features = X.shape[1]
1.04 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
1.04 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
1.04 logistic.py(343):                     dtype=X.dtype)
1.04 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
1.04 logistic.py(282):     n_classes = Y.shape[1]
1.04 logistic.py(283):     n_features = X.shape[1]
1.04 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
1.04 logistic.py(285):     w = w.reshape(n_classes, -1)
1.04 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
1.04 logistic.py(287):     if fit_intercept:
1.04 logistic.py(288):         intercept = w[:, -1]
1.04 logistic.py(289):         w = w[:, :-1]
1.04 logistic.py(292):     p = safe_sparse_dot(X, w.T)
1.04 logistic.py(293):     p += intercept
1.04 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
1.04 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
1.04 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
1.04 logistic.py(297):     p = np.exp(p, p)
1.04 logistic.py(298):     return loss, p, w
1.04 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
1.04 logistic.py(346):     diff = sample_weight * (p - Y)
1.04 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
1.04 logistic.py(348):     grad[:, :n_features] += alpha * w
1.04 logistic.py(349):     if fit_intercept:
1.04 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
1.04 logistic.py(351):     return loss, grad.ravel(), p
1.04 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
1.04 logistic.py(339):     n_classes = Y.shape[1]
1.04 logistic.py(340):     n_features = X.shape[1]
1.04 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
1.04 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
1.04 logistic.py(343):                     dtype=X.dtype)
1.04 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
1.04 logistic.py(282):     n_classes = Y.shape[1]
1.04 logistic.py(283):     n_features = X.shape[1]
1.04 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
1.04 logistic.py(285):     w = w.reshape(n_classes, -1)
1.04 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
1.04 logistic.py(287):     if fit_intercept:
1.04 logistic.py(288):         intercept = w[:, -1]
1.04 logistic.py(289):         w = w[:, :-1]
1.04 logistic.py(292):     p = safe_sparse_dot(X, w.T)
1.04 logistic.py(293):     p += intercept
1.04 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
1.04 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
1.04 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
1.04 logistic.py(297):     p = np.exp(p, p)
1.04 logistic.py(298):     return loss, p, w
1.04 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
1.04 logistic.py(346):     diff = sample_weight * (p - Y)
1.04 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
1.04 logistic.py(348):     grad[:, :n_features] += alpha * w
1.04 logistic.py(349):     if fit_intercept:
1.04 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
1.04 logistic.py(351):     return loss, grad.ravel(), p
1.04 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
1.04 logistic.py(339):     n_classes = Y.shape[1]
1.04 logistic.py(340):     n_features = X.shape[1]
1.04 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
1.04 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
1.04 logistic.py(343):                     dtype=X.dtype)
1.04 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
1.04 logistic.py(282):     n_classes = Y.shape[1]
1.04 logistic.py(283):     n_features = X.shape[1]
1.04 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
1.04 logistic.py(285):     w = w.reshape(n_classes, -1)
1.04 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
1.04 logistic.py(287):     if fit_intercept:
1.04 logistic.py(288):         intercept = w[:, -1]
1.04 logistic.py(289):         w = w[:, :-1]
1.04 logistic.py(292):     p = safe_sparse_dot(X, w.T)
1.04 logistic.py(293):     p += intercept
1.04 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
1.04 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
1.04 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
1.04 logistic.py(297):     p = np.exp(p, p)
1.04 logistic.py(298):     return loss, p, w
1.04 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
1.04 logistic.py(346):     diff = sample_weight * (p - Y)
1.04 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
1.04 logistic.py(348):     grad[:, :n_features] += alpha * w
1.04 logistic.py(349):     if fit_intercept:
1.04 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
1.04 logistic.py(351):     return loss, grad.ravel(), p
1.04 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
1.04 logistic.py(339):     n_classes = Y.shape[1]
1.04 logistic.py(340):     n_features = X.shape[1]
1.04 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
1.04 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
1.04 logistic.py(343):                     dtype=X.dtype)
1.04 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
1.04 logistic.py(282):     n_classes = Y.shape[1]
1.04 logistic.py(283):     n_features = X.shape[1]
1.04 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
1.04 logistic.py(285):     w = w.reshape(n_classes, -1)
1.04 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
1.04 logistic.py(287):     if fit_intercept:
1.04 logistic.py(288):         intercept = w[:, -1]
1.04 logistic.py(289):         w = w[:, :-1]
1.04 logistic.py(292):     p = safe_sparse_dot(X, w.T)
1.04 logistic.py(293):     p += intercept
1.04 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
1.04 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
1.04 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
1.04 logistic.py(297):     p = np.exp(p, p)
1.04 logistic.py(298):     return loss, p, w
1.04 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
1.04 logistic.py(346):     diff = sample_weight * (p - Y)
1.04 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
1.04 logistic.py(348):     grad[:, :n_features] += alpha * w
1.04 logistic.py(349):     if fit_intercept:
1.04 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
1.04 logistic.py(351):     return loss, grad.ravel(), p
1.04 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
1.04 logistic.py(339):     n_classes = Y.shape[1]
1.04 logistic.py(340):     n_features = X.shape[1]
1.04 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
1.04 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
1.04 logistic.py(343):                     dtype=X.dtype)
1.04 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
1.04 logistic.py(282):     n_classes = Y.shape[1]
1.04 logistic.py(283):     n_features = X.shape[1]
1.04 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
1.04 logistic.py(285):     w = w.reshape(n_classes, -1)
1.04 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
1.04 logistic.py(287):     if fit_intercept:
1.04 logistic.py(288):         intercept = w[:, -1]
1.04 logistic.py(289):         w = w[:, :-1]
1.04 logistic.py(292):     p = safe_sparse_dot(X, w.T)
1.04 logistic.py(293):     p += intercept
1.04 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
1.04 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
1.04 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
1.04 logistic.py(297):     p = np.exp(p, p)
1.04 logistic.py(298):     return loss, p, w
1.04 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
1.04 logistic.py(346):     diff = sample_weight * (p - Y)
1.04 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
1.04 logistic.py(348):     grad[:, :n_features] += alpha * w
1.04 logistic.py(349):     if fit_intercept:
1.04 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
1.04 logistic.py(351):     return loss, grad.ravel(), p
1.04 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
1.04 logistic.py(339):     n_classes = Y.shape[1]
1.04 logistic.py(340):     n_features = X.shape[1]
1.04 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
1.04 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
1.04 logistic.py(343):                     dtype=X.dtype)
1.04 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
1.04 logistic.py(282):     n_classes = Y.shape[1]
1.04 logistic.py(283):     n_features = X.shape[1]
1.04 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
1.04 logistic.py(285):     w = w.reshape(n_classes, -1)
1.04 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
1.04 logistic.py(287):     if fit_intercept:
1.04 logistic.py(288):         intercept = w[:, -1]
1.04 logistic.py(289):         w = w[:, :-1]
1.04 logistic.py(292):     p = safe_sparse_dot(X, w.T)
1.04 logistic.py(293):     p += intercept
1.04 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
1.04 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
1.04 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
1.04 logistic.py(297):     p = np.exp(p, p)
1.04 logistic.py(298):     return loss, p, w
1.04 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
1.04 logistic.py(346):     diff = sample_weight * (p - Y)
1.04 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
1.04 logistic.py(348):     grad[:, :n_features] += alpha * w
1.04 logistic.py(349):     if fit_intercept:
1.04 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
1.04 logistic.py(351):     return loss, grad.ravel(), p
1.04 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
1.04 logistic.py(339):     n_classes = Y.shape[1]
1.04 logistic.py(340):     n_features = X.shape[1]
1.04 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
1.04 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
1.04 logistic.py(343):                     dtype=X.dtype)
1.04 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
1.04 logistic.py(282):     n_classes = Y.shape[1]
1.04 logistic.py(283):     n_features = X.shape[1]
1.04 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
1.04 logistic.py(285):     w = w.reshape(n_classes, -1)
1.04 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
1.04 logistic.py(287):     if fit_intercept:
1.04 logistic.py(288):         intercept = w[:, -1]
1.04 logistic.py(289):         w = w[:, :-1]
1.04 logistic.py(292):     p = safe_sparse_dot(X, w.T)
1.04 logistic.py(293):     p += intercept
1.04 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
1.04 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
1.04 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
1.04 logistic.py(297):     p = np.exp(p, p)
1.04 logistic.py(298):     return loss, p, w
1.04 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
1.04 logistic.py(346):     diff = sample_weight * (p - Y)
1.04 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
1.04 logistic.py(348):     grad[:, :n_features] += alpha * w
1.04 logistic.py(349):     if fit_intercept:
1.04 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
1.04 logistic.py(351):     return loss, grad.ravel(), p
1.04 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
1.04 logistic.py(339):     n_classes = Y.shape[1]
1.04 logistic.py(340):     n_features = X.shape[1]
1.04 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
1.04 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
1.04 logistic.py(343):                     dtype=X.dtype)
1.04 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
1.04 logistic.py(282):     n_classes = Y.shape[1]
1.04 logistic.py(283):     n_features = X.shape[1]
1.04 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
1.04 logistic.py(285):     w = w.reshape(n_classes, -1)
1.04 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
1.04 logistic.py(287):     if fit_intercept:
1.04 logistic.py(288):         intercept = w[:, -1]
1.04 logistic.py(289):         w = w[:, :-1]
1.04 logistic.py(292):     p = safe_sparse_dot(X, w.T)
1.04 logistic.py(293):     p += intercept
1.04 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
1.04 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
1.04 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
1.04 logistic.py(297):     p = np.exp(p, p)
1.04 logistic.py(298):     return loss, p, w
1.04 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
1.04 logistic.py(346):     diff = sample_weight * (p - Y)
1.04 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
1.04 logistic.py(348):     grad[:, :n_features] += alpha * w
1.04 logistic.py(349):     if fit_intercept:
1.04 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
1.04 logistic.py(351):     return loss, grad.ravel(), p
1.04 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
1.04 logistic.py(339):     n_classes = Y.shape[1]
1.04 logistic.py(340):     n_features = X.shape[1]
1.04 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
1.04 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
1.04 logistic.py(343):                     dtype=X.dtype)
1.04 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
1.04 logistic.py(282):     n_classes = Y.shape[1]
1.04 logistic.py(283):     n_features = X.shape[1]
1.04 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
1.04 logistic.py(285):     w = w.reshape(n_classes, -1)
1.04 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
1.04 logistic.py(287):     if fit_intercept:
1.04 logistic.py(288):         intercept = w[:, -1]
1.04 logistic.py(289):         w = w[:, :-1]
1.04 logistic.py(292):     p = safe_sparse_dot(X, w.T)
1.04 logistic.py(293):     p += intercept
1.04 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
1.05 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
1.05 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
1.05 logistic.py(297):     p = np.exp(p, p)
1.05 logistic.py(298):     return loss, p, w
1.05 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
1.05 logistic.py(346):     diff = sample_weight * (p - Y)
1.05 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
1.05 logistic.py(348):     grad[:, :n_features] += alpha * w
1.05 logistic.py(349):     if fit_intercept:
1.05 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
1.05 logistic.py(351):     return loss, grad.ravel(), p
1.05 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
1.05 logistic.py(339):     n_classes = Y.shape[1]
1.05 logistic.py(340):     n_features = X.shape[1]
1.05 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
1.05 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
1.05 logistic.py(343):                     dtype=X.dtype)
1.05 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
1.05 logistic.py(282):     n_classes = Y.shape[1]
1.05 logistic.py(283):     n_features = X.shape[1]
1.05 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
1.05 logistic.py(285):     w = w.reshape(n_classes, -1)
1.05 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
1.05 logistic.py(287):     if fit_intercept:
1.05 logistic.py(288):         intercept = w[:, -1]
1.05 logistic.py(289):         w = w[:, :-1]
1.05 logistic.py(292):     p = safe_sparse_dot(X, w.T)
1.05 logistic.py(293):     p += intercept
1.05 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
1.05 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
1.05 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
1.05 logistic.py(297):     p = np.exp(p, p)
1.05 logistic.py(298):     return loss, p, w
1.05 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
1.05 logistic.py(346):     diff = sample_weight * (p - Y)
1.05 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
1.05 logistic.py(348):     grad[:, :n_features] += alpha * w
1.05 logistic.py(349):     if fit_intercept:
1.05 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
1.05 logistic.py(351):     return loss, grad.ravel(), p
1.05 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
1.05 logistic.py(339):     n_classes = Y.shape[1]
1.05 logistic.py(340):     n_features = X.shape[1]
1.05 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
1.05 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
1.05 logistic.py(343):                     dtype=X.dtype)
1.05 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
1.05 logistic.py(282):     n_classes = Y.shape[1]
1.05 logistic.py(283):     n_features = X.shape[1]
1.05 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
1.05 logistic.py(285):     w = w.reshape(n_classes, -1)
1.05 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
1.05 logistic.py(287):     if fit_intercept:
1.05 logistic.py(288):         intercept = w[:, -1]
1.05 logistic.py(289):         w = w[:, :-1]
1.05 logistic.py(292):     p = safe_sparse_dot(X, w.T)
1.05 logistic.py(293):     p += intercept
1.05 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
1.05 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
1.05 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
1.05 logistic.py(297):     p = np.exp(p, p)
1.05 logistic.py(298):     return loss, p, w
1.05 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
1.05 logistic.py(346):     diff = sample_weight * (p - Y)
1.05 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
1.05 logistic.py(348):     grad[:, :n_features] += alpha * w
1.05 logistic.py(349):     if fit_intercept:
1.05 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
1.05 logistic.py(351):     return loss, grad.ravel(), p
1.05 logistic.py(718):             if info["warnflag"] == 1:
1.05 logistic.py(723):             n_iter_i = min(info['nit'], max_iter)
1.05 logistic.py(760):         if multi_class == 'multinomial':
1.05 logistic.py(761):             multi_w0 = np.reshape(w0, (classes.size, -1))
1.05 logistic.py(762):             if classes.size == 2:
1.05 logistic.py(764):             coefs.append(multi_w0)
1.05 logistic.py(768):         n_iter[i] = n_iter_i
1.05 logistic.py(712):     for i, C in enumerate(Cs):
1.05 logistic.py(713):         if solver == 'lbfgs':
1.05 logistic.py(714):             w0, loss, info = optimize.fmin_l_bfgs_b(
1.05 logistic.py(715):                 func, w0, fprime=None,
1.05 logistic.py(716):                 args=(X, target, 1. / C, sample_weight),
1.05 logistic.py(717):                 iprint=(verbose > 0) - 1, pgtol=tol, maxiter=max_iter)
1.05 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
1.05 logistic.py(339):     n_classes = Y.shape[1]
1.05 logistic.py(340):     n_features = X.shape[1]
1.05 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
1.05 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
1.05 logistic.py(343):                     dtype=X.dtype)
1.05 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
1.05 logistic.py(282):     n_classes = Y.shape[1]
1.05 logistic.py(283):     n_features = X.shape[1]
1.05 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
1.05 logistic.py(285):     w = w.reshape(n_classes, -1)
1.05 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
1.05 logistic.py(287):     if fit_intercept:
1.05 logistic.py(288):         intercept = w[:, -1]
1.05 logistic.py(289):         w = w[:, :-1]
1.05 logistic.py(292):     p = safe_sparse_dot(X, w.T)
1.05 logistic.py(293):     p += intercept
1.05 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
1.05 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
1.05 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
1.05 logistic.py(297):     p = np.exp(p, p)
1.05 logistic.py(298):     return loss, p, w
1.05 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
1.05 logistic.py(346):     diff = sample_weight * (p - Y)
1.05 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
1.05 logistic.py(348):     grad[:, :n_features] += alpha * w
1.05 logistic.py(349):     if fit_intercept:
1.05 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
1.05 logistic.py(351):     return loss, grad.ravel(), p
1.05 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
1.05 logistic.py(339):     n_classes = Y.shape[1]
1.05 logistic.py(340):     n_features = X.shape[1]
1.05 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
1.05 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
1.05 logistic.py(343):                     dtype=X.dtype)
1.05 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
1.05 logistic.py(282):     n_classes = Y.shape[1]
1.05 logistic.py(283):     n_features = X.shape[1]
1.05 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
1.05 logistic.py(285):     w = w.reshape(n_classes, -1)
1.05 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
1.05 logistic.py(287):     if fit_intercept:
1.05 logistic.py(288):         intercept = w[:, -1]
1.05 logistic.py(289):         w = w[:, :-1]
1.05 logistic.py(292):     p = safe_sparse_dot(X, w.T)
1.05 logistic.py(293):     p += intercept
1.05 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
1.05 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
1.05 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
1.05 logistic.py(297):     p = np.exp(p, p)
1.05 logistic.py(298):     return loss, p, w
1.05 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
1.05 logistic.py(346):     diff = sample_weight * (p - Y)
1.05 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
1.05 logistic.py(348):     grad[:, :n_features] += alpha * w
1.05 logistic.py(349):     if fit_intercept:
1.05 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
1.05 logistic.py(351):     return loss, grad.ravel(), p
1.05 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
1.05 logistic.py(339):     n_classes = Y.shape[1]
1.05 logistic.py(340):     n_features = X.shape[1]
1.05 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
1.05 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
1.05 logistic.py(343):                     dtype=X.dtype)
1.05 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
1.05 logistic.py(282):     n_classes = Y.shape[1]
1.05 logistic.py(283):     n_features = X.shape[1]
1.05 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
1.05 logistic.py(285):     w = w.reshape(n_classes, -1)
1.05 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
1.05 logistic.py(287):     if fit_intercept:
1.05 logistic.py(288):         intercept = w[:, -1]
1.05 logistic.py(289):         w = w[:, :-1]
1.05 logistic.py(292):     p = safe_sparse_dot(X, w.T)
1.05 logistic.py(293):     p += intercept
1.05 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
1.05 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
1.05 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
1.05 logistic.py(297):     p = np.exp(p, p)
1.05 logistic.py(298):     return loss, p, w
1.05 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
1.05 logistic.py(346):     diff = sample_weight * (p - Y)
1.05 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
1.05 logistic.py(348):     grad[:, :n_features] += alpha * w
1.05 logistic.py(349):     if fit_intercept:
1.05 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
1.05 logistic.py(351):     return loss, grad.ravel(), p
1.05 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
1.05 logistic.py(339):     n_classes = Y.shape[1]
1.05 logistic.py(340):     n_features = X.shape[1]
1.05 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
1.05 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
1.05 logistic.py(343):                     dtype=X.dtype)
1.05 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
1.05 logistic.py(282):     n_classes = Y.shape[1]
1.05 logistic.py(283):     n_features = X.shape[1]
1.05 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
1.05 logistic.py(285):     w = w.reshape(n_classes, -1)
1.05 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
1.05 logistic.py(287):     if fit_intercept:
1.05 logistic.py(288):         intercept = w[:, -1]
1.05 logistic.py(289):         w = w[:, :-1]
1.05 logistic.py(292):     p = safe_sparse_dot(X, w.T)
1.05 logistic.py(293):     p += intercept
1.05 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
1.05 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
1.05 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
1.05 logistic.py(297):     p = np.exp(p, p)
1.05 logistic.py(298):     return loss, p, w
1.05 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
1.05 logistic.py(346):     diff = sample_weight * (p - Y)
1.05 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
1.05 logistic.py(348):     grad[:, :n_features] += alpha * w
1.05 logistic.py(349):     if fit_intercept:
1.05 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
1.05 logistic.py(351):     return loss, grad.ravel(), p
1.05 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
1.05 logistic.py(339):     n_classes = Y.shape[1]
1.05 logistic.py(340):     n_features = X.shape[1]
1.05 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
1.05 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
1.05 logistic.py(343):                     dtype=X.dtype)
1.05 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
1.05 logistic.py(282):     n_classes = Y.shape[1]
1.05 logistic.py(283):     n_features = X.shape[1]
1.05 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
1.05 logistic.py(285):     w = w.reshape(n_classes, -1)
1.05 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
1.05 logistic.py(287):     if fit_intercept:
1.05 logistic.py(288):         intercept = w[:, -1]
1.05 logistic.py(289):         w = w[:, :-1]
1.05 logistic.py(292):     p = safe_sparse_dot(X, w.T)
1.05 logistic.py(293):     p += intercept
1.05 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
1.05 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
1.05 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
1.05 logistic.py(297):     p = np.exp(p, p)
1.05 logistic.py(298):     return loss, p, w
1.05 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
1.05 logistic.py(346):     diff = sample_weight * (p - Y)
1.05 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
1.05 logistic.py(348):     grad[:, :n_features] += alpha * w
1.05 logistic.py(349):     if fit_intercept:
1.05 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
1.05 logistic.py(351):     return loss, grad.ravel(), p
1.05 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
1.05 logistic.py(339):     n_classes = Y.shape[1]
1.05 logistic.py(340):     n_features = X.shape[1]
1.05 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
1.05 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
1.05 logistic.py(343):                     dtype=X.dtype)
1.05 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
1.05 logistic.py(282):     n_classes = Y.shape[1]
1.05 logistic.py(283):     n_features = X.shape[1]
1.05 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
1.05 logistic.py(285):     w = w.reshape(n_classes, -1)
1.05 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
1.05 logistic.py(287):     if fit_intercept:
1.05 logistic.py(288):         intercept = w[:, -1]
1.05 logistic.py(289):         w = w[:, :-1]
1.05 logistic.py(292):     p = safe_sparse_dot(X, w.T)
1.05 logistic.py(293):     p += intercept
1.05 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
1.05 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
1.05 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
1.05 logistic.py(297):     p = np.exp(p, p)
1.05 logistic.py(298):     return loss, p, w
1.05 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
1.05 logistic.py(346):     diff = sample_weight * (p - Y)
1.05 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
1.05 logistic.py(348):     grad[:, :n_features] += alpha * w
1.05 logistic.py(349):     if fit_intercept:
1.05 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
1.05 logistic.py(351):     return loss, grad.ravel(), p
1.05 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
1.05 logistic.py(339):     n_classes = Y.shape[1]
1.05 logistic.py(340):     n_features = X.shape[1]
1.05 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
1.05 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
1.05 logistic.py(343):                     dtype=X.dtype)
1.05 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
1.05 logistic.py(282):     n_classes = Y.shape[1]
1.05 logistic.py(283):     n_features = X.shape[1]
1.05 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
1.05 logistic.py(285):     w = w.reshape(n_classes, -1)
1.05 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
1.05 logistic.py(287):     if fit_intercept:
1.05 logistic.py(288):         intercept = w[:, -1]
1.05 logistic.py(289):         w = w[:, :-1]
1.05 logistic.py(292):     p = safe_sparse_dot(X, w.T)
1.05 logistic.py(293):     p += intercept
1.05 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
1.05 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
1.05 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
1.05 logistic.py(297):     p = np.exp(p, p)
1.05 logistic.py(298):     return loss, p, w
1.05 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
1.05 logistic.py(346):     diff = sample_weight * (p - Y)
1.05 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
1.05 logistic.py(348):     grad[:, :n_features] += alpha * w
1.05 logistic.py(349):     if fit_intercept:
1.05 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
1.05 logistic.py(351):     return loss, grad.ravel(), p
1.05 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
1.05 logistic.py(339):     n_classes = Y.shape[1]
1.05 logistic.py(340):     n_features = X.shape[1]
1.05 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
1.05 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
1.05 logistic.py(343):                     dtype=X.dtype)
1.05 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
1.05 logistic.py(282):     n_classes = Y.shape[1]
1.05 logistic.py(283):     n_features = X.shape[1]
1.05 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
1.05 logistic.py(285):     w = w.reshape(n_classes, -1)
1.05 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
1.05 logistic.py(287):     if fit_intercept:
1.05 logistic.py(288):         intercept = w[:, -1]
1.05 logistic.py(289):         w = w[:, :-1]
1.05 logistic.py(292):     p = safe_sparse_dot(X, w.T)
1.05 logistic.py(293):     p += intercept
1.05 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
1.05 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
1.05 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
1.05 logistic.py(297):     p = np.exp(p, p)
1.05 logistic.py(298):     return loss, p, w
1.05 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
1.05 logistic.py(346):     diff = sample_weight * (p - Y)
1.05 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
1.05 logistic.py(348):     grad[:, :n_features] += alpha * w
1.05 logistic.py(349):     if fit_intercept:
1.05 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
1.05 logistic.py(351):     return loss, grad.ravel(), p
1.05 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
1.05 logistic.py(339):     n_classes = Y.shape[1]
1.05 logistic.py(340):     n_features = X.shape[1]
1.05 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
1.05 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
1.05 logistic.py(343):                     dtype=X.dtype)
1.05 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
1.05 logistic.py(282):     n_classes = Y.shape[1]
1.05 logistic.py(283):     n_features = X.shape[1]
1.05 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
1.05 logistic.py(285):     w = w.reshape(n_classes, -1)
1.05 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
1.05 logistic.py(287):     if fit_intercept:
1.05 logistic.py(288):         intercept = w[:, -1]
1.05 logistic.py(289):         w = w[:, :-1]
1.05 logistic.py(292):     p = safe_sparse_dot(X, w.T)
1.05 logistic.py(293):     p += intercept
1.05 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
1.05 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
1.05 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
1.05 logistic.py(297):     p = np.exp(p, p)
1.05 logistic.py(298):     return loss, p, w
1.05 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
1.05 logistic.py(346):     diff = sample_weight * (p - Y)
1.05 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
1.05 logistic.py(348):     grad[:, :n_features] += alpha * w
1.05 logistic.py(349):     if fit_intercept:
1.05 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
1.05 logistic.py(351):     return loss, grad.ravel(), p
1.05 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
1.05 logistic.py(339):     n_classes = Y.shape[1]
1.05 logistic.py(340):     n_features = X.shape[1]
1.05 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
1.05 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
1.05 logistic.py(343):                     dtype=X.dtype)
1.05 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
1.05 logistic.py(282):     n_classes = Y.shape[1]
1.05 logistic.py(283):     n_features = X.shape[1]
1.05 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
1.05 logistic.py(285):     w = w.reshape(n_classes, -1)
1.05 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
1.05 logistic.py(287):     if fit_intercept:
1.05 logistic.py(288):         intercept = w[:, -1]
1.05 logistic.py(289):         w = w[:, :-1]
1.05 logistic.py(292):     p = safe_sparse_dot(X, w.T)
1.05 logistic.py(293):     p += intercept
1.05 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
1.05 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
1.05 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
1.05 logistic.py(297):     p = np.exp(p, p)
1.05 logistic.py(298):     return loss, p, w
1.05 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
1.05 logistic.py(346):     diff = sample_weight * (p - Y)
1.05 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
1.05 logistic.py(348):     grad[:, :n_features] += alpha * w
1.05 logistic.py(349):     if fit_intercept:
1.05 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
1.05 logistic.py(351):     return loss, grad.ravel(), p
1.05 logistic.py(718):             if info["warnflag"] == 1:
1.05 logistic.py(723):             n_iter_i = min(info['nit'], max_iter)
1.05 logistic.py(760):         if multi_class == 'multinomial':
1.05 logistic.py(761):             multi_w0 = np.reshape(w0, (classes.size, -1))
1.05 logistic.py(762):             if classes.size == 2:
1.05 logistic.py(764):             coefs.append(multi_w0)
1.05 logistic.py(768):         n_iter[i] = n_iter_i
1.05 logistic.py(712):     for i, C in enumerate(Cs):
1.05 logistic.py(713):         if solver == 'lbfgs':
1.05 logistic.py(714):             w0, loss, info = optimize.fmin_l_bfgs_b(
1.05 logistic.py(715):                 func, w0, fprime=None,
1.05 logistic.py(716):                 args=(X, target, 1. / C, sample_weight),
1.05 logistic.py(717):                 iprint=(verbose > 0) - 1, pgtol=tol, maxiter=max_iter)
1.05 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
1.05 logistic.py(339):     n_classes = Y.shape[1]
1.05 logistic.py(340):     n_features = X.shape[1]
1.05 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
1.05 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
1.05 logistic.py(343):                     dtype=X.dtype)
1.05 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
1.05 logistic.py(282):     n_classes = Y.shape[1]
1.06 logistic.py(283):     n_features = X.shape[1]
1.06 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
1.06 logistic.py(285):     w = w.reshape(n_classes, -1)
1.06 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
1.06 logistic.py(287):     if fit_intercept:
1.06 logistic.py(288):         intercept = w[:, -1]
1.06 logistic.py(289):         w = w[:, :-1]
1.06 logistic.py(292):     p = safe_sparse_dot(X, w.T)
1.06 logistic.py(293):     p += intercept
1.06 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
1.06 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
1.06 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
1.06 logistic.py(297):     p = np.exp(p, p)
1.06 logistic.py(298):     return loss, p, w
1.06 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
1.06 logistic.py(346):     diff = sample_weight * (p - Y)
1.06 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
1.06 logistic.py(348):     grad[:, :n_features] += alpha * w
1.06 logistic.py(349):     if fit_intercept:
1.06 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
1.06 logistic.py(351):     return loss, grad.ravel(), p
1.06 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
1.06 logistic.py(339):     n_classes = Y.shape[1]
1.06 logistic.py(340):     n_features = X.shape[1]
1.06 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
1.06 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
1.06 logistic.py(343):                     dtype=X.dtype)
1.06 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
1.06 logistic.py(282):     n_classes = Y.shape[1]
1.06 logistic.py(283):     n_features = X.shape[1]
1.06 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
1.06 logistic.py(285):     w = w.reshape(n_classes, -1)
1.06 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
1.06 logistic.py(287):     if fit_intercept:
1.06 logistic.py(288):         intercept = w[:, -1]
1.06 logistic.py(289):         w = w[:, :-1]
1.06 logistic.py(292):     p = safe_sparse_dot(X, w.T)
1.06 logistic.py(293):     p += intercept
1.06 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
1.06 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
1.06 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
1.06 logistic.py(297):     p = np.exp(p, p)
1.06 logistic.py(298):     return loss, p, w
1.06 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
1.06 logistic.py(346):     diff = sample_weight * (p - Y)
1.06 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
1.06 logistic.py(348):     grad[:, :n_features] += alpha * w
1.06 logistic.py(349):     if fit_intercept:
1.06 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
1.06 logistic.py(351):     return loss, grad.ravel(), p
1.06 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
1.06 logistic.py(339):     n_classes = Y.shape[1]
1.06 logistic.py(340):     n_features = X.shape[1]
1.06 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
1.06 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
1.06 logistic.py(343):                     dtype=X.dtype)
1.06 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
1.06 logistic.py(282):     n_classes = Y.shape[1]
1.06 logistic.py(283):     n_features = X.shape[1]
1.06 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
1.06 logistic.py(285):     w = w.reshape(n_classes, -1)
1.06 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
1.06 logistic.py(287):     if fit_intercept:
1.06 logistic.py(288):         intercept = w[:, -1]
1.06 logistic.py(289):         w = w[:, :-1]
1.06 logistic.py(292):     p = safe_sparse_dot(X, w.T)
1.06 logistic.py(293):     p += intercept
1.06 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
1.06 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
1.06 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
1.06 logistic.py(297):     p = np.exp(p, p)
1.06 logistic.py(298):     return loss, p, w
1.06 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
1.06 logistic.py(346):     diff = sample_weight * (p - Y)
1.06 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
1.06 logistic.py(348):     grad[:, :n_features] += alpha * w
1.06 logistic.py(349):     if fit_intercept:
1.06 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
1.06 logistic.py(351):     return loss, grad.ravel(), p
1.06 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
1.06 logistic.py(339):     n_classes = Y.shape[1]
1.06 logistic.py(340):     n_features = X.shape[1]
1.06 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
1.06 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
1.06 logistic.py(343):                     dtype=X.dtype)
1.06 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
1.06 logistic.py(282):     n_classes = Y.shape[1]
1.06 logistic.py(283):     n_features = X.shape[1]
1.06 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
1.06 logistic.py(285):     w = w.reshape(n_classes, -1)
1.06 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
1.06 logistic.py(287):     if fit_intercept:
1.06 logistic.py(288):         intercept = w[:, -1]
1.06 logistic.py(289):         w = w[:, :-1]
1.06 logistic.py(292):     p = safe_sparse_dot(X, w.T)
1.06 logistic.py(293):     p += intercept
1.06 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
1.06 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
1.06 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
1.06 logistic.py(297):     p = np.exp(p, p)
1.06 logistic.py(298):     return loss, p, w
1.06 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
1.06 logistic.py(346):     diff = sample_weight * (p - Y)
1.06 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
1.06 logistic.py(348):     grad[:, :n_features] += alpha * w
1.06 logistic.py(349):     if fit_intercept:
1.06 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
1.06 logistic.py(351):     return loss, grad.ravel(), p
1.06 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
1.06 logistic.py(339):     n_classes = Y.shape[1]
1.06 logistic.py(340):     n_features = X.shape[1]
1.06 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
1.06 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
1.06 logistic.py(343):                     dtype=X.dtype)
1.06 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
1.06 logistic.py(282):     n_classes = Y.shape[1]
1.06 logistic.py(283):     n_features = X.shape[1]
1.06 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
1.06 logistic.py(285):     w = w.reshape(n_classes, -1)
1.06 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
1.06 logistic.py(287):     if fit_intercept:
1.06 logistic.py(288):         intercept = w[:, -1]
1.06 logistic.py(289):         w = w[:, :-1]
1.06 logistic.py(292):     p = safe_sparse_dot(X, w.T)
1.06 logistic.py(293):     p += intercept
1.06 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
1.06 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
1.06 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
1.06 logistic.py(297):     p = np.exp(p, p)
1.06 logistic.py(298):     return loss, p, w
1.06 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
1.06 logistic.py(346):     diff = sample_weight * (p - Y)
1.06 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
1.06 logistic.py(348):     grad[:, :n_features] += alpha * w
1.06 logistic.py(349):     if fit_intercept:
1.06 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
1.06 logistic.py(351):     return loss, grad.ravel(), p
1.06 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
1.06 logistic.py(339):     n_classes = Y.shape[1]
1.06 logistic.py(340):     n_features = X.shape[1]
1.06 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
1.06 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
1.06 logistic.py(343):                     dtype=X.dtype)
1.06 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
1.06 logistic.py(282):     n_classes = Y.shape[1]
1.06 logistic.py(283):     n_features = X.shape[1]
1.06 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
1.06 logistic.py(285):     w = w.reshape(n_classes, -1)
1.06 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
1.06 logistic.py(287):     if fit_intercept:
1.06 logistic.py(288):         intercept = w[:, -1]
1.06 logistic.py(289):         w = w[:, :-1]
1.06 logistic.py(292):     p = safe_sparse_dot(X, w.T)
1.06 logistic.py(293):     p += intercept
1.06 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
1.06 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
1.06 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
1.06 logistic.py(297):     p = np.exp(p, p)
1.06 logistic.py(298):     return loss, p, w
1.06 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
1.06 logistic.py(346):     diff = sample_weight * (p - Y)
1.06 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
1.06 logistic.py(348):     grad[:, :n_features] += alpha * w
1.06 logistic.py(349):     if fit_intercept:
1.06 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
1.06 logistic.py(351):     return loss, grad.ravel(), p
1.06 logistic.py(718):             if info["warnflag"] == 1:
1.06 logistic.py(723):             n_iter_i = min(info['nit'], max_iter)
1.06 logistic.py(760):         if multi_class == 'multinomial':
1.06 logistic.py(761):             multi_w0 = np.reshape(w0, (classes.size, -1))
1.06 logistic.py(762):             if classes.size == 2:
1.06 logistic.py(764):             coefs.append(multi_w0)
1.06 logistic.py(768):         n_iter[i] = n_iter_i
1.06 logistic.py(712):     for i, C in enumerate(Cs):
1.06 logistic.py(713):         if solver == 'lbfgs':
1.06 logistic.py(714):             w0, loss, info = optimize.fmin_l_bfgs_b(
1.06 logistic.py(715):                 func, w0, fprime=None,
1.06 logistic.py(716):                 args=(X, target, 1. / C, sample_weight),
1.06 logistic.py(717):                 iprint=(verbose > 0) - 1, pgtol=tol, maxiter=max_iter)
1.06 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
1.06 logistic.py(339):     n_classes = Y.shape[1]
1.06 logistic.py(340):     n_features = X.shape[1]
1.06 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
1.06 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
1.06 logistic.py(343):                     dtype=X.dtype)
1.06 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
1.06 logistic.py(282):     n_classes = Y.shape[1]
1.06 logistic.py(283):     n_features = X.shape[1]
1.06 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
1.06 logistic.py(285):     w = w.reshape(n_classes, -1)
1.06 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
1.06 logistic.py(287):     if fit_intercept:
1.06 logistic.py(288):         intercept = w[:, -1]
1.06 logistic.py(289):         w = w[:, :-1]
1.06 logistic.py(292):     p = safe_sparse_dot(X, w.T)
1.06 logistic.py(293):     p += intercept
1.06 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
1.06 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
1.06 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
1.06 logistic.py(297):     p = np.exp(p, p)
1.06 logistic.py(298):     return loss, p, w
1.06 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
1.06 logistic.py(346):     diff = sample_weight * (p - Y)
1.06 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
1.06 logistic.py(348):     grad[:, :n_features] += alpha * w
1.06 logistic.py(349):     if fit_intercept:
1.06 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
1.06 logistic.py(351):     return loss, grad.ravel(), p
1.06 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
1.06 logistic.py(339):     n_classes = Y.shape[1]
1.06 logistic.py(340):     n_features = X.shape[1]
1.06 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
1.06 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
1.06 logistic.py(343):                     dtype=X.dtype)
1.06 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
1.06 logistic.py(282):     n_classes = Y.shape[1]
1.06 logistic.py(283):     n_features = X.shape[1]
1.06 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
1.06 logistic.py(285):     w = w.reshape(n_classes, -1)
1.06 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
1.06 logistic.py(287):     if fit_intercept:
1.06 logistic.py(288):         intercept = w[:, -1]
1.06 logistic.py(289):         w = w[:, :-1]
1.06 logistic.py(292):     p = safe_sparse_dot(X, w.T)
1.06 logistic.py(293):     p += intercept
1.06 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
1.06 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
1.06 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
1.06 logistic.py(297):     p = np.exp(p, p)
1.06 logistic.py(298):     return loss, p, w
1.06 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
1.06 logistic.py(346):     diff = sample_weight * (p - Y)
1.06 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
1.06 logistic.py(348):     grad[:, :n_features] += alpha * w
1.06 logistic.py(349):     if fit_intercept:
1.06 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
1.06 logistic.py(351):     return loss, grad.ravel(), p
1.06 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
1.06 logistic.py(339):     n_classes = Y.shape[1]
1.06 logistic.py(340):     n_features = X.shape[1]
1.06 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
1.06 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
1.06 logistic.py(343):                     dtype=X.dtype)
1.06 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
1.06 logistic.py(282):     n_classes = Y.shape[1]
1.06 logistic.py(283):     n_features = X.shape[1]
1.06 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
1.06 logistic.py(285):     w = w.reshape(n_classes, -1)
1.06 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
1.06 logistic.py(287):     if fit_intercept:
1.06 logistic.py(288):         intercept = w[:, -1]
1.06 logistic.py(289):         w = w[:, :-1]
1.06 logistic.py(292):     p = safe_sparse_dot(X, w.T)
1.06 logistic.py(293):     p += intercept
1.06 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
1.06 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
1.06 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
1.06 logistic.py(297):     p = np.exp(p, p)
1.06 logistic.py(298):     return loss, p, w
1.06 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
1.06 logistic.py(346):     diff = sample_weight * (p - Y)
1.06 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
1.06 logistic.py(348):     grad[:, :n_features] += alpha * w
1.06 logistic.py(349):     if fit_intercept:
1.06 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
1.06 logistic.py(351):     return loss, grad.ravel(), p
1.06 logistic.py(718):             if info["warnflag"] == 1:
1.06 logistic.py(723):             n_iter_i = min(info['nit'], max_iter)
1.06 logistic.py(760):         if multi_class == 'multinomial':
1.06 logistic.py(761):             multi_w0 = np.reshape(w0, (classes.size, -1))
1.06 logistic.py(762):             if classes.size == 2:
1.06 logistic.py(764):             coefs.append(multi_w0)
1.06 logistic.py(768):         n_iter[i] = n_iter_i
1.06 logistic.py(712):     for i, C in enumerate(Cs):
1.06 logistic.py(770):     return coefs, np.array(Cs), n_iter
1.06 logistic.py(925):     log_reg = LogisticRegression(multi_class=multi_class)
1.06 logistic.py(1171):         self.penalty = penalty
1.06 logistic.py(1172):         self.dual = dual
1.06 logistic.py(1173):         self.tol = tol
1.06 logistic.py(1174):         self.C = C
1.06 logistic.py(1175):         self.fit_intercept = fit_intercept
1.06 logistic.py(1176):         self.intercept_scaling = intercept_scaling
1.06 logistic.py(1177):         self.class_weight = class_weight
1.06 logistic.py(1178):         self.random_state = random_state
1.06 logistic.py(1179):         self.solver = solver
1.06 logistic.py(1180):         self.max_iter = max_iter
1.06 logistic.py(1181):         self.multi_class = multi_class
1.06 logistic.py(1182):         self.verbose = verbose
1.06 logistic.py(1183):         self.warm_start = warm_start
1.06 logistic.py(1184):         self.n_jobs = n_jobs
1.06 logistic.py(928):     if multi_class == 'ovr':
1.06 logistic.py(930):     elif multi_class == 'multinomial':
1.06 logistic.py(931):         log_reg.classes_ = np.unique(y_train)
1.06 logistic.py(936):     if pos_class is not None:
1.06 logistic.py(941):     scores = list()
1.06 logistic.py(943):     if isinstance(scoring, six.string_types):
1.06 logistic.py(945):     for w in coefs:
1.06 logistic.py(946):         if multi_class == 'ovr':
1.06 logistic.py(948):         if fit_intercept:
1.06 logistic.py(949):             log_reg.coef_ = w[:, :-1]
1.06 logistic.py(950):             log_reg.intercept_ = w[:, -1]
1.06 logistic.py(955):         if scoring is None:
1.06 logistic.py(956):             scores.append(log_reg.score(X_test, y_test))
1.06 logistic.py(945):     for w in coefs:
1.06 logistic.py(946):         if multi_class == 'ovr':
1.06 logistic.py(948):         if fit_intercept:
1.06 logistic.py(949):             log_reg.coef_ = w[:, :-1]
1.06 logistic.py(950):             log_reg.intercept_ = w[:, -1]
1.06 logistic.py(955):         if scoring is None:
1.06 logistic.py(956):             scores.append(log_reg.score(X_test, y_test))
1.06 logistic.py(945):     for w in coefs:
1.06 logistic.py(946):         if multi_class == 'ovr':
1.06 logistic.py(948):         if fit_intercept:
1.06 logistic.py(949):             log_reg.coef_ = w[:, :-1]
1.06 logistic.py(950):             log_reg.intercept_ = w[:, -1]
1.06 logistic.py(955):         if scoring is None:
1.06 logistic.py(956):             scores.append(log_reg.score(X_test, y_test))
1.06 logistic.py(945):     for w in coefs:
1.06 logistic.py(946):         if multi_class == 'ovr':
1.06 logistic.py(948):         if fit_intercept:
1.06 logistic.py(949):             log_reg.coef_ = w[:, :-1]
1.06 logistic.py(950):             log_reg.intercept_ = w[:, -1]
1.06 logistic.py(955):         if scoring is None:
1.06 logistic.py(956):             scores.append(log_reg.score(X_test, y_test))
1.06 logistic.py(945):     for w in coefs:
1.06 logistic.py(946):         if multi_class == 'ovr':
1.06 logistic.py(948):         if fit_intercept:
1.06 logistic.py(949):             log_reg.coef_ = w[:, :-1]
1.06 logistic.py(950):             log_reg.intercept_ = w[:, -1]
1.06 logistic.py(955):         if scoring is None:
1.06 logistic.py(956):             scores.append(log_reg.score(X_test, y_test))
1.06 logistic.py(945):     for w in coefs:
1.06 logistic.py(946):         if multi_class == 'ovr':
1.06 logistic.py(948):         if fit_intercept:
1.06 logistic.py(949):             log_reg.coef_ = w[:, :-1]
1.06 logistic.py(950):             log_reg.intercept_ = w[:, -1]
1.06 logistic.py(955):         if scoring is None:
1.06 logistic.py(956):             scores.append(log_reg.score(X_test, y_test))
1.06 logistic.py(945):     for w in coefs:
1.06 logistic.py(946):         if multi_class == 'ovr':
1.06 logistic.py(948):         if fit_intercept:
1.06 logistic.py(949):             log_reg.coef_ = w[:, :-1]
1.06 logistic.py(950):             log_reg.intercept_ = w[:, -1]
1.06 logistic.py(955):         if scoring is None:
1.06 logistic.py(956):             scores.append(log_reg.score(X_test, y_test))
1.07 logistic.py(945):     for w in coefs:
1.07 logistic.py(946):         if multi_class == 'ovr':
1.07 logistic.py(948):         if fit_intercept:
1.07 logistic.py(949):             log_reg.coef_ = w[:, :-1]
1.07 logistic.py(950):             log_reg.intercept_ = w[:, -1]
1.07 logistic.py(955):         if scoring is None:
1.07 logistic.py(956):             scores.append(log_reg.score(X_test, y_test))
1.07 logistic.py(945):     for w in coefs:
1.07 logistic.py(946):         if multi_class == 'ovr':
1.07 logistic.py(948):         if fit_intercept:
1.07 logistic.py(949):             log_reg.coef_ = w[:, :-1]
1.07 logistic.py(950):             log_reg.intercept_ = w[:, -1]
1.07 logistic.py(955):         if scoring is None:
1.07 logistic.py(956):             scores.append(log_reg.score(X_test, y_test))
1.07 logistic.py(945):     for w in coefs:
1.07 logistic.py(946):         if multi_class == 'ovr':
1.07 logistic.py(948):         if fit_intercept:
1.07 logistic.py(949):             log_reg.coef_ = w[:, :-1]
1.07 logistic.py(950):             log_reg.intercept_ = w[:, -1]
1.07 logistic.py(955):         if scoring is None:
1.07 logistic.py(956):             scores.append(log_reg.score(X_test, y_test))
1.07 logistic.py(945):     for w in coefs:
1.07 logistic.py(959):     return coefs, Cs, np.array(scores), n_iter
1.07 logistic.py(1703):             for train, test in folds)
1.07 logistic.py(903):     _check_solver_option(solver, multi_class, penalty, dual)
1.07 logistic.py(428):     if solver not in ['liblinear', 'newton-cg', 'lbfgs', 'sag', 'saga']:
1.07 logistic.py(433):     if multi_class not in ['multinomial', 'ovr']:
1.07 logistic.py(437):     if multi_class == 'multinomial' and solver == 'liblinear':
1.07 logistic.py(441):     if solver not in ['liblinear', 'saga']:
1.07 logistic.py(442):         if penalty != 'l2':
1.07 logistic.py(445):     if solver != 'liblinear':
1.07 logistic.py(446):         if dual:
1.07 logistic.py(905):     X_train = X[train]
1.07 logistic.py(906):     X_test = X[test]
1.07 logistic.py(907):     y_train = y[train]
1.07 logistic.py(908):     y_test = y[test]
1.07 logistic.py(910):     if sample_weight is not None:
1.07 logistic.py(916):     coefs, Cs, n_iter = logistic_regression_path(
1.07 logistic.py(917):         X_train, y_train, Cs=Cs, fit_intercept=fit_intercept,
1.07 logistic.py(918):         solver=solver, max_iter=max_iter, class_weight=class_weight,
1.07 logistic.py(919):         pos_class=pos_class, multi_class=multi_class,
1.07 logistic.py(920):         tol=tol, verbose=verbose, dual=dual, penalty=penalty,
1.07 logistic.py(921):         intercept_scaling=intercept_scaling, random_state=random_state,
1.07 logistic.py(922):         check_input=False, max_squared_sum=max_squared_sum,
1.07 logistic.py(923):         sample_weight=sample_weight)
1.07 logistic.py(591):     if isinstance(Cs, numbers.Integral):
1.07 logistic.py(592):         Cs = np.logspace(-4, 4, Cs)
1.07 logistic.py(594):     _check_solver_option(solver, multi_class, penalty, dual)
1.07 logistic.py(428):     if solver not in ['liblinear', 'newton-cg', 'lbfgs', 'sag', 'saga']:
1.07 logistic.py(433):     if multi_class not in ['multinomial', 'ovr']:
1.07 logistic.py(437):     if multi_class == 'multinomial' and solver == 'liblinear':
1.07 logistic.py(441):     if solver not in ['liblinear', 'saga']:
1.07 logistic.py(442):         if penalty != 'l2':
1.07 logistic.py(445):     if solver != 'liblinear':
1.07 logistic.py(446):         if dual:
1.07 logistic.py(597):     if check_input:
1.07 logistic.py(602):     _, n_features = X.shape
1.07 logistic.py(603):     classes = np.unique(y)
1.07 logistic.py(604):     random_state = check_random_state(random_state)
1.07 logistic.py(606):     if pos_class is None and multi_class != 'multinomial':
1.07 logistic.py(615):     if sample_weight is not None:
1.07 logistic.py(619):         sample_weight = np.ones(X.shape[0], dtype=X.dtype)
1.07 logistic.py(624):     le = LabelEncoder()
1.07 logistic.py(625):     if isinstance(class_weight, dict) or multi_class == 'multinomial':
1.07 logistic.py(626):         class_weight_ = compute_class_weight(class_weight, classes, y)
1.07 logistic.py(627):         sample_weight *= class_weight_[le.fit_transform(y)]
1.07 logistic.py(631):     if multi_class == 'ovr':
1.07 logistic.py(645):         if solver not in ['sag', 'saga']:
1.07 logistic.py(646):             lbin = LabelBinarizer()
1.07 logistic.py(647):             Y_multi = lbin.fit_transform(y)
1.07 logistic.py(648):             if Y_multi.shape[1] == 1:
1.07 logistic.py(655):         w0 = np.zeros((classes.size, n_features + int(fit_intercept)),
1.07 logistic.py(656):                       order='F', dtype=X.dtype)
1.07 logistic.py(658):     if coef is not None:
1.07 logistic.py(688):     if multi_class == 'multinomial':
1.07 logistic.py(690):         if solver in ['lbfgs', 'newton-cg']:
1.07 logistic.py(691):             w0 = w0.ravel()
1.07 logistic.py(692):         target = Y_multi
1.07 logistic.py(693):         if solver == 'lbfgs':
1.07 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
1.07 logistic.py(699):         warm_start_sag = {'coef': w0.T}
1.07 logistic.py(710):     coefs = list()
1.07 logistic.py(711):     n_iter = np.zeros(len(Cs), dtype=np.int32)
1.07 logistic.py(712):     for i, C in enumerate(Cs):
1.07 logistic.py(713):         if solver == 'lbfgs':
1.07 logistic.py(714):             w0, loss, info = optimize.fmin_l_bfgs_b(
1.07 logistic.py(715):                 func, w0, fprime=None,
1.07 logistic.py(716):                 args=(X, target, 1. / C, sample_weight),
1.07 logistic.py(717):                 iprint=(verbose > 0) - 1, pgtol=tol, maxiter=max_iter)
1.07 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
1.07 logistic.py(339):     n_classes = Y.shape[1]
1.07 logistic.py(340):     n_features = X.shape[1]
1.07 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
1.07 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
1.07 logistic.py(343):                     dtype=X.dtype)
1.07 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
1.07 logistic.py(282):     n_classes = Y.shape[1]
1.07 logistic.py(283):     n_features = X.shape[1]
1.07 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
1.07 logistic.py(285):     w = w.reshape(n_classes, -1)
1.07 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
1.07 logistic.py(287):     if fit_intercept:
1.07 logistic.py(288):         intercept = w[:, -1]
1.07 logistic.py(289):         w = w[:, :-1]
1.07 logistic.py(292):     p = safe_sparse_dot(X, w.T)
1.07 logistic.py(293):     p += intercept
1.07 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
1.07 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
1.07 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
1.07 logistic.py(297):     p = np.exp(p, p)
1.07 logistic.py(298):     return loss, p, w
1.07 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
1.07 logistic.py(346):     diff = sample_weight * (p - Y)
1.07 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
1.07 logistic.py(348):     grad[:, :n_features] += alpha * w
1.07 logistic.py(349):     if fit_intercept:
1.07 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
1.07 logistic.py(351):     return loss, grad.ravel(), p
1.07 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
1.07 logistic.py(339):     n_classes = Y.shape[1]
1.07 logistic.py(340):     n_features = X.shape[1]
1.07 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
1.07 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
1.07 logistic.py(343):                     dtype=X.dtype)
1.07 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
1.07 logistic.py(282):     n_classes = Y.shape[1]
1.07 logistic.py(283):     n_features = X.shape[1]
1.07 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
1.07 logistic.py(285):     w = w.reshape(n_classes, -1)
1.07 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
1.07 logistic.py(287):     if fit_intercept:
1.07 logistic.py(288):         intercept = w[:, -1]
1.07 logistic.py(289):         w = w[:, :-1]
1.07 logistic.py(292):     p = safe_sparse_dot(X, w.T)
1.07 logistic.py(293):     p += intercept
1.07 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
1.07 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
1.07 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
1.07 logistic.py(297):     p = np.exp(p, p)
1.07 logistic.py(298):     return loss, p, w
1.07 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
1.07 logistic.py(346):     diff = sample_weight * (p - Y)
1.07 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
1.07 logistic.py(348):     grad[:, :n_features] += alpha * w
1.07 logistic.py(349):     if fit_intercept:
1.07 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
1.07 logistic.py(351):     return loss, grad.ravel(), p
1.07 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
1.07 logistic.py(339):     n_classes = Y.shape[1]
1.07 logistic.py(340):     n_features = X.shape[1]
1.07 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
1.07 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
1.07 logistic.py(343):                     dtype=X.dtype)
1.07 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
1.07 logistic.py(282):     n_classes = Y.shape[1]
1.07 logistic.py(283):     n_features = X.shape[1]
1.07 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
1.07 logistic.py(285):     w = w.reshape(n_classes, -1)
1.07 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
1.07 logistic.py(287):     if fit_intercept:
1.07 logistic.py(288):         intercept = w[:, -1]
1.07 logistic.py(289):         w = w[:, :-1]
1.07 logistic.py(292):     p = safe_sparse_dot(X, w.T)
1.07 logistic.py(293):     p += intercept
1.07 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
1.07 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
1.07 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
1.07 logistic.py(297):     p = np.exp(p, p)
1.07 logistic.py(298):     return loss, p, w
1.07 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
1.07 logistic.py(346):     diff = sample_weight * (p - Y)
1.07 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
1.07 logistic.py(348):     grad[:, :n_features] += alpha * w
1.07 logistic.py(349):     if fit_intercept:
1.07 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
1.07 logistic.py(351):     return loss, grad.ravel(), p
1.07 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
1.07 logistic.py(339):     n_classes = Y.shape[1]
1.07 logistic.py(340):     n_features = X.shape[1]
1.07 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
1.07 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
1.07 logistic.py(343):                     dtype=X.dtype)
1.07 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
1.07 logistic.py(282):     n_classes = Y.shape[1]
1.07 logistic.py(283):     n_features = X.shape[1]
1.07 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
1.07 logistic.py(285):     w = w.reshape(n_classes, -1)
1.07 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
1.07 logistic.py(287):     if fit_intercept:
1.07 logistic.py(288):         intercept = w[:, -1]
1.07 logistic.py(289):         w = w[:, :-1]
1.07 logistic.py(292):     p = safe_sparse_dot(X, w.T)
1.07 logistic.py(293):     p += intercept
1.07 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
1.07 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
1.07 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
1.07 logistic.py(297):     p = np.exp(p, p)
1.07 logistic.py(298):     return loss, p, w
1.07 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
1.07 logistic.py(346):     diff = sample_weight * (p - Y)
1.07 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
1.07 logistic.py(348):     grad[:, :n_features] += alpha * w
1.07 logistic.py(349):     if fit_intercept:
1.07 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
1.07 logistic.py(351):     return loss, grad.ravel(), p
1.07 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
1.07 logistic.py(339):     n_classes = Y.shape[1]
1.07 logistic.py(340):     n_features = X.shape[1]
1.07 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
1.07 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
1.07 logistic.py(343):                     dtype=X.dtype)
1.07 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
1.07 logistic.py(282):     n_classes = Y.shape[1]
1.07 logistic.py(283):     n_features = X.shape[1]
1.07 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
1.07 logistic.py(285):     w = w.reshape(n_classes, -1)
1.07 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
1.07 logistic.py(287):     if fit_intercept:
1.07 logistic.py(288):         intercept = w[:, -1]
1.07 logistic.py(289):         w = w[:, :-1]
1.07 logistic.py(292):     p = safe_sparse_dot(X, w.T)
1.07 logistic.py(293):     p += intercept
1.07 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
1.07 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
1.07 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
1.07 logistic.py(297):     p = np.exp(p, p)
1.07 logistic.py(298):     return loss, p, w
1.07 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
1.07 logistic.py(346):     diff = sample_weight * (p - Y)
1.07 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
1.07 logistic.py(348):     grad[:, :n_features] += alpha * w
1.07 logistic.py(349):     if fit_intercept:
1.07 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
1.07 logistic.py(351):     return loss, grad.ravel(), p
1.07 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
1.07 logistic.py(339):     n_classes = Y.shape[1]
1.07 logistic.py(340):     n_features = X.shape[1]
1.07 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
1.07 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
1.07 logistic.py(343):                     dtype=X.dtype)
1.07 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
1.07 logistic.py(282):     n_classes = Y.shape[1]
1.07 logistic.py(283):     n_features = X.shape[1]
1.07 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
1.07 logistic.py(285):     w = w.reshape(n_classes, -1)
1.07 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
1.07 logistic.py(287):     if fit_intercept:
1.07 logistic.py(288):         intercept = w[:, -1]
1.07 logistic.py(289):         w = w[:, :-1]
1.07 logistic.py(292):     p = safe_sparse_dot(X, w.T)
1.07 logistic.py(293):     p += intercept
1.07 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
1.07 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
1.07 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
1.07 logistic.py(297):     p = np.exp(p, p)
1.07 logistic.py(298):     return loss, p, w
1.07 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
1.07 logistic.py(346):     diff = sample_weight * (p - Y)
1.07 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
1.07 logistic.py(348):     grad[:, :n_features] += alpha * w
1.07 logistic.py(349):     if fit_intercept:
1.07 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
1.07 logistic.py(351):     return loss, grad.ravel(), p
1.07 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
1.07 logistic.py(339):     n_classes = Y.shape[1]
1.07 logistic.py(340):     n_features = X.shape[1]
1.07 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
1.07 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
1.07 logistic.py(343):                     dtype=X.dtype)
1.07 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
1.07 logistic.py(282):     n_classes = Y.shape[1]
1.07 logistic.py(283):     n_features = X.shape[1]
1.07 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
1.07 logistic.py(285):     w = w.reshape(n_classes, -1)
1.07 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
1.07 logistic.py(287):     if fit_intercept:
1.07 logistic.py(288):         intercept = w[:, -1]
1.07 logistic.py(289):         w = w[:, :-1]
1.07 logistic.py(292):     p = safe_sparse_dot(X, w.T)
1.07 logistic.py(293):     p += intercept
1.07 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
1.07 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
1.07 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
1.07 logistic.py(297):     p = np.exp(p, p)
1.07 logistic.py(298):     return loss, p, w
1.07 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
1.07 logistic.py(346):     diff = sample_weight * (p - Y)
1.07 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
1.07 logistic.py(348):     grad[:, :n_features] += alpha * w
1.07 logistic.py(349):     if fit_intercept:
1.07 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
1.07 logistic.py(351):     return loss, grad.ravel(), p
1.07 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
1.07 logistic.py(339):     n_classes = Y.shape[1]
1.07 logistic.py(340):     n_features = X.shape[1]
1.07 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
1.07 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
1.07 logistic.py(343):                     dtype=X.dtype)
1.07 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
1.07 logistic.py(282):     n_classes = Y.shape[1]
1.07 logistic.py(283):     n_features = X.shape[1]
1.07 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
1.07 logistic.py(285):     w = w.reshape(n_classes, -1)
1.07 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
1.07 logistic.py(287):     if fit_intercept:
1.07 logistic.py(288):         intercept = w[:, -1]
1.07 logistic.py(289):         w = w[:, :-1]
1.07 logistic.py(292):     p = safe_sparse_dot(X, w.T)
1.07 logistic.py(293):     p += intercept
1.07 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
1.07 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
1.07 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
1.07 logistic.py(297):     p = np.exp(p, p)
1.07 logistic.py(298):     return loss, p, w
1.07 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
1.07 logistic.py(346):     diff = sample_weight * (p - Y)
1.07 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
1.07 logistic.py(348):     grad[:, :n_features] += alpha * w
1.07 logistic.py(349):     if fit_intercept:
1.07 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
1.07 logistic.py(351):     return loss, grad.ravel(), p
1.07 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
1.07 logistic.py(339):     n_classes = Y.shape[1]
1.07 logistic.py(340):     n_features = X.shape[1]
1.07 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
1.07 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
1.07 logistic.py(343):                     dtype=X.dtype)
1.07 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
1.07 logistic.py(282):     n_classes = Y.shape[1]
1.07 logistic.py(283):     n_features = X.shape[1]
1.07 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
1.07 logistic.py(285):     w = w.reshape(n_classes, -1)
1.07 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
1.07 logistic.py(287):     if fit_intercept:
1.07 logistic.py(288):         intercept = w[:, -1]
1.07 logistic.py(289):         w = w[:, :-1]
1.07 logistic.py(292):     p = safe_sparse_dot(X, w.T)
1.07 logistic.py(293):     p += intercept
1.07 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
1.07 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
1.07 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
1.07 logistic.py(297):     p = np.exp(p, p)
1.07 logistic.py(298):     return loss, p, w
1.07 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
1.07 logistic.py(346):     diff = sample_weight * (p - Y)
1.07 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
1.07 logistic.py(348):     grad[:, :n_features] += alpha * w
1.07 logistic.py(349):     if fit_intercept:
1.08 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
1.08 logistic.py(351):     return loss, grad.ravel(), p
1.08 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
1.08 logistic.py(339):     n_classes = Y.shape[1]
1.08 logistic.py(340):     n_features = X.shape[1]
1.08 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
1.08 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
1.08 logistic.py(343):                     dtype=X.dtype)
1.08 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
1.08 logistic.py(282):     n_classes = Y.shape[1]
1.08 logistic.py(283):     n_features = X.shape[1]
1.08 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
1.08 logistic.py(285):     w = w.reshape(n_classes, -1)
1.08 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
1.08 logistic.py(287):     if fit_intercept:
1.08 logistic.py(288):         intercept = w[:, -1]
1.08 logistic.py(289):         w = w[:, :-1]
1.08 logistic.py(292):     p = safe_sparse_dot(X, w.T)
1.08 logistic.py(293):     p += intercept
1.08 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
1.08 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
1.08 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
1.08 logistic.py(297):     p = np.exp(p, p)
1.08 logistic.py(298):     return loss, p, w
1.08 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
1.08 logistic.py(346):     diff = sample_weight * (p - Y)
1.08 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
1.08 logistic.py(348):     grad[:, :n_features] += alpha * w
1.08 logistic.py(349):     if fit_intercept:
1.08 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
1.08 logistic.py(351):     return loss, grad.ravel(), p
1.08 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
1.08 logistic.py(339):     n_classes = Y.shape[1]
1.08 logistic.py(340):     n_features = X.shape[1]
1.08 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
1.08 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
1.08 logistic.py(343):                     dtype=X.dtype)
1.08 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
1.08 logistic.py(282):     n_classes = Y.shape[1]
1.08 logistic.py(283):     n_features = X.shape[1]
1.08 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
1.08 logistic.py(285):     w = w.reshape(n_classes, -1)
1.08 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
1.08 logistic.py(287):     if fit_intercept:
1.08 logistic.py(288):         intercept = w[:, -1]
1.08 logistic.py(289):         w = w[:, :-1]
1.08 logistic.py(292):     p = safe_sparse_dot(X, w.T)
1.08 logistic.py(293):     p += intercept
1.08 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
1.08 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
1.08 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
1.08 logistic.py(297):     p = np.exp(p, p)
1.08 logistic.py(298):     return loss, p, w
1.08 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
1.08 logistic.py(346):     diff = sample_weight * (p - Y)
1.08 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
1.08 logistic.py(348):     grad[:, :n_features] += alpha * w
1.08 logistic.py(349):     if fit_intercept:
1.08 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
1.08 logistic.py(351):     return loss, grad.ravel(), p
1.08 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
1.08 logistic.py(339):     n_classes = Y.shape[1]
1.08 logistic.py(340):     n_features = X.shape[1]
1.08 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
1.08 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
1.08 logistic.py(343):                     dtype=X.dtype)
1.08 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
1.08 logistic.py(282):     n_classes = Y.shape[1]
1.08 logistic.py(283):     n_features = X.shape[1]
1.08 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
1.08 logistic.py(285):     w = w.reshape(n_classes, -1)
1.08 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
1.08 logistic.py(287):     if fit_intercept:
1.08 logistic.py(288):         intercept = w[:, -1]
1.08 logistic.py(289):         w = w[:, :-1]
1.08 logistic.py(292):     p = safe_sparse_dot(X, w.T)
1.08 logistic.py(293):     p += intercept
1.08 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
1.08 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
1.08 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
1.08 logistic.py(297):     p = np.exp(p, p)
1.08 logistic.py(298):     return loss, p, w
1.08 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
1.08 logistic.py(346):     diff = sample_weight * (p - Y)
1.08 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
1.08 logistic.py(348):     grad[:, :n_features] += alpha * w
1.08 logistic.py(349):     if fit_intercept:
1.08 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
1.08 logistic.py(351):     return loss, grad.ravel(), p
1.08 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
1.08 logistic.py(339):     n_classes = Y.shape[1]
1.08 logistic.py(340):     n_features = X.shape[1]
1.08 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
1.08 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
1.08 logistic.py(343):                     dtype=X.dtype)
1.08 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
1.08 logistic.py(282):     n_classes = Y.shape[1]
1.08 logistic.py(283):     n_features = X.shape[1]
1.08 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
1.08 logistic.py(285):     w = w.reshape(n_classes, -1)
1.08 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
1.08 logistic.py(287):     if fit_intercept:
1.08 logistic.py(288):         intercept = w[:, -1]
1.08 logistic.py(289):         w = w[:, :-1]
1.08 logistic.py(292):     p = safe_sparse_dot(X, w.T)
1.08 logistic.py(293):     p += intercept
1.08 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
1.08 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
1.08 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
1.08 logistic.py(297):     p = np.exp(p, p)
1.08 logistic.py(298):     return loss, p, w
1.08 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
1.08 logistic.py(346):     diff = sample_weight * (p - Y)
1.08 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
1.08 logistic.py(348):     grad[:, :n_features] += alpha * w
1.08 logistic.py(349):     if fit_intercept:
1.08 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
1.08 logistic.py(351):     return loss, grad.ravel(), p
1.08 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
1.08 logistic.py(339):     n_classes = Y.shape[1]
1.08 logistic.py(340):     n_features = X.shape[1]
1.08 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
1.08 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
1.08 logistic.py(343):                     dtype=X.dtype)
1.08 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
1.08 logistic.py(282):     n_classes = Y.shape[1]
1.08 logistic.py(283):     n_features = X.shape[1]
1.08 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
1.08 logistic.py(285):     w = w.reshape(n_classes, -1)
1.08 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
1.08 logistic.py(287):     if fit_intercept:
1.08 logistic.py(288):         intercept = w[:, -1]
1.08 logistic.py(289):         w = w[:, :-1]
1.08 logistic.py(292):     p = safe_sparse_dot(X, w.T)
1.08 logistic.py(293):     p += intercept
1.08 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
1.08 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
1.08 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
1.08 logistic.py(297):     p = np.exp(p, p)
1.08 logistic.py(298):     return loss, p, w
1.08 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
1.08 logistic.py(346):     diff = sample_weight * (p - Y)
1.08 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
1.08 logistic.py(348):     grad[:, :n_features] += alpha * w
1.08 logistic.py(349):     if fit_intercept:
1.08 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
1.08 logistic.py(351):     return loss, grad.ravel(), p
1.08 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
1.08 logistic.py(339):     n_classes = Y.shape[1]
1.08 logistic.py(340):     n_features = X.shape[1]
1.08 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
1.08 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
1.08 logistic.py(343):                     dtype=X.dtype)
1.08 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
1.08 logistic.py(282):     n_classes = Y.shape[1]
1.08 logistic.py(283):     n_features = X.shape[1]
1.08 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
1.08 logistic.py(285):     w = w.reshape(n_classes, -1)
1.08 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
1.08 logistic.py(287):     if fit_intercept:
1.08 logistic.py(288):         intercept = w[:, -1]
1.08 logistic.py(289):         w = w[:, :-1]
1.08 logistic.py(292):     p = safe_sparse_dot(X, w.T)
1.08 logistic.py(293):     p += intercept
1.08 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
1.08 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
1.08 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
1.08 logistic.py(297):     p = np.exp(p, p)
1.08 logistic.py(298):     return loss, p, w
1.08 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
1.08 logistic.py(346):     diff = sample_weight * (p - Y)
1.08 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
1.08 logistic.py(348):     grad[:, :n_features] += alpha * w
1.08 logistic.py(349):     if fit_intercept:
1.08 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
1.08 logistic.py(351):     return loss, grad.ravel(), p
1.08 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
1.08 logistic.py(339):     n_classes = Y.shape[1]
1.08 logistic.py(340):     n_features = X.shape[1]
1.08 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
1.08 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
1.08 logistic.py(343):                     dtype=X.dtype)
1.08 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
1.08 logistic.py(282):     n_classes = Y.shape[1]
1.08 logistic.py(283):     n_features = X.shape[1]
1.08 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
1.08 logistic.py(285):     w = w.reshape(n_classes, -1)
1.08 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
1.08 logistic.py(287):     if fit_intercept:
1.08 logistic.py(288):         intercept = w[:, -1]
1.08 logistic.py(289):         w = w[:, :-1]
1.08 logistic.py(292):     p = safe_sparse_dot(X, w.T)
1.08 logistic.py(293):     p += intercept
1.08 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
1.08 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
1.08 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
1.08 logistic.py(297):     p = np.exp(p, p)
1.08 logistic.py(298):     return loss, p, w
1.08 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
1.08 logistic.py(346):     diff = sample_weight * (p - Y)
1.08 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
1.08 logistic.py(348):     grad[:, :n_features] += alpha * w
1.08 logistic.py(349):     if fit_intercept:
1.08 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
1.08 logistic.py(351):     return loss, grad.ravel(), p
1.08 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
1.08 logistic.py(339):     n_classes = Y.shape[1]
1.08 logistic.py(340):     n_features = X.shape[1]
1.08 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
1.08 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
1.08 logistic.py(343):                     dtype=X.dtype)
1.08 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
1.08 logistic.py(282):     n_classes = Y.shape[1]
1.08 logistic.py(283):     n_features = X.shape[1]
1.08 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
1.08 logistic.py(285):     w = w.reshape(n_classes, -1)
1.08 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
1.08 logistic.py(287):     if fit_intercept:
1.08 logistic.py(288):         intercept = w[:, -1]
1.08 logistic.py(289):         w = w[:, :-1]
1.08 logistic.py(292):     p = safe_sparse_dot(X, w.T)
1.08 logistic.py(293):     p += intercept
1.08 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
1.08 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
1.08 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
1.08 logistic.py(297):     p = np.exp(p, p)
1.08 logistic.py(298):     return loss, p, w
1.08 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
1.08 logistic.py(346):     diff = sample_weight * (p - Y)
1.08 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
1.08 logistic.py(348):     grad[:, :n_features] += alpha * w
1.08 logistic.py(349):     if fit_intercept:
1.08 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
1.08 logistic.py(351):     return loss, grad.ravel(), p
1.08 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
1.08 logistic.py(339):     n_classes = Y.shape[1]
1.08 logistic.py(340):     n_features = X.shape[1]
1.08 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
1.08 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
1.08 logistic.py(343):                     dtype=X.dtype)
1.08 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
1.08 logistic.py(282):     n_classes = Y.shape[1]
1.08 logistic.py(283):     n_features = X.shape[1]
1.08 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
1.08 logistic.py(285):     w = w.reshape(n_classes, -1)
1.08 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
1.08 logistic.py(287):     if fit_intercept:
1.08 logistic.py(288):         intercept = w[:, -1]
1.08 logistic.py(289):         w = w[:, :-1]
1.08 logistic.py(292):     p = safe_sparse_dot(X, w.T)
1.08 logistic.py(293):     p += intercept
1.08 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
1.08 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
1.08 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
1.08 logistic.py(297):     p = np.exp(p, p)
1.08 logistic.py(298):     return loss, p, w
1.08 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
1.08 logistic.py(346):     diff = sample_weight * (p - Y)
1.08 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
1.08 logistic.py(348):     grad[:, :n_features] += alpha * w
1.08 logistic.py(349):     if fit_intercept:
1.08 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
1.08 logistic.py(351):     return loss, grad.ravel(), p
1.08 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
1.08 logistic.py(339):     n_classes = Y.shape[1]
1.08 logistic.py(340):     n_features = X.shape[1]
1.08 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
1.08 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
1.08 logistic.py(343):                     dtype=X.dtype)
1.08 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
1.08 logistic.py(282):     n_classes = Y.shape[1]
1.08 logistic.py(283):     n_features = X.shape[1]
1.08 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
1.08 logistic.py(285):     w = w.reshape(n_classes, -1)
1.08 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
1.08 logistic.py(287):     if fit_intercept:
1.08 logistic.py(288):         intercept = w[:, -1]
1.08 logistic.py(289):         w = w[:, :-1]
1.08 logistic.py(292):     p = safe_sparse_dot(X, w.T)
1.08 logistic.py(293):     p += intercept
1.08 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
1.08 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
1.08 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
1.08 logistic.py(297):     p = np.exp(p, p)
1.08 logistic.py(298):     return loss, p, w
1.08 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
1.08 logistic.py(346):     diff = sample_weight * (p - Y)
1.08 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
1.08 logistic.py(348):     grad[:, :n_features] += alpha * w
1.08 logistic.py(349):     if fit_intercept:
1.08 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
1.08 logistic.py(351):     return loss, grad.ravel(), p
1.08 logistic.py(718):             if info["warnflag"] == 1:
1.08 logistic.py(723):             n_iter_i = min(info['nit'], max_iter)
1.08 logistic.py(760):         if multi_class == 'multinomial':
1.08 logistic.py(761):             multi_w0 = np.reshape(w0, (classes.size, -1))
1.08 logistic.py(762):             if classes.size == 2:
1.08 logistic.py(764):             coefs.append(multi_w0)
1.08 logistic.py(768):         n_iter[i] = n_iter_i
1.08 logistic.py(712):     for i, C in enumerate(Cs):
1.08 logistic.py(713):         if solver == 'lbfgs':
1.08 logistic.py(714):             w0, loss, info = optimize.fmin_l_bfgs_b(
1.08 logistic.py(715):                 func, w0, fprime=None,
1.08 logistic.py(716):                 args=(X, target, 1. / C, sample_weight),
1.08 logistic.py(717):                 iprint=(verbose > 0) - 1, pgtol=tol, maxiter=max_iter)
1.08 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
1.08 logistic.py(339):     n_classes = Y.shape[1]
1.08 logistic.py(340):     n_features = X.shape[1]
1.08 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
1.08 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
1.08 logistic.py(343):                     dtype=X.dtype)
1.08 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
1.08 logistic.py(282):     n_classes = Y.shape[1]
1.08 logistic.py(283):     n_features = X.shape[1]
1.08 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
1.08 logistic.py(285):     w = w.reshape(n_classes, -1)
1.08 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
1.08 logistic.py(287):     if fit_intercept:
1.08 logistic.py(288):         intercept = w[:, -1]
1.08 logistic.py(289):         w = w[:, :-1]
1.08 logistic.py(292):     p = safe_sparse_dot(X, w.T)
1.08 logistic.py(293):     p += intercept
1.08 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
1.08 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
1.08 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
1.08 logistic.py(297):     p = np.exp(p, p)
1.08 logistic.py(298):     return loss, p, w
1.08 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
1.08 logistic.py(346):     diff = sample_weight * (p - Y)
1.08 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
1.08 logistic.py(348):     grad[:, :n_features] += alpha * w
1.08 logistic.py(349):     if fit_intercept:
1.08 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
1.08 logistic.py(351):     return loss, grad.ravel(), p
1.08 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
1.08 logistic.py(339):     n_classes = Y.shape[1]
1.08 logistic.py(340):     n_features = X.shape[1]
1.08 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
1.08 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
1.08 logistic.py(343):                     dtype=X.dtype)
1.08 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
1.08 logistic.py(282):     n_classes = Y.shape[1]
1.08 logistic.py(283):     n_features = X.shape[1]
1.08 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
1.08 logistic.py(285):     w = w.reshape(n_classes, -1)
1.08 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
1.08 logistic.py(287):     if fit_intercept:
1.08 logistic.py(288):         intercept = w[:, -1]
1.08 logistic.py(289):         w = w[:, :-1]
1.08 logistic.py(292):     p = safe_sparse_dot(X, w.T)
1.08 logistic.py(293):     p += intercept
1.08 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
1.08 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
1.08 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
1.08 logistic.py(297):     p = np.exp(p, p)
1.08 logistic.py(298):     return loss, p, w
1.08 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
1.08 logistic.py(346):     diff = sample_weight * (p - Y)
1.08 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
1.08 logistic.py(348):     grad[:, :n_features] += alpha * w
1.08 logistic.py(349):     if fit_intercept:
1.08 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
1.08 logistic.py(351):     return loss, grad.ravel(), p
1.08 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
1.08 logistic.py(339):     n_classes = Y.shape[1]
1.08 logistic.py(340):     n_features = X.shape[1]
1.08 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
1.08 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
1.08 logistic.py(343):                     dtype=X.dtype)
1.08 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
1.08 logistic.py(282):     n_classes = Y.shape[1]
1.08 logistic.py(283):     n_features = X.shape[1]
1.08 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
1.08 logistic.py(285):     w = w.reshape(n_classes, -1)
1.08 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
1.08 logistic.py(287):     if fit_intercept:
1.08 logistic.py(288):         intercept = w[:, -1]
1.08 logistic.py(289):         w = w[:, :-1]
1.08 logistic.py(292):     p = safe_sparse_dot(X, w.T)
1.08 logistic.py(293):     p += intercept
1.08 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
1.08 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
1.08 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
1.08 logistic.py(297):     p = np.exp(p, p)
1.08 logistic.py(298):     return loss, p, w
1.08 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
1.08 logistic.py(346):     diff = sample_weight * (p - Y)
1.08 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
1.08 logistic.py(348):     grad[:, :n_features] += alpha * w
1.08 logistic.py(349):     if fit_intercept:
1.08 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
1.08 logistic.py(351):     return loss, grad.ravel(), p
1.08 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
1.08 logistic.py(339):     n_classes = Y.shape[1]
1.08 logistic.py(340):     n_features = X.shape[1]
1.08 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
1.08 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
1.08 logistic.py(343):                     dtype=X.dtype)
1.08 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
1.08 logistic.py(282):     n_classes = Y.shape[1]
1.08 logistic.py(283):     n_features = X.shape[1]
1.08 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
1.08 logistic.py(285):     w = w.reshape(n_classes, -1)
1.08 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
1.08 logistic.py(287):     if fit_intercept:
1.08 logistic.py(288):         intercept = w[:, -1]
1.08 logistic.py(289):         w = w[:, :-1]
1.08 logistic.py(292):     p = safe_sparse_dot(X, w.T)
1.08 logistic.py(293):     p += intercept
1.08 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
1.08 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
1.08 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
1.08 logistic.py(297):     p = np.exp(p, p)
1.08 logistic.py(298):     return loss, p, w
1.09 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
1.09 logistic.py(346):     diff = sample_weight * (p - Y)
1.09 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
1.09 logistic.py(348):     grad[:, :n_features] += alpha * w
1.09 logistic.py(349):     if fit_intercept:
1.09 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
1.09 logistic.py(351):     return loss, grad.ravel(), p
1.09 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
1.09 logistic.py(339):     n_classes = Y.shape[1]
1.09 logistic.py(340):     n_features = X.shape[1]
1.09 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
1.09 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
1.09 logistic.py(343):                     dtype=X.dtype)
1.09 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
1.09 logistic.py(282):     n_classes = Y.shape[1]
1.09 logistic.py(283):     n_features = X.shape[1]
1.09 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
1.09 logistic.py(285):     w = w.reshape(n_classes, -1)
1.09 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
1.09 logistic.py(287):     if fit_intercept:
1.09 logistic.py(288):         intercept = w[:, -1]
1.09 logistic.py(289):         w = w[:, :-1]
1.09 logistic.py(292):     p = safe_sparse_dot(X, w.T)
1.09 logistic.py(293):     p += intercept
1.09 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
1.09 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
1.09 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
1.09 logistic.py(297):     p = np.exp(p, p)
1.09 logistic.py(298):     return loss, p, w
1.09 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
1.09 logistic.py(346):     diff = sample_weight * (p - Y)
1.09 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
1.09 logistic.py(348):     grad[:, :n_features] += alpha * w
1.09 logistic.py(349):     if fit_intercept:
1.09 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
1.09 logistic.py(351):     return loss, grad.ravel(), p
1.09 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
1.09 logistic.py(339):     n_classes = Y.shape[1]
1.09 logistic.py(340):     n_features = X.shape[1]
1.09 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
1.09 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
1.09 logistic.py(343):                     dtype=X.dtype)
1.09 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
1.09 logistic.py(282):     n_classes = Y.shape[1]
1.09 logistic.py(283):     n_features = X.shape[1]
1.09 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
1.09 logistic.py(285):     w = w.reshape(n_classes, -1)
1.09 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
1.09 logistic.py(287):     if fit_intercept:
1.09 logistic.py(288):         intercept = w[:, -1]
1.09 logistic.py(289):         w = w[:, :-1]
1.09 logistic.py(292):     p = safe_sparse_dot(X, w.T)
1.09 logistic.py(293):     p += intercept
1.09 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
1.09 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
1.09 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
1.09 logistic.py(297):     p = np.exp(p, p)
1.09 logistic.py(298):     return loss, p, w
1.09 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
1.09 logistic.py(346):     diff = sample_weight * (p - Y)
1.09 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
1.09 logistic.py(348):     grad[:, :n_features] += alpha * w
1.09 logistic.py(349):     if fit_intercept:
1.09 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
1.09 logistic.py(351):     return loss, grad.ravel(), p
1.09 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
1.09 logistic.py(339):     n_classes = Y.shape[1]
1.09 logistic.py(340):     n_features = X.shape[1]
1.09 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
1.09 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
1.09 logistic.py(343):                     dtype=X.dtype)
1.09 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
1.09 logistic.py(282):     n_classes = Y.shape[1]
1.09 logistic.py(283):     n_features = X.shape[1]
1.09 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
1.09 logistic.py(285):     w = w.reshape(n_classes, -1)
1.09 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
1.09 logistic.py(287):     if fit_intercept:
1.09 logistic.py(288):         intercept = w[:, -1]
1.09 logistic.py(289):         w = w[:, :-1]
1.09 logistic.py(292):     p = safe_sparse_dot(X, w.T)
1.09 logistic.py(293):     p += intercept
1.09 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
1.09 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
1.09 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
1.09 logistic.py(297):     p = np.exp(p, p)
1.09 logistic.py(298):     return loss, p, w
1.09 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
1.09 logistic.py(346):     diff = sample_weight * (p - Y)
1.09 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
1.09 logistic.py(348):     grad[:, :n_features] += alpha * w
1.09 logistic.py(349):     if fit_intercept:
1.09 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
1.09 logistic.py(351):     return loss, grad.ravel(), p
1.09 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
1.09 logistic.py(339):     n_classes = Y.shape[1]
1.09 logistic.py(340):     n_features = X.shape[1]
1.09 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
1.09 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
1.09 logistic.py(343):                     dtype=X.dtype)
1.09 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
1.09 logistic.py(282):     n_classes = Y.shape[1]
1.09 logistic.py(283):     n_features = X.shape[1]
1.09 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
1.09 logistic.py(285):     w = w.reshape(n_classes, -1)
1.09 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
1.09 logistic.py(287):     if fit_intercept:
1.09 logistic.py(288):         intercept = w[:, -1]
1.09 logistic.py(289):         w = w[:, :-1]
1.09 logistic.py(292):     p = safe_sparse_dot(X, w.T)
1.09 logistic.py(293):     p += intercept
1.09 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
1.09 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
1.09 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
1.09 logistic.py(297):     p = np.exp(p, p)
1.09 logistic.py(298):     return loss, p, w
1.09 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
1.09 logistic.py(346):     diff = sample_weight * (p - Y)
1.09 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
1.09 logistic.py(348):     grad[:, :n_features] += alpha * w
1.09 logistic.py(349):     if fit_intercept:
1.09 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
1.09 logistic.py(351):     return loss, grad.ravel(), p
1.09 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
1.09 logistic.py(339):     n_classes = Y.shape[1]
1.09 logistic.py(340):     n_features = X.shape[1]
1.09 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
1.09 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
1.09 logistic.py(343):                     dtype=X.dtype)
1.09 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
1.09 logistic.py(282):     n_classes = Y.shape[1]
1.09 logistic.py(283):     n_features = X.shape[1]
1.09 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
1.09 logistic.py(285):     w = w.reshape(n_classes, -1)
1.09 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
1.09 logistic.py(287):     if fit_intercept:
1.09 logistic.py(288):         intercept = w[:, -1]
1.09 logistic.py(289):         w = w[:, :-1]
1.09 logistic.py(292):     p = safe_sparse_dot(X, w.T)
1.09 logistic.py(293):     p += intercept
1.09 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
1.09 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
1.09 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
1.09 logistic.py(297):     p = np.exp(p, p)
1.09 logistic.py(298):     return loss, p, w
1.09 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
1.09 logistic.py(346):     diff = sample_weight * (p - Y)
1.09 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
1.09 logistic.py(348):     grad[:, :n_features] += alpha * w
1.09 logistic.py(349):     if fit_intercept:
1.09 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
1.09 logistic.py(351):     return loss, grad.ravel(), p
1.09 logistic.py(718):             if info["warnflag"] == 1:
1.09 logistic.py(723):             n_iter_i = min(info['nit'], max_iter)
1.09 logistic.py(760):         if multi_class == 'multinomial':
1.09 logistic.py(761):             multi_w0 = np.reshape(w0, (classes.size, -1))
1.09 logistic.py(762):             if classes.size == 2:
1.09 logistic.py(764):             coefs.append(multi_w0)
1.09 logistic.py(768):         n_iter[i] = n_iter_i
1.09 logistic.py(712):     for i, C in enumerate(Cs):
1.09 logistic.py(713):         if solver == 'lbfgs':
1.09 logistic.py(714):             w0, loss, info = optimize.fmin_l_bfgs_b(
1.09 logistic.py(715):                 func, w0, fprime=None,
1.09 logistic.py(716):                 args=(X, target, 1. / C, sample_weight),
1.09 logistic.py(717):                 iprint=(verbose > 0) - 1, pgtol=tol, maxiter=max_iter)
1.09 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
1.09 logistic.py(339):     n_classes = Y.shape[1]
1.09 logistic.py(340):     n_features = X.shape[1]
1.09 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
1.09 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
1.09 logistic.py(343):                     dtype=X.dtype)
1.09 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
1.09 logistic.py(282):     n_classes = Y.shape[1]
1.09 logistic.py(283):     n_features = X.shape[1]
1.09 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
1.09 logistic.py(285):     w = w.reshape(n_classes, -1)
1.09 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
1.09 logistic.py(287):     if fit_intercept:
1.09 logistic.py(288):         intercept = w[:, -1]
1.09 logistic.py(289):         w = w[:, :-1]
1.09 logistic.py(292):     p = safe_sparse_dot(X, w.T)
1.09 logistic.py(293):     p += intercept
1.09 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
1.09 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
1.09 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
1.09 logistic.py(297):     p = np.exp(p, p)
1.09 logistic.py(298):     return loss, p, w
1.09 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
1.09 logistic.py(346):     diff = sample_weight * (p - Y)
1.09 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
1.09 logistic.py(348):     grad[:, :n_features] += alpha * w
1.09 logistic.py(349):     if fit_intercept:
1.09 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
1.09 logistic.py(351):     return loss, grad.ravel(), p
1.09 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
1.09 logistic.py(339):     n_classes = Y.shape[1]
1.09 logistic.py(340):     n_features = X.shape[1]
1.09 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
1.09 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
1.09 logistic.py(343):                     dtype=X.dtype)
1.09 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
1.09 logistic.py(282):     n_classes = Y.shape[1]
1.09 logistic.py(283):     n_features = X.shape[1]
1.09 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
1.09 logistic.py(285):     w = w.reshape(n_classes, -1)
1.09 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
1.09 logistic.py(287):     if fit_intercept:
1.09 logistic.py(288):         intercept = w[:, -1]
1.09 logistic.py(289):         w = w[:, :-1]
1.09 logistic.py(292):     p = safe_sparse_dot(X, w.T)
1.09 logistic.py(293):     p += intercept
1.09 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
1.09 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
1.09 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
1.09 logistic.py(297):     p = np.exp(p, p)
1.09 logistic.py(298):     return loss, p, w
1.09 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
1.09 logistic.py(346):     diff = sample_weight * (p - Y)
1.09 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
1.09 logistic.py(348):     grad[:, :n_features] += alpha * w
1.09 logistic.py(349):     if fit_intercept:
1.09 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
1.09 logistic.py(351):     return loss, grad.ravel(), p
1.09 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
1.09 logistic.py(339):     n_classes = Y.shape[1]
1.09 logistic.py(340):     n_features = X.shape[1]
1.09 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
1.09 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
1.09 logistic.py(343):                     dtype=X.dtype)
1.09 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
1.09 logistic.py(282):     n_classes = Y.shape[1]
1.09 logistic.py(283):     n_features = X.shape[1]
1.09 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
1.09 logistic.py(285):     w = w.reshape(n_classes, -1)
1.09 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
1.09 logistic.py(287):     if fit_intercept:
1.09 logistic.py(288):         intercept = w[:, -1]
1.09 logistic.py(289):         w = w[:, :-1]
1.09 logistic.py(292):     p = safe_sparse_dot(X, w.T)
1.09 logistic.py(293):     p += intercept
1.09 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
1.09 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
1.09 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
1.09 logistic.py(297):     p = np.exp(p, p)
1.09 logistic.py(298):     return loss, p, w
1.09 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
1.09 logistic.py(346):     diff = sample_weight * (p - Y)
1.09 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
1.09 logistic.py(348):     grad[:, :n_features] += alpha * w
1.09 logistic.py(349):     if fit_intercept:
1.09 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
1.09 logistic.py(351):     return loss, grad.ravel(), p
1.09 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
1.09 logistic.py(339):     n_classes = Y.shape[1]
1.09 logistic.py(340):     n_features = X.shape[1]
1.09 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
1.09 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
1.09 logistic.py(343):                     dtype=X.dtype)
1.09 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
1.09 logistic.py(282):     n_classes = Y.shape[1]
1.09 logistic.py(283):     n_features = X.shape[1]
1.09 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
1.09 logistic.py(285):     w = w.reshape(n_classes, -1)
1.09 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
1.09 logistic.py(287):     if fit_intercept:
1.09 logistic.py(288):         intercept = w[:, -1]
1.09 logistic.py(289):         w = w[:, :-1]
1.09 logistic.py(292):     p = safe_sparse_dot(X, w.T)
1.09 logistic.py(293):     p += intercept
1.09 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
1.09 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
1.09 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
1.09 logistic.py(297):     p = np.exp(p, p)
1.09 logistic.py(298):     return loss, p, w
1.09 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
1.09 logistic.py(346):     diff = sample_weight * (p - Y)
1.09 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
1.09 logistic.py(348):     grad[:, :n_features] += alpha * w
1.09 logistic.py(349):     if fit_intercept:
1.09 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
1.09 logistic.py(351):     return loss, grad.ravel(), p
1.09 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
1.09 logistic.py(339):     n_classes = Y.shape[1]
1.09 logistic.py(340):     n_features = X.shape[1]
1.09 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
1.09 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
1.09 logistic.py(343):                     dtype=X.dtype)
1.09 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
1.09 logistic.py(282):     n_classes = Y.shape[1]
1.09 logistic.py(283):     n_features = X.shape[1]
1.09 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
1.09 logistic.py(285):     w = w.reshape(n_classes, -1)
1.09 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
1.09 logistic.py(287):     if fit_intercept:
1.09 logistic.py(288):         intercept = w[:, -1]
1.09 logistic.py(289):         w = w[:, :-1]
1.09 logistic.py(292):     p = safe_sparse_dot(X, w.T)
1.09 logistic.py(293):     p += intercept
1.09 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
1.09 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
1.09 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
1.09 logistic.py(297):     p = np.exp(p, p)
1.09 logistic.py(298):     return loss, p, w
1.09 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
1.09 logistic.py(346):     diff = sample_weight * (p - Y)
1.09 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
1.09 logistic.py(348):     grad[:, :n_features] += alpha * w
1.09 logistic.py(349):     if fit_intercept:
1.09 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
1.09 logistic.py(351):     return loss, grad.ravel(), p
1.09 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
1.09 logistic.py(339):     n_classes = Y.shape[1]
1.09 logistic.py(340):     n_features = X.shape[1]
1.09 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
1.09 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
1.09 logistic.py(343):                     dtype=X.dtype)
1.09 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
1.09 logistic.py(282):     n_classes = Y.shape[1]
1.09 logistic.py(283):     n_features = X.shape[1]
1.09 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
1.09 logistic.py(285):     w = w.reshape(n_classes, -1)
1.09 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
1.09 logistic.py(287):     if fit_intercept:
1.09 logistic.py(288):         intercept = w[:, -1]
1.09 logistic.py(289):         w = w[:, :-1]
1.09 logistic.py(292):     p = safe_sparse_dot(X, w.T)
1.09 logistic.py(293):     p += intercept
1.09 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
1.09 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
1.09 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
1.09 logistic.py(297):     p = np.exp(p, p)
1.09 logistic.py(298):     return loss, p, w
1.09 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
1.09 logistic.py(346):     diff = sample_weight * (p - Y)
1.09 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
1.09 logistic.py(348):     grad[:, :n_features] += alpha * w
1.09 logistic.py(349):     if fit_intercept:
1.09 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
1.09 logistic.py(351):     return loss, grad.ravel(), p
1.09 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
1.09 logistic.py(339):     n_classes = Y.shape[1]
1.09 logistic.py(340):     n_features = X.shape[1]
1.09 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
1.09 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
1.09 logistic.py(343):                     dtype=X.dtype)
1.09 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
1.09 logistic.py(282):     n_classes = Y.shape[1]
1.09 logistic.py(283):     n_features = X.shape[1]
1.09 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
1.09 logistic.py(285):     w = w.reshape(n_classes, -1)
1.09 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
1.09 logistic.py(287):     if fit_intercept:
1.09 logistic.py(288):         intercept = w[:, -1]
1.09 logistic.py(289):         w = w[:, :-1]
1.09 logistic.py(292):     p = safe_sparse_dot(X, w.T)
1.09 logistic.py(293):     p += intercept
1.09 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
1.09 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
1.09 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
1.09 logistic.py(297):     p = np.exp(p, p)
1.09 logistic.py(298):     return loss, p, w
1.09 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
1.09 logistic.py(346):     diff = sample_weight * (p - Y)
1.09 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
1.09 logistic.py(348):     grad[:, :n_features] += alpha * w
1.09 logistic.py(349):     if fit_intercept:
1.09 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
1.09 logistic.py(351):     return loss, grad.ravel(), p
1.09 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
1.09 logistic.py(339):     n_classes = Y.shape[1]
1.09 logistic.py(340):     n_features = X.shape[1]
1.09 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
1.09 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
1.09 logistic.py(343):                     dtype=X.dtype)
1.09 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
1.09 logistic.py(282):     n_classes = Y.shape[1]
1.09 logistic.py(283):     n_features = X.shape[1]
1.09 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
1.09 logistic.py(285):     w = w.reshape(n_classes, -1)
1.09 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
1.09 logistic.py(287):     if fit_intercept:
1.09 logistic.py(288):         intercept = w[:, -1]
1.09 logistic.py(289):         w = w[:, :-1]
1.09 logistic.py(292):     p = safe_sparse_dot(X, w.T)
1.10 logistic.py(293):     p += intercept
1.10 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
1.10 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
1.10 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
1.10 logistic.py(297):     p = np.exp(p, p)
1.10 logistic.py(298):     return loss, p, w
1.10 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
1.10 logistic.py(346):     diff = sample_weight * (p - Y)
1.10 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
1.10 logistic.py(348):     grad[:, :n_features] += alpha * w
1.10 logistic.py(349):     if fit_intercept:
1.10 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
1.10 logistic.py(351):     return loss, grad.ravel(), p
1.10 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
1.10 logistic.py(339):     n_classes = Y.shape[1]
1.10 logistic.py(340):     n_features = X.shape[1]
1.10 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
1.10 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
1.10 logistic.py(343):                     dtype=X.dtype)
1.10 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
1.10 logistic.py(282):     n_classes = Y.shape[1]
1.10 logistic.py(283):     n_features = X.shape[1]
1.10 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
1.10 logistic.py(285):     w = w.reshape(n_classes, -1)
1.10 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
1.10 logistic.py(287):     if fit_intercept:
1.10 logistic.py(288):         intercept = w[:, -1]
1.10 logistic.py(289):         w = w[:, :-1]
1.10 logistic.py(292):     p = safe_sparse_dot(X, w.T)
1.10 logistic.py(293):     p += intercept
1.10 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
1.10 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
1.10 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
1.10 logistic.py(297):     p = np.exp(p, p)
1.10 logistic.py(298):     return loss, p, w
1.10 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
1.10 logistic.py(346):     diff = sample_weight * (p - Y)
1.10 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
1.10 logistic.py(348):     grad[:, :n_features] += alpha * w
1.10 logistic.py(349):     if fit_intercept:
1.10 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
1.10 logistic.py(351):     return loss, grad.ravel(), p
1.10 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
1.10 logistic.py(339):     n_classes = Y.shape[1]
1.10 logistic.py(340):     n_features = X.shape[1]
1.10 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
1.10 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
1.10 logistic.py(343):                     dtype=X.dtype)
1.10 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
1.10 logistic.py(282):     n_classes = Y.shape[1]
1.10 logistic.py(283):     n_features = X.shape[1]
1.10 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
1.10 logistic.py(285):     w = w.reshape(n_classes, -1)
1.10 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
1.10 logistic.py(287):     if fit_intercept:
1.10 logistic.py(288):         intercept = w[:, -1]
1.10 logistic.py(289):         w = w[:, :-1]
1.10 logistic.py(292):     p = safe_sparse_dot(X, w.T)
1.10 logistic.py(293):     p += intercept
1.10 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
1.10 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
1.10 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
1.10 logistic.py(297):     p = np.exp(p, p)
1.10 logistic.py(298):     return loss, p, w
1.10 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
1.10 logistic.py(346):     diff = sample_weight * (p - Y)
1.10 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
1.10 logistic.py(348):     grad[:, :n_features] += alpha * w
1.10 logistic.py(349):     if fit_intercept:
1.10 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
1.10 logistic.py(351):     return loss, grad.ravel(), p
1.10 logistic.py(718):             if info["warnflag"] == 1:
1.10 logistic.py(723):             n_iter_i = min(info['nit'], max_iter)
1.10 logistic.py(760):         if multi_class == 'multinomial':
1.10 logistic.py(761):             multi_w0 = np.reshape(w0, (classes.size, -1))
1.10 logistic.py(762):             if classes.size == 2:
1.10 logistic.py(764):             coefs.append(multi_w0)
1.10 logistic.py(768):         n_iter[i] = n_iter_i
1.10 logistic.py(712):     for i, C in enumerate(Cs):
1.10 logistic.py(713):         if solver == 'lbfgs':
1.10 logistic.py(714):             w0, loss, info = optimize.fmin_l_bfgs_b(
1.10 logistic.py(715):                 func, w0, fprime=None,
1.10 logistic.py(716):                 args=(X, target, 1. / C, sample_weight),
1.10 logistic.py(717):                 iprint=(verbose > 0) - 1, pgtol=tol, maxiter=max_iter)
1.10 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
1.10 logistic.py(339):     n_classes = Y.shape[1]
1.10 logistic.py(340):     n_features = X.shape[1]
1.10 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
1.10 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
1.10 logistic.py(343):                     dtype=X.dtype)
1.10 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
1.10 logistic.py(282):     n_classes = Y.shape[1]
1.10 logistic.py(283):     n_features = X.shape[1]
1.10 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
1.10 logistic.py(285):     w = w.reshape(n_classes, -1)
1.10 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
1.10 logistic.py(287):     if fit_intercept:
1.10 logistic.py(288):         intercept = w[:, -1]
1.10 logistic.py(289):         w = w[:, :-1]
1.10 logistic.py(292):     p = safe_sparse_dot(X, w.T)
1.10 logistic.py(293):     p += intercept
1.10 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
1.10 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
1.10 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
1.10 logistic.py(297):     p = np.exp(p, p)
1.10 logistic.py(298):     return loss, p, w
1.10 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
1.10 logistic.py(346):     diff = sample_weight * (p - Y)
1.10 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
1.10 logistic.py(348):     grad[:, :n_features] += alpha * w
1.10 logistic.py(349):     if fit_intercept:
1.10 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
1.10 logistic.py(351):     return loss, grad.ravel(), p
1.10 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
1.10 logistic.py(339):     n_classes = Y.shape[1]
1.10 logistic.py(340):     n_features = X.shape[1]
1.10 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
1.10 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
1.10 logistic.py(343):                     dtype=X.dtype)
1.10 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
1.10 logistic.py(282):     n_classes = Y.shape[1]
1.10 logistic.py(283):     n_features = X.shape[1]
1.10 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
1.10 logistic.py(285):     w = w.reshape(n_classes, -1)
1.10 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
1.10 logistic.py(287):     if fit_intercept:
1.10 logistic.py(288):         intercept = w[:, -1]
1.10 logistic.py(289):         w = w[:, :-1]
1.10 logistic.py(292):     p = safe_sparse_dot(X, w.T)
1.10 logistic.py(293):     p += intercept
1.10 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
1.10 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
1.10 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
1.10 logistic.py(297):     p = np.exp(p, p)
1.10 logistic.py(298):     return loss, p, w
1.10 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
1.10 logistic.py(346):     diff = sample_weight * (p - Y)
1.10 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
1.10 logistic.py(348):     grad[:, :n_features] += alpha * w
1.10 logistic.py(349):     if fit_intercept:
1.10 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
1.10 logistic.py(351):     return loss, grad.ravel(), p
1.10 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
1.10 logistic.py(339):     n_classes = Y.shape[1]
1.10 logistic.py(340):     n_features = X.shape[1]
1.10 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
1.10 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
1.10 logistic.py(343):                     dtype=X.dtype)
1.10 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
1.10 logistic.py(282):     n_classes = Y.shape[1]
1.10 logistic.py(283):     n_features = X.shape[1]
1.10 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
1.10 logistic.py(285):     w = w.reshape(n_classes, -1)
1.10 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
1.10 logistic.py(287):     if fit_intercept:
1.10 logistic.py(288):         intercept = w[:, -1]
1.10 logistic.py(289):         w = w[:, :-1]
1.10 logistic.py(292):     p = safe_sparse_dot(X, w.T)
1.10 logistic.py(293):     p += intercept
1.10 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
1.10 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
1.10 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
1.10 logistic.py(297):     p = np.exp(p, p)
1.10 logistic.py(298):     return loss, p, w
1.10 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
1.10 logistic.py(346):     diff = sample_weight * (p - Y)
1.10 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
1.10 logistic.py(348):     grad[:, :n_features] += alpha * w
1.10 logistic.py(349):     if fit_intercept:
1.10 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
1.10 logistic.py(351):     return loss, grad.ravel(), p
1.10 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
1.10 logistic.py(339):     n_classes = Y.shape[1]
1.10 logistic.py(340):     n_features = X.shape[1]
1.10 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
1.10 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
1.10 logistic.py(343):                     dtype=X.dtype)
1.10 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
1.10 logistic.py(282):     n_classes = Y.shape[1]
1.10 logistic.py(283):     n_features = X.shape[1]
1.10 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
1.10 logistic.py(285):     w = w.reshape(n_classes, -1)
1.10 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
1.10 logistic.py(287):     if fit_intercept:
1.10 logistic.py(288):         intercept = w[:, -1]
1.10 logistic.py(289):         w = w[:, :-1]
1.10 logistic.py(292):     p = safe_sparse_dot(X, w.T)
1.10 logistic.py(293):     p += intercept
1.10 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
1.10 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
1.10 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
1.10 logistic.py(297):     p = np.exp(p, p)
1.10 logistic.py(298):     return loss, p, w
1.10 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
1.10 logistic.py(346):     diff = sample_weight * (p - Y)
1.10 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
1.10 logistic.py(348):     grad[:, :n_features] += alpha * w
1.10 logistic.py(349):     if fit_intercept:
1.10 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
1.10 logistic.py(351):     return loss, grad.ravel(), p
1.10 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
1.10 logistic.py(339):     n_classes = Y.shape[1]
1.10 logistic.py(340):     n_features = X.shape[1]
1.10 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
1.10 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
1.10 logistic.py(343):                     dtype=X.dtype)
1.10 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
1.10 logistic.py(282):     n_classes = Y.shape[1]
1.10 logistic.py(283):     n_features = X.shape[1]
1.10 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
1.10 logistic.py(285):     w = w.reshape(n_classes, -1)
1.10 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
1.10 logistic.py(287):     if fit_intercept:
1.10 logistic.py(288):         intercept = w[:, -1]
1.10 logistic.py(289):         w = w[:, :-1]
1.10 logistic.py(292):     p = safe_sparse_dot(X, w.T)
1.10 logistic.py(293):     p += intercept
1.10 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
1.10 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
1.10 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
1.10 logistic.py(297):     p = np.exp(p, p)
1.10 logistic.py(298):     return loss, p, w
1.10 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
1.10 logistic.py(346):     diff = sample_weight * (p - Y)
1.10 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
1.10 logistic.py(348):     grad[:, :n_features] += alpha * w
1.10 logistic.py(349):     if fit_intercept:
1.10 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
1.10 logistic.py(351):     return loss, grad.ravel(), p
1.10 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
1.10 logistic.py(339):     n_classes = Y.shape[1]
1.10 logistic.py(340):     n_features = X.shape[1]
1.10 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
1.10 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
1.10 logistic.py(343):                     dtype=X.dtype)
1.10 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
1.10 logistic.py(282):     n_classes = Y.shape[1]
1.10 logistic.py(283):     n_features = X.shape[1]
1.10 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
1.10 logistic.py(285):     w = w.reshape(n_classes, -1)
1.10 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
1.10 logistic.py(287):     if fit_intercept:
1.10 logistic.py(288):         intercept = w[:, -1]
1.10 logistic.py(289):         w = w[:, :-1]
1.10 logistic.py(292):     p = safe_sparse_dot(X, w.T)
1.10 logistic.py(293):     p += intercept
1.10 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
1.10 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
1.10 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
1.10 logistic.py(297):     p = np.exp(p, p)
1.10 logistic.py(298):     return loss, p, w
1.10 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
1.10 logistic.py(346):     diff = sample_weight * (p - Y)
1.10 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
1.10 logistic.py(348):     grad[:, :n_features] += alpha * w
1.10 logistic.py(349):     if fit_intercept:
1.10 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
1.10 logistic.py(351):     return loss, grad.ravel(), p
1.10 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
1.10 logistic.py(339):     n_classes = Y.shape[1]
1.10 logistic.py(340):     n_features = X.shape[1]
1.10 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
1.10 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
1.10 logistic.py(343):                     dtype=X.dtype)
1.10 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
1.10 logistic.py(282):     n_classes = Y.shape[1]
1.10 logistic.py(283):     n_features = X.shape[1]
1.10 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
1.10 logistic.py(285):     w = w.reshape(n_classes, -1)
1.10 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
1.10 logistic.py(287):     if fit_intercept:
1.10 logistic.py(288):         intercept = w[:, -1]
1.10 logistic.py(289):         w = w[:, :-1]
1.10 logistic.py(292):     p = safe_sparse_dot(X, w.T)
1.10 logistic.py(293):     p += intercept
1.10 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
1.10 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
1.10 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
1.10 logistic.py(297):     p = np.exp(p, p)
1.10 logistic.py(298):     return loss, p, w
1.10 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
1.10 logistic.py(346):     diff = sample_weight * (p - Y)
1.10 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
1.10 logistic.py(348):     grad[:, :n_features] += alpha * w
1.10 logistic.py(349):     if fit_intercept:
1.10 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
1.10 logistic.py(351):     return loss, grad.ravel(), p
1.10 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
1.10 logistic.py(339):     n_classes = Y.shape[1]
1.10 logistic.py(340):     n_features = X.shape[1]
1.10 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
1.10 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
1.10 logistic.py(343):                     dtype=X.dtype)
1.10 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
1.10 logistic.py(282):     n_classes = Y.shape[1]
1.10 logistic.py(283):     n_features = X.shape[1]
1.10 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
1.10 logistic.py(285):     w = w.reshape(n_classes, -1)
1.10 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
1.10 logistic.py(287):     if fit_intercept:
1.10 logistic.py(288):         intercept = w[:, -1]
1.10 logistic.py(289):         w = w[:, :-1]
1.10 logistic.py(292):     p = safe_sparse_dot(X, w.T)
1.10 logistic.py(293):     p += intercept
1.10 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
1.10 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
1.10 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
1.10 logistic.py(297):     p = np.exp(p, p)
1.10 logistic.py(298):     return loss, p, w
1.10 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
1.10 logistic.py(346):     diff = sample_weight * (p - Y)
1.10 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
1.10 logistic.py(348):     grad[:, :n_features] += alpha * w
1.10 logistic.py(349):     if fit_intercept:
1.10 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
1.10 logistic.py(351):     return loss, grad.ravel(), p
1.10 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
1.10 logistic.py(339):     n_classes = Y.shape[1]
1.10 logistic.py(340):     n_features = X.shape[1]
1.10 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
1.10 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
1.10 logistic.py(343):                     dtype=X.dtype)
1.10 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
1.10 logistic.py(282):     n_classes = Y.shape[1]
1.10 logistic.py(283):     n_features = X.shape[1]
1.10 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
1.10 logistic.py(285):     w = w.reshape(n_classes, -1)
1.10 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
1.10 logistic.py(287):     if fit_intercept:
1.10 logistic.py(288):         intercept = w[:, -1]
1.10 logistic.py(289):         w = w[:, :-1]
1.10 logistic.py(292):     p = safe_sparse_dot(X, w.T)
1.10 logistic.py(293):     p += intercept
1.10 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
1.10 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
1.10 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
1.10 logistic.py(297):     p = np.exp(p, p)
1.10 logistic.py(298):     return loss, p, w
1.10 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
1.10 logistic.py(346):     diff = sample_weight * (p - Y)
1.10 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
1.10 logistic.py(348):     grad[:, :n_features] += alpha * w
1.10 logistic.py(349):     if fit_intercept:
1.10 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
1.10 logistic.py(351):     return loss, grad.ravel(), p
1.10 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
1.10 logistic.py(339):     n_classes = Y.shape[1]
1.10 logistic.py(340):     n_features = X.shape[1]
1.10 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
1.10 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
1.10 logistic.py(343):                     dtype=X.dtype)
1.10 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
1.10 logistic.py(282):     n_classes = Y.shape[1]
1.10 logistic.py(283):     n_features = X.shape[1]
1.10 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
1.10 logistic.py(285):     w = w.reshape(n_classes, -1)
1.10 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
1.10 logistic.py(287):     if fit_intercept:
1.10 logistic.py(288):         intercept = w[:, -1]
1.10 logistic.py(289):         w = w[:, :-1]
1.10 logistic.py(292):     p = safe_sparse_dot(X, w.T)
1.10 logistic.py(293):     p += intercept
1.10 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
1.10 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
1.10 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
1.10 logistic.py(297):     p = np.exp(p, p)
1.10 logistic.py(298):     return loss, p, w
1.10 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
1.10 logistic.py(346):     diff = sample_weight * (p - Y)
1.10 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
1.10 logistic.py(348):     grad[:, :n_features] += alpha * w
1.10 logistic.py(349):     if fit_intercept:
1.10 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
1.10 logistic.py(351):     return loss, grad.ravel(), p
1.10 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
1.10 logistic.py(339):     n_classes = Y.shape[1]
1.10 logistic.py(340):     n_features = X.shape[1]
1.10 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
1.10 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
1.10 logistic.py(343):                     dtype=X.dtype)
1.10 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
1.10 logistic.py(282):     n_classes = Y.shape[1]
1.10 logistic.py(283):     n_features = X.shape[1]
1.10 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
1.10 logistic.py(285):     w = w.reshape(n_classes, -1)
1.10 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
1.10 logistic.py(287):     if fit_intercept:
1.10 logistic.py(288):         intercept = w[:, -1]
1.10 logistic.py(289):         w = w[:, :-1]
1.10 logistic.py(292):     p = safe_sparse_dot(X, w.T)
1.10 logistic.py(293):     p += intercept
1.10 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
1.10 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
1.11 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
1.11 logistic.py(297):     p = np.exp(p, p)
1.11 logistic.py(298):     return loss, p, w
1.11 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
1.11 logistic.py(346):     diff = sample_weight * (p - Y)
1.11 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
1.11 logistic.py(348):     grad[:, :n_features] += alpha * w
1.11 logistic.py(349):     if fit_intercept:
1.11 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
1.11 logistic.py(351):     return loss, grad.ravel(), p
1.11 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
1.11 logistic.py(339):     n_classes = Y.shape[1]
1.11 logistic.py(340):     n_features = X.shape[1]
1.11 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
1.11 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
1.11 logistic.py(343):                     dtype=X.dtype)
1.11 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
1.11 logistic.py(282):     n_classes = Y.shape[1]
1.11 logistic.py(283):     n_features = X.shape[1]
1.11 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
1.11 logistic.py(285):     w = w.reshape(n_classes, -1)
1.11 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
1.11 logistic.py(287):     if fit_intercept:
1.11 logistic.py(288):         intercept = w[:, -1]
1.11 logistic.py(289):         w = w[:, :-1]
1.11 logistic.py(292):     p = safe_sparse_dot(X, w.T)
1.11 logistic.py(293):     p += intercept
1.11 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
1.11 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
1.11 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
1.11 logistic.py(297):     p = np.exp(p, p)
1.11 logistic.py(298):     return loss, p, w
1.11 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
1.11 logistic.py(346):     diff = sample_weight * (p - Y)
1.11 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
1.11 logistic.py(348):     grad[:, :n_features] += alpha * w
1.11 logistic.py(349):     if fit_intercept:
1.11 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
1.11 logistic.py(351):     return loss, grad.ravel(), p
1.11 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
1.11 logistic.py(339):     n_classes = Y.shape[1]
1.11 logistic.py(340):     n_features = X.shape[1]
1.11 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
1.11 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
1.11 logistic.py(343):                     dtype=X.dtype)
1.11 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
1.11 logistic.py(282):     n_classes = Y.shape[1]
1.11 logistic.py(283):     n_features = X.shape[1]
1.11 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
1.11 logistic.py(285):     w = w.reshape(n_classes, -1)
1.11 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
1.11 logistic.py(287):     if fit_intercept:
1.11 logistic.py(288):         intercept = w[:, -1]
1.11 logistic.py(289):         w = w[:, :-1]
1.11 logistic.py(292):     p = safe_sparse_dot(X, w.T)
1.11 logistic.py(293):     p += intercept
1.11 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
1.11 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
1.11 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
1.11 logistic.py(297):     p = np.exp(p, p)
1.11 logistic.py(298):     return loss, p, w
1.11 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
1.11 logistic.py(346):     diff = sample_weight * (p - Y)
1.11 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
1.11 logistic.py(348):     grad[:, :n_features] += alpha * w
1.11 logistic.py(349):     if fit_intercept:
1.11 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
1.11 logistic.py(351):     return loss, grad.ravel(), p
1.11 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
1.11 logistic.py(339):     n_classes = Y.shape[1]
1.11 logistic.py(340):     n_features = X.shape[1]
1.11 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
1.11 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
1.11 logistic.py(343):                     dtype=X.dtype)
1.11 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
1.11 logistic.py(282):     n_classes = Y.shape[1]
1.11 logistic.py(283):     n_features = X.shape[1]
1.11 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
1.11 logistic.py(285):     w = w.reshape(n_classes, -1)
1.11 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
1.11 logistic.py(287):     if fit_intercept:
1.11 logistic.py(288):         intercept = w[:, -1]
1.11 logistic.py(289):         w = w[:, :-1]
1.11 logistic.py(292):     p = safe_sparse_dot(X, w.T)
1.11 logistic.py(293):     p += intercept
1.11 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
1.11 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
1.11 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
1.11 logistic.py(297):     p = np.exp(p, p)
1.11 logistic.py(298):     return loss, p, w
1.11 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
1.11 logistic.py(346):     diff = sample_weight * (p - Y)
1.11 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
1.11 logistic.py(348):     grad[:, :n_features] += alpha * w
1.11 logistic.py(349):     if fit_intercept:
1.11 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
1.11 logistic.py(351):     return loss, grad.ravel(), p
1.11 logistic.py(718):             if info["warnflag"] == 1:
1.11 logistic.py(723):             n_iter_i = min(info['nit'], max_iter)
1.11 logistic.py(760):         if multi_class == 'multinomial':
1.11 logistic.py(761):             multi_w0 = np.reshape(w0, (classes.size, -1))
1.11 logistic.py(762):             if classes.size == 2:
1.11 logistic.py(764):             coefs.append(multi_w0)
1.11 logistic.py(768):         n_iter[i] = n_iter_i
1.11 logistic.py(712):     for i, C in enumerate(Cs):
1.11 logistic.py(713):         if solver == 'lbfgs':
1.11 logistic.py(714):             w0, loss, info = optimize.fmin_l_bfgs_b(
1.11 logistic.py(715):                 func, w0, fprime=None,
1.11 logistic.py(716):                 args=(X, target, 1. / C, sample_weight),
1.11 logistic.py(717):                 iprint=(verbose > 0) - 1, pgtol=tol, maxiter=max_iter)
1.11 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
1.11 logistic.py(339):     n_classes = Y.shape[1]
1.11 logistic.py(340):     n_features = X.shape[1]
1.11 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
1.11 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
1.11 logistic.py(343):                     dtype=X.dtype)
1.11 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
1.11 logistic.py(282):     n_classes = Y.shape[1]
1.11 logistic.py(283):     n_features = X.shape[1]
1.11 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
1.11 logistic.py(285):     w = w.reshape(n_classes, -1)
1.11 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
1.11 logistic.py(287):     if fit_intercept:
1.11 logistic.py(288):         intercept = w[:, -1]
1.11 logistic.py(289):         w = w[:, :-1]
1.11 logistic.py(292):     p = safe_sparse_dot(X, w.T)
1.11 logistic.py(293):     p += intercept
1.11 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
1.11 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
1.11 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
1.11 logistic.py(297):     p = np.exp(p, p)
1.11 logistic.py(298):     return loss, p, w
1.11 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
1.11 logistic.py(346):     diff = sample_weight * (p - Y)
1.11 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
1.11 logistic.py(348):     grad[:, :n_features] += alpha * w
1.11 logistic.py(349):     if fit_intercept:
1.11 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
1.11 logistic.py(351):     return loss, grad.ravel(), p
1.11 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
1.11 logistic.py(339):     n_classes = Y.shape[1]
1.11 logistic.py(340):     n_features = X.shape[1]
1.11 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
1.11 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
1.11 logistic.py(343):                     dtype=X.dtype)
1.11 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
1.11 logistic.py(282):     n_classes = Y.shape[1]
1.11 logistic.py(283):     n_features = X.shape[1]
1.11 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
1.11 logistic.py(285):     w = w.reshape(n_classes, -1)
1.11 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
1.11 logistic.py(287):     if fit_intercept:
1.11 logistic.py(288):         intercept = w[:, -1]
1.11 logistic.py(289):         w = w[:, :-1]
1.11 logistic.py(292):     p = safe_sparse_dot(X, w.T)
1.11 logistic.py(293):     p += intercept
1.11 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
1.11 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
1.11 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
1.11 logistic.py(297):     p = np.exp(p, p)
1.11 logistic.py(298):     return loss, p, w
1.11 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
1.11 logistic.py(346):     diff = sample_weight * (p - Y)
1.11 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
1.11 logistic.py(348):     grad[:, :n_features] += alpha * w
1.11 logistic.py(349):     if fit_intercept:
1.11 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
1.11 logistic.py(351):     return loss, grad.ravel(), p
1.11 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
1.11 logistic.py(339):     n_classes = Y.shape[1]
1.11 logistic.py(340):     n_features = X.shape[1]
1.11 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
1.11 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
1.11 logistic.py(343):                     dtype=X.dtype)
1.11 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
1.11 logistic.py(282):     n_classes = Y.shape[1]
1.11 logistic.py(283):     n_features = X.shape[1]
1.11 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
1.11 logistic.py(285):     w = w.reshape(n_classes, -1)
1.11 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
1.11 logistic.py(287):     if fit_intercept:
1.11 logistic.py(288):         intercept = w[:, -1]
1.11 logistic.py(289):         w = w[:, :-1]
1.11 logistic.py(292):     p = safe_sparse_dot(X, w.T)
1.11 logistic.py(293):     p += intercept
1.11 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
1.11 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
1.11 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
1.11 logistic.py(297):     p = np.exp(p, p)
1.11 logistic.py(298):     return loss, p, w
1.11 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
1.11 logistic.py(346):     diff = sample_weight * (p - Y)
1.11 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
1.11 logistic.py(348):     grad[:, :n_features] += alpha * w
1.11 logistic.py(349):     if fit_intercept:
1.11 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
1.11 logistic.py(351):     return loss, grad.ravel(), p
1.11 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
1.11 logistic.py(339):     n_classes = Y.shape[1]
1.11 logistic.py(340):     n_features = X.shape[1]
1.11 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
1.11 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
1.11 logistic.py(343):                     dtype=X.dtype)
1.11 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
1.11 logistic.py(282):     n_classes = Y.shape[1]
1.11 logistic.py(283):     n_features = X.shape[1]
1.11 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
1.11 logistic.py(285):     w = w.reshape(n_classes, -1)
1.11 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
1.11 logistic.py(287):     if fit_intercept:
1.11 logistic.py(288):         intercept = w[:, -1]
1.11 logistic.py(289):         w = w[:, :-1]
1.11 logistic.py(292):     p = safe_sparse_dot(X, w.T)
1.11 logistic.py(293):     p += intercept
1.11 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
1.11 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
1.11 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
1.11 logistic.py(297):     p = np.exp(p, p)
1.11 logistic.py(298):     return loss, p, w
1.11 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
1.11 logistic.py(346):     diff = sample_weight * (p - Y)
1.11 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
1.11 logistic.py(348):     grad[:, :n_features] += alpha * w
1.11 logistic.py(349):     if fit_intercept:
1.11 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
1.11 logistic.py(351):     return loss, grad.ravel(), p
1.11 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
1.11 logistic.py(339):     n_classes = Y.shape[1]
1.11 logistic.py(340):     n_features = X.shape[1]
1.11 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
1.11 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
1.11 logistic.py(343):                     dtype=X.dtype)
1.11 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
1.11 logistic.py(282):     n_classes = Y.shape[1]
1.11 logistic.py(283):     n_features = X.shape[1]
1.11 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
1.11 logistic.py(285):     w = w.reshape(n_classes, -1)
1.11 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
1.11 logistic.py(287):     if fit_intercept:
1.11 logistic.py(288):         intercept = w[:, -1]
1.11 logistic.py(289):         w = w[:, :-1]
1.11 logistic.py(292):     p = safe_sparse_dot(X, w.T)
1.11 logistic.py(293):     p += intercept
1.11 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
1.11 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
1.11 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
1.11 logistic.py(297):     p = np.exp(p, p)
1.11 logistic.py(298):     return loss, p, w
1.11 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
1.11 logistic.py(346):     diff = sample_weight * (p - Y)
1.11 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
1.11 logistic.py(348):     grad[:, :n_features] += alpha * w
1.11 logistic.py(349):     if fit_intercept:
1.11 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
1.11 logistic.py(351):     return loss, grad.ravel(), p
1.11 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
1.11 logistic.py(339):     n_classes = Y.shape[1]
1.11 logistic.py(340):     n_features = X.shape[1]
1.11 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
1.11 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
1.11 logistic.py(343):                     dtype=X.dtype)
1.11 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
1.11 logistic.py(282):     n_classes = Y.shape[1]
1.11 logistic.py(283):     n_features = X.shape[1]
1.11 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
1.11 logistic.py(285):     w = w.reshape(n_classes, -1)
1.11 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
1.11 logistic.py(287):     if fit_intercept:
1.11 logistic.py(288):         intercept = w[:, -1]
1.11 logistic.py(289):         w = w[:, :-1]
1.11 logistic.py(292):     p = safe_sparse_dot(X, w.T)
1.11 logistic.py(293):     p += intercept
1.11 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
1.11 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
1.11 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
1.11 logistic.py(297):     p = np.exp(p, p)
1.11 logistic.py(298):     return loss, p, w
1.11 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
1.11 logistic.py(346):     diff = sample_weight * (p - Y)
1.11 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
1.11 logistic.py(348):     grad[:, :n_features] += alpha * w
1.11 logistic.py(349):     if fit_intercept:
1.11 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
1.11 logistic.py(351):     return loss, grad.ravel(), p
1.11 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
1.11 logistic.py(339):     n_classes = Y.shape[1]
1.11 logistic.py(340):     n_features = X.shape[1]
1.11 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
1.11 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
1.11 logistic.py(343):                     dtype=X.dtype)
1.11 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
1.11 logistic.py(282):     n_classes = Y.shape[1]
1.11 logistic.py(283):     n_features = X.shape[1]
1.11 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
1.11 logistic.py(285):     w = w.reshape(n_classes, -1)
1.11 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
1.11 logistic.py(287):     if fit_intercept:
1.11 logistic.py(288):         intercept = w[:, -1]
1.11 logistic.py(289):         w = w[:, :-1]
1.11 logistic.py(292):     p = safe_sparse_dot(X, w.T)
1.11 logistic.py(293):     p += intercept
1.11 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
1.11 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
1.11 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
1.11 logistic.py(297):     p = np.exp(p, p)
1.11 logistic.py(298):     return loss, p, w
1.11 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
1.11 logistic.py(346):     diff = sample_weight * (p - Y)
1.11 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
1.11 logistic.py(348):     grad[:, :n_features] += alpha * w
1.11 logistic.py(349):     if fit_intercept:
1.11 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
1.11 logistic.py(351):     return loss, grad.ravel(), p
1.11 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
1.11 logistic.py(339):     n_classes = Y.shape[1]
1.11 logistic.py(340):     n_features = X.shape[1]
1.11 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
1.11 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
1.11 logistic.py(343):                     dtype=X.dtype)
1.11 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
1.11 logistic.py(282):     n_classes = Y.shape[1]
1.11 logistic.py(283):     n_features = X.shape[1]
1.11 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
1.11 logistic.py(285):     w = w.reshape(n_classes, -1)
1.11 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
1.11 logistic.py(287):     if fit_intercept:
1.11 logistic.py(288):         intercept = w[:, -1]
1.11 logistic.py(289):         w = w[:, :-1]
1.11 logistic.py(292):     p = safe_sparse_dot(X, w.T)
1.11 logistic.py(293):     p += intercept
1.11 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
1.11 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
1.11 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
1.11 logistic.py(297):     p = np.exp(p, p)
1.11 logistic.py(298):     return loss, p, w
1.11 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
1.11 logistic.py(346):     diff = sample_weight * (p - Y)
1.11 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
1.11 logistic.py(348):     grad[:, :n_features] += alpha * w
1.11 logistic.py(349):     if fit_intercept:
1.11 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
1.11 logistic.py(351):     return loss, grad.ravel(), p
1.11 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
1.11 logistic.py(339):     n_classes = Y.shape[1]
1.11 logistic.py(340):     n_features = X.shape[1]
1.11 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
1.11 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
1.11 logistic.py(343):                     dtype=X.dtype)
1.11 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
1.11 logistic.py(282):     n_classes = Y.shape[1]
1.11 logistic.py(283):     n_features = X.shape[1]
1.11 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
1.11 logistic.py(285):     w = w.reshape(n_classes, -1)
1.11 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
1.11 logistic.py(287):     if fit_intercept:
1.11 logistic.py(288):         intercept = w[:, -1]
1.11 logistic.py(289):         w = w[:, :-1]
1.11 logistic.py(292):     p = safe_sparse_dot(X, w.T)
1.11 logistic.py(293):     p += intercept
1.11 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
1.11 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
1.11 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
1.11 logistic.py(297):     p = np.exp(p, p)
1.11 logistic.py(298):     return loss, p, w
1.11 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
1.11 logistic.py(346):     diff = sample_weight * (p - Y)
1.11 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
1.11 logistic.py(348):     grad[:, :n_features] += alpha * w
1.11 logistic.py(349):     if fit_intercept:
1.11 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
1.11 logistic.py(351):     return loss, grad.ravel(), p
1.11 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
1.11 logistic.py(339):     n_classes = Y.shape[1]
1.11 logistic.py(340):     n_features = X.shape[1]
1.11 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
1.11 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
1.11 logistic.py(343):                     dtype=X.dtype)
1.11 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
1.11 logistic.py(282):     n_classes = Y.shape[1]
1.11 logistic.py(283):     n_features = X.shape[1]
1.11 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
1.11 logistic.py(285):     w = w.reshape(n_classes, -1)
1.11 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
1.11 logistic.py(287):     if fit_intercept:
1.11 logistic.py(288):         intercept = w[:, -1]
1.11 logistic.py(289):         w = w[:, :-1]
1.11 logistic.py(292):     p = safe_sparse_dot(X, w.T)
1.11 logistic.py(293):     p += intercept
1.11 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
1.11 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
1.11 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
1.11 logistic.py(297):     p = np.exp(p, p)
1.11 logistic.py(298):     return loss, p, w
1.11 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
1.11 logistic.py(346):     diff = sample_weight * (p - Y)
1.11 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
1.11 logistic.py(348):     grad[:, :n_features] += alpha * w
1.11 logistic.py(349):     if fit_intercept:
1.11 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
1.11 logistic.py(351):     return loss, grad.ravel(), p
1.11 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
1.11 logistic.py(339):     n_classes = Y.shape[1]
1.11 logistic.py(340):     n_features = X.shape[1]
1.11 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
1.11 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
1.11 logistic.py(343):                     dtype=X.dtype)
1.11 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
1.11 logistic.py(282):     n_classes = Y.shape[1]
1.11 logistic.py(283):     n_features = X.shape[1]
1.11 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
1.11 logistic.py(285):     w = w.reshape(n_classes, -1)
1.12 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
1.12 logistic.py(287):     if fit_intercept:
1.12 logistic.py(288):         intercept = w[:, -1]
1.12 logistic.py(289):         w = w[:, :-1]
1.12 logistic.py(292):     p = safe_sparse_dot(X, w.T)
1.12 logistic.py(293):     p += intercept
1.12 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
1.12 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
1.12 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
1.12 logistic.py(297):     p = np.exp(p, p)
1.12 logistic.py(298):     return loss, p, w
1.12 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
1.12 logistic.py(346):     diff = sample_weight * (p - Y)
1.12 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
1.12 logistic.py(348):     grad[:, :n_features] += alpha * w
1.12 logistic.py(349):     if fit_intercept:
1.12 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
1.12 logistic.py(351):     return loss, grad.ravel(), p
1.12 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
1.12 logistic.py(339):     n_classes = Y.shape[1]
1.12 logistic.py(340):     n_features = X.shape[1]
1.12 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
1.12 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
1.12 logistic.py(343):                     dtype=X.dtype)
1.12 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
1.12 logistic.py(282):     n_classes = Y.shape[1]
1.12 logistic.py(283):     n_features = X.shape[1]
1.12 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
1.12 logistic.py(285):     w = w.reshape(n_classes, -1)
1.12 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
1.12 logistic.py(287):     if fit_intercept:
1.12 logistic.py(288):         intercept = w[:, -1]
1.12 logistic.py(289):         w = w[:, :-1]
1.12 logistic.py(292):     p = safe_sparse_dot(X, w.T)
1.12 logistic.py(293):     p += intercept
1.12 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
1.12 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
1.12 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
1.12 logistic.py(297):     p = np.exp(p, p)
1.12 logistic.py(298):     return loss, p, w
1.12 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
1.12 logistic.py(346):     diff = sample_weight * (p - Y)
1.12 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
1.12 logistic.py(348):     grad[:, :n_features] += alpha * w
1.12 logistic.py(349):     if fit_intercept:
1.12 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
1.12 logistic.py(351):     return loss, grad.ravel(), p
1.12 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
1.12 logistic.py(339):     n_classes = Y.shape[1]
1.12 logistic.py(340):     n_features = X.shape[1]
1.12 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
1.12 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
1.12 logistic.py(343):                     dtype=X.dtype)
1.12 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
1.12 logistic.py(282):     n_classes = Y.shape[1]
1.12 logistic.py(283):     n_features = X.shape[1]
1.12 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
1.12 logistic.py(285):     w = w.reshape(n_classes, -1)
1.12 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
1.12 logistic.py(287):     if fit_intercept:
1.12 logistic.py(288):         intercept = w[:, -1]
1.12 logistic.py(289):         w = w[:, :-1]
1.12 logistic.py(292):     p = safe_sparse_dot(X, w.T)
1.12 logistic.py(293):     p += intercept
1.12 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
1.12 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
1.12 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
1.12 logistic.py(297):     p = np.exp(p, p)
1.12 logistic.py(298):     return loss, p, w
1.12 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
1.12 logistic.py(346):     diff = sample_weight * (p - Y)
1.12 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
1.12 logistic.py(348):     grad[:, :n_features] += alpha * w
1.12 logistic.py(349):     if fit_intercept:
1.12 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
1.12 logistic.py(351):     return loss, grad.ravel(), p
1.12 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
1.12 logistic.py(339):     n_classes = Y.shape[1]
1.12 logistic.py(340):     n_features = X.shape[1]
1.12 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
1.12 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
1.12 logistic.py(343):                     dtype=X.dtype)
1.12 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
1.12 logistic.py(282):     n_classes = Y.shape[1]
1.12 logistic.py(283):     n_features = X.shape[1]
1.12 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
1.12 logistic.py(285):     w = w.reshape(n_classes, -1)
1.12 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
1.12 logistic.py(287):     if fit_intercept:
1.12 logistic.py(288):         intercept = w[:, -1]
1.12 logistic.py(289):         w = w[:, :-1]
1.12 logistic.py(292):     p = safe_sparse_dot(X, w.T)
1.12 logistic.py(293):     p += intercept
1.12 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
1.12 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
1.12 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
1.12 logistic.py(297):     p = np.exp(p, p)
1.12 logistic.py(298):     return loss, p, w
1.12 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
1.12 logistic.py(346):     diff = sample_weight * (p - Y)
1.12 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
1.12 logistic.py(348):     grad[:, :n_features] += alpha * w
1.12 logistic.py(349):     if fit_intercept:
1.12 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
1.12 logistic.py(351):     return loss, grad.ravel(), p
1.12 logistic.py(718):             if info["warnflag"] == 1:
1.12 logistic.py(723):             n_iter_i = min(info['nit'], max_iter)
1.12 logistic.py(760):         if multi_class == 'multinomial':
1.12 logistic.py(761):             multi_w0 = np.reshape(w0, (classes.size, -1))
1.12 logistic.py(762):             if classes.size == 2:
1.12 logistic.py(764):             coefs.append(multi_w0)
1.12 logistic.py(768):         n_iter[i] = n_iter_i
1.12 logistic.py(712):     for i, C in enumerate(Cs):
1.12 logistic.py(713):         if solver == 'lbfgs':
1.12 logistic.py(714):             w0, loss, info = optimize.fmin_l_bfgs_b(
1.12 logistic.py(715):                 func, w0, fprime=None,
1.12 logistic.py(716):                 args=(X, target, 1. / C, sample_weight),
1.12 logistic.py(717):                 iprint=(verbose > 0) - 1, pgtol=tol, maxiter=max_iter)
1.12 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
1.12 logistic.py(339):     n_classes = Y.shape[1]
1.12 logistic.py(340):     n_features = X.shape[1]
1.12 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
1.12 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
1.12 logistic.py(343):                     dtype=X.dtype)
1.12 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
1.12 logistic.py(282):     n_classes = Y.shape[1]
1.12 logistic.py(283):     n_features = X.shape[1]
1.12 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
1.12 logistic.py(285):     w = w.reshape(n_classes, -1)
1.12 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
1.12 logistic.py(287):     if fit_intercept:
1.12 logistic.py(288):         intercept = w[:, -1]
1.12 logistic.py(289):         w = w[:, :-1]
1.12 logistic.py(292):     p = safe_sparse_dot(X, w.T)
1.12 logistic.py(293):     p += intercept
1.12 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
1.12 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
1.12 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
1.12 logistic.py(297):     p = np.exp(p, p)
1.12 logistic.py(298):     return loss, p, w
1.12 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
1.12 logistic.py(346):     diff = sample_weight * (p - Y)
1.12 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
1.12 logistic.py(348):     grad[:, :n_features] += alpha * w
1.12 logistic.py(349):     if fit_intercept:
1.12 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
1.12 logistic.py(351):     return loss, grad.ravel(), p
1.12 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
1.12 logistic.py(339):     n_classes = Y.shape[1]
1.12 logistic.py(340):     n_features = X.shape[1]
1.12 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
1.12 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
1.12 logistic.py(343):                     dtype=X.dtype)
1.12 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
1.12 logistic.py(282):     n_classes = Y.shape[1]
1.12 logistic.py(283):     n_features = X.shape[1]
1.12 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
1.12 logistic.py(285):     w = w.reshape(n_classes, -1)
1.12 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
1.12 logistic.py(287):     if fit_intercept:
1.12 logistic.py(288):         intercept = w[:, -1]
1.12 logistic.py(289):         w = w[:, :-1]
1.12 logistic.py(292):     p = safe_sparse_dot(X, w.T)
1.12 logistic.py(293):     p += intercept
1.12 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
1.12 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
1.12 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
1.12 logistic.py(297):     p = np.exp(p, p)
1.12 logistic.py(298):     return loss, p, w
1.12 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
1.12 logistic.py(346):     diff = sample_weight * (p - Y)
1.12 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
1.12 logistic.py(348):     grad[:, :n_features] += alpha * w
1.12 logistic.py(349):     if fit_intercept:
1.12 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
1.12 logistic.py(351):     return loss, grad.ravel(), p
1.12 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
1.12 logistic.py(339):     n_classes = Y.shape[1]
1.12 logistic.py(340):     n_features = X.shape[1]
1.12 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
1.12 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
1.12 logistic.py(343):                     dtype=X.dtype)
1.12 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
1.12 logistic.py(282):     n_classes = Y.shape[1]
1.12 logistic.py(283):     n_features = X.shape[1]
1.12 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
1.12 logistic.py(285):     w = w.reshape(n_classes, -1)
1.12 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
1.12 logistic.py(287):     if fit_intercept:
1.12 logistic.py(288):         intercept = w[:, -1]
1.12 logistic.py(289):         w = w[:, :-1]
1.12 logistic.py(292):     p = safe_sparse_dot(X, w.T)
1.12 logistic.py(293):     p += intercept
1.12 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
1.12 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
1.12 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
1.12 logistic.py(297):     p = np.exp(p, p)
1.12 logistic.py(298):     return loss, p, w
1.12 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
1.12 logistic.py(346):     diff = sample_weight * (p - Y)
1.12 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
1.12 logistic.py(348):     grad[:, :n_features] += alpha * w
1.12 logistic.py(349):     if fit_intercept:
1.12 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
1.12 logistic.py(351):     return loss, grad.ravel(), p
1.12 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
1.12 logistic.py(339):     n_classes = Y.shape[1]
1.12 logistic.py(340):     n_features = X.shape[1]
1.12 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
1.12 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
1.12 logistic.py(343):                     dtype=X.dtype)
1.12 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
1.12 logistic.py(282):     n_classes = Y.shape[1]
1.12 logistic.py(283):     n_features = X.shape[1]
1.12 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
1.12 logistic.py(285):     w = w.reshape(n_classes, -1)
1.12 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
1.12 logistic.py(287):     if fit_intercept:
1.12 logistic.py(288):         intercept = w[:, -1]
1.12 logistic.py(289):         w = w[:, :-1]
1.12 logistic.py(292):     p = safe_sparse_dot(X, w.T)
1.12 logistic.py(293):     p += intercept
1.12 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
1.12 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
1.12 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
1.12 logistic.py(297):     p = np.exp(p, p)
1.12 logistic.py(298):     return loss, p, w
1.12 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
1.12 logistic.py(346):     diff = sample_weight * (p - Y)
1.12 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
1.12 logistic.py(348):     grad[:, :n_features] += alpha * w
1.12 logistic.py(349):     if fit_intercept:
1.12 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
1.12 logistic.py(351):     return loss, grad.ravel(), p
1.12 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
1.12 logistic.py(339):     n_classes = Y.shape[1]
1.12 logistic.py(340):     n_features = X.shape[1]
1.12 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
1.12 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
1.12 logistic.py(343):                     dtype=X.dtype)
1.12 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
1.12 logistic.py(282):     n_classes = Y.shape[1]
1.12 logistic.py(283):     n_features = X.shape[1]
1.12 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
1.12 logistic.py(285):     w = w.reshape(n_classes, -1)
1.12 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
1.12 logistic.py(287):     if fit_intercept:
1.12 logistic.py(288):         intercept = w[:, -1]
1.12 logistic.py(289):         w = w[:, :-1]
1.12 logistic.py(292):     p = safe_sparse_dot(X, w.T)
1.12 logistic.py(293):     p += intercept
1.12 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
1.12 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
1.12 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
1.12 logistic.py(297):     p = np.exp(p, p)
1.12 logistic.py(298):     return loss, p, w
1.12 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
1.12 logistic.py(346):     diff = sample_weight * (p - Y)
1.12 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
1.12 logistic.py(348):     grad[:, :n_features] += alpha * w
1.12 logistic.py(349):     if fit_intercept:
1.12 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
1.12 logistic.py(351):     return loss, grad.ravel(), p
1.12 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
1.12 logistic.py(339):     n_classes = Y.shape[1]
1.12 logistic.py(340):     n_features = X.shape[1]
1.12 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
1.12 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
1.12 logistic.py(343):                     dtype=X.dtype)
1.12 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
1.12 logistic.py(282):     n_classes = Y.shape[1]
1.12 logistic.py(283):     n_features = X.shape[1]
1.12 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
1.12 logistic.py(285):     w = w.reshape(n_classes, -1)
1.12 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
1.12 logistic.py(287):     if fit_intercept:
1.12 logistic.py(288):         intercept = w[:, -1]
1.12 logistic.py(289):         w = w[:, :-1]
1.12 logistic.py(292):     p = safe_sparse_dot(X, w.T)
1.12 logistic.py(293):     p += intercept
1.12 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
1.12 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
1.12 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
1.12 logistic.py(297):     p = np.exp(p, p)
1.12 logistic.py(298):     return loss, p, w
1.12 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
1.12 logistic.py(346):     diff = sample_weight * (p - Y)
1.12 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
1.12 logistic.py(348):     grad[:, :n_features] += alpha * w
1.12 logistic.py(349):     if fit_intercept:
1.12 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
1.12 logistic.py(351):     return loss, grad.ravel(), p
1.12 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
1.12 logistic.py(339):     n_classes = Y.shape[1]
1.12 logistic.py(340):     n_features = X.shape[1]
1.12 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
1.12 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
1.12 logistic.py(343):                     dtype=X.dtype)
1.12 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
1.12 logistic.py(282):     n_classes = Y.shape[1]
1.12 logistic.py(283):     n_features = X.shape[1]
1.12 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
1.12 logistic.py(285):     w = w.reshape(n_classes, -1)
1.12 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
1.12 logistic.py(287):     if fit_intercept:
1.12 logistic.py(288):         intercept = w[:, -1]
1.12 logistic.py(289):         w = w[:, :-1]
1.12 logistic.py(292):     p = safe_sparse_dot(X, w.T)
1.12 logistic.py(293):     p += intercept
1.12 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
1.12 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
1.12 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
1.12 logistic.py(297):     p = np.exp(p, p)
1.12 logistic.py(298):     return loss, p, w
1.12 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
1.12 logistic.py(346):     diff = sample_weight * (p - Y)
1.12 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
1.12 logistic.py(348):     grad[:, :n_features] += alpha * w
1.12 logistic.py(349):     if fit_intercept:
1.12 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
1.12 logistic.py(351):     return loss, grad.ravel(), p
1.12 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
1.12 logistic.py(339):     n_classes = Y.shape[1]
1.12 logistic.py(340):     n_features = X.shape[1]
1.12 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
1.12 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
1.12 logistic.py(343):                     dtype=X.dtype)
1.12 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
1.12 logistic.py(282):     n_classes = Y.shape[1]
1.12 logistic.py(283):     n_features = X.shape[1]
1.12 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
1.12 logistic.py(285):     w = w.reshape(n_classes, -1)
1.12 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
1.12 logistic.py(287):     if fit_intercept:
1.12 logistic.py(288):         intercept = w[:, -1]
1.12 logistic.py(289):         w = w[:, :-1]
1.12 logistic.py(292):     p = safe_sparse_dot(X, w.T)
1.12 logistic.py(293):     p += intercept
1.12 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
1.12 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
1.12 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
1.12 logistic.py(297):     p = np.exp(p, p)
1.12 logistic.py(298):     return loss, p, w
1.12 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
1.12 logistic.py(346):     diff = sample_weight * (p - Y)
1.12 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
1.12 logistic.py(348):     grad[:, :n_features] += alpha * w
1.12 logistic.py(349):     if fit_intercept:
1.12 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
1.12 logistic.py(351):     return loss, grad.ravel(), p
1.12 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
1.12 logistic.py(339):     n_classes = Y.shape[1]
1.12 logistic.py(340):     n_features = X.shape[1]
1.12 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
1.12 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
1.12 logistic.py(343):                     dtype=X.dtype)
1.12 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
1.12 logistic.py(282):     n_classes = Y.shape[1]
1.12 logistic.py(283):     n_features = X.shape[1]
1.12 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
1.12 logistic.py(285):     w = w.reshape(n_classes, -1)
1.12 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
1.12 logistic.py(287):     if fit_intercept:
1.12 logistic.py(288):         intercept = w[:, -1]
1.12 logistic.py(289):         w = w[:, :-1]
1.12 logistic.py(292):     p = safe_sparse_dot(X, w.T)
1.12 logistic.py(293):     p += intercept
1.12 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
1.12 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
1.12 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
1.12 logistic.py(297):     p = np.exp(p, p)
1.12 logistic.py(298):     return loss, p, w
1.12 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
1.12 logistic.py(346):     diff = sample_weight * (p - Y)
1.12 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
1.12 logistic.py(348):     grad[:, :n_features] += alpha * w
1.12 logistic.py(349):     if fit_intercept:
1.12 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
1.12 logistic.py(351):     return loss, grad.ravel(), p
1.12 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
1.12 logistic.py(339):     n_classes = Y.shape[1]
1.12 logistic.py(340):     n_features = X.shape[1]
1.12 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
1.12 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
1.12 logistic.py(343):                     dtype=X.dtype)
1.13 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
1.13 logistic.py(282):     n_classes = Y.shape[1]
1.13 logistic.py(283):     n_features = X.shape[1]
1.13 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
1.13 logistic.py(285):     w = w.reshape(n_classes, -1)
1.13 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
1.13 logistic.py(287):     if fit_intercept:
1.13 logistic.py(288):         intercept = w[:, -1]
1.13 logistic.py(289):         w = w[:, :-1]
1.13 logistic.py(292):     p = safe_sparse_dot(X, w.T)
1.13 logistic.py(293):     p += intercept
1.13 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
1.13 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
1.13 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
1.13 logistic.py(297):     p = np.exp(p, p)
1.13 logistic.py(298):     return loss, p, w
1.13 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
1.13 logistic.py(346):     diff = sample_weight * (p - Y)
1.13 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
1.13 logistic.py(348):     grad[:, :n_features] += alpha * w
1.13 logistic.py(349):     if fit_intercept:
1.13 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
1.13 logistic.py(351):     return loss, grad.ravel(), p
1.13 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
1.13 logistic.py(339):     n_classes = Y.shape[1]
1.13 logistic.py(340):     n_features = X.shape[1]
1.13 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
1.13 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
1.13 logistic.py(343):                     dtype=X.dtype)
1.13 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
1.13 logistic.py(282):     n_classes = Y.shape[1]
1.13 logistic.py(283):     n_features = X.shape[1]
1.13 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
1.13 logistic.py(285):     w = w.reshape(n_classes, -1)
1.13 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
1.13 logistic.py(287):     if fit_intercept:
1.13 logistic.py(288):         intercept = w[:, -1]
1.13 logistic.py(289):         w = w[:, :-1]
1.13 logistic.py(292):     p = safe_sparse_dot(X, w.T)
1.13 logistic.py(293):     p += intercept
1.13 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
1.13 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
1.13 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
1.13 logistic.py(297):     p = np.exp(p, p)
1.13 logistic.py(298):     return loss, p, w
1.13 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
1.13 logistic.py(346):     diff = sample_weight * (p - Y)
1.13 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
1.13 logistic.py(348):     grad[:, :n_features] += alpha * w
1.13 logistic.py(349):     if fit_intercept:
1.13 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
1.13 logistic.py(351):     return loss, grad.ravel(), p
1.13 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
1.13 logistic.py(339):     n_classes = Y.shape[1]
1.13 logistic.py(340):     n_features = X.shape[1]
1.13 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
1.13 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
1.13 logistic.py(343):                     dtype=X.dtype)
1.13 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
1.13 logistic.py(282):     n_classes = Y.shape[1]
1.13 logistic.py(283):     n_features = X.shape[1]
1.13 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
1.13 logistic.py(285):     w = w.reshape(n_classes, -1)
1.13 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
1.13 logistic.py(287):     if fit_intercept:
1.13 logistic.py(288):         intercept = w[:, -1]
1.13 logistic.py(289):         w = w[:, :-1]
1.13 logistic.py(292):     p = safe_sparse_dot(X, w.T)
1.13 logistic.py(293):     p += intercept
1.13 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
1.13 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
1.13 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
1.13 logistic.py(297):     p = np.exp(p, p)
1.13 logistic.py(298):     return loss, p, w
1.13 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
1.13 logistic.py(346):     diff = sample_weight * (p - Y)
1.13 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
1.13 logistic.py(348):     grad[:, :n_features] += alpha * w
1.13 logistic.py(349):     if fit_intercept:
1.13 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
1.13 logistic.py(351):     return loss, grad.ravel(), p
1.13 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
1.13 logistic.py(339):     n_classes = Y.shape[1]
1.13 logistic.py(340):     n_features = X.shape[1]
1.13 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
1.13 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
1.13 logistic.py(343):                     dtype=X.dtype)
1.13 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
1.13 logistic.py(282):     n_classes = Y.shape[1]
1.13 logistic.py(283):     n_features = X.shape[1]
1.13 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
1.13 logistic.py(285):     w = w.reshape(n_classes, -1)
1.13 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
1.13 logistic.py(287):     if fit_intercept:
1.13 logistic.py(288):         intercept = w[:, -1]
1.13 logistic.py(289):         w = w[:, :-1]
1.13 logistic.py(292):     p = safe_sparse_dot(X, w.T)
1.13 logistic.py(293):     p += intercept
1.13 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
1.13 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
1.13 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
1.13 logistic.py(297):     p = np.exp(p, p)
1.13 logistic.py(298):     return loss, p, w
1.13 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
1.13 logistic.py(346):     diff = sample_weight * (p - Y)
1.13 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
1.13 logistic.py(348):     grad[:, :n_features] += alpha * w
1.13 logistic.py(349):     if fit_intercept:
1.13 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
1.13 logistic.py(351):     return loss, grad.ravel(), p
1.13 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
1.13 logistic.py(339):     n_classes = Y.shape[1]
1.13 logistic.py(340):     n_features = X.shape[1]
1.13 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
1.13 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
1.13 logistic.py(343):                     dtype=X.dtype)
1.13 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
1.13 logistic.py(282):     n_classes = Y.shape[1]
1.13 logistic.py(283):     n_features = X.shape[1]
1.13 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
1.13 logistic.py(285):     w = w.reshape(n_classes, -1)
1.13 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
1.13 logistic.py(287):     if fit_intercept:
1.13 logistic.py(288):         intercept = w[:, -1]
1.13 logistic.py(289):         w = w[:, :-1]
1.13 logistic.py(292):     p = safe_sparse_dot(X, w.T)
1.13 logistic.py(293):     p += intercept
1.13 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
1.13 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
1.13 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
1.13 logistic.py(297):     p = np.exp(p, p)
1.13 logistic.py(298):     return loss, p, w
1.13 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
1.13 logistic.py(346):     diff = sample_weight * (p - Y)
1.13 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
1.13 logistic.py(348):     grad[:, :n_features] += alpha * w
1.13 logistic.py(349):     if fit_intercept:
1.13 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
1.13 logistic.py(351):     return loss, grad.ravel(), p
1.13 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
1.13 logistic.py(339):     n_classes = Y.shape[1]
1.13 logistic.py(340):     n_features = X.shape[1]
1.13 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
1.13 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
1.13 logistic.py(343):                     dtype=X.dtype)
1.13 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
1.13 logistic.py(282):     n_classes = Y.shape[1]
1.13 logistic.py(283):     n_features = X.shape[1]
1.13 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
1.13 logistic.py(285):     w = w.reshape(n_classes, -1)
1.13 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
1.13 logistic.py(287):     if fit_intercept:
1.13 logistic.py(288):         intercept = w[:, -1]
1.13 logistic.py(289):         w = w[:, :-1]
1.13 logistic.py(292):     p = safe_sparse_dot(X, w.T)
1.13 logistic.py(293):     p += intercept
1.13 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
1.13 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
1.13 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
1.13 logistic.py(297):     p = np.exp(p, p)
1.13 logistic.py(298):     return loss, p, w
1.13 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
1.13 logistic.py(346):     diff = sample_weight * (p - Y)
1.13 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
1.13 logistic.py(348):     grad[:, :n_features] += alpha * w
1.13 logistic.py(349):     if fit_intercept:
1.13 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
1.13 logistic.py(351):     return loss, grad.ravel(), p
1.13 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
1.13 logistic.py(339):     n_classes = Y.shape[1]
1.13 logistic.py(340):     n_features = X.shape[1]
1.13 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
1.13 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
1.13 logistic.py(343):                     dtype=X.dtype)
1.13 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
1.13 logistic.py(282):     n_classes = Y.shape[1]
1.13 logistic.py(283):     n_features = X.shape[1]
1.13 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
1.13 logistic.py(285):     w = w.reshape(n_classes, -1)
1.13 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
1.13 logistic.py(287):     if fit_intercept:
1.13 logistic.py(288):         intercept = w[:, -1]
1.13 logistic.py(289):         w = w[:, :-1]
1.13 logistic.py(292):     p = safe_sparse_dot(X, w.T)
1.13 logistic.py(293):     p += intercept
1.13 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
1.13 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
1.13 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
1.13 logistic.py(297):     p = np.exp(p, p)
1.13 logistic.py(298):     return loss, p, w
1.13 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
1.13 logistic.py(346):     diff = sample_weight * (p - Y)
1.13 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
1.13 logistic.py(348):     grad[:, :n_features] += alpha * w
1.13 logistic.py(349):     if fit_intercept:
1.13 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
1.13 logistic.py(351):     return loss, grad.ravel(), p
1.13 logistic.py(718):             if info["warnflag"] == 1:
1.13 logistic.py(723):             n_iter_i = min(info['nit'], max_iter)
1.13 logistic.py(760):         if multi_class == 'multinomial':
1.13 logistic.py(761):             multi_w0 = np.reshape(w0, (classes.size, -1))
1.13 logistic.py(762):             if classes.size == 2:
1.13 logistic.py(764):             coefs.append(multi_w0)
1.13 logistic.py(768):         n_iter[i] = n_iter_i
1.13 logistic.py(712):     for i, C in enumerate(Cs):
1.13 logistic.py(713):         if solver == 'lbfgs':
1.13 logistic.py(714):             w0, loss, info = optimize.fmin_l_bfgs_b(
1.13 logistic.py(715):                 func, w0, fprime=None,
1.13 logistic.py(716):                 args=(X, target, 1. / C, sample_weight),
1.13 logistic.py(717):                 iprint=(verbose > 0) - 1, pgtol=tol, maxiter=max_iter)
1.13 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
1.13 logistic.py(339):     n_classes = Y.shape[1]
1.13 logistic.py(340):     n_features = X.shape[1]
1.13 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
1.13 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
1.13 logistic.py(343):                     dtype=X.dtype)
1.13 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
1.13 logistic.py(282):     n_classes = Y.shape[1]
1.13 logistic.py(283):     n_features = X.shape[1]
1.13 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
1.13 logistic.py(285):     w = w.reshape(n_classes, -1)
1.13 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
1.13 logistic.py(287):     if fit_intercept:
1.13 logistic.py(288):         intercept = w[:, -1]
1.13 logistic.py(289):         w = w[:, :-1]
1.13 logistic.py(292):     p = safe_sparse_dot(X, w.T)
1.13 logistic.py(293):     p += intercept
1.13 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
1.13 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
1.13 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
1.13 logistic.py(297):     p = np.exp(p, p)
1.13 logistic.py(298):     return loss, p, w
1.13 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
1.13 logistic.py(346):     diff = sample_weight * (p - Y)
1.13 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
1.13 logistic.py(348):     grad[:, :n_features] += alpha * w
1.13 logistic.py(349):     if fit_intercept:
1.13 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
1.13 logistic.py(351):     return loss, grad.ravel(), p
1.13 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
1.13 logistic.py(339):     n_classes = Y.shape[1]
1.13 logistic.py(340):     n_features = X.shape[1]
1.13 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
1.13 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
1.13 logistic.py(343):                     dtype=X.dtype)
1.13 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
1.13 logistic.py(282):     n_classes = Y.shape[1]
1.13 logistic.py(283):     n_features = X.shape[1]
1.13 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
1.13 logistic.py(285):     w = w.reshape(n_classes, -1)
1.13 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
1.13 logistic.py(287):     if fit_intercept:
1.13 logistic.py(288):         intercept = w[:, -1]
1.13 logistic.py(289):         w = w[:, :-1]
1.13 logistic.py(292):     p = safe_sparse_dot(X, w.T)
1.13 logistic.py(293):     p += intercept
1.13 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
1.13 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
1.13 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
1.13 logistic.py(297):     p = np.exp(p, p)
1.13 logistic.py(298):     return loss, p, w
1.13 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
1.13 logistic.py(346):     diff = sample_weight * (p - Y)
1.13 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
1.13 logistic.py(348):     grad[:, :n_features] += alpha * w
1.13 logistic.py(349):     if fit_intercept:
1.13 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
1.13 logistic.py(351):     return loss, grad.ravel(), p
1.13 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
1.13 logistic.py(339):     n_classes = Y.shape[1]
1.13 logistic.py(340):     n_features = X.shape[1]
1.13 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
1.13 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
1.13 logistic.py(343):                     dtype=X.dtype)
1.13 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
1.13 logistic.py(282):     n_classes = Y.shape[1]
1.13 logistic.py(283):     n_features = X.shape[1]
1.13 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
1.13 logistic.py(285):     w = w.reshape(n_classes, -1)
1.13 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
1.13 logistic.py(287):     if fit_intercept:
1.13 logistic.py(288):         intercept = w[:, -1]
1.13 logistic.py(289):         w = w[:, :-1]
1.13 logistic.py(292):     p = safe_sparse_dot(X, w.T)
1.13 logistic.py(293):     p += intercept
1.13 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
1.13 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
1.13 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
1.13 logistic.py(297):     p = np.exp(p, p)
1.13 logistic.py(298):     return loss, p, w
1.13 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
1.13 logistic.py(346):     diff = sample_weight * (p - Y)
1.13 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
1.13 logistic.py(348):     grad[:, :n_features] += alpha * w
1.13 logistic.py(349):     if fit_intercept:
1.13 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
1.13 logistic.py(351):     return loss, grad.ravel(), p
1.13 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
1.13 logistic.py(339):     n_classes = Y.shape[1]
1.13 logistic.py(340):     n_features = X.shape[1]
1.13 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
1.13 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
1.13 logistic.py(343):                     dtype=X.dtype)
1.13 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
1.13 logistic.py(282):     n_classes = Y.shape[1]
1.13 logistic.py(283):     n_features = X.shape[1]
1.13 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
1.13 logistic.py(285):     w = w.reshape(n_classes, -1)
1.13 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
1.13 logistic.py(287):     if fit_intercept:
1.13 logistic.py(288):         intercept = w[:, -1]
1.13 logistic.py(289):         w = w[:, :-1]
1.13 logistic.py(292):     p = safe_sparse_dot(X, w.T)
1.13 logistic.py(293):     p += intercept
1.13 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
1.13 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
1.13 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
1.13 logistic.py(297):     p = np.exp(p, p)
1.13 logistic.py(298):     return loss, p, w
1.13 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
1.13 logistic.py(346):     diff = sample_weight * (p - Y)
1.13 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
1.13 logistic.py(348):     grad[:, :n_features] += alpha * w
1.13 logistic.py(349):     if fit_intercept:
1.13 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
1.13 logistic.py(351):     return loss, grad.ravel(), p
1.13 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
1.13 logistic.py(339):     n_classes = Y.shape[1]
1.13 logistic.py(340):     n_features = X.shape[1]
1.13 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
1.13 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
1.13 logistic.py(343):                     dtype=X.dtype)
1.13 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
1.13 logistic.py(282):     n_classes = Y.shape[1]
1.13 logistic.py(283):     n_features = X.shape[1]
1.13 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
1.13 logistic.py(285):     w = w.reshape(n_classes, -1)
1.13 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
1.13 logistic.py(287):     if fit_intercept:
1.13 logistic.py(288):         intercept = w[:, -1]
1.13 logistic.py(289):         w = w[:, :-1]
1.13 logistic.py(292):     p = safe_sparse_dot(X, w.T)
1.13 logistic.py(293):     p += intercept
1.13 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
1.13 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
1.13 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
1.13 logistic.py(297):     p = np.exp(p, p)
1.13 logistic.py(298):     return loss, p, w
1.13 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
1.13 logistic.py(346):     diff = sample_weight * (p - Y)
1.13 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
1.13 logistic.py(348):     grad[:, :n_features] += alpha * w
1.13 logistic.py(349):     if fit_intercept:
1.13 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
1.13 logistic.py(351):     return loss, grad.ravel(), p
1.13 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
1.13 logistic.py(339):     n_classes = Y.shape[1]
1.13 logistic.py(340):     n_features = X.shape[1]
1.13 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
1.13 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
1.13 logistic.py(343):                     dtype=X.dtype)
1.13 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
1.13 logistic.py(282):     n_classes = Y.shape[1]
1.13 logistic.py(283):     n_features = X.shape[1]
1.13 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
1.13 logistic.py(285):     w = w.reshape(n_classes, -1)
1.13 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
1.13 logistic.py(287):     if fit_intercept:
1.13 logistic.py(288):         intercept = w[:, -1]
1.13 logistic.py(289):         w = w[:, :-1]
1.13 logistic.py(292):     p = safe_sparse_dot(X, w.T)
1.13 logistic.py(293):     p += intercept
1.13 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
1.13 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
1.13 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
1.13 logistic.py(297):     p = np.exp(p, p)
1.13 logistic.py(298):     return loss, p, w
1.13 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
1.13 logistic.py(346):     diff = sample_weight * (p - Y)
1.13 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
1.13 logistic.py(348):     grad[:, :n_features] += alpha * w
1.13 logistic.py(349):     if fit_intercept:
1.13 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
1.13 logistic.py(351):     return loss, grad.ravel(), p
1.13 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
1.13 logistic.py(339):     n_classes = Y.shape[1]
1.13 logistic.py(340):     n_features = X.shape[1]
1.13 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
1.13 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
1.13 logistic.py(343):                     dtype=X.dtype)
1.13 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
1.13 logistic.py(282):     n_classes = Y.shape[1]
1.13 logistic.py(283):     n_features = X.shape[1]
1.13 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
1.13 logistic.py(285):     w = w.reshape(n_classes, -1)
1.13 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
1.13 logistic.py(287):     if fit_intercept:
1.13 logistic.py(288):         intercept = w[:, -1]
1.13 logistic.py(289):         w = w[:, :-1]
1.13 logistic.py(292):     p = safe_sparse_dot(X, w.T)
1.13 logistic.py(293):     p += intercept
1.13 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
1.13 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
1.13 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
1.13 logistic.py(297):     p = np.exp(p, p)
1.13 logistic.py(298):     return loss, p, w
1.13 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
1.13 logistic.py(346):     diff = sample_weight * (p - Y)
1.13 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
1.13 logistic.py(348):     grad[:, :n_features] += alpha * w
1.13 logistic.py(349):     if fit_intercept:
1.13 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
1.13 logistic.py(351):     return loss, grad.ravel(), p
1.13 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
1.13 logistic.py(339):     n_classes = Y.shape[1]
1.13 logistic.py(340):     n_features = X.shape[1]
1.13 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
1.13 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
1.13 logistic.py(343):                     dtype=X.dtype)
1.13 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
1.13 logistic.py(282):     n_classes = Y.shape[1]
1.13 logistic.py(283):     n_features = X.shape[1]
1.13 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
1.13 logistic.py(285):     w = w.reshape(n_classes, -1)
1.13 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
1.13 logistic.py(287):     if fit_intercept:
1.13 logistic.py(288):         intercept = w[:, -1]
1.13 logistic.py(289):         w = w[:, :-1]
1.13 logistic.py(292):     p = safe_sparse_dot(X, w.T)
1.13 logistic.py(293):     p += intercept
1.13 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
1.13 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
1.13 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
1.13 logistic.py(297):     p = np.exp(p, p)
1.13 logistic.py(298):     return loss, p, w
1.13 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
1.13 logistic.py(346):     diff = sample_weight * (p - Y)
1.13 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
1.13 logistic.py(348):     grad[:, :n_features] += alpha * w
1.13 logistic.py(349):     if fit_intercept:
1.13 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
1.13 logistic.py(351):     return loss, grad.ravel(), p
1.13 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
1.13 logistic.py(339):     n_classes = Y.shape[1]
1.13 logistic.py(340):     n_features = X.shape[1]
1.13 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
1.13 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
1.13 logistic.py(343):                     dtype=X.dtype)
1.13 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
1.13 logistic.py(282):     n_classes = Y.shape[1]
1.13 logistic.py(283):     n_features = X.shape[1]
1.13 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
1.13 logistic.py(285):     w = w.reshape(n_classes, -1)
1.13 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
1.13 logistic.py(287):     if fit_intercept:
1.13 logistic.py(288):         intercept = w[:, -1]
1.13 logistic.py(289):         w = w[:, :-1]
1.13 logistic.py(292):     p = safe_sparse_dot(X, w.T)
1.13 logistic.py(293):     p += intercept
1.13 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
1.13 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
1.13 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
1.13 logistic.py(297):     p = np.exp(p, p)
1.13 logistic.py(298):     return loss, p, w
1.13 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
1.14 logistic.py(346):     diff = sample_weight * (p - Y)
1.14 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
1.14 logistic.py(348):     grad[:, :n_features] += alpha * w
1.14 logistic.py(349):     if fit_intercept:
1.14 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
1.14 logistic.py(351):     return loss, grad.ravel(), p
1.14 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
1.14 logistic.py(339):     n_classes = Y.shape[1]
1.14 logistic.py(340):     n_features = X.shape[1]
1.14 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
1.14 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
1.14 logistic.py(343):                     dtype=X.dtype)
1.14 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
1.14 logistic.py(282):     n_classes = Y.shape[1]
1.14 logistic.py(283):     n_features = X.shape[1]
1.14 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
1.14 logistic.py(285):     w = w.reshape(n_classes, -1)
1.14 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
1.14 logistic.py(287):     if fit_intercept:
1.14 logistic.py(288):         intercept = w[:, -1]
1.14 logistic.py(289):         w = w[:, :-1]
1.14 logistic.py(292):     p = safe_sparse_dot(X, w.T)
1.14 logistic.py(293):     p += intercept
1.14 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
1.14 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
1.14 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
1.14 logistic.py(297):     p = np.exp(p, p)
1.14 logistic.py(298):     return loss, p, w
1.14 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
1.14 logistic.py(346):     diff = sample_weight * (p - Y)
1.14 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
1.14 logistic.py(348):     grad[:, :n_features] += alpha * w
1.14 logistic.py(349):     if fit_intercept:
1.14 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
1.14 logistic.py(351):     return loss, grad.ravel(), p
1.14 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
1.14 logistic.py(339):     n_classes = Y.shape[1]
1.14 logistic.py(340):     n_features = X.shape[1]
1.14 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
1.14 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
1.14 logistic.py(343):                     dtype=X.dtype)
1.14 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
1.14 logistic.py(282):     n_classes = Y.shape[1]
1.14 logistic.py(283):     n_features = X.shape[1]
1.14 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
1.14 logistic.py(285):     w = w.reshape(n_classes, -1)
1.14 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
1.14 logistic.py(287):     if fit_intercept:
1.14 logistic.py(288):         intercept = w[:, -1]
1.14 logistic.py(289):         w = w[:, :-1]
1.14 logistic.py(292):     p = safe_sparse_dot(X, w.T)
1.14 logistic.py(293):     p += intercept
1.14 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
1.14 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
1.14 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
1.14 logistic.py(297):     p = np.exp(p, p)
1.14 logistic.py(298):     return loss, p, w
1.14 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
1.14 logistic.py(346):     diff = sample_weight * (p - Y)
1.14 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
1.14 logistic.py(348):     grad[:, :n_features] += alpha * w
1.14 logistic.py(349):     if fit_intercept:
1.14 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
1.14 logistic.py(351):     return loss, grad.ravel(), p
1.14 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
1.14 logistic.py(339):     n_classes = Y.shape[1]
1.14 logistic.py(340):     n_features = X.shape[1]
1.14 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
1.14 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
1.14 logistic.py(343):                     dtype=X.dtype)
1.14 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
1.14 logistic.py(282):     n_classes = Y.shape[1]
1.14 logistic.py(283):     n_features = X.shape[1]
1.14 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
1.14 logistic.py(285):     w = w.reshape(n_classes, -1)
1.14 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
1.14 logistic.py(287):     if fit_intercept:
1.14 logistic.py(288):         intercept = w[:, -1]
1.14 logistic.py(289):         w = w[:, :-1]
1.14 logistic.py(292):     p = safe_sparse_dot(X, w.T)
1.14 logistic.py(293):     p += intercept
1.14 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
1.14 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
1.14 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
1.14 logistic.py(297):     p = np.exp(p, p)
1.14 logistic.py(298):     return loss, p, w
1.14 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
1.14 logistic.py(346):     diff = sample_weight * (p - Y)
1.14 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
1.14 logistic.py(348):     grad[:, :n_features] += alpha * w
1.14 logistic.py(349):     if fit_intercept:
1.14 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
1.14 logistic.py(351):     return loss, grad.ravel(), p
1.14 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
1.14 logistic.py(339):     n_classes = Y.shape[1]
1.14 logistic.py(340):     n_features = X.shape[1]
1.14 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
1.14 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
1.14 logistic.py(343):                     dtype=X.dtype)
1.14 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
1.14 logistic.py(282):     n_classes = Y.shape[1]
1.14 logistic.py(283):     n_features = X.shape[1]
1.14 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
1.14 logistic.py(285):     w = w.reshape(n_classes, -1)
1.14 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
1.14 logistic.py(287):     if fit_intercept:
1.14 logistic.py(288):         intercept = w[:, -1]
1.14 logistic.py(289):         w = w[:, :-1]
1.14 logistic.py(292):     p = safe_sparse_dot(X, w.T)
1.14 logistic.py(293):     p += intercept
1.14 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
1.14 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
1.14 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
1.14 logistic.py(297):     p = np.exp(p, p)
1.14 logistic.py(298):     return loss, p, w
1.14 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
1.14 logistic.py(346):     diff = sample_weight * (p - Y)
1.14 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
1.14 logistic.py(348):     grad[:, :n_features] += alpha * w
1.14 logistic.py(349):     if fit_intercept:
1.14 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
1.14 logistic.py(351):     return loss, grad.ravel(), p
1.14 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
1.14 logistic.py(339):     n_classes = Y.shape[1]
1.14 logistic.py(340):     n_features = X.shape[1]
1.14 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
1.14 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
1.14 logistic.py(343):                     dtype=X.dtype)
1.14 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
1.14 logistic.py(282):     n_classes = Y.shape[1]
1.14 logistic.py(283):     n_features = X.shape[1]
1.14 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
1.14 logistic.py(285):     w = w.reshape(n_classes, -1)
1.14 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
1.14 logistic.py(287):     if fit_intercept:
1.14 logistic.py(288):         intercept = w[:, -1]
1.14 logistic.py(289):         w = w[:, :-1]
1.14 logistic.py(292):     p = safe_sparse_dot(X, w.T)
1.14 logistic.py(293):     p += intercept
1.14 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
1.14 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
1.14 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
1.14 logistic.py(297):     p = np.exp(p, p)
1.14 logistic.py(298):     return loss, p, w
1.14 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
1.14 logistic.py(346):     diff = sample_weight * (p - Y)
1.14 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
1.14 logistic.py(348):     grad[:, :n_features] += alpha * w
1.14 logistic.py(349):     if fit_intercept:
1.14 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
1.14 logistic.py(351):     return loss, grad.ravel(), p
1.14 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
1.14 logistic.py(339):     n_classes = Y.shape[1]
1.14 logistic.py(340):     n_features = X.shape[1]
1.14 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
1.14 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
1.14 logistic.py(343):                     dtype=X.dtype)
1.14 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
1.14 logistic.py(282):     n_classes = Y.shape[1]
1.14 logistic.py(283):     n_features = X.shape[1]
1.14 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
1.14 logistic.py(285):     w = w.reshape(n_classes, -1)
1.14 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
1.14 logistic.py(287):     if fit_intercept:
1.14 logistic.py(288):         intercept = w[:, -1]
1.14 logistic.py(289):         w = w[:, :-1]
1.14 logistic.py(292):     p = safe_sparse_dot(X, w.T)
1.14 logistic.py(293):     p += intercept
1.14 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
1.14 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
1.14 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
1.14 logistic.py(297):     p = np.exp(p, p)
1.14 logistic.py(298):     return loss, p, w
1.14 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
1.14 logistic.py(346):     diff = sample_weight * (p - Y)
1.14 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
1.14 logistic.py(348):     grad[:, :n_features] += alpha * w
1.14 logistic.py(349):     if fit_intercept:
1.14 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
1.14 logistic.py(351):     return loss, grad.ravel(), p
1.14 logistic.py(718):             if info["warnflag"] == 1:
1.14 logistic.py(723):             n_iter_i = min(info['nit'], max_iter)
1.14 logistic.py(760):         if multi_class == 'multinomial':
1.14 logistic.py(761):             multi_w0 = np.reshape(w0, (classes.size, -1))
1.14 logistic.py(762):             if classes.size == 2:
1.14 logistic.py(764):             coefs.append(multi_w0)
1.14 logistic.py(768):         n_iter[i] = n_iter_i
1.14 logistic.py(712):     for i, C in enumerate(Cs):
1.14 logistic.py(713):         if solver == 'lbfgs':
1.14 logistic.py(714):             w0, loss, info = optimize.fmin_l_bfgs_b(
1.14 logistic.py(715):                 func, w0, fprime=None,
1.14 logistic.py(716):                 args=(X, target, 1. / C, sample_weight),
1.14 logistic.py(717):                 iprint=(verbose > 0) - 1, pgtol=tol, maxiter=max_iter)
1.14 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
1.14 logistic.py(339):     n_classes = Y.shape[1]
1.14 logistic.py(340):     n_features = X.shape[1]
1.14 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
1.14 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
1.14 logistic.py(343):                     dtype=X.dtype)
1.14 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
1.14 logistic.py(282):     n_classes = Y.shape[1]
1.14 logistic.py(283):     n_features = X.shape[1]
1.14 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
1.14 logistic.py(285):     w = w.reshape(n_classes, -1)
1.14 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
1.14 logistic.py(287):     if fit_intercept:
1.14 logistic.py(288):         intercept = w[:, -1]
1.14 logistic.py(289):         w = w[:, :-1]
1.14 logistic.py(292):     p = safe_sparse_dot(X, w.T)
1.14 logistic.py(293):     p += intercept
1.14 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
1.14 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
1.14 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
1.14 logistic.py(297):     p = np.exp(p, p)
1.14 logistic.py(298):     return loss, p, w
1.14 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
1.14 logistic.py(346):     diff = sample_weight * (p - Y)
1.14 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
1.14 logistic.py(348):     grad[:, :n_features] += alpha * w
1.14 logistic.py(349):     if fit_intercept:
1.14 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
1.14 logistic.py(351):     return loss, grad.ravel(), p
1.14 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
1.14 logistic.py(339):     n_classes = Y.shape[1]
1.14 logistic.py(340):     n_features = X.shape[1]
1.14 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
1.14 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
1.14 logistic.py(343):                     dtype=X.dtype)
1.14 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
1.14 logistic.py(282):     n_classes = Y.shape[1]
1.14 logistic.py(283):     n_features = X.shape[1]
1.14 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
1.14 logistic.py(285):     w = w.reshape(n_classes, -1)
1.14 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
1.14 logistic.py(287):     if fit_intercept:
1.14 logistic.py(288):         intercept = w[:, -1]
1.14 logistic.py(289):         w = w[:, :-1]
1.14 logistic.py(292):     p = safe_sparse_dot(X, w.T)
1.14 logistic.py(293):     p += intercept
1.14 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
1.14 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
1.14 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
1.14 logistic.py(297):     p = np.exp(p, p)
1.14 logistic.py(298):     return loss, p, w
1.14 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
1.14 logistic.py(346):     diff = sample_weight * (p - Y)
1.14 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
1.14 logistic.py(348):     grad[:, :n_features] += alpha * w
1.14 logistic.py(349):     if fit_intercept:
1.14 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
1.14 logistic.py(351):     return loss, grad.ravel(), p
1.14 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
1.14 logistic.py(339):     n_classes = Y.shape[1]
1.14 logistic.py(340):     n_features = X.shape[1]
1.14 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
1.14 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
1.14 logistic.py(343):                     dtype=X.dtype)
1.14 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
1.14 logistic.py(282):     n_classes = Y.shape[1]
1.14 logistic.py(283):     n_features = X.shape[1]
1.14 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
1.14 logistic.py(285):     w = w.reshape(n_classes, -1)
1.14 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
1.14 logistic.py(287):     if fit_intercept:
1.14 logistic.py(288):         intercept = w[:, -1]
1.14 logistic.py(289):         w = w[:, :-1]
1.14 logistic.py(292):     p = safe_sparse_dot(X, w.T)
1.14 logistic.py(293):     p += intercept
1.14 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
1.14 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
1.14 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
1.14 logistic.py(297):     p = np.exp(p, p)
1.14 logistic.py(298):     return loss, p, w
1.14 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
1.14 logistic.py(346):     diff = sample_weight * (p - Y)
1.14 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
1.14 logistic.py(348):     grad[:, :n_features] += alpha * w
1.14 logistic.py(349):     if fit_intercept:
1.14 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
1.14 logistic.py(351):     return loss, grad.ravel(), p
1.14 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
1.14 logistic.py(339):     n_classes = Y.shape[1]
1.14 logistic.py(340):     n_features = X.shape[1]
1.14 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
1.14 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
1.14 logistic.py(343):                     dtype=X.dtype)
1.14 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
1.14 logistic.py(282):     n_classes = Y.shape[1]
1.14 logistic.py(283):     n_features = X.shape[1]
1.14 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
1.14 logistic.py(285):     w = w.reshape(n_classes, -1)
1.14 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
1.14 logistic.py(287):     if fit_intercept:
1.14 logistic.py(288):         intercept = w[:, -1]
1.14 logistic.py(289):         w = w[:, :-1]
1.14 logistic.py(292):     p = safe_sparse_dot(X, w.T)
1.14 logistic.py(293):     p += intercept
1.14 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
1.14 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
1.14 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
1.14 logistic.py(297):     p = np.exp(p, p)
1.14 logistic.py(298):     return loss, p, w
1.14 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
1.14 logistic.py(346):     diff = sample_weight * (p - Y)
1.14 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
1.14 logistic.py(348):     grad[:, :n_features] += alpha * w
1.14 logistic.py(349):     if fit_intercept:
1.14 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
1.14 logistic.py(351):     return loss, grad.ravel(), p
1.14 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
1.14 logistic.py(339):     n_classes = Y.shape[1]
1.14 logistic.py(340):     n_features = X.shape[1]
1.14 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
1.14 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
1.14 logistic.py(343):                     dtype=X.dtype)
1.14 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
1.14 logistic.py(282):     n_classes = Y.shape[1]
1.14 logistic.py(283):     n_features = X.shape[1]
1.14 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
1.14 logistic.py(285):     w = w.reshape(n_classes, -1)
1.14 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
1.14 logistic.py(287):     if fit_intercept:
1.14 logistic.py(288):         intercept = w[:, -1]
1.14 logistic.py(289):         w = w[:, :-1]
1.14 logistic.py(292):     p = safe_sparse_dot(X, w.T)
1.14 logistic.py(293):     p += intercept
1.14 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
1.14 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
1.14 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
1.14 logistic.py(297):     p = np.exp(p, p)
1.14 logistic.py(298):     return loss, p, w
1.14 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
1.14 logistic.py(346):     diff = sample_weight * (p - Y)
1.14 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
1.14 logistic.py(348):     grad[:, :n_features] += alpha * w
1.14 logistic.py(349):     if fit_intercept:
1.14 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
1.14 logistic.py(351):     return loss, grad.ravel(), p
1.14 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
1.14 logistic.py(339):     n_classes = Y.shape[1]
1.14 logistic.py(340):     n_features = X.shape[1]
1.14 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
1.14 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
1.14 logistic.py(343):                     dtype=X.dtype)
1.14 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
1.14 logistic.py(282):     n_classes = Y.shape[1]
1.14 logistic.py(283):     n_features = X.shape[1]
1.14 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
1.14 logistic.py(285):     w = w.reshape(n_classes, -1)
1.14 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
1.14 logistic.py(287):     if fit_intercept:
1.14 logistic.py(288):         intercept = w[:, -1]
1.14 logistic.py(289):         w = w[:, :-1]
1.14 logistic.py(292):     p = safe_sparse_dot(X, w.T)
1.14 logistic.py(293):     p += intercept
1.14 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
1.14 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
1.14 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
1.14 logistic.py(297):     p = np.exp(p, p)
1.14 logistic.py(298):     return loss, p, w
1.14 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
1.14 logistic.py(346):     diff = sample_weight * (p - Y)
1.14 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
1.14 logistic.py(348):     grad[:, :n_features] += alpha * w
1.14 logistic.py(349):     if fit_intercept:
1.14 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
1.14 logistic.py(351):     return loss, grad.ravel(), p
1.14 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
1.14 logistic.py(339):     n_classes = Y.shape[1]
1.14 logistic.py(340):     n_features = X.shape[1]
1.14 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
1.14 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
1.14 logistic.py(343):                     dtype=X.dtype)
1.14 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
1.14 logistic.py(282):     n_classes = Y.shape[1]
1.14 logistic.py(283):     n_features = X.shape[1]
1.14 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
1.14 logistic.py(285):     w = w.reshape(n_classes, -1)
1.14 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
1.14 logistic.py(287):     if fit_intercept:
1.14 logistic.py(288):         intercept = w[:, -1]
1.14 logistic.py(289):         w = w[:, :-1]
1.14 logistic.py(292):     p = safe_sparse_dot(X, w.T)
1.14 logistic.py(293):     p += intercept
1.14 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
1.14 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
1.14 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
1.14 logistic.py(297):     p = np.exp(p, p)
1.14 logistic.py(298):     return loss, p, w
1.14 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
1.14 logistic.py(346):     diff = sample_weight * (p - Y)
1.14 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
1.14 logistic.py(348):     grad[:, :n_features] += alpha * w
1.14 logistic.py(349):     if fit_intercept:
1.14 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
1.14 logistic.py(351):     return loss, grad.ravel(), p
1.14 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
1.14 logistic.py(339):     n_classes = Y.shape[1]
1.14 logistic.py(340):     n_features = X.shape[1]
1.14 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
1.14 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
1.14 logistic.py(343):                     dtype=X.dtype)
1.14 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
1.14 logistic.py(282):     n_classes = Y.shape[1]
1.14 logistic.py(283):     n_features = X.shape[1]
1.14 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
1.14 logistic.py(285):     w = w.reshape(n_classes, -1)
1.14 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
1.14 logistic.py(287):     if fit_intercept:
1.14 logistic.py(288):         intercept = w[:, -1]
1.14 logistic.py(289):         w = w[:, :-1]
1.14 logistic.py(292):     p = safe_sparse_dot(X, w.T)
1.14 logistic.py(293):     p += intercept
1.14 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
1.14 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
1.14 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
1.14 logistic.py(297):     p = np.exp(p, p)
1.14 logistic.py(298):     return loss, p, w
1.14 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
1.14 logistic.py(346):     diff = sample_weight * (p - Y)
1.14 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
1.14 logistic.py(348):     grad[:, :n_features] += alpha * w
1.14 logistic.py(349):     if fit_intercept:
1.14 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
1.14 logistic.py(351):     return loss, grad.ravel(), p
1.14 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
1.14 logistic.py(339):     n_classes = Y.shape[1]
1.14 logistic.py(340):     n_features = X.shape[1]
1.14 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
1.14 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
1.14 logistic.py(343):                     dtype=X.dtype)
1.14 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
1.14 logistic.py(282):     n_classes = Y.shape[1]
1.14 logistic.py(283):     n_features = X.shape[1]
1.14 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
1.14 logistic.py(285):     w = w.reshape(n_classes, -1)
1.14 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
1.14 logistic.py(287):     if fit_intercept:
1.14 logistic.py(288):         intercept = w[:, -1]
1.14 logistic.py(289):         w = w[:, :-1]
1.14 logistic.py(292):     p = safe_sparse_dot(X, w.T)
1.14 logistic.py(293):     p += intercept
1.14 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
1.14 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
1.14 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
1.14 logistic.py(297):     p = np.exp(p, p)
1.14 logistic.py(298):     return loss, p, w
1.14 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
1.14 logistic.py(346):     diff = sample_weight * (p - Y)
1.14 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
1.14 logistic.py(348):     grad[:, :n_features] += alpha * w
1.14 logistic.py(349):     if fit_intercept:
1.14 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
1.14 logistic.py(351):     return loss, grad.ravel(), p
1.14 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
1.14 logistic.py(339):     n_classes = Y.shape[1]
1.14 logistic.py(340):     n_features = X.shape[1]
1.14 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
1.14 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
1.14 logistic.py(343):                     dtype=X.dtype)
1.14 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
1.14 logistic.py(282):     n_classes = Y.shape[1]
1.15 logistic.py(283):     n_features = X.shape[1]
1.15 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
1.15 logistic.py(285):     w = w.reshape(n_classes, -1)
1.15 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
1.15 logistic.py(287):     if fit_intercept:
1.15 logistic.py(288):         intercept = w[:, -1]
1.15 logistic.py(289):         w = w[:, :-1]
1.15 logistic.py(292):     p = safe_sparse_dot(X, w.T)
1.15 logistic.py(293):     p += intercept
1.15 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
1.15 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
1.15 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
1.15 logistic.py(297):     p = np.exp(p, p)
1.15 logistic.py(298):     return loss, p, w
1.15 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
1.15 logistic.py(346):     diff = sample_weight * (p - Y)
1.15 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
1.15 logistic.py(348):     grad[:, :n_features] += alpha * w
1.15 logistic.py(349):     if fit_intercept:
1.15 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
1.15 logistic.py(351):     return loss, grad.ravel(), p
1.15 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
1.15 logistic.py(339):     n_classes = Y.shape[1]
1.15 logistic.py(340):     n_features = X.shape[1]
1.15 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
1.15 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
1.15 logistic.py(343):                     dtype=X.dtype)
1.15 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
1.15 logistic.py(282):     n_classes = Y.shape[1]
1.15 logistic.py(283):     n_features = X.shape[1]
1.15 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
1.15 logistic.py(285):     w = w.reshape(n_classes, -1)
1.15 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
1.15 logistic.py(287):     if fit_intercept:
1.15 logistic.py(288):         intercept = w[:, -1]
1.15 logistic.py(289):         w = w[:, :-1]
1.15 logistic.py(292):     p = safe_sparse_dot(X, w.T)
1.15 logistic.py(293):     p += intercept
1.15 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
1.15 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
1.15 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
1.15 logistic.py(297):     p = np.exp(p, p)
1.15 logistic.py(298):     return loss, p, w
1.15 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
1.15 logistic.py(346):     diff = sample_weight * (p - Y)
1.15 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
1.15 logistic.py(348):     grad[:, :n_features] += alpha * w
1.15 logistic.py(349):     if fit_intercept:
1.15 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
1.15 logistic.py(351):     return loss, grad.ravel(), p
1.15 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
1.15 logistic.py(339):     n_classes = Y.shape[1]
1.15 logistic.py(340):     n_features = X.shape[1]
1.15 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
1.15 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
1.15 logistic.py(343):                     dtype=X.dtype)
1.15 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
1.15 logistic.py(282):     n_classes = Y.shape[1]
1.15 logistic.py(283):     n_features = X.shape[1]
1.15 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
1.15 logistic.py(285):     w = w.reshape(n_classes, -1)
1.15 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
1.15 logistic.py(287):     if fit_intercept:
1.15 logistic.py(288):         intercept = w[:, -1]
1.15 logistic.py(289):         w = w[:, :-1]
1.15 logistic.py(292):     p = safe_sparse_dot(X, w.T)
1.15 logistic.py(293):     p += intercept
1.15 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
1.15 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
1.15 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
1.15 logistic.py(297):     p = np.exp(p, p)
1.15 logistic.py(298):     return loss, p, w
1.15 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
1.15 logistic.py(346):     diff = sample_weight * (p - Y)
1.15 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
1.15 logistic.py(348):     grad[:, :n_features] += alpha * w
1.15 logistic.py(349):     if fit_intercept:
1.15 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
1.15 logistic.py(351):     return loss, grad.ravel(), p
1.15 logistic.py(718):             if info["warnflag"] == 1:
1.15 logistic.py(723):             n_iter_i = min(info['nit'], max_iter)
1.15 logistic.py(760):         if multi_class == 'multinomial':
1.15 logistic.py(761):             multi_w0 = np.reshape(w0, (classes.size, -1))
1.15 logistic.py(762):             if classes.size == 2:
1.15 logistic.py(764):             coefs.append(multi_w0)
1.15 logistic.py(768):         n_iter[i] = n_iter_i
1.15 logistic.py(712):     for i, C in enumerate(Cs):
1.15 logistic.py(713):         if solver == 'lbfgs':
1.15 logistic.py(714):             w0, loss, info = optimize.fmin_l_bfgs_b(
1.15 logistic.py(715):                 func, w0, fprime=None,
1.15 logistic.py(716):                 args=(X, target, 1. / C, sample_weight),
1.15 logistic.py(717):                 iprint=(verbose > 0) - 1, pgtol=tol, maxiter=max_iter)
1.15 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
1.15 logistic.py(339):     n_classes = Y.shape[1]
1.15 logistic.py(340):     n_features = X.shape[1]
1.15 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
1.15 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
1.15 logistic.py(343):                     dtype=X.dtype)
1.15 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
1.15 logistic.py(282):     n_classes = Y.shape[1]
1.15 logistic.py(283):     n_features = X.shape[1]
1.15 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
1.15 logistic.py(285):     w = w.reshape(n_classes, -1)
1.15 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
1.15 logistic.py(287):     if fit_intercept:
1.15 logistic.py(288):         intercept = w[:, -1]
1.15 logistic.py(289):         w = w[:, :-1]
1.15 logistic.py(292):     p = safe_sparse_dot(X, w.T)
1.15 logistic.py(293):     p += intercept
1.15 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
1.15 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
1.15 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
1.15 logistic.py(297):     p = np.exp(p, p)
1.15 logistic.py(298):     return loss, p, w
1.15 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
1.15 logistic.py(346):     diff = sample_weight * (p - Y)
1.15 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
1.15 logistic.py(348):     grad[:, :n_features] += alpha * w
1.15 logistic.py(349):     if fit_intercept:
1.15 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
1.15 logistic.py(351):     return loss, grad.ravel(), p
1.15 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
1.15 logistic.py(339):     n_classes = Y.shape[1]
1.15 logistic.py(340):     n_features = X.shape[1]
1.15 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
1.15 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
1.15 logistic.py(343):                     dtype=X.dtype)
1.15 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
1.15 logistic.py(282):     n_classes = Y.shape[1]
1.15 logistic.py(283):     n_features = X.shape[1]
1.15 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
1.15 logistic.py(285):     w = w.reshape(n_classes, -1)
1.15 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
1.15 logistic.py(287):     if fit_intercept:
1.15 logistic.py(288):         intercept = w[:, -1]
1.15 logistic.py(289):         w = w[:, :-1]
1.15 logistic.py(292):     p = safe_sparse_dot(X, w.T)
1.15 logistic.py(293):     p += intercept
1.15 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
1.15 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
1.15 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
1.15 logistic.py(297):     p = np.exp(p, p)
1.15 logistic.py(298):     return loss, p, w
1.15 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
1.15 logistic.py(346):     diff = sample_weight * (p - Y)
1.15 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
1.15 logistic.py(348):     grad[:, :n_features] += alpha * w
1.15 logistic.py(349):     if fit_intercept:
1.15 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
1.15 logistic.py(351):     return loss, grad.ravel(), p
1.15 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
1.15 logistic.py(339):     n_classes = Y.shape[1]
1.15 logistic.py(340):     n_features = X.shape[1]
1.15 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
1.15 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
1.15 logistic.py(343):                     dtype=X.dtype)
1.15 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
1.15 logistic.py(282):     n_classes = Y.shape[1]
1.15 logistic.py(283):     n_features = X.shape[1]
1.15 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
1.15 logistic.py(285):     w = w.reshape(n_classes, -1)
1.15 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
1.15 logistic.py(287):     if fit_intercept:
1.15 logistic.py(288):         intercept = w[:, -1]
1.15 logistic.py(289):         w = w[:, :-1]
1.15 logistic.py(292):     p = safe_sparse_dot(X, w.T)
1.15 logistic.py(293):     p += intercept
1.15 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
1.15 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
1.15 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
1.15 logistic.py(297):     p = np.exp(p, p)
1.15 logistic.py(298):     return loss, p, w
1.15 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
1.15 logistic.py(346):     diff = sample_weight * (p - Y)
1.15 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
1.15 logistic.py(348):     grad[:, :n_features] += alpha * w
1.15 logistic.py(349):     if fit_intercept:
1.15 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
1.15 logistic.py(351):     return loss, grad.ravel(), p
1.15 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
1.15 logistic.py(339):     n_classes = Y.shape[1]
1.15 logistic.py(340):     n_features = X.shape[1]
1.15 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
1.15 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
1.15 logistic.py(343):                     dtype=X.dtype)
1.15 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
1.15 logistic.py(282):     n_classes = Y.shape[1]
1.15 logistic.py(283):     n_features = X.shape[1]
1.15 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
1.15 logistic.py(285):     w = w.reshape(n_classes, -1)
1.15 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
1.15 logistic.py(287):     if fit_intercept:
1.15 logistic.py(288):         intercept = w[:, -1]
1.15 logistic.py(289):         w = w[:, :-1]
1.15 logistic.py(292):     p = safe_sparse_dot(X, w.T)
1.15 logistic.py(293):     p += intercept
1.15 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
1.15 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
1.15 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
1.15 logistic.py(297):     p = np.exp(p, p)
1.15 logistic.py(298):     return loss, p, w
1.15 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
1.15 logistic.py(346):     diff = sample_weight * (p - Y)
1.15 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
1.15 logistic.py(348):     grad[:, :n_features] += alpha * w
1.15 logistic.py(349):     if fit_intercept:
1.15 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
1.15 logistic.py(351):     return loss, grad.ravel(), p
1.15 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
1.15 logistic.py(339):     n_classes = Y.shape[1]
1.15 logistic.py(340):     n_features = X.shape[1]
1.15 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
1.15 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
1.15 logistic.py(343):                     dtype=X.dtype)
1.15 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
1.15 logistic.py(282):     n_classes = Y.shape[1]
1.15 logistic.py(283):     n_features = X.shape[1]
1.15 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
1.15 logistic.py(285):     w = w.reshape(n_classes, -1)
1.15 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
1.15 logistic.py(287):     if fit_intercept:
1.15 logistic.py(288):         intercept = w[:, -1]
1.15 logistic.py(289):         w = w[:, :-1]
1.15 logistic.py(292):     p = safe_sparse_dot(X, w.T)
1.15 logistic.py(293):     p += intercept
1.15 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
1.15 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
1.15 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
1.15 logistic.py(297):     p = np.exp(p, p)
1.15 logistic.py(298):     return loss, p, w
1.15 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
1.15 logistic.py(346):     diff = sample_weight * (p - Y)
1.15 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
1.15 logistic.py(348):     grad[:, :n_features] += alpha * w
1.15 logistic.py(349):     if fit_intercept:
1.15 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
1.15 logistic.py(351):     return loss, grad.ravel(), p
1.15 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
1.15 logistic.py(339):     n_classes = Y.shape[1]
1.15 logistic.py(340):     n_features = X.shape[1]
1.15 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
1.15 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
1.15 logistic.py(343):                     dtype=X.dtype)
1.15 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
1.15 logistic.py(282):     n_classes = Y.shape[1]
1.15 logistic.py(283):     n_features = X.shape[1]
1.15 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
1.15 logistic.py(285):     w = w.reshape(n_classes, -1)
1.15 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
1.15 logistic.py(287):     if fit_intercept:
1.15 logistic.py(288):         intercept = w[:, -1]
1.15 logistic.py(289):         w = w[:, :-1]
1.15 logistic.py(292):     p = safe_sparse_dot(X, w.T)
1.15 logistic.py(293):     p += intercept
1.15 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
1.15 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
1.15 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
1.15 logistic.py(297):     p = np.exp(p, p)
1.15 logistic.py(298):     return loss, p, w
1.15 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
1.15 logistic.py(346):     diff = sample_weight * (p - Y)
1.15 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
1.15 logistic.py(348):     grad[:, :n_features] += alpha * w
1.15 logistic.py(349):     if fit_intercept:
1.15 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
1.15 logistic.py(351):     return loss, grad.ravel(), p
1.15 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
1.15 logistic.py(339):     n_classes = Y.shape[1]
1.15 logistic.py(340):     n_features = X.shape[1]
1.15 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
1.15 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
1.15 logistic.py(343):                     dtype=X.dtype)
1.15 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
1.15 logistic.py(282):     n_classes = Y.shape[1]
1.15 logistic.py(283):     n_features = X.shape[1]
1.15 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
1.15 logistic.py(285):     w = w.reshape(n_classes, -1)
1.15 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
1.15 logistic.py(287):     if fit_intercept:
1.15 logistic.py(288):         intercept = w[:, -1]
1.15 logistic.py(289):         w = w[:, :-1]
1.15 logistic.py(292):     p = safe_sparse_dot(X, w.T)
1.15 logistic.py(293):     p += intercept
1.15 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
1.15 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
1.15 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
1.15 logistic.py(297):     p = np.exp(p, p)
1.15 logistic.py(298):     return loss, p, w
1.15 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
1.15 logistic.py(346):     diff = sample_weight * (p - Y)
1.15 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
1.15 logistic.py(348):     grad[:, :n_features] += alpha * w
1.15 logistic.py(349):     if fit_intercept:
1.15 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
1.15 logistic.py(351):     return loss, grad.ravel(), p
1.15 logistic.py(718):             if info["warnflag"] == 1:
1.15 logistic.py(723):             n_iter_i = min(info['nit'], max_iter)
1.15 logistic.py(760):         if multi_class == 'multinomial':
1.15 logistic.py(761):             multi_w0 = np.reshape(w0, (classes.size, -1))
1.15 logistic.py(762):             if classes.size == 2:
1.15 logistic.py(764):             coefs.append(multi_w0)
1.15 logistic.py(768):         n_iter[i] = n_iter_i
1.15 logistic.py(712):     for i, C in enumerate(Cs):
1.15 logistic.py(713):         if solver == 'lbfgs':
1.15 logistic.py(714):             w0, loss, info = optimize.fmin_l_bfgs_b(
1.15 logistic.py(715):                 func, w0, fprime=None,
1.15 logistic.py(716):                 args=(X, target, 1. / C, sample_weight),
1.15 logistic.py(717):                 iprint=(verbose > 0) - 1, pgtol=tol, maxiter=max_iter)
1.15 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
1.15 logistic.py(339):     n_classes = Y.shape[1]
1.15 logistic.py(340):     n_features = X.shape[1]
1.15 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
1.15 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
1.15 logistic.py(343):                     dtype=X.dtype)
1.15 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
1.15 logistic.py(282):     n_classes = Y.shape[1]
1.15 logistic.py(283):     n_features = X.shape[1]
1.15 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
1.15 logistic.py(285):     w = w.reshape(n_classes, -1)
1.15 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
1.15 logistic.py(287):     if fit_intercept:
1.15 logistic.py(288):         intercept = w[:, -1]
1.15 logistic.py(289):         w = w[:, :-1]
1.15 logistic.py(292):     p = safe_sparse_dot(X, w.T)
1.15 logistic.py(293):     p += intercept
1.15 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
1.15 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
1.15 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
1.15 logistic.py(297):     p = np.exp(p, p)
1.15 logistic.py(298):     return loss, p, w
1.15 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
1.15 logistic.py(346):     diff = sample_weight * (p - Y)
1.15 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
1.15 logistic.py(348):     grad[:, :n_features] += alpha * w
1.15 logistic.py(349):     if fit_intercept:
1.15 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
1.15 logistic.py(351):     return loss, grad.ravel(), p
1.15 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
1.15 logistic.py(339):     n_classes = Y.shape[1]
1.15 logistic.py(340):     n_features = X.shape[1]
1.15 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
1.15 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
1.15 logistic.py(343):                     dtype=X.dtype)
1.15 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
1.15 logistic.py(282):     n_classes = Y.shape[1]
1.15 logistic.py(283):     n_features = X.shape[1]
1.15 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
1.15 logistic.py(285):     w = w.reshape(n_classes, -1)
1.15 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
1.15 logistic.py(287):     if fit_intercept:
1.15 logistic.py(288):         intercept = w[:, -1]
1.15 logistic.py(289):         w = w[:, :-1]
1.15 logistic.py(292):     p = safe_sparse_dot(X, w.T)
1.15 logistic.py(293):     p += intercept
1.15 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
1.15 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
1.15 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
1.15 logistic.py(297):     p = np.exp(p, p)
1.15 logistic.py(298):     return loss, p, w
1.15 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
1.15 logistic.py(346):     diff = sample_weight * (p - Y)
1.15 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
1.15 logistic.py(348):     grad[:, :n_features] += alpha * w
1.15 logistic.py(349):     if fit_intercept:
1.15 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
1.15 logistic.py(351):     return loss, grad.ravel(), p
1.15 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
1.15 logistic.py(339):     n_classes = Y.shape[1]
1.15 logistic.py(340):     n_features = X.shape[1]
1.15 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
1.15 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
1.15 logistic.py(343):                     dtype=X.dtype)
1.15 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
1.15 logistic.py(282):     n_classes = Y.shape[1]
1.15 logistic.py(283):     n_features = X.shape[1]
1.15 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
1.15 logistic.py(285):     w = w.reshape(n_classes, -1)
1.15 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
1.15 logistic.py(287):     if fit_intercept:
1.15 logistic.py(288):         intercept = w[:, -1]
1.15 logistic.py(289):         w = w[:, :-1]
1.15 logistic.py(292):     p = safe_sparse_dot(X, w.T)
1.15 logistic.py(293):     p += intercept
1.15 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
1.15 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
1.15 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
1.15 logistic.py(297):     p = np.exp(p, p)
1.15 logistic.py(298):     return loss, p, w
1.15 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
1.15 logistic.py(346):     diff = sample_weight * (p - Y)
1.15 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
1.15 logistic.py(348):     grad[:, :n_features] += alpha * w
1.15 logistic.py(349):     if fit_intercept:
1.15 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
1.15 logistic.py(351):     return loss, grad.ravel(), p
1.15 logistic.py(718):             if info["warnflag"] == 1:
1.15 logistic.py(723):             n_iter_i = min(info['nit'], max_iter)
1.15 logistic.py(760):         if multi_class == 'multinomial':
1.15 logistic.py(761):             multi_w0 = np.reshape(w0, (classes.size, -1))
1.15 logistic.py(762):             if classes.size == 2:
1.15 logistic.py(764):             coefs.append(multi_w0)
1.15 logistic.py(768):         n_iter[i] = n_iter_i
1.15 logistic.py(712):     for i, C in enumerate(Cs):
1.15 logistic.py(770):     return coefs, np.array(Cs), n_iter
1.15 logistic.py(925):     log_reg = LogisticRegression(multi_class=multi_class)
1.15 logistic.py(1171):         self.penalty = penalty
1.15 logistic.py(1172):         self.dual = dual
1.15 logistic.py(1173):         self.tol = tol
1.15 logistic.py(1174):         self.C = C
1.15 logistic.py(1175):         self.fit_intercept = fit_intercept
1.15 logistic.py(1176):         self.intercept_scaling = intercept_scaling
1.15 logistic.py(1177):         self.class_weight = class_weight
1.15 logistic.py(1178):         self.random_state = random_state
1.15 logistic.py(1179):         self.solver = solver
1.15 logistic.py(1180):         self.max_iter = max_iter
1.15 logistic.py(1181):         self.multi_class = multi_class
1.15 logistic.py(1182):         self.verbose = verbose
1.15 logistic.py(1183):         self.warm_start = warm_start
1.15 logistic.py(1184):         self.n_jobs = n_jobs
1.15 logistic.py(928):     if multi_class == 'ovr':
1.15 logistic.py(930):     elif multi_class == 'multinomial':
1.15 logistic.py(931):         log_reg.classes_ = np.unique(y_train)
1.15 logistic.py(936):     if pos_class is not None:
1.15 logistic.py(941):     scores = list()
1.15 logistic.py(943):     if isinstance(scoring, six.string_types):
1.15 logistic.py(945):     for w in coefs:
1.15 logistic.py(946):         if multi_class == 'ovr':
1.15 logistic.py(948):         if fit_intercept:
1.15 logistic.py(949):             log_reg.coef_ = w[:, :-1]
1.15 logistic.py(950):             log_reg.intercept_ = w[:, -1]
1.15 logistic.py(955):         if scoring is None:
1.15 logistic.py(956):             scores.append(log_reg.score(X_test, y_test))
1.15 logistic.py(945):     for w in coefs:
1.15 logistic.py(946):         if multi_class == 'ovr':
1.15 logistic.py(948):         if fit_intercept:
1.15 logistic.py(949):             log_reg.coef_ = w[:, :-1]
1.15 logistic.py(950):             log_reg.intercept_ = w[:, -1]
1.15 logistic.py(955):         if scoring is None:
1.15 logistic.py(956):             scores.append(log_reg.score(X_test, y_test))
1.16 logistic.py(945):     for w in coefs:
1.16 logistic.py(946):         if multi_class == 'ovr':
1.16 logistic.py(948):         if fit_intercept:
1.16 logistic.py(949):             log_reg.coef_ = w[:, :-1]
1.16 logistic.py(950):             log_reg.intercept_ = w[:, -1]
1.16 logistic.py(955):         if scoring is None:
1.16 logistic.py(956):             scores.append(log_reg.score(X_test, y_test))
1.16 logistic.py(945):     for w in coefs:
1.16 logistic.py(946):         if multi_class == 'ovr':
1.16 logistic.py(948):         if fit_intercept:
1.16 logistic.py(949):             log_reg.coef_ = w[:, :-1]
1.16 logistic.py(950):             log_reg.intercept_ = w[:, -1]
1.16 logistic.py(955):         if scoring is None:
1.16 logistic.py(956):             scores.append(log_reg.score(X_test, y_test))
1.16 logistic.py(945):     for w in coefs:
1.16 logistic.py(946):         if multi_class == 'ovr':
1.16 logistic.py(948):         if fit_intercept:
1.16 logistic.py(949):             log_reg.coef_ = w[:, :-1]
1.16 logistic.py(950):             log_reg.intercept_ = w[:, -1]
1.16 logistic.py(955):         if scoring is None:
1.16 logistic.py(956):             scores.append(log_reg.score(X_test, y_test))
1.16 logistic.py(945):     for w in coefs:
1.16 logistic.py(946):         if multi_class == 'ovr':
1.16 logistic.py(948):         if fit_intercept:
1.16 logistic.py(949):             log_reg.coef_ = w[:, :-1]
1.16 logistic.py(950):             log_reg.intercept_ = w[:, -1]
1.16 logistic.py(955):         if scoring is None:
1.16 logistic.py(956):             scores.append(log_reg.score(X_test, y_test))
1.16 logistic.py(945):     for w in coefs:
1.16 logistic.py(946):         if multi_class == 'ovr':
1.16 logistic.py(948):         if fit_intercept:
1.16 logistic.py(949):             log_reg.coef_ = w[:, :-1]
1.16 logistic.py(950):             log_reg.intercept_ = w[:, -1]
1.16 logistic.py(955):         if scoring is None:
1.16 logistic.py(956):             scores.append(log_reg.score(X_test, y_test))
1.16 logistic.py(945):     for w in coefs:
1.16 logistic.py(946):         if multi_class == 'ovr':
1.16 logistic.py(948):         if fit_intercept:
1.16 logistic.py(949):             log_reg.coef_ = w[:, :-1]
1.16 logistic.py(950):             log_reg.intercept_ = w[:, -1]
1.16 logistic.py(955):         if scoring is None:
1.16 logistic.py(956):             scores.append(log_reg.score(X_test, y_test))
1.16 logistic.py(945):     for w in coefs:
1.16 logistic.py(946):         if multi_class == 'ovr':
1.16 logistic.py(948):         if fit_intercept:
1.16 logistic.py(949):             log_reg.coef_ = w[:, :-1]
1.16 logistic.py(950):             log_reg.intercept_ = w[:, -1]
1.16 logistic.py(955):         if scoring is None:
1.16 logistic.py(956):             scores.append(log_reg.score(X_test, y_test))
1.16 logistic.py(945):     for w in coefs:
1.16 logistic.py(946):         if multi_class == 'ovr':
1.16 logistic.py(948):         if fit_intercept:
1.16 logistic.py(949):             log_reg.coef_ = w[:, :-1]
1.16 logistic.py(950):             log_reg.intercept_ = w[:, -1]
1.16 logistic.py(955):         if scoring is None:
1.16 logistic.py(956):             scores.append(log_reg.score(X_test, y_test))
1.16 logistic.py(945):     for w in coefs:
1.16 logistic.py(959):     return coefs, Cs, np.array(scores), n_iter
1.16 logistic.py(1703):             for train, test in folds)
1.16 logistic.py(903):     _check_solver_option(solver, multi_class, penalty, dual)
1.16 logistic.py(428):     if solver not in ['liblinear', 'newton-cg', 'lbfgs', 'sag', 'saga']:
1.16 logistic.py(433):     if multi_class not in ['multinomial', 'ovr']:
1.16 logistic.py(437):     if multi_class == 'multinomial' and solver == 'liblinear':
1.16 logistic.py(441):     if solver not in ['liblinear', 'saga']:
1.16 logistic.py(442):         if penalty != 'l2':
1.16 logistic.py(445):     if solver != 'liblinear':
1.16 logistic.py(446):         if dual:
1.16 logistic.py(905):     X_train = X[train]
1.16 logistic.py(906):     X_test = X[test]
1.16 logistic.py(907):     y_train = y[train]
1.16 logistic.py(908):     y_test = y[test]
1.16 logistic.py(910):     if sample_weight is not None:
1.16 logistic.py(916):     coefs, Cs, n_iter = logistic_regression_path(
1.16 logistic.py(917):         X_train, y_train, Cs=Cs, fit_intercept=fit_intercept,
1.16 logistic.py(918):         solver=solver, max_iter=max_iter, class_weight=class_weight,
1.16 logistic.py(919):         pos_class=pos_class, multi_class=multi_class,
1.16 logistic.py(920):         tol=tol, verbose=verbose, dual=dual, penalty=penalty,
1.16 logistic.py(921):         intercept_scaling=intercept_scaling, random_state=random_state,
1.16 logistic.py(922):         check_input=False, max_squared_sum=max_squared_sum,
1.16 logistic.py(923):         sample_weight=sample_weight)
1.16 logistic.py(591):     if isinstance(Cs, numbers.Integral):
1.16 logistic.py(592):         Cs = np.logspace(-4, 4, Cs)
1.16 logistic.py(594):     _check_solver_option(solver, multi_class, penalty, dual)
1.16 logistic.py(428):     if solver not in ['liblinear', 'newton-cg', 'lbfgs', 'sag', 'saga']:
1.16 logistic.py(433):     if multi_class not in ['multinomial', 'ovr']:
1.16 logistic.py(437):     if multi_class == 'multinomial' and solver == 'liblinear':
1.16 logistic.py(441):     if solver not in ['liblinear', 'saga']:
1.16 logistic.py(442):         if penalty != 'l2':
1.16 logistic.py(445):     if solver != 'liblinear':
1.16 logistic.py(446):         if dual:
1.16 logistic.py(597):     if check_input:
1.16 logistic.py(602):     _, n_features = X.shape
1.16 logistic.py(603):     classes = np.unique(y)
1.16 logistic.py(604):     random_state = check_random_state(random_state)
1.16 logistic.py(606):     if pos_class is None and multi_class != 'multinomial':
1.16 logistic.py(615):     if sample_weight is not None:
1.16 logistic.py(619):         sample_weight = np.ones(X.shape[0], dtype=X.dtype)
1.16 logistic.py(624):     le = LabelEncoder()
1.16 logistic.py(625):     if isinstance(class_weight, dict) or multi_class == 'multinomial':
1.16 logistic.py(626):         class_weight_ = compute_class_weight(class_weight, classes, y)
1.16 logistic.py(627):         sample_weight *= class_weight_[le.fit_transform(y)]
1.16 logistic.py(631):     if multi_class == 'ovr':
1.16 logistic.py(645):         if solver not in ['sag', 'saga']:
1.16 logistic.py(646):             lbin = LabelBinarizer()
1.16 logistic.py(647):             Y_multi = lbin.fit_transform(y)
1.16 logistic.py(648):             if Y_multi.shape[1] == 1:
1.16 logistic.py(655):         w0 = np.zeros((classes.size, n_features + int(fit_intercept)),
1.16 logistic.py(656):                       order='F', dtype=X.dtype)
1.16 logistic.py(658):     if coef is not None:
1.16 logistic.py(688):     if multi_class == 'multinomial':
1.16 logistic.py(690):         if solver in ['lbfgs', 'newton-cg']:
1.16 logistic.py(691):             w0 = w0.ravel()
1.16 logistic.py(692):         target = Y_multi
1.16 logistic.py(693):         if solver == 'lbfgs':
1.16 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
1.16 logistic.py(699):         warm_start_sag = {'coef': w0.T}
1.16 logistic.py(710):     coefs = list()
1.16 logistic.py(711):     n_iter = np.zeros(len(Cs), dtype=np.int32)
1.16 logistic.py(712):     for i, C in enumerate(Cs):
1.16 logistic.py(713):         if solver == 'lbfgs':
1.16 logistic.py(714):             w0, loss, info = optimize.fmin_l_bfgs_b(
1.16 logistic.py(715):                 func, w0, fprime=None,
1.16 logistic.py(716):                 args=(X, target, 1. / C, sample_weight),
1.16 logistic.py(717):                 iprint=(verbose > 0) - 1, pgtol=tol, maxiter=max_iter)
1.16 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
1.16 logistic.py(339):     n_classes = Y.shape[1]
1.16 logistic.py(340):     n_features = X.shape[1]
1.16 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
1.16 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
1.16 logistic.py(343):                     dtype=X.dtype)
1.16 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
1.16 logistic.py(282):     n_classes = Y.shape[1]
1.16 logistic.py(283):     n_features = X.shape[1]
1.16 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
1.16 logistic.py(285):     w = w.reshape(n_classes, -1)
1.16 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
1.16 logistic.py(287):     if fit_intercept:
1.16 logistic.py(288):         intercept = w[:, -1]
1.16 logistic.py(289):         w = w[:, :-1]
1.16 logistic.py(292):     p = safe_sparse_dot(X, w.T)
1.16 logistic.py(293):     p += intercept
1.16 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
1.16 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
1.16 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
1.16 logistic.py(297):     p = np.exp(p, p)
1.16 logistic.py(298):     return loss, p, w
1.16 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
1.16 logistic.py(346):     diff = sample_weight * (p - Y)
1.16 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
1.16 logistic.py(348):     grad[:, :n_features] += alpha * w
1.16 logistic.py(349):     if fit_intercept:
1.16 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
1.16 logistic.py(351):     return loss, grad.ravel(), p
1.16 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
1.16 logistic.py(339):     n_classes = Y.shape[1]
1.16 logistic.py(340):     n_features = X.shape[1]
1.16 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
1.16 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
1.16 logistic.py(343):                     dtype=X.dtype)
1.16 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
1.16 logistic.py(282):     n_classes = Y.shape[1]
1.16 logistic.py(283):     n_features = X.shape[1]
1.16 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
1.16 logistic.py(285):     w = w.reshape(n_classes, -1)
1.16 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
1.16 logistic.py(287):     if fit_intercept:
1.16 logistic.py(288):         intercept = w[:, -1]
1.16 logistic.py(289):         w = w[:, :-1]
1.16 logistic.py(292):     p = safe_sparse_dot(X, w.T)
1.16 logistic.py(293):     p += intercept
1.16 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
1.16 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
1.16 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
1.16 logistic.py(297):     p = np.exp(p, p)
1.16 logistic.py(298):     return loss, p, w
1.16 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
1.16 logistic.py(346):     diff = sample_weight * (p - Y)
1.16 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
1.16 logistic.py(348):     grad[:, :n_features] += alpha * w
1.16 logistic.py(349):     if fit_intercept:
1.16 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
1.16 logistic.py(351):     return loss, grad.ravel(), p
1.16 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
1.16 logistic.py(339):     n_classes = Y.shape[1]
1.16 logistic.py(340):     n_features = X.shape[1]
1.16 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
1.16 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
1.16 logistic.py(343):                     dtype=X.dtype)
1.16 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
1.16 logistic.py(282):     n_classes = Y.shape[1]
1.16 logistic.py(283):     n_features = X.shape[1]
1.16 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
1.16 logistic.py(285):     w = w.reshape(n_classes, -1)
1.16 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
1.16 logistic.py(287):     if fit_intercept:
1.16 logistic.py(288):         intercept = w[:, -1]
1.16 logistic.py(289):         w = w[:, :-1]
1.16 logistic.py(292):     p = safe_sparse_dot(X, w.T)
1.16 logistic.py(293):     p += intercept
1.16 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
1.16 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
1.16 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
1.16 logistic.py(297):     p = np.exp(p, p)
1.16 logistic.py(298):     return loss, p, w
1.16 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
1.16 logistic.py(346):     diff = sample_weight * (p - Y)
1.16 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
1.16 logistic.py(348):     grad[:, :n_features] += alpha * w
1.16 logistic.py(349):     if fit_intercept:
1.16 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
1.16 logistic.py(351):     return loss, grad.ravel(), p
1.16 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
1.16 logistic.py(339):     n_classes = Y.shape[1]
1.16 logistic.py(340):     n_features = X.shape[1]
1.16 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
1.16 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
1.16 logistic.py(343):                     dtype=X.dtype)
1.16 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
1.16 logistic.py(282):     n_classes = Y.shape[1]
1.16 logistic.py(283):     n_features = X.shape[1]
1.16 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
1.16 logistic.py(285):     w = w.reshape(n_classes, -1)
1.16 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
1.16 logistic.py(287):     if fit_intercept:
1.16 logistic.py(288):         intercept = w[:, -1]
1.16 logistic.py(289):         w = w[:, :-1]
1.16 logistic.py(292):     p = safe_sparse_dot(X, w.T)
1.16 logistic.py(293):     p += intercept
1.16 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
1.16 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
1.16 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
1.16 logistic.py(297):     p = np.exp(p, p)
1.16 logistic.py(298):     return loss, p, w
1.16 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
1.16 logistic.py(346):     diff = sample_weight * (p - Y)
1.16 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
1.16 logistic.py(348):     grad[:, :n_features] += alpha * w
1.16 logistic.py(349):     if fit_intercept:
1.16 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
1.16 logistic.py(351):     return loss, grad.ravel(), p
1.16 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
1.16 logistic.py(339):     n_classes = Y.shape[1]
1.16 logistic.py(340):     n_features = X.shape[1]
1.16 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
1.16 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
1.16 logistic.py(343):                     dtype=X.dtype)
1.16 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
1.16 logistic.py(282):     n_classes = Y.shape[1]
1.16 logistic.py(283):     n_features = X.shape[1]
1.16 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
1.16 logistic.py(285):     w = w.reshape(n_classes, -1)
1.16 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
1.16 logistic.py(287):     if fit_intercept:
1.16 logistic.py(288):         intercept = w[:, -1]
1.16 logistic.py(289):         w = w[:, :-1]
1.16 logistic.py(292):     p = safe_sparse_dot(X, w.T)
1.16 logistic.py(293):     p += intercept
1.16 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
1.16 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
1.16 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
1.16 logistic.py(297):     p = np.exp(p, p)
1.16 logistic.py(298):     return loss, p, w
1.16 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
1.16 logistic.py(346):     diff = sample_weight * (p - Y)
1.16 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
1.16 logistic.py(348):     grad[:, :n_features] += alpha * w
1.16 logistic.py(349):     if fit_intercept:
1.16 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
1.16 logistic.py(351):     return loss, grad.ravel(), p
1.16 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
1.16 logistic.py(339):     n_classes = Y.shape[1]
1.16 logistic.py(340):     n_features = X.shape[1]
1.16 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
1.16 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
1.16 logistic.py(343):                     dtype=X.dtype)
1.16 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
1.16 logistic.py(282):     n_classes = Y.shape[1]
1.16 logistic.py(283):     n_features = X.shape[1]
1.16 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
1.16 logistic.py(285):     w = w.reshape(n_classes, -1)
1.16 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
1.16 logistic.py(287):     if fit_intercept:
1.16 logistic.py(288):         intercept = w[:, -1]
1.16 logistic.py(289):         w = w[:, :-1]
1.16 logistic.py(292):     p = safe_sparse_dot(X, w.T)
1.16 logistic.py(293):     p += intercept
1.16 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
1.16 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
1.16 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
1.16 logistic.py(297):     p = np.exp(p, p)
1.16 logistic.py(298):     return loss, p, w
1.16 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
1.16 logistic.py(346):     diff = sample_weight * (p - Y)
1.16 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
1.16 logistic.py(348):     grad[:, :n_features] += alpha * w
1.16 logistic.py(349):     if fit_intercept:
1.16 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
1.16 logistic.py(351):     return loss, grad.ravel(), p
1.16 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
1.16 logistic.py(339):     n_classes = Y.shape[1]
1.16 logistic.py(340):     n_features = X.shape[1]
1.16 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
1.16 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
1.16 logistic.py(343):                     dtype=X.dtype)
1.16 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
1.16 logistic.py(282):     n_classes = Y.shape[1]
1.16 logistic.py(283):     n_features = X.shape[1]
1.16 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
1.16 logistic.py(285):     w = w.reshape(n_classes, -1)
1.16 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
1.16 logistic.py(287):     if fit_intercept:
1.16 logistic.py(288):         intercept = w[:, -1]
1.16 logistic.py(289):         w = w[:, :-1]
1.16 logistic.py(292):     p = safe_sparse_dot(X, w.T)
1.16 logistic.py(293):     p += intercept
1.16 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
1.16 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
1.16 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
1.16 logistic.py(297):     p = np.exp(p, p)
1.16 logistic.py(298):     return loss, p, w
1.16 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
1.16 logistic.py(346):     diff = sample_weight * (p - Y)
1.16 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
1.16 logistic.py(348):     grad[:, :n_features] += alpha * w
1.16 logistic.py(349):     if fit_intercept:
1.16 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
1.16 logistic.py(351):     return loss, grad.ravel(), p
1.16 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
1.16 logistic.py(339):     n_classes = Y.shape[1]
1.16 logistic.py(340):     n_features = X.shape[1]
1.16 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
1.16 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
1.16 logistic.py(343):                     dtype=X.dtype)
1.16 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
1.16 logistic.py(282):     n_classes = Y.shape[1]
1.16 logistic.py(283):     n_features = X.shape[1]
1.16 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
1.16 logistic.py(285):     w = w.reshape(n_classes, -1)
1.16 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
1.16 logistic.py(287):     if fit_intercept:
1.16 logistic.py(288):         intercept = w[:, -1]
1.16 logistic.py(289):         w = w[:, :-1]
1.16 logistic.py(292):     p = safe_sparse_dot(X, w.T)
1.16 logistic.py(293):     p += intercept
1.16 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
1.16 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
1.16 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
1.16 logistic.py(297):     p = np.exp(p, p)
1.16 logistic.py(298):     return loss, p, w
1.16 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
1.16 logistic.py(346):     diff = sample_weight * (p - Y)
1.16 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
1.16 logistic.py(348):     grad[:, :n_features] += alpha * w
1.16 logistic.py(349):     if fit_intercept:
1.16 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
1.16 logistic.py(351):     return loss, grad.ravel(), p
1.16 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
1.16 logistic.py(339):     n_classes = Y.shape[1]
1.16 logistic.py(340):     n_features = X.shape[1]
1.16 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
1.16 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
1.16 logistic.py(343):                     dtype=X.dtype)
1.16 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
1.16 logistic.py(282):     n_classes = Y.shape[1]
1.16 logistic.py(283):     n_features = X.shape[1]
1.16 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
1.16 logistic.py(285):     w = w.reshape(n_classes, -1)
1.16 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
1.16 logistic.py(287):     if fit_intercept:
1.16 logistic.py(288):         intercept = w[:, -1]
1.16 logistic.py(289):         w = w[:, :-1]
1.16 logistic.py(292):     p = safe_sparse_dot(X, w.T)
1.16 logistic.py(293):     p += intercept
1.16 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
1.16 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
1.16 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
1.16 logistic.py(297):     p = np.exp(p, p)
1.16 logistic.py(298):     return loss, p, w
1.16 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
1.16 logistic.py(346):     diff = sample_weight * (p - Y)
1.16 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
1.16 logistic.py(348):     grad[:, :n_features] += alpha * w
1.16 logistic.py(349):     if fit_intercept:
1.16 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
1.16 logistic.py(351):     return loss, grad.ravel(), p
1.16 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
1.16 logistic.py(339):     n_classes = Y.shape[1]
1.16 logistic.py(340):     n_features = X.shape[1]
1.16 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
1.16 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
1.16 logistic.py(343):                     dtype=X.dtype)
1.16 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
1.16 logistic.py(282):     n_classes = Y.shape[1]
1.16 logistic.py(283):     n_features = X.shape[1]
1.16 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
1.16 logistic.py(285):     w = w.reshape(n_classes, -1)
1.16 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
1.16 logistic.py(287):     if fit_intercept:
1.16 logistic.py(288):         intercept = w[:, -1]
1.16 logistic.py(289):         w = w[:, :-1]
1.16 logistic.py(292):     p = safe_sparse_dot(X, w.T)
1.16 logistic.py(293):     p += intercept
1.16 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
1.16 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
1.16 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
1.16 logistic.py(297):     p = np.exp(p, p)
1.16 logistic.py(298):     return loss, p, w
1.16 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
1.16 logistic.py(346):     diff = sample_weight * (p - Y)
1.17 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
1.17 logistic.py(348):     grad[:, :n_features] += alpha * w
1.17 logistic.py(349):     if fit_intercept:
1.17 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
1.17 logistic.py(351):     return loss, grad.ravel(), p
1.17 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
1.17 logistic.py(339):     n_classes = Y.shape[1]
1.17 logistic.py(340):     n_features = X.shape[1]
1.17 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
1.17 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
1.17 logistic.py(343):                     dtype=X.dtype)
1.17 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
1.17 logistic.py(282):     n_classes = Y.shape[1]
1.17 logistic.py(283):     n_features = X.shape[1]
1.17 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
1.17 logistic.py(285):     w = w.reshape(n_classes, -1)
1.17 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
1.17 logistic.py(287):     if fit_intercept:
1.17 logistic.py(288):         intercept = w[:, -1]
1.17 logistic.py(289):         w = w[:, :-1]
1.17 logistic.py(292):     p = safe_sparse_dot(X, w.T)
1.17 logistic.py(293):     p += intercept
1.17 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
1.17 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
1.17 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
1.17 logistic.py(297):     p = np.exp(p, p)
1.17 logistic.py(298):     return loss, p, w
1.17 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
1.17 logistic.py(346):     diff = sample_weight * (p - Y)
1.17 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
1.17 logistic.py(348):     grad[:, :n_features] += alpha * w
1.17 logistic.py(349):     if fit_intercept:
1.17 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
1.17 logistic.py(351):     return loss, grad.ravel(), p
1.17 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
1.17 logistic.py(339):     n_classes = Y.shape[1]
1.17 logistic.py(340):     n_features = X.shape[1]
1.17 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
1.17 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
1.17 logistic.py(343):                     dtype=X.dtype)
1.17 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
1.17 logistic.py(282):     n_classes = Y.shape[1]
1.17 logistic.py(283):     n_features = X.shape[1]
1.17 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
1.17 logistic.py(285):     w = w.reshape(n_classes, -1)
1.17 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
1.17 logistic.py(287):     if fit_intercept:
1.17 logistic.py(288):         intercept = w[:, -1]
1.17 logistic.py(289):         w = w[:, :-1]
1.17 logistic.py(292):     p = safe_sparse_dot(X, w.T)
1.17 logistic.py(293):     p += intercept
1.17 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
1.17 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
1.17 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
1.17 logistic.py(297):     p = np.exp(p, p)
1.17 logistic.py(298):     return loss, p, w
1.17 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
1.17 logistic.py(346):     diff = sample_weight * (p - Y)
1.17 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
1.17 logistic.py(348):     grad[:, :n_features] += alpha * w
1.17 logistic.py(349):     if fit_intercept:
1.17 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
1.17 logistic.py(351):     return loss, grad.ravel(), p
1.17 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
1.17 logistic.py(339):     n_classes = Y.shape[1]
1.17 logistic.py(340):     n_features = X.shape[1]
1.17 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
1.17 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
1.17 logistic.py(343):                     dtype=X.dtype)
1.17 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
1.17 logistic.py(282):     n_classes = Y.shape[1]
1.17 logistic.py(283):     n_features = X.shape[1]
1.17 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
1.17 logistic.py(285):     w = w.reshape(n_classes, -1)
1.17 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
1.17 logistic.py(287):     if fit_intercept:
1.17 logistic.py(288):         intercept = w[:, -1]
1.17 logistic.py(289):         w = w[:, :-1]
1.17 logistic.py(292):     p = safe_sparse_dot(X, w.T)
1.17 logistic.py(293):     p += intercept
1.17 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
1.17 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
1.17 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
1.17 logistic.py(297):     p = np.exp(p, p)
1.17 logistic.py(298):     return loss, p, w
1.17 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
1.17 logistic.py(346):     diff = sample_weight * (p - Y)
1.17 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
1.17 logistic.py(348):     grad[:, :n_features] += alpha * w
1.17 logistic.py(349):     if fit_intercept:
1.17 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
1.17 logistic.py(351):     return loss, grad.ravel(), p
1.17 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
1.17 logistic.py(339):     n_classes = Y.shape[1]
1.17 logistic.py(340):     n_features = X.shape[1]
1.17 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
1.17 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
1.17 logistic.py(343):                     dtype=X.dtype)
1.17 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
1.17 logistic.py(282):     n_classes = Y.shape[1]
1.17 logistic.py(283):     n_features = X.shape[1]
1.17 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
1.17 logistic.py(285):     w = w.reshape(n_classes, -1)
1.17 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
1.17 logistic.py(287):     if fit_intercept:
1.17 logistic.py(288):         intercept = w[:, -1]
1.17 logistic.py(289):         w = w[:, :-1]
1.17 logistic.py(292):     p = safe_sparse_dot(X, w.T)
1.17 logistic.py(293):     p += intercept
1.17 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
1.17 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
1.17 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
1.17 logistic.py(297):     p = np.exp(p, p)
1.17 logistic.py(298):     return loss, p, w
1.17 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
1.17 logistic.py(346):     diff = sample_weight * (p - Y)
1.17 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
1.17 logistic.py(348):     grad[:, :n_features] += alpha * w
1.17 logistic.py(349):     if fit_intercept:
1.17 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
1.17 logistic.py(351):     return loss, grad.ravel(), p
1.17 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
1.17 logistic.py(339):     n_classes = Y.shape[1]
1.17 logistic.py(340):     n_features = X.shape[1]
1.17 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
1.17 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
1.17 logistic.py(343):                     dtype=X.dtype)
1.17 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
1.17 logistic.py(282):     n_classes = Y.shape[1]
1.17 logistic.py(283):     n_features = X.shape[1]
1.17 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
1.17 logistic.py(285):     w = w.reshape(n_classes, -1)
1.17 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
1.17 logistic.py(287):     if fit_intercept:
1.17 logistic.py(288):         intercept = w[:, -1]
1.17 logistic.py(289):         w = w[:, :-1]
1.17 logistic.py(292):     p = safe_sparse_dot(X, w.T)
1.17 logistic.py(293):     p += intercept
1.17 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
1.17 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
1.17 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
1.17 logistic.py(297):     p = np.exp(p, p)
1.17 logistic.py(298):     return loss, p, w
1.17 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
1.17 logistic.py(346):     diff = sample_weight * (p - Y)
1.17 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
1.17 logistic.py(348):     grad[:, :n_features] += alpha * w
1.17 logistic.py(349):     if fit_intercept:
1.17 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
1.17 logistic.py(351):     return loss, grad.ravel(), p
1.17 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
1.17 logistic.py(339):     n_classes = Y.shape[1]
1.17 logistic.py(340):     n_features = X.shape[1]
1.17 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
1.17 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
1.17 logistic.py(343):                     dtype=X.dtype)
1.17 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
1.17 logistic.py(282):     n_classes = Y.shape[1]
1.17 logistic.py(283):     n_features = X.shape[1]
1.17 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
1.17 logistic.py(285):     w = w.reshape(n_classes, -1)
1.17 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
1.17 logistic.py(287):     if fit_intercept:
1.17 logistic.py(288):         intercept = w[:, -1]
1.17 logistic.py(289):         w = w[:, :-1]
1.17 logistic.py(292):     p = safe_sparse_dot(X, w.T)
1.17 logistic.py(293):     p += intercept
1.17 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
1.17 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
1.17 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
1.17 logistic.py(297):     p = np.exp(p, p)
1.17 logistic.py(298):     return loss, p, w
1.17 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
1.17 logistic.py(346):     diff = sample_weight * (p - Y)
1.17 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
1.17 logistic.py(348):     grad[:, :n_features] += alpha * w
1.17 logistic.py(349):     if fit_intercept:
1.17 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
1.17 logistic.py(351):     return loss, grad.ravel(), p
1.17 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
1.17 logistic.py(339):     n_classes = Y.shape[1]
1.17 logistic.py(340):     n_features = X.shape[1]
1.17 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
1.17 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
1.17 logistic.py(343):                     dtype=X.dtype)
1.17 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
1.17 logistic.py(282):     n_classes = Y.shape[1]
1.17 logistic.py(283):     n_features = X.shape[1]
1.17 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
1.17 logistic.py(285):     w = w.reshape(n_classes, -1)
1.17 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
1.17 logistic.py(287):     if fit_intercept:
1.17 logistic.py(288):         intercept = w[:, -1]
1.17 logistic.py(289):         w = w[:, :-1]
1.17 logistic.py(292):     p = safe_sparse_dot(X, w.T)
1.17 logistic.py(293):     p += intercept
1.17 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
1.17 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
1.17 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
1.17 logistic.py(297):     p = np.exp(p, p)
1.17 logistic.py(298):     return loss, p, w
1.17 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
1.17 logistic.py(346):     diff = sample_weight * (p - Y)
1.17 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
1.17 logistic.py(348):     grad[:, :n_features] += alpha * w
1.17 logistic.py(349):     if fit_intercept:
1.17 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
1.17 logistic.py(351):     return loss, grad.ravel(), p
1.17 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
1.17 logistic.py(339):     n_classes = Y.shape[1]
1.17 logistic.py(340):     n_features = X.shape[1]
1.17 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
1.17 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
1.17 logistic.py(343):                     dtype=X.dtype)
1.17 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
1.17 logistic.py(282):     n_classes = Y.shape[1]
1.17 logistic.py(283):     n_features = X.shape[1]
1.17 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
1.17 logistic.py(285):     w = w.reshape(n_classes, -1)
1.17 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
1.17 logistic.py(287):     if fit_intercept:
1.17 logistic.py(288):         intercept = w[:, -1]
1.17 logistic.py(289):         w = w[:, :-1]
1.17 logistic.py(292):     p = safe_sparse_dot(X, w.T)
1.17 logistic.py(293):     p += intercept
1.17 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
1.17 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
1.17 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
1.17 logistic.py(297):     p = np.exp(p, p)
1.17 logistic.py(298):     return loss, p, w
1.17 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
1.17 logistic.py(346):     diff = sample_weight * (p - Y)
1.17 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
1.17 logistic.py(348):     grad[:, :n_features] += alpha * w
1.17 logistic.py(349):     if fit_intercept:
1.17 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
1.17 logistic.py(351):     return loss, grad.ravel(), p
1.17 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
1.17 logistic.py(339):     n_classes = Y.shape[1]
1.17 logistic.py(340):     n_features = X.shape[1]
1.17 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
1.17 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
1.17 logistic.py(343):                     dtype=X.dtype)
1.17 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
1.17 logistic.py(282):     n_classes = Y.shape[1]
1.17 logistic.py(283):     n_features = X.shape[1]
1.17 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
1.17 logistic.py(285):     w = w.reshape(n_classes, -1)
1.17 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
1.17 logistic.py(287):     if fit_intercept:
1.17 logistic.py(288):         intercept = w[:, -1]
1.17 logistic.py(289):         w = w[:, :-1]
1.17 logistic.py(292):     p = safe_sparse_dot(X, w.T)
1.17 logistic.py(293):     p += intercept
1.17 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
1.17 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
1.17 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
1.17 logistic.py(297):     p = np.exp(p, p)
1.17 logistic.py(298):     return loss, p, w
1.17 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
1.17 logistic.py(346):     diff = sample_weight * (p - Y)
1.17 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
1.17 logistic.py(348):     grad[:, :n_features] += alpha * w
1.17 logistic.py(349):     if fit_intercept:
1.17 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
1.17 logistic.py(351):     return loss, grad.ravel(), p
1.17 logistic.py(718):             if info["warnflag"] == 1:
1.17 logistic.py(723):             n_iter_i = min(info['nit'], max_iter)
1.17 logistic.py(760):         if multi_class == 'multinomial':
1.17 logistic.py(761):             multi_w0 = np.reshape(w0, (classes.size, -1))
1.17 logistic.py(762):             if classes.size == 2:
1.17 logistic.py(764):             coefs.append(multi_w0)
1.17 logistic.py(768):         n_iter[i] = n_iter_i
1.17 logistic.py(712):     for i, C in enumerate(Cs):
1.17 logistic.py(713):         if solver == 'lbfgs':
1.17 logistic.py(714):             w0, loss, info = optimize.fmin_l_bfgs_b(
1.17 logistic.py(715):                 func, w0, fprime=None,
1.17 logistic.py(716):                 args=(X, target, 1. / C, sample_weight),
1.17 logistic.py(717):                 iprint=(verbose > 0) - 1, pgtol=tol, maxiter=max_iter)
1.17 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
1.17 logistic.py(339):     n_classes = Y.shape[1]
1.17 logistic.py(340):     n_features = X.shape[1]
1.17 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
1.17 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
1.17 logistic.py(343):                     dtype=X.dtype)
1.17 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
1.17 logistic.py(282):     n_classes = Y.shape[1]
1.17 logistic.py(283):     n_features = X.shape[1]
1.17 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
1.17 logistic.py(285):     w = w.reshape(n_classes, -1)
1.17 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
1.17 logistic.py(287):     if fit_intercept:
1.17 logistic.py(288):         intercept = w[:, -1]
1.17 logistic.py(289):         w = w[:, :-1]
1.17 logistic.py(292):     p = safe_sparse_dot(X, w.T)
1.17 logistic.py(293):     p += intercept
1.17 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
1.17 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
1.17 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
1.17 logistic.py(297):     p = np.exp(p, p)
1.17 logistic.py(298):     return loss, p, w
1.17 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
1.17 logistic.py(346):     diff = sample_weight * (p - Y)
1.17 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
1.17 logistic.py(348):     grad[:, :n_features] += alpha * w
1.17 logistic.py(349):     if fit_intercept:
1.17 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
1.17 logistic.py(351):     return loss, grad.ravel(), p
1.17 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
1.17 logistic.py(339):     n_classes = Y.shape[1]
1.17 logistic.py(340):     n_features = X.shape[1]
1.17 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
1.17 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
1.17 logistic.py(343):                     dtype=X.dtype)
1.17 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
1.17 logistic.py(282):     n_classes = Y.shape[1]
1.17 logistic.py(283):     n_features = X.shape[1]
1.17 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
1.17 logistic.py(285):     w = w.reshape(n_classes, -1)
1.17 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
1.17 logistic.py(287):     if fit_intercept:
1.17 logistic.py(288):         intercept = w[:, -1]
1.17 logistic.py(289):         w = w[:, :-1]
1.17 logistic.py(292):     p = safe_sparse_dot(X, w.T)
1.17 logistic.py(293):     p += intercept
1.17 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
1.17 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
1.17 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
1.17 logistic.py(297):     p = np.exp(p, p)
1.17 logistic.py(298):     return loss, p, w
1.17 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
1.17 logistic.py(346):     diff = sample_weight * (p - Y)
1.17 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
1.17 logistic.py(348):     grad[:, :n_features] += alpha * w
1.17 logistic.py(349):     if fit_intercept:
1.17 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
1.17 logistic.py(351):     return loss, grad.ravel(), p
1.17 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
1.17 logistic.py(339):     n_classes = Y.shape[1]
1.17 logistic.py(340):     n_features = X.shape[1]
1.17 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
1.17 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
1.17 logistic.py(343):                     dtype=X.dtype)
1.17 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
1.17 logistic.py(282):     n_classes = Y.shape[1]
1.17 logistic.py(283):     n_features = X.shape[1]
1.17 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
1.17 logistic.py(285):     w = w.reshape(n_classes, -1)
1.17 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
1.17 logistic.py(287):     if fit_intercept:
1.17 logistic.py(288):         intercept = w[:, -1]
1.17 logistic.py(289):         w = w[:, :-1]
1.17 logistic.py(292):     p = safe_sparse_dot(X, w.T)
1.17 logistic.py(293):     p += intercept
1.17 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
1.17 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
1.17 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
1.17 logistic.py(297):     p = np.exp(p, p)
1.17 logistic.py(298):     return loss, p, w
1.17 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
1.17 logistic.py(346):     diff = sample_weight * (p - Y)
1.17 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
1.17 logistic.py(348):     grad[:, :n_features] += alpha * w
1.17 logistic.py(349):     if fit_intercept:
1.17 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
1.17 logistic.py(351):     return loss, grad.ravel(), p
1.17 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
1.17 logistic.py(339):     n_classes = Y.shape[1]
1.17 logistic.py(340):     n_features = X.shape[1]
1.17 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
1.17 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
1.17 logistic.py(343):                     dtype=X.dtype)
1.17 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
1.17 logistic.py(282):     n_classes = Y.shape[1]
1.17 logistic.py(283):     n_features = X.shape[1]
1.17 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
1.17 logistic.py(285):     w = w.reshape(n_classes, -1)
1.17 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
1.17 logistic.py(287):     if fit_intercept:
1.17 logistic.py(288):         intercept = w[:, -1]
1.17 logistic.py(289):         w = w[:, :-1]
1.17 logistic.py(292):     p = safe_sparse_dot(X, w.T)
1.17 logistic.py(293):     p += intercept
1.17 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
1.17 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
1.17 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
1.17 logistic.py(297):     p = np.exp(p, p)
1.17 logistic.py(298):     return loss, p, w
1.17 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
1.17 logistic.py(346):     diff = sample_weight * (p - Y)
1.17 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
1.17 logistic.py(348):     grad[:, :n_features] += alpha * w
1.17 logistic.py(349):     if fit_intercept:
1.17 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
1.17 logistic.py(351):     return loss, grad.ravel(), p
1.17 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
1.17 logistic.py(339):     n_classes = Y.shape[1]
1.17 logistic.py(340):     n_features = X.shape[1]
1.17 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
1.17 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
1.17 logistic.py(343):                     dtype=X.dtype)
1.17 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
1.17 logistic.py(282):     n_classes = Y.shape[1]
1.17 logistic.py(283):     n_features = X.shape[1]
1.17 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
1.17 logistic.py(285):     w = w.reshape(n_classes, -1)
1.17 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
1.17 logistic.py(287):     if fit_intercept:
1.17 logistic.py(288):         intercept = w[:, -1]
1.17 logistic.py(289):         w = w[:, :-1]
1.17 logistic.py(292):     p = safe_sparse_dot(X, w.T)
1.17 logistic.py(293):     p += intercept
1.17 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
1.17 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
1.17 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
1.17 logistic.py(297):     p = np.exp(p, p)
1.17 logistic.py(298):     return loss, p, w
1.17 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
1.17 logistic.py(346):     diff = sample_weight * (p - Y)
1.17 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
1.17 logistic.py(348):     grad[:, :n_features] += alpha * w
1.17 logistic.py(349):     if fit_intercept:
1.17 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
1.17 logistic.py(351):     return loss, grad.ravel(), p
1.17 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
1.17 logistic.py(339):     n_classes = Y.shape[1]
1.17 logistic.py(340):     n_features = X.shape[1]
1.17 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
1.17 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
1.17 logistic.py(343):                     dtype=X.dtype)
1.17 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
1.17 logistic.py(282):     n_classes = Y.shape[1]
1.17 logistic.py(283):     n_features = X.shape[1]
1.17 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
1.17 logistic.py(285):     w = w.reshape(n_classes, -1)
1.17 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
1.17 logistic.py(287):     if fit_intercept:
1.17 logistic.py(288):         intercept = w[:, -1]
1.17 logistic.py(289):         w = w[:, :-1]
1.17 logistic.py(292):     p = safe_sparse_dot(X, w.T)
1.17 logistic.py(293):     p += intercept
1.17 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
1.17 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
1.17 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
1.17 logistic.py(297):     p = np.exp(p, p)
1.17 logistic.py(298):     return loss, p, w
1.17 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
1.17 logistic.py(346):     diff = sample_weight * (p - Y)
1.17 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
1.17 logistic.py(348):     grad[:, :n_features] += alpha * w
1.17 logistic.py(349):     if fit_intercept:
1.17 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
1.17 logistic.py(351):     return loss, grad.ravel(), p
1.17 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
1.17 logistic.py(339):     n_classes = Y.shape[1]
1.17 logistic.py(340):     n_features = X.shape[1]
1.17 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
1.17 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
1.17 logistic.py(343):                     dtype=X.dtype)
1.17 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
1.17 logistic.py(282):     n_classes = Y.shape[1]
1.17 logistic.py(283):     n_features = X.shape[1]
1.17 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
1.17 logistic.py(285):     w = w.reshape(n_classes, -1)
1.17 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
1.17 logistic.py(287):     if fit_intercept:
1.17 logistic.py(288):         intercept = w[:, -1]
1.17 logistic.py(289):         w = w[:, :-1]
1.17 logistic.py(292):     p = safe_sparse_dot(X, w.T)
1.17 logistic.py(293):     p += intercept
1.17 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
1.17 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
1.17 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
1.17 logistic.py(297):     p = np.exp(p, p)
1.17 logistic.py(298):     return loss, p, w
1.17 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
1.17 logistic.py(346):     diff = sample_weight * (p - Y)
1.17 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
1.17 logistic.py(348):     grad[:, :n_features] += alpha * w
1.17 logistic.py(349):     if fit_intercept:
1.17 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
1.18 logistic.py(351):     return loss, grad.ravel(), p
1.18 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
1.18 logistic.py(339):     n_classes = Y.shape[1]
1.18 logistic.py(340):     n_features = X.shape[1]
1.18 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
1.18 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
1.18 logistic.py(343):                     dtype=X.dtype)
1.18 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
1.18 logistic.py(282):     n_classes = Y.shape[1]
1.18 logistic.py(283):     n_features = X.shape[1]
1.18 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
1.18 logistic.py(285):     w = w.reshape(n_classes, -1)
1.18 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
1.18 logistic.py(287):     if fit_intercept:
1.18 logistic.py(288):         intercept = w[:, -1]
1.18 logistic.py(289):         w = w[:, :-1]
1.18 logistic.py(292):     p = safe_sparse_dot(X, w.T)
1.18 logistic.py(293):     p += intercept
1.18 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
1.18 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
1.18 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
1.18 logistic.py(297):     p = np.exp(p, p)
1.18 logistic.py(298):     return loss, p, w
1.18 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
1.18 logistic.py(346):     diff = sample_weight * (p - Y)
1.18 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
1.18 logistic.py(348):     grad[:, :n_features] += alpha * w
1.18 logistic.py(349):     if fit_intercept:
1.18 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
1.18 logistic.py(351):     return loss, grad.ravel(), p
1.18 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
1.18 logistic.py(339):     n_classes = Y.shape[1]
1.18 logistic.py(340):     n_features = X.shape[1]
1.18 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
1.18 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
1.18 logistic.py(343):                     dtype=X.dtype)
1.18 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
1.18 logistic.py(282):     n_classes = Y.shape[1]
1.18 logistic.py(283):     n_features = X.shape[1]
1.18 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
1.18 logistic.py(285):     w = w.reshape(n_classes, -1)
1.18 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
1.18 logistic.py(287):     if fit_intercept:
1.18 logistic.py(288):         intercept = w[:, -1]
1.18 logistic.py(289):         w = w[:, :-1]
1.18 logistic.py(292):     p = safe_sparse_dot(X, w.T)
1.18 logistic.py(293):     p += intercept
1.18 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
1.18 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
1.18 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
1.18 logistic.py(297):     p = np.exp(p, p)
1.18 logistic.py(298):     return loss, p, w
1.18 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
1.18 logistic.py(346):     diff = sample_weight * (p - Y)
1.18 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
1.18 logistic.py(348):     grad[:, :n_features] += alpha * w
1.18 logistic.py(349):     if fit_intercept:
1.18 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
1.18 logistic.py(351):     return loss, grad.ravel(), p
1.18 logistic.py(718):             if info["warnflag"] == 1:
1.18 logistic.py(723):             n_iter_i = min(info['nit'], max_iter)
1.18 logistic.py(760):         if multi_class == 'multinomial':
1.18 logistic.py(761):             multi_w0 = np.reshape(w0, (classes.size, -1))
1.18 logistic.py(762):             if classes.size == 2:
1.18 logistic.py(764):             coefs.append(multi_w0)
1.18 logistic.py(768):         n_iter[i] = n_iter_i
1.18 logistic.py(712):     for i, C in enumerate(Cs):
1.18 logistic.py(713):         if solver == 'lbfgs':
1.18 logistic.py(714):             w0, loss, info = optimize.fmin_l_bfgs_b(
1.18 logistic.py(715):                 func, w0, fprime=None,
1.18 logistic.py(716):                 args=(X, target, 1. / C, sample_weight),
1.18 logistic.py(717):                 iprint=(verbose > 0) - 1, pgtol=tol, maxiter=max_iter)
1.18 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
1.18 logistic.py(339):     n_classes = Y.shape[1]
1.18 logistic.py(340):     n_features = X.shape[1]
1.18 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
1.18 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
1.18 logistic.py(343):                     dtype=X.dtype)
1.18 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
1.18 logistic.py(282):     n_classes = Y.shape[1]
1.18 logistic.py(283):     n_features = X.shape[1]
1.18 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
1.18 logistic.py(285):     w = w.reshape(n_classes, -1)
1.18 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
1.18 logistic.py(287):     if fit_intercept:
1.18 logistic.py(288):         intercept = w[:, -1]
1.18 logistic.py(289):         w = w[:, :-1]
1.18 logistic.py(292):     p = safe_sparse_dot(X, w.T)
1.18 logistic.py(293):     p += intercept
1.18 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
1.18 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
1.18 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
1.18 logistic.py(297):     p = np.exp(p, p)
1.18 logistic.py(298):     return loss, p, w
1.18 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
1.18 logistic.py(346):     diff = sample_weight * (p - Y)
1.18 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
1.18 logistic.py(348):     grad[:, :n_features] += alpha * w
1.18 logistic.py(349):     if fit_intercept:
1.18 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
1.18 logistic.py(351):     return loss, grad.ravel(), p
1.18 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
1.18 logistic.py(339):     n_classes = Y.shape[1]
1.18 logistic.py(340):     n_features = X.shape[1]
1.18 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
1.18 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
1.18 logistic.py(343):                     dtype=X.dtype)
1.18 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
1.18 logistic.py(282):     n_classes = Y.shape[1]
1.18 logistic.py(283):     n_features = X.shape[1]
1.18 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
1.18 logistic.py(285):     w = w.reshape(n_classes, -1)
1.18 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
1.18 logistic.py(287):     if fit_intercept:
1.18 logistic.py(288):         intercept = w[:, -1]
1.18 logistic.py(289):         w = w[:, :-1]
1.18 logistic.py(292):     p = safe_sparse_dot(X, w.T)
1.18 logistic.py(293):     p += intercept
1.18 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
1.18 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
1.18 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
1.18 logistic.py(297):     p = np.exp(p, p)
1.18 logistic.py(298):     return loss, p, w
1.18 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
1.18 logistic.py(346):     diff = sample_weight * (p - Y)
1.18 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
1.18 logistic.py(348):     grad[:, :n_features] += alpha * w
1.18 logistic.py(349):     if fit_intercept:
1.18 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
1.18 logistic.py(351):     return loss, grad.ravel(), p
1.18 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
1.18 logistic.py(339):     n_classes = Y.shape[1]
1.18 logistic.py(340):     n_features = X.shape[1]
1.18 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
1.18 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
1.18 logistic.py(343):                     dtype=X.dtype)
1.18 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
1.18 logistic.py(282):     n_classes = Y.shape[1]
1.18 logistic.py(283):     n_features = X.shape[1]
1.18 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
1.18 logistic.py(285):     w = w.reshape(n_classes, -1)
1.18 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
1.18 logistic.py(287):     if fit_intercept:
1.18 logistic.py(288):         intercept = w[:, -1]
1.18 logistic.py(289):         w = w[:, :-1]
1.18 logistic.py(292):     p = safe_sparse_dot(X, w.T)
1.18 logistic.py(293):     p += intercept
1.18 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
1.18 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
1.18 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
1.18 logistic.py(297):     p = np.exp(p, p)
1.18 logistic.py(298):     return loss, p, w
1.18 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
1.18 logistic.py(346):     diff = sample_weight * (p - Y)
1.18 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
1.18 logistic.py(348):     grad[:, :n_features] += alpha * w
1.18 logistic.py(349):     if fit_intercept:
1.18 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
1.18 logistic.py(351):     return loss, grad.ravel(), p
1.18 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
1.18 logistic.py(339):     n_classes = Y.shape[1]
1.18 logistic.py(340):     n_features = X.shape[1]
1.18 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
1.18 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
1.18 logistic.py(343):                     dtype=X.dtype)
1.18 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
1.18 logistic.py(282):     n_classes = Y.shape[1]
1.18 logistic.py(283):     n_features = X.shape[1]
1.18 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
1.18 logistic.py(285):     w = w.reshape(n_classes, -1)
1.18 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
1.18 logistic.py(287):     if fit_intercept:
1.18 logistic.py(288):         intercept = w[:, -1]
1.18 logistic.py(289):         w = w[:, :-1]
1.18 logistic.py(292):     p = safe_sparse_dot(X, w.T)
1.18 logistic.py(293):     p += intercept
1.18 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
1.18 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
1.18 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
1.18 logistic.py(297):     p = np.exp(p, p)
1.18 logistic.py(298):     return loss, p, w
1.18 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
1.18 logistic.py(346):     diff = sample_weight * (p - Y)
1.18 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
1.18 logistic.py(348):     grad[:, :n_features] += alpha * w
1.18 logistic.py(349):     if fit_intercept:
1.18 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
1.18 logistic.py(351):     return loss, grad.ravel(), p
1.18 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
1.18 logistic.py(339):     n_classes = Y.shape[1]
1.18 logistic.py(340):     n_features = X.shape[1]
1.18 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
1.18 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
1.18 logistic.py(343):                     dtype=X.dtype)
1.18 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
1.18 logistic.py(282):     n_classes = Y.shape[1]
1.18 logistic.py(283):     n_features = X.shape[1]
1.18 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
1.18 logistic.py(285):     w = w.reshape(n_classes, -1)
1.18 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
1.18 logistic.py(287):     if fit_intercept:
1.18 logistic.py(288):         intercept = w[:, -1]
1.18 logistic.py(289):         w = w[:, :-1]
1.18 logistic.py(292):     p = safe_sparse_dot(X, w.T)
1.18 logistic.py(293):     p += intercept
1.18 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
1.18 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
1.18 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
1.18 logistic.py(297):     p = np.exp(p, p)
1.18 logistic.py(298):     return loss, p, w
1.18 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
1.18 logistic.py(346):     diff = sample_weight * (p - Y)
1.18 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
1.18 logistic.py(348):     grad[:, :n_features] += alpha * w
1.18 logistic.py(349):     if fit_intercept:
1.18 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
1.18 logistic.py(351):     return loss, grad.ravel(), p
1.18 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
1.18 logistic.py(339):     n_classes = Y.shape[1]
1.18 logistic.py(340):     n_features = X.shape[1]
1.18 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
1.18 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
1.18 logistic.py(343):                     dtype=X.dtype)
1.18 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
1.18 logistic.py(282):     n_classes = Y.shape[1]
1.18 logistic.py(283):     n_features = X.shape[1]
1.18 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
1.18 logistic.py(285):     w = w.reshape(n_classes, -1)
1.18 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
1.18 logistic.py(287):     if fit_intercept:
1.18 logistic.py(288):         intercept = w[:, -1]
1.18 logistic.py(289):         w = w[:, :-1]
1.18 logistic.py(292):     p = safe_sparse_dot(X, w.T)
1.18 logistic.py(293):     p += intercept
1.18 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
1.18 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
1.18 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
1.18 logistic.py(297):     p = np.exp(p, p)
1.18 logistic.py(298):     return loss, p, w
1.18 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
1.18 logistic.py(346):     diff = sample_weight * (p - Y)
1.18 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
1.18 logistic.py(348):     grad[:, :n_features] += alpha * w
1.18 logistic.py(349):     if fit_intercept:
1.18 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
1.18 logistic.py(351):     return loss, grad.ravel(), p
1.18 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
1.18 logistic.py(339):     n_classes = Y.shape[1]
1.18 logistic.py(340):     n_features = X.shape[1]
1.18 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
1.18 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
1.18 logistic.py(343):                     dtype=X.dtype)
1.18 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
1.18 logistic.py(282):     n_classes = Y.shape[1]
1.18 logistic.py(283):     n_features = X.shape[1]
1.18 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
1.18 logistic.py(285):     w = w.reshape(n_classes, -1)
1.18 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
1.18 logistic.py(287):     if fit_intercept:
1.18 logistic.py(288):         intercept = w[:, -1]
1.18 logistic.py(289):         w = w[:, :-1]
1.18 logistic.py(292):     p = safe_sparse_dot(X, w.T)
1.18 logistic.py(293):     p += intercept
1.18 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
1.18 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
1.18 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
1.18 logistic.py(297):     p = np.exp(p, p)
1.18 logistic.py(298):     return loss, p, w
1.18 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
1.18 logistic.py(346):     diff = sample_weight * (p - Y)
1.18 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
1.18 logistic.py(348):     grad[:, :n_features] += alpha * w
1.18 logistic.py(349):     if fit_intercept:
1.18 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
1.18 logistic.py(351):     return loss, grad.ravel(), p
1.18 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
1.18 logistic.py(339):     n_classes = Y.shape[1]
1.18 logistic.py(340):     n_features = X.shape[1]
1.18 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
1.18 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
1.18 logistic.py(343):                     dtype=X.dtype)
1.18 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
1.18 logistic.py(282):     n_classes = Y.shape[1]
1.18 logistic.py(283):     n_features = X.shape[1]
1.18 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
1.18 logistic.py(285):     w = w.reshape(n_classes, -1)
1.18 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
1.18 logistic.py(287):     if fit_intercept:
1.18 logistic.py(288):         intercept = w[:, -1]
1.18 logistic.py(289):         w = w[:, :-1]
1.18 logistic.py(292):     p = safe_sparse_dot(X, w.T)
1.18 logistic.py(293):     p += intercept
1.18 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
1.18 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
1.18 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
1.18 logistic.py(297):     p = np.exp(p, p)
1.18 logistic.py(298):     return loss, p, w
1.18 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
1.18 logistic.py(346):     diff = sample_weight * (p - Y)
1.18 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
1.18 logistic.py(348):     grad[:, :n_features] += alpha * w
1.18 logistic.py(349):     if fit_intercept:
1.18 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
1.18 logistic.py(351):     return loss, grad.ravel(), p
1.18 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
1.18 logistic.py(339):     n_classes = Y.shape[1]
1.18 logistic.py(340):     n_features = X.shape[1]
1.18 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
1.18 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
1.18 logistic.py(343):                     dtype=X.dtype)
1.18 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
1.18 logistic.py(282):     n_classes = Y.shape[1]
1.18 logistic.py(283):     n_features = X.shape[1]
1.18 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
1.18 logistic.py(285):     w = w.reshape(n_classes, -1)
1.18 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
1.18 logistic.py(287):     if fit_intercept:
1.18 logistic.py(288):         intercept = w[:, -1]
1.18 logistic.py(289):         w = w[:, :-1]
1.18 logistic.py(292):     p = safe_sparse_dot(X, w.T)
1.18 logistic.py(293):     p += intercept
1.18 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
1.18 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
1.18 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
1.18 logistic.py(297):     p = np.exp(p, p)
1.18 logistic.py(298):     return loss, p, w
1.18 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
1.18 logistic.py(346):     diff = sample_weight * (p - Y)
1.18 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
1.18 logistic.py(348):     grad[:, :n_features] += alpha * w
1.18 logistic.py(349):     if fit_intercept:
1.18 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
1.18 logistic.py(351):     return loss, grad.ravel(), p
1.18 logistic.py(718):             if info["warnflag"] == 1:
1.18 logistic.py(723):             n_iter_i = min(info['nit'], max_iter)
1.18 logistic.py(760):         if multi_class == 'multinomial':
1.18 logistic.py(761):             multi_w0 = np.reshape(w0, (classes.size, -1))
1.18 logistic.py(762):             if classes.size == 2:
1.18 logistic.py(764):             coefs.append(multi_w0)
1.18 logistic.py(768):         n_iter[i] = n_iter_i
1.18 logistic.py(712):     for i, C in enumerate(Cs):
1.18 logistic.py(713):         if solver == 'lbfgs':
1.18 logistic.py(714):             w0, loss, info = optimize.fmin_l_bfgs_b(
1.18 logistic.py(715):                 func, w0, fprime=None,
1.18 logistic.py(716):                 args=(X, target, 1. / C, sample_weight),
1.18 logistic.py(717):                 iprint=(verbose > 0) - 1, pgtol=tol, maxiter=max_iter)
1.18 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
1.18 logistic.py(339):     n_classes = Y.shape[1]
1.18 logistic.py(340):     n_features = X.shape[1]
1.18 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
1.18 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
1.18 logistic.py(343):                     dtype=X.dtype)
1.18 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
1.18 logistic.py(282):     n_classes = Y.shape[1]
1.18 logistic.py(283):     n_features = X.shape[1]
1.18 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
1.18 logistic.py(285):     w = w.reshape(n_classes, -1)
1.18 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
1.18 logistic.py(287):     if fit_intercept:
1.18 logistic.py(288):         intercept = w[:, -1]
1.18 logistic.py(289):         w = w[:, :-1]
1.18 logistic.py(292):     p = safe_sparse_dot(X, w.T)
1.18 logistic.py(293):     p += intercept
1.18 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
1.18 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
1.18 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
1.18 logistic.py(297):     p = np.exp(p, p)
1.18 logistic.py(298):     return loss, p, w
1.18 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
1.18 logistic.py(346):     diff = sample_weight * (p - Y)
1.18 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
1.18 logistic.py(348):     grad[:, :n_features] += alpha * w
1.18 logistic.py(349):     if fit_intercept:
1.18 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
1.18 logistic.py(351):     return loss, grad.ravel(), p
1.18 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
1.18 logistic.py(339):     n_classes = Y.shape[1]
1.18 logistic.py(340):     n_features = X.shape[1]
1.18 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
1.18 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
1.18 logistic.py(343):                     dtype=X.dtype)
1.18 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
1.18 logistic.py(282):     n_classes = Y.shape[1]
1.18 logistic.py(283):     n_features = X.shape[1]
1.18 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
1.18 logistic.py(285):     w = w.reshape(n_classes, -1)
1.18 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
1.18 logistic.py(287):     if fit_intercept:
1.18 logistic.py(288):         intercept = w[:, -1]
1.18 logistic.py(289):         w = w[:, :-1]
1.18 logistic.py(292):     p = safe_sparse_dot(X, w.T)
1.18 logistic.py(293):     p += intercept
1.18 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
1.18 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
1.18 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
1.18 logistic.py(297):     p = np.exp(p, p)
1.18 logistic.py(298):     return loss, p, w
1.18 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
1.18 logistic.py(346):     diff = sample_weight * (p - Y)
1.18 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
1.18 logistic.py(348):     grad[:, :n_features] += alpha * w
1.18 logistic.py(349):     if fit_intercept:
1.18 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
1.18 logistic.py(351):     return loss, grad.ravel(), p
1.18 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
1.18 logistic.py(339):     n_classes = Y.shape[1]
1.18 logistic.py(340):     n_features = X.shape[1]
1.18 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
1.18 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
1.18 logistic.py(343):                     dtype=X.dtype)
1.18 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
1.18 logistic.py(282):     n_classes = Y.shape[1]
1.18 logistic.py(283):     n_features = X.shape[1]
1.18 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
1.18 logistic.py(285):     w = w.reshape(n_classes, -1)
1.18 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
1.18 logistic.py(287):     if fit_intercept:
1.18 logistic.py(288):         intercept = w[:, -1]
1.18 logistic.py(289):         w = w[:, :-1]
1.18 logistic.py(292):     p = safe_sparse_dot(X, w.T)
1.18 logistic.py(293):     p += intercept
1.18 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
1.18 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
1.18 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
1.18 logistic.py(297):     p = np.exp(p, p)
1.18 logistic.py(298):     return loss, p, w
1.18 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
1.18 logistic.py(346):     diff = sample_weight * (p - Y)
1.18 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
1.18 logistic.py(348):     grad[:, :n_features] += alpha * w
1.18 logistic.py(349):     if fit_intercept:
1.18 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
1.18 logistic.py(351):     return loss, grad.ravel(), p
1.18 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
1.18 logistic.py(339):     n_classes = Y.shape[1]
1.18 logistic.py(340):     n_features = X.shape[1]
1.18 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
1.18 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
1.18 logistic.py(343):                     dtype=X.dtype)
1.18 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
1.18 logistic.py(282):     n_classes = Y.shape[1]
1.18 logistic.py(283):     n_features = X.shape[1]
1.18 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
1.18 logistic.py(285):     w = w.reshape(n_classes, -1)
1.18 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
1.18 logistic.py(287):     if fit_intercept:
1.18 logistic.py(288):         intercept = w[:, -1]
1.18 logistic.py(289):         w = w[:, :-1]
1.18 logistic.py(292):     p = safe_sparse_dot(X, w.T)
1.18 logistic.py(293):     p += intercept
1.18 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
1.18 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
1.18 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
1.18 logistic.py(297):     p = np.exp(p, p)
1.18 logistic.py(298):     return loss, p, w
1.18 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
1.18 logistic.py(346):     diff = sample_weight * (p - Y)
1.18 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
1.18 logistic.py(348):     grad[:, :n_features] += alpha * w
1.18 logistic.py(349):     if fit_intercept:
1.18 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
1.18 logistic.py(351):     return loss, grad.ravel(), p
1.18 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
1.18 logistic.py(339):     n_classes = Y.shape[1]
1.18 logistic.py(340):     n_features = X.shape[1]
1.18 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
1.18 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
1.18 logistic.py(343):                     dtype=X.dtype)
1.18 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
1.18 logistic.py(282):     n_classes = Y.shape[1]
1.18 logistic.py(283):     n_features = X.shape[1]
1.18 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
1.18 logistic.py(285):     w = w.reshape(n_classes, -1)
1.18 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
1.19 logistic.py(287):     if fit_intercept:
1.19 logistic.py(288):         intercept = w[:, -1]
1.19 logistic.py(289):         w = w[:, :-1]
1.19 logistic.py(292):     p = safe_sparse_dot(X, w.T)
1.19 logistic.py(293):     p += intercept
1.19 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
1.19 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
1.19 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
1.19 logistic.py(297):     p = np.exp(p, p)
1.19 logistic.py(298):     return loss, p, w
1.19 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
1.19 logistic.py(346):     diff = sample_weight * (p - Y)
1.19 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
1.19 logistic.py(348):     grad[:, :n_features] += alpha * w
1.19 logistic.py(349):     if fit_intercept:
1.19 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
1.19 logistic.py(351):     return loss, grad.ravel(), p
1.19 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
1.19 logistic.py(339):     n_classes = Y.shape[1]
1.19 logistic.py(340):     n_features = X.shape[1]
1.19 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
1.19 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
1.19 logistic.py(343):                     dtype=X.dtype)
1.19 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
1.19 logistic.py(282):     n_classes = Y.shape[1]
1.19 logistic.py(283):     n_features = X.shape[1]
1.19 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
1.19 logistic.py(285):     w = w.reshape(n_classes, -1)
1.19 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
1.19 logistic.py(287):     if fit_intercept:
1.19 logistic.py(288):         intercept = w[:, -1]
1.19 logistic.py(289):         w = w[:, :-1]
1.19 logistic.py(292):     p = safe_sparse_dot(X, w.T)
1.19 logistic.py(293):     p += intercept
1.19 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
1.19 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
1.19 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
1.19 logistic.py(297):     p = np.exp(p, p)
1.19 logistic.py(298):     return loss, p, w
1.19 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
1.19 logistic.py(346):     diff = sample_weight * (p - Y)
1.19 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
1.19 logistic.py(348):     grad[:, :n_features] += alpha * w
1.19 logistic.py(349):     if fit_intercept:
1.19 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
1.19 logistic.py(351):     return loss, grad.ravel(), p
1.19 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
1.19 logistic.py(339):     n_classes = Y.shape[1]
1.19 logistic.py(340):     n_features = X.shape[1]
1.19 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
1.19 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
1.19 logistic.py(343):                     dtype=X.dtype)
1.19 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
1.19 logistic.py(282):     n_classes = Y.shape[1]
1.19 logistic.py(283):     n_features = X.shape[1]
1.19 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
1.19 logistic.py(285):     w = w.reshape(n_classes, -1)
1.19 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
1.19 logistic.py(287):     if fit_intercept:
1.19 logistic.py(288):         intercept = w[:, -1]
1.19 logistic.py(289):         w = w[:, :-1]
1.19 logistic.py(292):     p = safe_sparse_dot(X, w.T)
1.19 logistic.py(293):     p += intercept
1.19 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
1.19 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
1.19 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
1.19 logistic.py(297):     p = np.exp(p, p)
1.19 logistic.py(298):     return loss, p, w
1.19 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
1.19 logistic.py(346):     diff = sample_weight * (p - Y)
1.19 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
1.19 logistic.py(348):     grad[:, :n_features] += alpha * w
1.19 logistic.py(349):     if fit_intercept:
1.19 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
1.19 logistic.py(351):     return loss, grad.ravel(), p
1.19 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
1.19 logistic.py(339):     n_classes = Y.shape[1]
1.19 logistic.py(340):     n_features = X.shape[1]
1.19 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
1.19 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
1.19 logistic.py(343):                     dtype=X.dtype)
1.19 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
1.19 logistic.py(282):     n_classes = Y.shape[1]
1.19 logistic.py(283):     n_features = X.shape[1]
1.19 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
1.19 logistic.py(285):     w = w.reshape(n_classes, -1)
1.19 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
1.19 logistic.py(287):     if fit_intercept:
1.19 logistic.py(288):         intercept = w[:, -1]
1.19 logistic.py(289):         w = w[:, :-1]
1.19 logistic.py(292):     p = safe_sparse_dot(X, w.T)
1.19 logistic.py(293):     p += intercept
1.19 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
1.19 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
1.19 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
1.19 logistic.py(297):     p = np.exp(p, p)
1.19 logistic.py(298):     return loss, p, w
1.19 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
1.19 logistic.py(346):     diff = sample_weight * (p - Y)
1.19 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
1.19 logistic.py(348):     grad[:, :n_features] += alpha * w
1.19 logistic.py(349):     if fit_intercept:
1.19 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
1.19 logistic.py(351):     return loss, grad.ravel(), p
1.19 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
1.19 logistic.py(339):     n_classes = Y.shape[1]
1.19 logistic.py(340):     n_features = X.shape[1]
1.19 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
1.19 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
1.19 logistic.py(343):                     dtype=X.dtype)
1.19 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
1.19 logistic.py(282):     n_classes = Y.shape[1]
1.19 logistic.py(283):     n_features = X.shape[1]
1.19 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
1.19 logistic.py(285):     w = w.reshape(n_classes, -1)
1.19 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
1.19 logistic.py(287):     if fit_intercept:
1.19 logistic.py(288):         intercept = w[:, -1]
1.19 logistic.py(289):         w = w[:, :-1]
1.19 logistic.py(292):     p = safe_sparse_dot(X, w.T)
1.19 logistic.py(293):     p += intercept
1.19 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
1.19 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
1.19 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
1.19 logistic.py(297):     p = np.exp(p, p)
1.19 logistic.py(298):     return loss, p, w
1.19 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
1.19 logistic.py(346):     diff = sample_weight * (p - Y)
1.19 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
1.19 logistic.py(348):     grad[:, :n_features] += alpha * w
1.19 logistic.py(349):     if fit_intercept:
1.19 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
1.19 logistic.py(351):     return loss, grad.ravel(), p
1.19 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
1.19 logistic.py(339):     n_classes = Y.shape[1]
1.19 logistic.py(340):     n_features = X.shape[1]
1.19 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
1.19 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
1.19 logistic.py(343):                     dtype=X.dtype)
1.19 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
1.19 logistic.py(282):     n_classes = Y.shape[1]
1.19 logistic.py(283):     n_features = X.shape[1]
1.19 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
1.19 logistic.py(285):     w = w.reshape(n_classes, -1)
1.19 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
1.19 logistic.py(287):     if fit_intercept:
1.19 logistic.py(288):         intercept = w[:, -1]
1.19 logistic.py(289):         w = w[:, :-1]
1.19 logistic.py(292):     p = safe_sparse_dot(X, w.T)
1.19 logistic.py(293):     p += intercept
1.19 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
1.19 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
1.19 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
1.19 logistic.py(297):     p = np.exp(p, p)
1.19 logistic.py(298):     return loss, p, w
1.19 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
1.19 logistic.py(346):     diff = sample_weight * (p - Y)
1.19 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
1.19 logistic.py(348):     grad[:, :n_features] += alpha * w
1.19 logistic.py(349):     if fit_intercept:
1.19 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
1.19 logistic.py(351):     return loss, grad.ravel(), p
1.19 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
1.19 logistic.py(339):     n_classes = Y.shape[1]
1.19 logistic.py(340):     n_features = X.shape[1]
1.19 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
1.19 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
1.19 logistic.py(343):                     dtype=X.dtype)
1.19 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
1.19 logistic.py(282):     n_classes = Y.shape[1]
1.19 logistic.py(283):     n_features = X.shape[1]
1.19 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
1.19 logistic.py(285):     w = w.reshape(n_classes, -1)
1.19 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
1.19 logistic.py(287):     if fit_intercept:
1.19 logistic.py(288):         intercept = w[:, -1]
1.19 logistic.py(289):         w = w[:, :-1]
1.19 logistic.py(292):     p = safe_sparse_dot(X, w.T)
1.19 logistic.py(293):     p += intercept
1.19 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
1.19 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
1.19 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
1.19 logistic.py(297):     p = np.exp(p, p)
1.19 logistic.py(298):     return loss, p, w
1.19 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
1.19 logistic.py(346):     diff = sample_weight * (p - Y)
1.19 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
1.19 logistic.py(348):     grad[:, :n_features] += alpha * w
1.19 logistic.py(349):     if fit_intercept:
1.19 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
1.19 logistic.py(351):     return loss, grad.ravel(), p
1.19 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
1.19 logistic.py(339):     n_classes = Y.shape[1]
1.19 logistic.py(340):     n_features = X.shape[1]
1.19 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
1.19 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
1.19 logistic.py(343):                     dtype=X.dtype)
1.19 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
1.19 logistic.py(282):     n_classes = Y.shape[1]
1.19 logistic.py(283):     n_features = X.shape[1]
1.19 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
1.19 logistic.py(285):     w = w.reshape(n_classes, -1)
1.19 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
1.19 logistic.py(287):     if fit_intercept:
1.19 logistic.py(288):         intercept = w[:, -1]
1.19 logistic.py(289):         w = w[:, :-1]
1.19 logistic.py(292):     p = safe_sparse_dot(X, w.T)
1.19 logistic.py(293):     p += intercept
1.19 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
1.19 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
1.19 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
1.19 logistic.py(297):     p = np.exp(p, p)
1.19 logistic.py(298):     return loss, p, w
1.19 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
1.19 logistic.py(346):     diff = sample_weight * (p - Y)
1.19 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
1.19 logistic.py(348):     grad[:, :n_features] += alpha * w
1.19 logistic.py(349):     if fit_intercept:
1.19 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
1.19 logistic.py(351):     return loss, grad.ravel(), p
1.19 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
1.19 logistic.py(339):     n_classes = Y.shape[1]
1.19 logistic.py(340):     n_features = X.shape[1]
1.19 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
1.19 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
1.19 logistic.py(343):                     dtype=X.dtype)
1.19 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
1.19 logistic.py(282):     n_classes = Y.shape[1]
1.19 logistic.py(283):     n_features = X.shape[1]
1.19 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
1.19 logistic.py(285):     w = w.reshape(n_classes, -1)
1.19 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
1.19 logistic.py(287):     if fit_intercept:
1.19 logistic.py(288):         intercept = w[:, -1]
1.19 logistic.py(289):         w = w[:, :-1]
1.19 logistic.py(292):     p = safe_sparse_dot(X, w.T)
1.19 logistic.py(293):     p += intercept
1.19 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
1.19 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
1.19 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
1.19 logistic.py(297):     p = np.exp(p, p)
1.19 logistic.py(298):     return loss, p, w
1.19 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
1.19 logistic.py(346):     diff = sample_weight * (p - Y)
1.19 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
1.19 logistic.py(348):     grad[:, :n_features] += alpha * w
1.19 logistic.py(349):     if fit_intercept:
1.19 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
1.19 logistic.py(351):     return loss, grad.ravel(), p
1.19 logistic.py(718):             if info["warnflag"] == 1:
1.19 logistic.py(723):             n_iter_i = min(info['nit'], max_iter)
1.19 logistic.py(760):         if multi_class == 'multinomial':
1.19 logistic.py(761):             multi_w0 = np.reshape(w0, (classes.size, -1))
1.19 logistic.py(762):             if classes.size == 2:
1.19 logistic.py(764):             coefs.append(multi_w0)
1.19 logistic.py(768):         n_iter[i] = n_iter_i
1.19 logistic.py(712):     for i, C in enumerate(Cs):
1.19 logistic.py(713):         if solver == 'lbfgs':
1.19 logistic.py(714):             w0, loss, info = optimize.fmin_l_bfgs_b(
1.19 logistic.py(715):                 func, w0, fprime=None,
1.19 logistic.py(716):                 args=(X, target, 1. / C, sample_weight),
1.19 logistic.py(717):                 iprint=(verbose > 0) - 1, pgtol=tol, maxiter=max_iter)
1.19 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
1.19 logistic.py(339):     n_classes = Y.shape[1]
1.19 logistic.py(340):     n_features = X.shape[1]
1.19 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
1.19 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
1.19 logistic.py(343):                     dtype=X.dtype)
1.19 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
1.19 logistic.py(282):     n_classes = Y.shape[1]
1.19 logistic.py(283):     n_features = X.shape[1]
1.19 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
1.19 logistic.py(285):     w = w.reshape(n_classes, -1)
1.19 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
1.19 logistic.py(287):     if fit_intercept:
1.19 logistic.py(288):         intercept = w[:, -1]
1.19 logistic.py(289):         w = w[:, :-1]
1.19 logistic.py(292):     p = safe_sparse_dot(X, w.T)
1.19 logistic.py(293):     p += intercept
1.19 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
1.19 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
1.19 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
1.19 logistic.py(297):     p = np.exp(p, p)
1.19 logistic.py(298):     return loss, p, w
1.19 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
1.19 logistic.py(346):     diff = sample_weight * (p - Y)
1.19 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
1.19 logistic.py(348):     grad[:, :n_features] += alpha * w
1.19 logistic.py(349):     if fit_intercept:
1.19 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
1.19 logistic.py(351):     return loss, grad.ravel(), p
1.19 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
1.19 logistic.py(339):     n_classes = Y.shape[1]
1.19 logistic.py(340):     n_features = X.shape[1]
1.19 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
1.19 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
1.19 logistic.py(343):                     dtype=X.dtype)
1.19 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
1.19 logistic.py(282):     n_classes = Y.shape[1]
1.19 logistic.py(283):     n_features = X.shape[1]
1.19 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
1.19 logistic.py(285):     w = w.reshape(n_classes, -1)
1.19 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
1.19 logistic.py(287):     if fit_intercept:
1.19 logistic.py(288):         intercept = w[:, -1]
1.19 logistic.py(289):         w = w[:, :-1]
1.19 logistic.py(292):     p = safe_sparse_dot(X, w.T)
1.19 logistic.py(293):     p += intercept
1.19 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
1.19 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
1.19 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
1.19 logistic.py(297):     p = np.exp(p, p)
1.19 logistic.py(298):     return loss, p, w
1.19 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
1.19 logistic.py(346):     diff = sample_weight * (p - Y)
1.19 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
1.19 logistic.py(348):     grad[:, :n_features] += alpha * w
1.19 logistic.py(349):     if fit_intercept:
1.19 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
1.19 logistic.py(351):     return loss, grad.ravel(), p
1.19 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
1.19 logistic.py(339):     n_classes = Y.shape[1]
1.19 logistic.py(340):     n_features = X.shape[1]
1.19 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
1.19 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
1.19 logistic.py(343):                     dtype=X.dtype)
1.19 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
1.19 logistic.py(282):     n_classes = Y.shape[1]
1.19 logistic.py(283):     n_features = X.shape[1]
1.19 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
1.19 logistic.py(285):     w = w.reshape(n_classes, -1)
1.19 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
1.19 logistic.py(287):     if fit_intercept:
1.19 logistic.py(288):         intercept = w[:, -1]
1.19 logistic.py(289):         w = w[:, :-1]
1.19 logistic.py(292):     p = safe_sparse_dot(X, w.T)
1.19 logistic.py(293):     p += intercept
1.19 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
1.19 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
1.19 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
1.19 logistic.py(297):     p = np.exp(p, p)
1.19 logistic.py(298):     return loss, p, w
1.19 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
1.19 logistic.py(346):     diff = sample_weight * (p - Y)
1.19 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
1.19 logistic.py(348):     grad[:, :n_features] += alpha * w
1.19 logistic.py(349):     if fit_intercept:
1.19 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
1.19 logistic.py(351):     return loss, grad.ravel(), p
1.19 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
1.19 logistic.py(339):     n_classes = Y.shape[1]
1.19 logistic.py(340):     n_features = X.shape[1]
1.19 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
1.19 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
1.19 logistic.py(343):                     dtype=X.dtype)
1.19 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
1.19 logistic.py(282):     n_classes = Y.shape[1]
1.19 logistic.py(283):     n_features = X.shape[1]
1.19 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
1.19 logistic.py(285):     w = w.reshape(n_classes, -1)
1.19 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
1.19 logistic.py(287):     if fit_intercept:
1.19 logistic.py(288):         intercept = w[:, -1]
1.19 logistic.py(289):         w = w[:, :-1]
1.19 logistic.py(292):     p = safe_sparse_dot(X, w.T)
1.19 logistic.py(293):     p += intercept
1.19 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
1.19 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
1.19 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
1.19 logistic.py(297):     p = np.exp(p, p)
1.19 logistic.py(298):     return loss, p, w
1.19 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
1.19 logistic.py(346):     diff = sample_weight * (p - Y)
1.19 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
1.19 logistic.py(348):     grad[:, :n_features] += alpha * w
1.19 logistic.py(349):     if fit_intercept:
1.19 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
1.19 logistic.py(351):     return loss, grad.ravel(), p
1.19 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
1.19 logistic.py(339):     n_classes = Y.shape[1]
1.19 logistic.py(340):     n_features = X.shape[1]
1.19 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
1.19 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
1.19 logistic.py(343):                     dtype=X.dtype)
1.19 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
1.19 logistic.py(282):     n_classes = Y.shape[1]
1.19 logistic.py(283):     n_features = X.shape[1]
1.19 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
1.19 logistic.py(285):     w = w.reshape(n_classes, -1)
1.19 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
1.19 logistic.py(287):     if fit_intercept:
1.19 logistic.py(288):         intercept = w[:, -1]
1.19 logistic.py(289):         w = w[:, :-1]
1.19 logistic.py(292):     p = safe_sparse_dot(X, w.T)
1.19 logistic.py(293):     p += intercept
1.19 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
1.19 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
1.19 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
1.19 logistic.py(297):     p = np.exp(p, p)
1.19 logistic.py(298):     return loss, p, w
1.19 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
1.19 logistic.py(346):     diff = sample_weight * (p - Y)
1.19 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
1.19 logistic.py(348):     grad[:, :n_features] += alpha * w
1.19 logistic.py(349):     if fit_intercept:
1.19 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
1.19 logistic.py(351):     return loss, grad.ravel(), p
1.19 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
1.19 logistic.py(339):     n_classes = Y.shape[1]
1.19 logistic.py(340):     n_features = X.shape[1]
1.19 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
1.19 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
1.19 logistic.py(343):                     dtype=X.dtype)
1.19 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
1.19 logistic.py(282):     n_classes = Y.shape[1]
1.19 logistic.py(283):     n_features = X.shape[1]
1.19 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
1.19 logistic.py(285):     w = w.reshape(n_classes, -1)
1.19 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
1.19 logistic.py(287):     if fit_intercept:
1.19 logistic.py(288):         intercept = w[:, -1]
1.19 logistic.py(289):         w = w[:, :-1]
1.19 logistic.py(292):     p = safe_sparse_dot(X, w.T)
1.19 logistic.py(293):     p += intercept
1.19 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
1.19 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
1.19 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
1.19 logistic.py(297):     p = np.exp(p, p)
1.19 logistic.py(298):     return loss, p, w
1.19 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
1.19 logistic.py(346):     diff = sample_weight * (p - Y)
1.19 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
1.19 logistic.py(348):     grad[:, :n_features] += alpha * w
1.19 logistic.py(349):     if fit_intercept:
1.19 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
1.19 logistic.py(351):     return loss, grad.ravel(), p
1.19 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
1.19 logistic.py(339):     n_classes = Y.shape[1]
1.19 logistic.py(340):     n_features = X.shape[1]
1.19 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
1.19 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
1.19 logistic.py(343):                     dtype=X.dtype)
1.19 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
1.19 logistic.py(282):     n_classes = Y.shape[1]
1.19 logistic.py(283):     n_features = X.shape[1]
1.19 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
1.19 logistic.py(285):     w = w.reshape(n_classes, -1)
1.19 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
1.19 logistic.py(287):     if fit_intercept:
1.19 logistic.py(288):         intercept = w[:, -1]
1.19 logistic.py(289):         w = w[:, :-1]
1.19 logistic.py(292):     p = safe_sparse_dot(X, w.T)
1.19 logistic.py(293):     p += intercept
1.19 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
1.19 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
1.19 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
1.19 logistic.py(297):     p = np.exp(p, p)
1.19 logistic.py(298):     return loss, p, w
1.19 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
1.19 logistic.py(346):     diff = sample_weight * (p - Y)
1.19 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
1.19 logistic.py(348):     grad[:, :n_features] += alpha * w
1.19 logistic.py(349):     if fit_intercept:
1.19 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
1.19 logistic.py(351):     return loss, grad.ravel(), p
1.19 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
1.19 logistic.py(339):     n_classes = Y.shape[1]
1.19 logistic.py(340):     n_features = X.shape[1]
1.19 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
1.19 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
1.19 logistic.py(343):                     dtype=X.dtype)
1.19 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
1.19 logistic.py(282):     n_classes = Y.shape[1]
1.19 logistic.py(283):     n_features = X.shape[1]
1.19 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
1.19 logistic.py(285):     w = w.reshape(n_classes, -1)
1.19 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
1.20 logistic.py(287):     if fit_intercept:
1.20 logistic.py(288):         intercept = w[:, -1]
1.20 logistic.py(289):         w = w[:, :-1]
1.20 logistic.py(292):     p = safe_sparse_dot(X, w.T)
1.20 logistic.py(293):     p += intercept
1.20 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
1.20 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
1.20 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
1.20 logistic.py(297):     p = np.exp(p, p)
1.20 logistic.py(298):     return loss, p, w
1.20 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
1.20 logistic.py(346):     diff = sample_weight * (p - Y)
1.20 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
1.20 logistic.py(348):     grad[:, :n_features] += alpha * w
1.20 logistic.py(349):     if fit_intercept:
1.20 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
1.20 logistic.py(351):     return loss, grad.ravel(), p
1.20 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
1.20 logistic.py(339):     n_classes = Y.shape[1]
1.20 logistic.py(340):     n_features = X.shape[1]
1.20 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
1.20 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
1.20 logistic.py(343):                     dtype=X.dtype)
1.20 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
1.20 logistic.py(282):     n_classes = Y.shape[1]
1.20 logistic.py(283):     n_features = X.shape[1]
1.20 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
1.20 logistic.py(285):     w = w.reshape(n_classes, -1)
1.20 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
1.20 logistic.py(287):     if fit_intercept:
1.20 logistic.py(288):         intercept = w[:, -1]
1.20 logistic.py(289):         w = w[:, :-1]
1.20 logistic.py(292):     p = safe_sparse_dot(X, w.T)
1.20 logistic.py(293):     p += intercept
1.20 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
1.20 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
1.20 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
1.20 logistic.py(297):     p = np.exp(p, p)
1.20 logistic.py(298):     return loss, p, w
1.20 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
1.20 logistic.py(346):     diff = sample_weight * (p - Y)
1.20 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
1.20 logistic.py(348):     grad[:, :n_features] += alpha * w
1.20 logistic.py(349):     if fit_intercept:
1.20 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
1.20 logistic.py(351):     return loss, grad.ravel(), p
1.20 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
1.20 logistic.py(339):     n_classes = Y.shape[1]
1.20 logistic.py(340):     n_features = X.shape[1]
1.20 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
1.20 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
1.20 logistic.py(343):                     dtype=X.dtype)
1.20 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
1.20 logistic.py(282):     n_classes = Y.shape[1]
1.20 logistic.py(283):     n_features = X.shape[1]
1.20 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
1.20 logistic.py(285):     w = w.reshape(n_classes, -1)
1.20 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
1.20 logistic.py(287):     if fit_intercept:
1.20 logistic.py(288):         intercept = w[:, -1]
1.20 logistic.py(289):         w = w[:, :-1]
1.20 logistic.py(292):     p = safe_sparse_dot(X, w.T)
1.20 logistic.py(293):     p += intercept
1.20 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
1.20 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
1.20 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
1.20 logistic.py(297):     p = np.exp(p, p)
1.20 logistic.py(298):     return loss, p, w
1.20 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
1.20 logistic.py(346):     diff = sample_weight * (p - Y)
1.20 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
1.20 logistic.py(348):     grad[:, :n_features] += alpha * w
1.20 logistic.py(349):     if fit_intercept:
1.20 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
1.20 logistic.py(351):     return loss, grad.ravel(), p
1.20 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
1.20 logistic.py(339):     n_classes = Y.shape[1]
1.20 logistic.py(340):     n_features = X.shape[1]
1.20 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
1.20 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
1.20 logistic.py(343):                     dtype=X.dtype)
1.20 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
1.20 logistic.py(282):     n_classes = Y.shape[1]
1.20 logistic.py(283):     n_features = X.shape[1]
1.20 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
1.20 logistic.py(285):     w = w.reshape(n_classes, -1)
1.20 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
1.20 logistic.py(287):     if fit_intercept:
1.20 logistic.py(288):         intercept = w[:, -1]
1.20 logistic.py(289):         w = w[:, :-1]
1.20 logistic.py(292):     p = safe_sparse_dot(X, w.T)
1.20 logistic.py(293):     p += intercept
1.20 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
1.20 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
1.20 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
1.20 logistic.py(297):     p = np.exp(p, p)
1.20 logistic.py(298):     return loss, p, w
1.20 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
1.20 logistic.py(346):     diff = sample_weight * (p - Y)
1.20 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
1.20 logistic.py(348):     grad[:, :n_features] += alpha * w
1.20 logistic.py(349):     if fit_intercept:
1.20 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
1.20 logistic.py(351):     return loss, grad.ravel(), p
1.20 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
1.20 logistic.py(339):     n_classes = Y.shape[1]
1.20 logistic.py(340):     n_features = X.shape[1]
1.20 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
1.20 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
1.20 logistic.py(343):                     dtype=X.dtype)
1.20 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
1.20 logistic.py(282):     n_classes = Y.shape[1]
1.20 logistic.py(283):     n_features = X.shape[1]
1.20 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
1.20 logistic.py(285):     w = w.reshape(n_classes, -1)
1.20 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
1.20 logistic.py(287):     if fit_intercept:
1.20 logistic.py(288):         intercept = w[:, -1]
1.20 logistic.py(289):         w = w[:, :-1]
1.20 logistic.py(292):     p = safe_sparse_dot(X, w.T)
1.20 logistic.py(293):     p += intercept
1.20 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
1.20 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
1.20 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
1.20 logistic.py(297):     p = np.exp(p, p)
1.20 logistic.py(298):     return loss, p, w
1.20 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
1.20 logistic.py(346):     diff = sample_weight * (p - Y)
1.20 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
1.20 logistic.py(348):     grad[:, :n_features] += alpha * w
1.20 logistic.py(349):     if fit_intercept:
1.20 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
1.20 logistic.py(351):     return loss, grad.ravel(), p
1.20 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
1.20 logistic.py(339):     n_classes = Y.shape[1]
1.20 logistic.py(340):     n_features = X.shape[1]
1.20 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
1.20 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
1.20 logistic.py(343):                     dtype=X.dtype)
1.20 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
1.20 logistic.py(282):     n_classes = Y.shape[1]
1.20 logistic.py(283):     n_features = X.shape[1]
1.20 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
1.20 logistic.py(285):     w = w.reshape(n_classes, -1)
1.20 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
1.20 logistic.py(287):     if fit_intercept:
1.20 logistic.py(288):         intercept = w[:, -1]
1.20 logistic.py(289):         w = w[:, :-1]
1.20 logistic.py(292):     p = safe_sparse_dot(X, w.T)
1.20 logistic.py(293):     p += intercept
1.20 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
1.20 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
1.20 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
1.20 logistic.py(297):     p = np.exp(p, p)
1.20 logistic.py(298):     return loss, p, w
1.20 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
1.20 logistic.py(346):     diff = sample_weight * (p - Y)
1.20 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
1.20 logistic.py(348):     grad[:, :n_features] += alpha * w
1.20 logistic.py(349):     if fit_intercept:
1.20 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
1.20 logistic.py(351):     return loss, grad.ravel(), p
1.20 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
1.20 logistic.py(339):     n_classes = Y.shape[1]
1.20 logistic.py(340):     n_features = X.shape[1]
1.20 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
1.20 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
1.20 logistic.py(343):                     dtype=X.dtype)
1.20 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
1.20 logistic.py(282):     n_classes = Y.shape[1]
1.20 logistic.py(283):     n_features = X.shape[1]
1.20 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
1.20 logistic.py(285):     w = w.reshape(n_classes, -1)
1.20 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
1.20 logistic.py(287):     if fit_intercept:
1.20 logistic.py(288):         intercept = w[:, -1]
1.20 logistic.py(289):         w = w[:, :-1]
1.20 logistic.py(292):     p = safe_sparse_dot(X, w.T)
1.20 logistic.py(293):     p += intercept
1.20 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
1.20 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
1.20 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
1.20 logistic.py(297):     p = np.exp(p, p)
1.20 logistic.py(298):     return loss, p, w
1.20 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
1.20 logistic.py(346):     diff = sample_weight * (p - Y)
1.20 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
1.20 logistic.py(348):     grad[:, :n_features] += alpha * w
1.20 logistic.py(349):     if fit_intercept:
1.20 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
1.20 logistic.py(351):     return loss, grad.ravel(), p
1.20 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
1.20 logistic.py(339):     n_classes = Y.shape[1]
1.20 logistic.py(340):     n_features = X.shape[1]
1.20 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
1.20 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
1.20 logistic.py(343):                     dtype=X.dtype)
1.20 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
1.20 logistic.py(282):     n_classes = Y.shape[1]
1.20 logistic.py(283):     n_features = X.shape[1]
1.20 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
1.20 logistic.py(285):     w = w.reshape(n_classes, -1)
1.20 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
1.20 logistic.py(287):     if fit_intercept:
1.20 logistic.py(288):         intercept = w[:, -1]
1.20 logistic.py(289):         w = w[:, :-1]
1.20 logistic.py(292):     p = safe_sparse_dot(X, w.T)
1.20 logistic.py(293):     p += intercept
1.20 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
1.20 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
1.20 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
1.20 logistic.py(297):     p = np.exp(p, p)
1.20 logistic.py(298):     return loss, p, w
1.20 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
1.20 logistic.py(346):     diff = sample_weight * (p - Y)
1.20 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
1.20 logistic.py(348):     grad[:, :n_features] += alpha * w
1.20 logistic.py(349):     if fit_intercept:
1.20 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
1.20 logistic.py(351):     return loss, grad.ravel(), p
1.20 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
1.20 logistic.py(339):     n_classes = Y.shape[1]
1.20 logistic.py(340):     n_features = X.shape[1]
1.20 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
1.20 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
1.20 logistic.py(343):                     dtype=X.dtype)
1.20 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
1.20 logistic.py(282):     n_classes = Y.shape[1]
1.20 logistic.py(283):     n_features = X.shape[1]
1.20 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
1.20 logistic.py(285):     w = w.reshape(n_classes, -1)
1.20 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
1.20 logistic.py(287):     if fit_intercept:
1.20 logistic.py(288):         intercept = w[:, -1]
1.20 logistic.py(289):         w = w[:, :-1]
1.20 logistic.py(292):     p = safe_sparse_dot(X, w.T)
1.20 logistic.py(293):     p += intercept
1.20 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
1.20 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
1.20 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
1.20 logistic.py(297):     p = np.exp(p, p)
1.20 logistic.py(298):     return loss, p, w
1.20 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
1.20 logistic.py(346):     diff = sample_weight * (p - Y)
1.20 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
1.20 logistic.py(348):     grad[:, :n_features] += alpha * w
1.20 logistic.py(349):     if fit_intercept:
1.20 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
1.20 logistic.py(351):     return loss, grad.ravel(), p
1.20 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
1.20 logistic.py(339):     n_classes = Y.shape[1]
1.20 logistic.py(340):     n_features = X.shape[1]
1.20 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
1.20 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
1.20 logistic.py(343):                     dtype=X.dtype)
1.20 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
1.20 logistic.py(282):     n_classes = Y.shape[1]
1.20 logistic.py(283):     n_features = X.shape[1]
1.20 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
1.20 logistic.py(285):     w = w.reshape(n_classes, -1)
1.20 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
1.20 logistic.py(287):     if fit_intercept:
1.20 logistic.py(288):         intercept = w[:, -1]
1.20 logistic.py(289):         w = w[:, :-1]
1.20 logistic.py(292):     p = safe_sparse_dot(X, w.T)
1.20 logistic.py(293):     p += intercept
1.20 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
1.20 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
1.20 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
1.20 logistic.py(297):     p = np.exp(p, p)
1.20 logistic.py(298):     return loss, p, w
1.20 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
1.20 logistic.py(346):     diff = sample_weight * (p - Y)
1.20 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
1.20 logistic.py(348):     grad[:, :n_features] += alpha * w
1.20 logistic.py(349):     if fit_intercept:
1.20 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
1.20 logistic.py(351):     return loss, grad.ravel(), p
1.20 logistic.py(718):             if info["warnflag"] == 1:
1.20 logistic.py(723):             n_iter_i = min(info['nit'], max_iter)
1.20 logistic.py(760):         if multi_class == 'multinomial':
1.20 logistic.py(761):             multi_w0 = np.reshape(w0, (classes.size, -1))
1.20 logistic.py(762):             if classes.size == 2:
1.20 logistic.py(764):             coefs.append(multi_w0)
1.20 logistic.py(768):         n_iter[i] = n_iter_i
1.20 logistic.py(712):     for i, C in enumerate(Cs):
1.20 logistic.py(713):         if solver == 'lbfgs':
1.20 logistic.py(714):             w0, loss, info = optimize.fmin_l_bfgs_b(
1.20 logistic.py(715):                 func, w0, fprime=None,
1.20 logistic.py(716):                 args=(X, target, 1. / C, sample_weight),
1.20 logistic.py(717):                 iprint=(verbose > 0) - 1, pgtol=tol, maxiter=max_iter)
1.20 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
1.20 logistic.py(339):     n_classes = Y.shape[1]
1.20 logistic.py(340):     n_features = X.shape[1]
1.20 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
1.20 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
1.20 logistic.py(343):                     dtype=X.dtype)
1.20 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
1.20 logistic.py(282):     n_classes = Y.shape[1]
1.20 logistic.py(283):     n_features = X.shape[1]
1.20 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
1.20 logistic.py(285):     w = w.reshape(n_classes, -1)
1.20 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
1.20 logistic.py(287):     if fit_intercept:
1.20 logistic.py(288):         intercept = w[:, -1]
1.20 logistic.py(289):         w = w[:, :-1]
1.20 logistic.py(292):     p = safe_sparse_dot(X, w.T)
1.20 logistic.py(293):     p += intercept
1.20 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
1.20 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
1.20 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
1.20 logistic.py(297):     p = np.exp(p, p)
1.20 logistic.py(298):     return loss, p, w
1.20 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
1.20 logistic.py(346):     diff = sample_weight * (p - Y)
1.20 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
1.20 logistic.py(348):     grad[:, :n_features] += alpha * w
1.20 logistic.py(349):     if fit_intercept:
1.20 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
1.20 logistic.py(351):     return loss, grad.ravel(), p
1.20 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
1.20 logistic.py(339):     n_classes = Y.shape[1]
1.20 logistic.py(340):     n_features = X.shape[1]
1.20 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
1.20 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
1.20 logistic.py(343):                     dtype=X.dtype)
1.20 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
1.20 logistic.py(282):     n_classes = Y.shape[1]
1.20 logistic.py(283):     n_features = X.shape[1]
1.20 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
1.20 logistic.py(285):     w = w.reshape(n_classes, -1)
1.20 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
1.20 logistic.py(287):     if fit_intercept:
1.20 logistic.py(288):         intercept = w[:, -1]
1.20 logistic.py(289):         w = w[:, :-1]
1.20 logistic.py(292):     p = safe_sparse_dot(X, w.T)
1.20 logistic.py(293):     p += intercept
1.20 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
1.20 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
1.20 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
1.20 logistic.py(297):     p = np.exp(p, p)
1.20 logistic.py(298):     return loss, p, w
1.20 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
1.20 logistic.py(346):     diff = sample_weight * (p - Y)
1.20 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
1.20 logistic.py(348):     grad[:, :n_features] += alpha * w
1.20 logistic.py(349):     if fit_intercept:
1.20 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
1.20 logistic.py(351):     return loss, grad.ravel(), p
1.20 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
1.20 logistic.py(339):     n_classes = Y.shape[1]
1.20 logistic.py(340):     n_features = X.shape[1]
1.20 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
1.20 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
1.20 logistic.py(343):                     dtype=X.dtype)
1.20 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
1.20 logistic.py(282):     n_classes = Y.shape[1]
1.20 logistic.py(283):     n_features = X.shape[1]
1.20 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
1.20 logistic.py(285):     w = w.reshape(n_classes, -1)
1.20 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
1.20 logistic.py(287):     if fit_intercept:
1.20 logistic.py(288):         intercept = w[:, -1]
1.20 logistic.py(289):         w = w[:, :-1]
1.20 logistic.py(292):     p = safe_sparse_dot(X, w.T)
1.20 logistic.py(293):     p += intercept
1.20 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
1.20 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
1.20 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
1.20 logistic.py(297):     p = np.exp(p, p)
1.20 logistic.py(298):     return loss, p, w
1.20 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
1.20 logistic.py(346):     diff = sample_weight * (p - Y)
1.20 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
1.20 logistic.py(348):     grad[:, :n_features] += alpha * w
1.20 logistic.py(349):     if fit_intercept:
1.20 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
1.20 logistic.py(351):     return loss, grad.ravel(), p
1.20 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
1.20 logistic.py(339):     n_classes = Y.shape[1]
1.20 logistic.py(340):     n_features = X.shape[1]
1.20 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
1.20 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
1.20 logistic.py(343):                     dtype=X.dtype)
1.20 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
1.20 logistic.py(282):     n_classes = Y.shape[1]
1.20 logistic.py(283):     n_features = X.shape[1]
1.20 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
1.20 logistic.py(285):     w = w.reshape(n_classes, -1)
1.20 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
1.20 logistic.py(287):     if fit_intercept:
1.20 logistic.py(288):         intercept = w[:, -1]
1.20 logistic.py(289):         w = w[:, :-1]
1.20 logistic.py(292):     p = safe_sparse_dot(X, w.T)
1.20 logistic.py(293):     p += intercept
1.20 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
1.20 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
1.20 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
1.20 logistic.py(297):     p = np.exp(p, p)
1.20 logistic.py(298):     return loss, p, w
1.20 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
1.20 logistic.py(346):     diff = sample_weight * (p - Y)
1.20 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
1.20 logistic.py(348):     grad[:, :n_features] += alpha * w
1.20 logistic.py(349):     if fit_intercept:
1.20 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
1.20 logistic.py(351):     return loss, grad.ravel(), p
1.20 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
1.20 logistic.py(339):     n_classes = Y.shape[1]
1.20 logistic.py(340):     n_features = X.shape[1]
1.20 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
1.20 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
1.20 logistic.py(343):                     dtype=X.dtype)
1.20 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
1.20 logistic.py(282):     n_classes = Y.shape[1]
1.20 logistic.py(283):     n_features = X.shape[1]
1.20 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
1.20 logistic.py(285):     w = w.reshape(n_classes, -1)
1.20 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
1.20 logistic.py(287):     if fit_intercept:
1.20 logistic.py(288):         intercept = w[:, -1]
1.20 logistic.py(289):         w = w[:, :-1]
1.20 logistic.py(292):     p = safe_sparse_dot(X, w.T)
1.20 logistic.py(293):     p += intercept
1.20 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
1.20 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
1.20 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
1.20 logistic.py(297):     p = np.exp(p, p)
1.20 logistic.py(298):     return loss, p, w
1.20 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
1.20 logistic.py(346):     diff = sample_weight * (p - Y)
1.20 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
1.20 logistic.py(348):     grad[:, :n_features] += alpha * w
1.20 logistic.py(349):     if fit_intercept:
1.20 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
1.20 logistic.py(351):     return loss, grad.ravel(), p
1.20 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
1.20 logistic.py(339):     n_classes = Y.shape[1]
1.20 logistic.py(340):     n_features = X.shape[1]
1.20 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
1.20 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
1.20 logistic.py(343):                     dtype=X.dtype)
1.20 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
1.20 logistic.py(282):     n_classes = Y.shape[1]
1.20 logistic.py(283):     n_features = X.shape[1]
1.20 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
1.20 logistic.py(285):     w = w.reshape(n_classes, -1)
1.20 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
1.20 logistic.py(287):     if fit_intercept:
1.20 logistic.py(288):         intercept = w[:, -1]
1.20 logistic.py(289):         w = w[:, :-1]
1.20 logistic.py(292):     p = safe_sparse_dot(X, w.T)
1.20 logistic.py(293):     p += intercept
1.20 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
1.20 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
1.20 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
1.20 logistic.py(297):     p = np.exp(p, p)
1.20 logistic.py(298):     return loss, p, w
1.20 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
1.20 logistic.py(346):     diff = sample_weight * (p - Y)
1.20 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
1.20 logistic.py(348):     grad[:, :n_features] += alpha * w
1.20 logistic.py(349):     if fit_intercept:
1.20 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
1.20 logistic.py(351):     return loss, grad.ravel(), p
1.20 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
1.20 logistic.py(339):     n_classes = Y.shape[1]
1.20 logistic.py(340):     n_features = X.shape[1]
1.20 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
1.20 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
1.20 logistic.py(343):                     dtype=X.dtype)
1.20 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
1.20 logistic.py(282):     n_classes = Y.shape[1]
1.20 logistic.py(283):     n_features = X.shape[1]
1.20 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
1.20 logistic.py(285):     w = w.reshape(n_classes, -1)
1.21 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
1.21 logistic.py(287):     if fit_intercept:
1.21 logistic.py(288):         intercept = w[:, -1]
1.21 logistic.py(289):         w = w[:, :-1]
1.21 logistic.py(292):     p = safe_sparse_dot(X, w.T)
1.21 logistic.py(293):     p += intercept
1.21 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
1.21 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
1.21 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
1.21 logistic.py(297):     p = np.exp(p, p)
1.21 logistic.py(298):     return loss, p, w
1.21 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
1.21 logistic.py(346):     diff = sample_weight * (p - Y)
1.21 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
1.21 logistic.py(348):     grad[:, :n_features] += alpha * w
1.21 logistic.py(349):     if fit_intercept:
1.21 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
1.21 logistic.py(351):     return loss, grad.ravel(), p
1.21 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
1.21 logistic.py(339):     n_classes = Y.shape[1]
1.21 logistic.py(340):     n_features = X.shape[1]
1.21 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
1.21 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
1.21 logistic.py(343):                     dtype=X.dtype)
1.21 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
1.21 logistic.py(282):     n_classes = Y.shape[1]
1.21 logistic.py(283):     n_features = X.shape[1]
1.21 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
1.21 logistic.py(285):     w = w.reshape(n_classes, -1)
1.21 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
1.21 logistic.py(287):     if fit_intercept:
1.21 logistic.py(288):         intercept = w[:, -1]
1.21 logistic.py(289):         w = w[:, :-1]
1.21 logistic.py(292):     p = safe_sparse_dot(X, w.T)
1.21 logistic.py(293):     p += intercept
1.21 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
1.21 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
1.21 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
1.21 logistic.py(297):     p = np.exp(p, p)
1.21 logistic.py(298):     return loss, p, w
1.21 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
1.21 logistic.py(346):     diff = sample_weight * (p - Y)
1.21 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
1.21 logistic.py(348):     grad[:, :n_features] += alpha * w
1.21 logistic.py(349):     if fit_intercept:
1.21 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
1.21 logistic.py(351):     return loss, grad.ravel(), p
1.21 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
1.21 logistic.py(339):     n_classes = Y.shape[1]
1.21 logistic.py(340):     n_features = X.shape[1]
1.21 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
1.21 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
1.21 logistic.py(343):                     dtype=X.dtype)
1.21 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
1.21 logistic.py(282):     n_classes = Y.shape[1]
1.21 logistic.py(283):     n_features = X.shape[1]
1.21 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
1.21 logistic.py(285):     w = w.reshape(n_classes, -1)
1.21 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
1.21 logistic.py(287):     if fit_intercept:
1.21 logistic.py(288):         intercept = w[:, -1]
1.21 logistic.py(289):         w = w[:, :-1]
1.21 logistic.py(292):     p = safe_sparse_dot(X, w.T)
1.21 logistic.py(293):     p += intercept
1.21 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
1.21 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
1.21 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
1.21 logistic.py(297):     p = np.exp(p, p)
1.21 logistic.py(298):     return loss, p, w
1.21 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
1.21 logistic.py(346):     diff = sample_weight * (p - Y)
1.21 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
1.21 logistic.py(348):     grad[:, :n_features] += alpha * w
1.21 logistic.py(349):     if fit_intercept:
1.21 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
1.21 logistic.py(351):     return loss, grad.ravel(), p
1.21 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
1.21 logistic.py(339):     n_classes = Y.shape[1]
1.21 logistic.py(340):     n_features = X.shape[1]
1.21 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
1.21 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
1.21 logistic.py(343):                     dtype=X.dtype)
1.21 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
1.21 logistic.py(282):     n_classes = Y.shape[1]
1.21 logistic.py(283):     n_features = X.shape[1]
1.21 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
1.21 logistic.py(285):     w = w.reshape(n_classes, -1)
1.21 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
1.21 logistic.py(287):     if fit_intercept:
1.21 logistic.py(288):         intercept = w[:, -1]
1.21 logistic.py(289):         w = w[:, :-1]
1.21 logistic.py(292):     p = safe_sparse_dot(X, w.T)
1.21 logistic.py(293):     p += intercept
1.21 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
1.21 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
1.21 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
1.21 logistic.py(297):     p = np.exp(p, p)
1.21 logistic.py(298):     return loss, p, w
1.21 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
1.21 logistic.py(346):     diff = sample_weight * (p - Y)
1.21 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
1.21 logistic.py(348):     grad[:, :n_features] += alpha * w
1.21 logistic.py(349):     if fit_intercept:
1.21 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
1.21 logistic.py(351):     return loss, grad.ravel(), p
1.21 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
1.21 logistic.py(339):     n_classes = Y.shape[1]
1.21 logistic.py(340):     n_features = X.shape[1]
1.21 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
1.21 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
1.21 logistic.py(343):                     dtype=X.dtype)
1.21 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
1.21 logistic.py(282):     n_classes = Y.shape[1]
1.21 logistic.py(283):     n_features = X.shape[1]
1.21 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
1.21 logistic.py(285):     w = w.reshape(n_classes, -1)
1.21 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
1.21 logistic.py(287):     if fit_intercept:
1.21 logistic.py(288):         intercept = w[:, -1]
1.21 logistic.py(289):         w = w[:, :-1]
1.21 logistic.py(292):     p = safe_sparse_dot(X, w.T)
1.21 logistic.py(293):     p += intercept
1.21 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
1.21 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
1.21 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
1.21 logistic.py(297):     p = np.exp(p, p)
1.21 logistic.py(298):     return loss, p, w
1.21 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
1.21 logistic.py(346):     diff = sample_weight * (p - Y)
1.21 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
1.21 logistic.py(348):     grad[:, :n_features] += alpha * w
1.21 logistic.py(349):     if fit_intercept:
1.21 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
1.21 logistic.py(351):     return loss, grad.ravel(), p
1.21 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
1.21 logistic.py(339):     n_classes = Y.shape[1]
1.21 logistic.py(340):     n_features = X.shape[1]
1.21 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
1.21 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
1.21 logistic.py(343):                     dtype=X.dtype)
1.21 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
1.21 logistic.py(282):     n_classes = Y.shape[1]
1.21 logistic.py(283):     n_features = X.shape[1]
1.21 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
1.21 logistic.py(285):     w = w.reshape(n_classes, -1)
1.21 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
1.21 logistic.py(287):     if fit_intercept:
1.21 logistic.py(288):         intercept = w[:, -1]
1.21 logistic.py(289):         w = w[:, :-1]
1.21 logistic.py(292):     p = safe_sparse_dot(X, w.T)
1.21 logistic.py(293):     p += intercept
1.21 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
1.21 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
1.21 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
1.21 logistic.py(297):     p = np.exp(p, p)
1.21 logistic.py(298):     return loss, p, w
1.21 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
1.21 logistic.py(346):     diff = sample_weight * (p - Y)
1.21 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
1.21 logistic.py(348):     grad[:, :n_features] += alpha * w
1.21 logistic.py(349):     if fit_intercept:
1.21 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
1.21 logistic.py(351):     return loss, grad.ravel(), p
1.21 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
1.21 logistic.py(339):     n_classes = Y.shape[1]
1.21 logistic.py(340):     n_features = X.shape[1]
1.21 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
1.21 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
1.21 logistic.py(343):                     dtype=X.dtype)
1.21 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
1.21 logistic.py(282):     n_classes = Y.shape[1]
1.21 logistic.py(283):     n_features = X.shape[1]
1.21 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
1.21 logistic.py(285):     w = w.reshape(n_classes, -1)
1.21 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
1.21 logistic.py(287):     if fit_intercept:
1.21 logistic.py(288):         intercept = w[:, -1]
1.21 logistic.py(289):         w = w[:, :-1]
1.21 logistic.py(292):     p = safe_sparse_dot(X, w.T)
1.21 logistic.py(293):     p += intercept
1.21 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
1.21 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
1.21 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
1.21 logistic.py(297):     p = np.exp(p, p)
1.21 logistic.py(298):     return loss, p, w
1.21 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
1.21 logistic.py(346):     diff = sample_weight * (p - Y)
1.21 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
1.21 logistic.py(348):     grad[:, :n_features] += alpha * w
1.21 logistic.py(349):     if fit_intercept:
1.21 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
1.21 logistic.py(351):     return loss, grad.ravel(), p
1.21 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
1.21 logistic.py(339):     n_classes = Y.shape[1]
1.21 logistic.py(340):     n_features = X.shape[1]
1.21 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
1.21 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
1.21 logistic.py(343):                     dtype=X.dtype)
1.21 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
1.21 logistic.py(282):     n_classes = Y.shape[1]
1.21 logistic.py(283):     n_features = X.shape[1]
1.21 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
1.21 logistic.py(285):     w = w.reshape(n_classes, -1)
1.21 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
1.21 logistic.py(287):     if fit_intercept:
1.21 logistic.py(288):         intercept = w[:, -1]
1.21 logistic.py(289):         w = w[:, :-1]
1.21 logistic.py(292):     p = safe_sparse_dot(X, w.T)
1.21 logistic.py(293):     p += intercept
1.21 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
1.21 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
1.21 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
1.21 logistic.py(297):     p = np.exp(p, p)
1.21 logistic.py(298):     return loss, p, w
1.21 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
1.21 logistic.py(346):     diff = sample_weight * (p - Y)
1.21 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
1.21 logistic.py(348):     grad[:, :n_features] += alpha * w
1.21 logistic.py(349):     if fit_intercept:
1.21 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
1.21 logistic.py(351):     return loss, grad.ravel(), p
1.21 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
1.21 logistic.py(339):     n_classes = Y.shape[1]
1.21 logistic.py(340):     n_features = X.shape[1]
1.21 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
1.21 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
1.21 logistic.py(343):                     dtype=X.dtype)
1.21 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
1.21 logistic.py(282):     n_classes = Y.shape[1]
1.21 logistic.py(283):     n_features = X.shape[1]
1.21 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
1.21 logistic.py(285):     w = w.reshape(n_classes, -1)
1.21 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
1.21 logistic.py(287):     if fit_intercept:
1.21 logistic.py(288):         intercept = w[:, -1]
1.21 logistic.py(289):         w = w[:, :-1]
1.21 logistic.py(292):     p = safe_sparse_dot(X, w.T)
1.21 logistic.py(293):     p += intercept
1.21 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
1.21 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
1.21 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
1.21 logistic.py(297):     p = np.exp(p, p)
1.21 logistic.py(298):     return loss, p, w
1.21 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
1.21 logistic.py(346):     diff = sample_weight * (p - Y)
1.21 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
1.21 logistic.py(348):     grad[:, :n_features] += alpha * w
1.21 logistic.py(349):     if fit_intercept:
1.21 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
1.21 logistic.py(351):     return loss, grad.ravel(), p
1.21 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
1.21 logistic.py(339):     n_classes = Y.shape[1]
1.21 logistic.py(340):     n_features = X.shape[1]
1.21 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
1.21 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
1.21 logistic.py(343):                     dtype=X.dtype)
1.21 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
1.21 logistic.py(282):     n_classes = Y.shape[1]
1.21 logistic.py(283):     n_features = X.shape[1]
1.21 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
1.21 logistic.py(285):     w = w.reshape(n_classes, -1)
1.21 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
1.21 logistic.py(287):     if fit_intercept:
1.21 logistic.py(288):         intercept = w[:, -1]
1.21 logistic.py(289):         w = w[:, :-1]
1.21 logistic.py(292):     p = safe_sparse_dot(X, w.T)
1.21 logistic.py(293):     p += intercept
1.21 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
1.21 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
1.21 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
1.21 logistic.py(297):     p = np.exp(p, p)
1.21 logistic.py(298):     return loss, p, w
1.21 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
1.21 logistic.py(346):     diff = sample_weight * (p - Y)
1.21 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
1.21 logistic.py(348):     grad[:, :n_features] += alpha * w
1.21 logistic.py(349):     if fit_intercept:
1.21 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
1.21 logistic.py(351):     return loss, grad.ravel(), p
1.21 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
1.21 logistic.py(339):     n_classes = Y.shape[1]
1.21 logistic.py(340):     n_features = X.shape[1]
1.21 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
1.21 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
1.21 logistic.py(343):                     dtype=X.dtype)
1.21 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
1.21 logistic.py(282):     n_classes = Y.shape[1]
1.21 logistic.py(283):     n_features = X.shape[1]
1.21 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
1.21 logistic.py(285):     w = w.reshape(n_classes, -1)
1.21 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
1.21 logistic.py(287):     if fit_intercept:
1.21 logistic.py(288):         intercept = w[:, -1]
1.21 logistic.py(289):         w = w[:, :-1]
1.21 logistic.py(292):     p = safe_sparse_dot(X, w.T)
1.21 logistic.py(293):     p += intercept
1.21 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
1.21 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
1.21 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
1.21 logistic.py(297):     p = np.exp(p, p)
1.21 logistic.py(298):     return loss, p, w
1.21 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
1.21 logistic.py(346):     diff = sample_weight * (p - Y)
1.21 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
1.21 logistic.py(348):     grad[:, :n_features] += alpha * w
1.21 logistic.py(349):     if fit_intercept:
1.21 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
1.21 logistic.py(351):     return loss, grad.ravel(), p
1.21 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
1.21 logistic.py(339):     n_classes = Y.shape[1]
1.21 logistic.py(340):     n_features = X.shape[1]
1.21 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
1.21 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
1.21 logistic.py(343):                     dtype=X.dtype)
1.21 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
1.21 logistic.py(282):     n_classes = Y.shape[1]
1.21 logistic.py(283):     n_features = X.shape[1]
1.21 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
1.21 logistic.py(285):     w = w.reshape(n_classes, -1)
1.21 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
1.21 logistic.py(287):     if fit_intercept:
1.21 logistic.py(288):         intercept = w[:, -1]
1.21 logistic.py(289):         w = w[:, :-1]
1.21 logistic.py(292):     p = safe_sparse_dot(X, w.T)
1.21 logistic.py(293):     p += intercept
1.21 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
1.21 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
1.21 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
1.21 logistic.py(297):     p = np.exp(p, p)
1.21 logistic.py(298):     return loss, p, w
1.21 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
1.21 logistic.py(346):     diff = sample_weight * (p - Y)
1.21 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
1.21 logistic.py(348):     grad[:, :n_features] += alpha * w
1.21 logistic.py(349):     if fit_intercept:
1.21 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
1.21 logistic.py(351):     return loss, grad.ravel(), p
1.21 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
1.21 logistic.py(339):     n_classes = Y.shape[1]
1.21 logistic.py(340):     n_features = X.shape[1]
1.21 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
1.21 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
1.21 logistic.py(343):                     dtype=X.dtype)
1.21 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
1.21 logistic.py(282):     n_classes = Y.shape[1]
1.21 logistic.py(283):     n_features = X.shape[1]
1.21 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
1.21 logistic.py(285):     w = w.reshape(n_classes, -1)
1.21 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
1.21 logistic.py(287):     if fit_intercept:
1.21 logistic.py(288):         intercept = w[:, -1]
1.21 logistic.py(289):         w = w[:, :-1]
1.21 logistic.py(292):     p = safe_sparse_dot(X, w.T)
1.21 logistic.py(293):     p += intercept
1.21 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
1.21 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
1.21 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
1.21 logistic.py(297):     p = np.exp(p, p)
1.21 logistic.py(298):     return loss, p, w
1.21 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
1.21 logistic.py(346):     diff = sample_weight * (p - Y)
1.21 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
1.21 logistic.py(348):     grad[:, :n_features] += alpha * w
1.21 logistic.py(349):     if fit_intercept:
1.21 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
1.21 logistic.py(351):     return loss, grad.ravel(), p
1.21 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
1.21 logistic.py(339):     n_classes = Y.shape[1]
1.21 logistic.py(340):     n_features = X.shape[1]
1.21 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
1.21 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
1.21 logistic.py(343):                     dtype=X.dtype)
1.21 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
1.21 logistic.py(282):     n_classes = Y.shape[1]
1.21 logistic.py(283):     n_features = X.shape[1]
1.21 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
1.21 logistic.py(285):     w = w.reshape(n_classes, -1)
1.21 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
1.21 logistic.py(287):     if fit_intercept:
1.21 logistic.py(288):         intercept = w[:, -1]
1.21 logistic.py(289):         w = w[:, :-1]
1.21 logistic.py(292):     p = safe_sparse_dot(X, w.T)
1.21 logistic.py(293):     p += intercept
1.21 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
1.21 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
1.21 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
1.21 logistic.py(297):     p = np.exp(p, p)
1.21 logistic.py(298):     return loss, p, w
1.21 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
1.21 logistic.py(346):     diff = sample_weight * (p - Y)
1.21 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
1.21 logistic.py(348):     grad[:, :n_features] += alpha * w
1.21 logistic.py(349):     if fit_intercept:
1.21 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
1.21 logistic.py(351):     return loss, grad.ravel(), p
1.21 logistic.py(718):             if info["warnflag"] == 1:
1.21 logistic.py(723):             n_iter_i = min(info['nit'], max_iter)
1.21 logistic.py(760):         if multi_class == 'multinomial':
1.21 logistic.py(761):             multi_w0 = np.reshape(w0, (classes.size, -1))
1.21 logistic.py(762):             if classes.size == 2:
1.21 logistic.py(764):             coefs.append(multi_w0)
1.21 logistic.py(768):         n_iter[i] = n_iter_i
1.21 logistic.py(712):     for i, C in enumerate(Cs):
1.21 logistic.py(713):         if solver == 'lbfgs':
1.21 logistic.py(714):             w0, loss, info = optimize.fmin_l_bfgs_b(
1.21 logistic.py(715):                 func, w0, fprime=None,
1.21 logistic.py(716):                 args=(X, target, 1. / C, sample_weight),
1.21 logistic.py(717):                 iprint=(verbose > 0) - 1, pgtol=tol, maxiter=max_iter)
1.21 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
1.21 logistic.py(339):     n_classes = Y.shape[1]
1.21 logistic.py(340):     n_features = X.shape[1]
1.21 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
1.21 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
1.21 logistic.py(343):                     dtype=X.dtype)
1.21 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
1.21 logistic.py(282):     n_classes = Y.shape[1]
1.21 logistic.py(283):     n_features = X.shape[1]
1.21 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
1.21 logistic.py(285):     w = w.reshape(n_classes, -1)
1.21 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
1.21 logistic.py(287):     if fit_intercept:
1.21 logistic.py(288):         intercept = w[:, -1]
1.21 logistic.py(289):         w = w[:, :-1]
1.21 logistic.py(292):     p = safe_sparse_dot(X, w.T)
1.21 logistic.py(293):     p += intercept
1.21 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
1.21 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
1.21 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
1.21 logistic.py(297):     p = np.exp(p, p)
1.21 logistic.py(298):     return loss, p, w
1.21 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
1.21 logistic.py(346):     diff = sample_weight * (p - Y)
1.21 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
1.21 logistic.py(348):     grad[:, :n_features] += alpha * w
1.21 logistic.py(349):     if fit_intercept:
1.21 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
1.21 logistic.py(351):     return loss, grad.ravel(), p
1.21 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
1.21 logistic.py(339):     n_classes = Y.shape[1]
1.21 logistic.py(340):     n_features = X.shape[1]
1.21 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
1.21 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
1.21 logistic.py(343):                     dtype=X.dtype)
1.21 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
1.21 logistic.py(282):     n_classes = Y.shape[1]
1.21 logistic.py(283):     n_features = X.shape[1]
1.21 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
1.21 logistic.py(285):     w = w.reshape(n_classes, -1)
1.21 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
1.21 logistic.py(287):     if fit_intercept:
1.21 logistic.py(288):         intercept = w[:, -1]
1.21 logistic.py(289):         w = w[:, :-1]
1.21 logistic.py(292):     p = safe_sparse_dot(X, w.T)
1.21 logistic.py(293):     p += intercept
1.21 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
1.21 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
1.21 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
1.21 logistic.py(297):     p = np.exp(p, p)
1.21 logistic.py(298):     return loss, p, w
1.21 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
1.21 logistic.py(346):     diff = sample_weight * (p - Y)
1.21 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
1.21 logistic.py(348):     grad[:, :n_features] += alpha * w
1.21 logistic.py(349):     if fit_intercept:
1.21 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
1.21 logistic.py(351):     return loss, grad.ravel(), p
1.21 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
1.21 logistic.py(339):     n_classes = Y.shape[1]
1.21 logistic.py(340):     n_features = X.shape[1]
1.21 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
1.21 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
1.21 logistic.py(343):                     dtype=X.dtype)
1.21 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
1.21 logistic.py(282):     n_classes = Y.shape[1]
1.21 logistic.py(283):     n_features = X.shape[1]
1.21 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
1.21 logistic.py(285):     w = w.reshape(n_classes, -1)
1.21 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
1.21 logistic.py(287):     if fit_intercept:
1.21 logistic.py(288):         intercept = w[:, -1]
1.21 logistic.py(289):         w = w[:, :-1]
1.21 logistic.py(292):     p = safe_sparse_dot(X, w.T)
1.21 logistic.py(293):     p += intercept
1.21 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
1.22 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
1.22 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
1.22 logistic.py(297):     p = np.exp(p, p)
1.22 logistic.py(298):     return loss, p, w
1.22 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
1.22 logistic.py(346):     diff = sample_weight * (p - Y)
1.22 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
1.22 logistic.py(348):     grad[:, :n_features] += alpha * w
1.22 logistic.py(349):     if fit_intercept:
1.22 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
1.22 logistic.py(351):     return loss, grad.ravel(), p
1.22 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
1.22 logistic.py(339):     n_classes = Y.shape[1]
1.22 logistic.py(340):     n_features = X.shape[1]
1.22 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
1.22 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
1.22 logistic.py(343):                     dtype=X.dtype)
1.22 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
1.22 logistic.py(282):     n_classes = Y.shape[1]
1.22 logistic.py(283):     n_features = X.shape[1]
1.22 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
1.22 logistic.py(285):     w = w.reshape(n_classes, -1)
1.22 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
1.22 logistic.py(287):     if fit_intercept:
1.22 logistic.py(288):         intercept = w[:, -1]
1.22 logistic.py(289):         w = w[:, :-1]
1.22 logistic.py(292):     p = safe_sparse_dot(X, w.T)
1.22 logistic.py(293):     p += intercept
1.22 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
1.22 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
1.22 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
1.22 logistic.py(297):     p = np.exp(p, p)
1.22 logistic.py(298):     return loss, p, w
1.22 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
1.22 logistic.py(346):     diff = sample_weight * (p - Y)
1.22 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
1.22 logistic.py(348):     grad[:, :n_features] += alpha * w
1.22 logistic.py(349):     if fit_intercept:
1.22 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
1.22 logistic.py(351):     return loss, grad.ravel(), p
1.22 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
1.22 logistic.py(339):     n_classes = Y.shape[1]
1.22 logistic.py(340):     n_features = X.shape[1]
1.22 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
1.22 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
1.22 logistic.py(343):                     dtype=X.dtype)
1.22 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
1.22 logistic.py(282):     n_classes = Y.shape[1]
1.22 logistic.py(283):     n_features = X.shape[1]
1.22 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
1.22 logistic.py(285):     w = w.reshape(n_classes, -1)
1.22 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
1.22 logistic.py(287):     if fit_intercept:
1.22 logistic.py(288):         intercept = w[:, -1]
1.22 logistic.py(289):         w = w[:, :-1]
1.22 logistic.py(292):     p = safe_sparse_dot(X, w.T)
1.22 logistic.py(293):     p += intercept
1.22 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
1.22 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
1.22 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
1.22 logistic.py(297):     p = np.exp(p, p)
1.22 logistic.py(298):     return loss, p, w
1.22 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
1.22 logistic.py(346):     diff = sample_weight * (p - Y)
1.22 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
1.22 logistic.py(348):     grad[:, :n_features] += alpha * w
1.22 logistic.py(349):     if fit_intercept:
1.22 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
1.22 logistic.py(351):     return loss, grad.ravel(), p
1.22 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
1.22 logistic.py(339):     n_classes = Y.shape[1]
1.22 logistic.py(340):     n_features = X.shape[1]
1.22 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
1.22 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
1.22 logistic.py(343):                     dtype=X.dtype)
1.22 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
1.22 logistic.py(282):     n_classes = Y.shape[1]
1.22 logistic.py(283):     n_features = X.shape[1]
1.22 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
1.22 logistic.py(285):     w = w.reshape(n_classes, -1)
1.22 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
1.22 logistic.py(287):     if fit_intercept:
1.22 logistic.py(288):         intercept = w[:, -1]
1.22 logistic.py(289):         w = w[:, :-1]
1.22 logistic.py(292):     p = safe_sparse_dot(X, w.T)
1.22 logistic.py(293):     p += intercept
1.22 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
1.22 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
1.22 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
1.22 logistic.py(297):     p = np.exp(p, p)
1.22 logistic.py(298):     return loss, p, w
1.22 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
1.22 logistic.py(346):     diff = sample_weight * (p - Y)
1.22 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
1.22 logistic.py(348):     grad[:, :n_features] += alpha * w
1.22 logistic.py(349):     if fit_intercept:
1.22 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
1.22 logistic.py(351):     return loss, grad.ravel(), p
1.22 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
1.22 logistic.py(339):     n_classes = Y.shape[1]
1.22 logistic.py(340):     n_features = X.shape[1]
1.22 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
1.22 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
1.22 logistic.py(343):                     dtype=X.dtype)
1.22 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
1.22 logistic.py(282):     n_classes = Y.shape[1]
1.22 logistic.py(283):     n_features = X.shape[1]
1.22 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
1.22 logistic.py(285):     w = w.reshape(n_classes, -1)
1.22 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
1.22 logistic.py(287):     if fit_intercept:
1.22 logistic.py(288):         intercept = w[:, -1]
1.22 logistic.py(289):         w = w[:, :-1]
1.22 logistic.py(292):     p = safe_sparse_dot(X, w.T)
1.22 logistic.py(293):     p += intercept
1.22 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
1.22 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
1.22 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
1.22 logistic.py(297):     p = np.exp(p, p)
1.22 logistic.py(298):     return loss, p, w
1.22 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
1.22 logistic.py(346):     diff = sample_weight * (p - Y)
1.22 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
1.22 logistic.py(348):     grad[:, :n_features] += alpha * w
1.22 logistic.py(349):     if fit_intercept:
1.22 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
1.22 logistic.py(351):     return loss, grad.ravel(), p
1.22 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
1.22 logistic.py(339):     n_classes = Y.shape[1]
1.22 logistic.py(340):     n_features = X.shape[1]
1.22 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
1.22 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
1.22 logistic.py(343):                     dtype=X.dtype)
1.22 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
1.22 logistic.py(282):     n_classes = Y.shape[1]
1.22 logistic.py(283):     n_features = X.shape[1]
1.22 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
1.22 logistic.py(285):     w = w.reshape(n_classes, -1)
1.22 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
1.22 logistic.py(287):     if fit_intercept:
1.22 logistic.py(288):         intercept = w[:, -1]
1.22 logistic.py(289):         w = w[:, :-1]
1.22 logistic.py(292):     p = safe_sparse_dot(X, w.T)
1.22 logistic.py(293):     p += intercept
1.22 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
1.22 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
1.22 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
1.22 logistic.py(297):     p = np.exp(p, p)
1.22 logistic.py(298):     return loss, p, w
1.22 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
1.22 logistic.py(346):     diff = sample_weight * (p - Y)
1.22 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
1.22 logistic.py(348):     grad[:, :n_features] += alpha * w
1.22 logistic.py(349):     if fit_intercept:
1.22 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
1.22 logistic.py(351):     return loss, grad.ravel(), p
1.22 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
1.22 logistic.py(339):     n_classes = Y.shape[1]
1.22 logistic.py(340):     n_features = X.shape[1]
1.22 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
1.22 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
1.22 logistic.py(343):                     dtype=X.dtype)
1.22 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
1.22 logistic.py(282):     n_classes = Y.shape[1]
1.22 logistic.py(283):     n_features = X.shape[1]
1.22 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
1.22 logistic.py(285):     w = w.reshape(n_classes, -1)
1.22 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
1.22 logistic.py(287):     if fit_intercept:
1.22 logistic.py(288):         intercept = w[:, -1]
1.22 logistic.py(289):         w = w[:, :-1]
1.22 logistic.py(292):     p = safe_sparse_dot(X, w.T)
1.22 logistic.py(293):     p += intercept
1.22 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
1.22 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
1.22 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
1.22 logistic.py(297):     p = np.exp(p, p)
1.22 logistic.py(298):     return loss, p, w
1.22 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
1.22 logistic.py(346):     diff = sample_weight * (p - Y)
1.22 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
1.22 logistic.py(348):     grad[:, :n_features] += alpha * w
1.22 logistic.py(349):     if fit_intercept:
1.22 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
1.22 logistic.py(351):     return loss, grad.ravel(), p
1.22 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
1.22 logistic.py(339):     n_classes = Y.shape[1]
1.22 logistic.py(340):     n_features = X.shape[1]
1.22 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
1.22 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
1.22 logistic.py(343):                     dtype=X.dtype)
1.22 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
1.22 logistic.py(282):     n_classes = Y.shape[1]
1.22 logistic.py(283):     n_features = X.shape[1]
1.22 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
1.22 logistic.py(285):     w = w.reshape(n_classes, -1)
1.22 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
1.22 logistic.py(287):     if fit_intercept:
1.22 logistic.py(288):         intercept = w[:, -1]
1.22 logistic.py(289):         w = w[:, :-1]
1.22 logistic.py(292):     p = safe_sparse_dot(X, w.T)
1.22 logistic.py(293):     p += intercept
1.22 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
1.22 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
1.22 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
1.22 logistic.py(297):     p = np.exp(p, p)
1.22 logistic.py(298):     return loss, p, w
1.22 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
1.22 logistic.py(346):     diff = sample_weight * (p - Y)
1.22 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
1.22 logistic.py(348):     grad[:, :n_features] += alpha * w
1.22 logistic.py(349):     if fit_intercept:
1.22 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
1.22 logistic.py(351):     return loss, grad.ravel(), p
1.22 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
1.22 logistic.py(339):     n_classes = Y.shape[1]
1.22 logistic.py(340):     n_features = X.shape[1]
1.22 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
1.22 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
1.22 logistic.py(343):                     dtype=X.dtype)
1.22 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
1.22 logistic.py(282):     n_classes = Y.shape[1]
1.22 logistic.py(283):     n_features = X.shape[1]
1.22 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
1.22 logistic.py(285):     w = w.reshape(n_classes, -1)
1.22 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
1.22 logistic.py(287):     if fit_intercept:
1.22 logistic.py(288):         intercept = w[:, -1]
1.22 logistic.py(289):         w = w[:, :-1]
1.22 logistic.py(292):     p = safe_sparse_dot(X, w.T)
1.22 logistic.py(293):     p += intercept
1.22 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
1.22 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
1.22 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
1.22 logistic.py(297):     p = np.exp(p, p)
1.22 logistic.py(298):     return loss, p, w
1.22 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
1.22 logistic.py(346):     diff = sample_weight * (p - Y)
1.22 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
1.22 logistic.py(348):     grad[:, :n_features] += alpha * w
1.22 logistic.py(349):     if fit_intercept:
1.22 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
1.22 logistic.py(351):     return loss, grad.ravel(), p
1.22 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
1.22 logistic.py(339):     n_classes = Y.shape[1]
1.22 logistic.py(340):     n_features = X.shape[1]
1.22 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
1.22 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
1.22 logistic.py(343):                     dtype=X.dtype)
1.22 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
1.22 logistic.py(282):     n_classes = Y.shape[1]
1.22 logistic.py(283):     n_features = X.shape[1]
1.22 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
1.22 logistic.py(285):     w = w.reshape(n_classes, -1)
1.22 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
1.22 logistic.py(287):     if fit_intercept:
1.22 logistic.py(288):         intercept = w[:, -1]
1.22 logistic.py(289):         w = w[:, :-1]
1.22 logistic.py(292):     p = safe_sparse_dot(X, w.T)
1.22 logistic.py(293):     p += intercept
1.22 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
1.22 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
1.22 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
1.22 logistic.py(297):     p = np.exp(p, p)
1.22 logistic.py(298):     return loss, p, w
1.22 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
1.22 logistic.py(346):     diff = sample_weight * (p - Y)
1.22 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
1.22 logistic.py(348):     grad[:, :n_features] += alpha * w
1.22 logistic.py(349):     if fit_intercept:
1.22 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
1.22 logistic.py(351):     return loss, grad.ravel(), p
1.22 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
1.22 logistic.py(339):     n_classes = Y.shape[1]
1.22 logistic.py(340):     n_features = X.shape[1]
1.22 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
1.22 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
1.22 logistic.py(343):                     dtype=X.dtype)
1.22 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
1.22 logistic.py(282):     n_classes = Y.shape[1]
1.22 logistic.py(283):     n_features = X.shape[1]
1.22 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
1.22 logistic.py(285):     w = w.reshape(n_classes, -1)
1.22 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
1.22 logistic.py(287):     if fit_intercept:
1.22 logistic.py(288):         intercept = w[:, -1]
1.22 logistic.py(289):         w = w[:, :-1]
1.22 logistic.py(292):     p = safe_sparse_dot(X, w.T)
1.22 logistic.py(293):     p += intercept
1.22 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
1.22 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
1.22 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
1.22 logistic.py(297):     p = np.exp(p, p)
1.22 logistic.py(298):     return loss, p, w
1.22 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
1.22 logistic.py(346):     diff = sample_weight * (p - Y)
1.22 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
1.22 logistic.py(348):     grad[:, :n_features] += alpha * w
1.22 logistic.py(349):     if fit_intercept:
1.22 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
1.22 logistic.py(351):     return loss, grad.ravel(), p
1.22 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
1.22 logistic.py(339):     n_classes = Y.shape[1]
1.22 logistic.py(340):     n_features = X.shape[1]
1.22 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
1.22 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
1.22 logistic.py(343):                     dtype=X.dtype)
1.22 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
1.22 logistic.py(282):     n_classes = Y.shape[1]
1.22 logistic.py(283):     n_features = X.shape[1]
1.22 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
1.22 logistic.py(285):     w = w.reshape(n_classes, -1)
1.22 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
1.22 logistic.py(287):     if fit_intercept:
1.22 logistic.py(288):         intercept = w[:, -1]
1.22 logistic.py(289):         w = w[:, :-1]
1.22 logistic.py(292):     p = safe_sparse_dot(X, w.T)
1.22 logistic.py(293):     p += intercept
1.22 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
1.22 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
1.22 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
1.22 logistic.py(297):     p = np.exp(p, p)
1.22 logistic.py(298):     return loss, p, w
1.22 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
1.22 logistic.py(346):     diff = sample_weight * (p - Y)
1.22 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
1.22 logistic.py(348):     grad[:, :n_features] += alpha * w
1.22 logistic.py(349):     if fit_intercept:
1.22 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
1.22 logistic.py(351):     return loss, grad.ravel(), p
1.22 logistic.py(718):             if info["warnflag"] == 1:
1.22 logistic.py(723):             n_iter_i = min(info['nit'], max_iter)
1.22 logistic.py(760):         if multi_class == 'multinomial':
1.22 logistic.py(761):             multi_w0 = np.reshape(w0, (classes.size, -1))
1.22 logistic.py(762):             if classes.size == 2:
1.22 logistic.py(764):             coefs.append(multi_w0)
1.22 logistic.py(768):         n_iter[i] = n_iter_i
1.22 logistic.py(712):     for i, C in enumerate(Cs):
1.22 logistic.py(713):         if solver == 'lbfgs':
1.22 logistic.py(714):             w0, loss, info = optimize.fmin_l_bfgs_b(
1.22 logistic.py(715):                 func, w0, fprime=None,
1.22 logistic.py(716):                 args=(X, target, 1. / C, sample_weight),
1.22 logistic.py(717):                 iprint=(verbose > 0) - 1, pgtol=tol, maxiter=max_iter)
1.22 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
1.22 logistic.py(339):     n_classes = Y.shape[1]
1.22 logistic.py(340):     n_features = X.shape[1]
1.22 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
1.22 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
1.22 logistic.py(343):                     dtype=X.dtype)
1.22 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
1.22 logistic.py(282):     n_classes = Y.shape[1]
1.22 logistic.py(283):     n_features = X.shape[1]
1.22 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
1.22 logistic.py(285):     w = w.reshape(n_classes, -1)
1.22 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
1.22 logistic.py(287):     if fit_intercept:
1.22 logistic.py(288):         intercept = w[:, -1]
1.22 logistic.py(289):         w = w[:, :-1]
1.22 logistic.py(292):     p = safe_sparse_dot(X, w.T)
1.22 logistic.py(293):     p += intercept
1.22 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
1.22 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
1.22 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
1.22 logistic.py(297):     p = np.exp(p, p)
1.22 logistic.py(298):     return loss, p, w
1.22 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
1.22 logistic.py(346):     diff = sample_weight * (p - Y)
1.22 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
1.22 logistic.py(348):     grad[:, :n_features] += alpha * w
1.22 logistic.py(349):     if fit_intercept:
1.22 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
1.22 logistic.py(351):     return loss, grad.ravel(), p
1.22 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
1.22 logistic.py(339):     n_classes = Y.shape[1]
1.22 logistic.py(340):     n_features = X.shape[1]
1.22 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
1.22 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
1.22 logistic.py(343):                     dtype=X.dtype)
1.22 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
1.22 logistic.py(282):     n_classes = Y.shape[1]
1.22 logistic.py(283):     n_features = X.shape[1]
1.22 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
1.22 logistic.py(285):     w = w.reshape(n_classes, -1)
1.22 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
1.22 logistic.py(287):     if fit_intercept:
1.22 logistic.py(288):         intercept = w[:, -1]
1.22 logistic.py(289):         w = w[:, :-1]
1.22 logistic.py(292):     p = safe_sparse_dot(X, w.T)
1.22 logistic.py(293):     p += intercept
1.22 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
1.22 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
1.22 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
1.22 logistic.py(297):     p = np.exp(p, p)
1.22 logistic.py(298):     return loss, p, w
1.22 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
1.22 logistic.py(346):     diff = sample_weight * (p - Y)
1.22 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
1.22 logistic.py(348):     grad[:, :n_features] += alpha * w
1.22 logistic.py(349):     if fit_intercept:
1.22 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
1.22 logistic.py(351):     return loss, grad.ravel(), p
1.22 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
1.22 logistic.py(339):     n_classes = Y.shape[1]
1.22 logistic.py(340):     n_features = X.shape[1]
1.22 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
1.22 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
1.22 logistic.py(343):                     dtype=X.dtype)
1.22 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
1.22 logistic.py(282):     n_classes = Y.shape[1]
1.22 logistic.py(283):     n_features = X.shape[1]
1.22 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
1.22 logistic.py(285):     w = w.reshape(n_classes, -1)
1.22 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
1.22 logistic.py(287):     if fit_intercept:
1.22 logistic.py(288):         intercept = w[:, -1]
1.22 logistic.py(289):         w = w[:, :-1]
1.22 logistic.py(292):     p = safe_sparse_dot(X, w.T)
1.22 logistic.py(293):     p += intercept
1.22 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
1.22 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
1.22 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
1.22 logistic.py(297):     p = np.exp(p, p)
1.22 logistic.py(298):     return loss, p, w
1.22 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
1.22 logistic.py(346):     diff = sample_weight * (p - Y)
1.22 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
1.22 logistic.py(348):     grad[:, :n_features] += alpha * w
1.22 logistic.py(349):     if fit_intercept:
1.22 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
1.22 logistic.py(351):     return loss, grad.ravel(), p
1.22 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
1.22 logistic.py(339):     n_classes = Y.shape[1]
1.22 logistic.py(340):     n_features = X.shape[1]
1.22 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
1.22 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
1.22 logistic.py(343):                     dtype=X.dtype)
1.22 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
1.22 logistic.py(282):     n_classes = Y.shape[1]
1.22 logistic.py(283):     n_features = X.shape[1]
1.22 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
1.22 logistic.py(285):     w = w.reshape(n_classes, -1)
1.22 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
1.22 logistic.py(287):     if fit_intercept:
1.22 logistic.py(288):         intercept = w[:, -1]
1.22 logistic.py(289):         w = w[:, :-1]
1.22 logistic.py(292):     p = safe_sparse_dot(X, w.T)
1.22 logistic.py(293):     p += intercept
1.22 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
1.22 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
1.22 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
1.22 logistic.py(297):     p = np.exp(p, p)
1.22 logistic.py(298):     return loss, p, w
1.22 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
1.22 logistic.py(346):     diff = sample_weight * (p - Y)
1.22 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
1.22 logistic.py(348):     grad[:, :n_features] += alpha * w
1.22 logistic.py(349):     if fit_intercept:
1.22 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
1.22 logistic.py(351):     return loss, grad.ravel(), p
1.22 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
1.22 logistic.py(339):     n_classes = Y.shape[1]
1.22 logistic.py(340):     n_features = X.shape[1]
1.22 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
1.22 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
1.22 logistic.py(343):                     dtype=X.dtype)
1.22 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
1.22 logistic.py(282):     n_classes = Y.shape[1]
1.22 logistic.py(283):     n_features = X.shape[1]
1.22 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
1.22 logistic.py(285):     w = w.reshape(n_classes, -1)
1.22 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
1.22 logistic.py(287):     if fit_intercept:
1.22 logistic.py(288):         intercept = w[:, -1]
1.22 logistic.py(289):         w = w[:, :-1]
1.22 logistic.py(292):     p = safe_sparse_dot(X, w.T)
1.23 logistic.py(293):     p += intercept
1.23 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
1.23 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
1.23 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
1.23 logistic.py(297):     p = np.exp(p, p)
1.23 logistic.py(298):     return loss, p, w
1.23 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
1.23 logistic.py(346):     diff = sample_weight * (p - Y)
1.23 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
1.23 logistic.py(348):     grad[:, :n_features] += alpha * w
1.23 logistic.py(349):     if fit_intercept:
1.23 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
1.23 logistic.py(351):     return loss, grad.ravel(), p
1.23 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
1.23 logistic.py(339):     n_classes = Y.shape[1]
1.23 logistic.py(340):     n_features = X.shape[1]
1.23 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
1.23 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
1.23 logistic.py(343):                     dtype=X.dtype)
1.23 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
1.23 logistic.py(282):     n_classes = Y.shape[1]
1.23 logistic.py(283):     n_features = X.shape[1]
1.23 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
1.23 logistic.py(285):     w = w.reshape(n_classes, -1)
1.23 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
1.23 logistic.py(287):     if fit_intercept:
1.23 logistic.py(288):         intercept = w[:, -1]
1.23 logistic.py(289):         w = w[:, :-1]
1.23 logistic.py(292):     p = safe_sparse_dot(X, w.T)
1.23 logistic.py(293):     p += intercept
1.23 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
1.23 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
1.23 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
1.23 logistic.py(297):     p = np.exp(p, p)
1.23 logistic.py(298):     return loss, p, w
1.23 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
1.23 logistic.py(346):     diff = sample_weight * (p - Y)
1.23 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
1.23 logistic.py(348):     grad[:, :n_features] += alpha * w
1.23 logistic.py(349):     if fit_intercept:
1.23 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
1.23 logistic.py(351):     return loss, grad.ravel(), p
1.23 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
1.23 logistic.py(339):     n_classes = Y.shape[1]
1.23 logistic.py(340):     n_features = X.shape[1]
1.23 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
1.23 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
1.23 logistic.py(343):                     dtype=X.dtype)
1.23 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
1.23 logistic.py(282):     n_classes = Y.shape[1]
1.23 logistic.py(283):     n_features = X.shape[1]
1.23 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
1.23 logistic.py(285):     w = w.reshape(n_classes, -1)
1.23 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
1.23 logistic.py(287):     if fit_intercept:
1.23 logistic.py(288):         intercept = w[:, -1]
1.23 logistic.py(289):         w = w[:, :-1]
1.23 logistic.py(292):     p = safe_sparse_dot(X, w.T)
1.23 logistic.py(293):     p += intercept
1.23 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
1.23 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
1.23 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
1.23 logistic.py(297):     p = np.exp(p, p)
1.23 logistic.py(298):     return loss, p, w
1.23 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
1.23 logistic.py(346):     diff = sample_weight * (p - Y)
1.23 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
1.23 logistic.py(348):     grad[:, :n_features] += alpha * w
1.23 logistic.py(349):     if fit_intercept:
1.23 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
1.23 logistic.py(351):     return loss, grad.ravel(), p
1.23 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
1.23 logistic.py(339):     n_classes = Y.shape[1]
1.23 logistic.py(340):     n_features = X.shape[1]
1.23 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
1.23 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
1.23 logistic.py(343):                     dtype=X.dtype)
1.23 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
1.23 logistic.py(282):     n_classes = Y.shape[1]
1.23 logistic.py(283):     n_features = X.shape[1]
1.23 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
1.23 logistic.py(285):     w = w.reshape(n_classes, -1)
1.23 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
1.23 logistic.py(287):     if fit_intercept:
1.23 logistic.py(288):         intercept = w[:, -1]
1.23 logistic.py(289):         w = w[:, :-1]
1.23 logistic.py(292):     p = safe_sparse_dot(X, w.T)
1.23 logistic.py(293):     p += intercept
1.23 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
1.23 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
1.23 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
1.23 logistic.py(297):     p = np.exp(p, p)
1.23 logistic.py(298):     return loss, p, w
1.23 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
1.23 logistic.py(346):     diff = sample_weight * (p - Y)
1.23 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
1.23 logistic.py(348):     grad[:, :n_features] += alpha * w
1.23 logistic.py(349):     if fit_intercept:
1.23 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
1.23 logistic.py(351):     return loss, grad.ravel(), p
1.23 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
1.23 logistic.py(339):     n_classes = Y.shape[1]
1.23 logistic.py(340):     n_features = X.shape[1]
1.23 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
1.23 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
1.23 logistic.py(343):                     dtype=X.dtype)
1.23 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
1.23 logistic.py(282):     n_classes = Y.shape[1]
1.23 logistic.py(283):     n_features = X.shape[1]
1.23 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
1.23 logistic.py(285):     w = w.reshape(n_classes, -1)
1.23 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
1.23 logistic.py(287):     if fit_intercept:
1.23 logistic.py(288):         intercept = w[:, -1]
1.23 logistic.py(289):         w = w[:, :-1]
1.23 logistic.py(292):     p = safe_sparse_dot(X, w.T)
1.23 logistic.py(293):     p += intercept
1.23 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
1.23 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
1.23 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
1.23 logistic.py(297):     p = np.exp(p, p)
1.23 logistic.py(298):     return loss, p, w
1.23 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
1.23 logistic.py(346):     diff = sample_weight * (p - Y)
1.23 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
1.23 logistic.py(348):     grad[:, :n_features] += alpha * w
1.23 logistic.py(349):     if fit_intercept:
1.23 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
1.23 logistic.py(351):     return loss, grad.ravel(), p
1.23 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
1.23 logistic.py(339):     n_classes = Y.shape[1]
1.23 logistic.py(340):     n_features = X.shape[1]
1.23 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
1.23 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
1.23 logistic.py(343):                     dtype=X.dtype)
1.23 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
1.23 logistic.py(282):     n_classes = Y.shape[1]
1.23 logistic.py(283):     n_features = X.shape[1]
1.23 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
1.23 logistic.py(285):     w = w.reshape(n_classes, -1)
1.23 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
1.23 logistic.py(287):     if fit_intercept:
1.23 logistic.py(288):         intercept = w[:, -1]
1.23 logistic.py(289):         w = w[:, :-1]
1.23 logistic.py(292):     p = safe_sparse_dot(X, w.T)
1.23 logistic.py(293):     p += intercept
1.23 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
1.23 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
1.23 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
1.23 logistic.py(297):     p = np.exp(p, p)
1.23 logistic.py(298):     return loss, p, w
1.23 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
1.23 logistic.py(346):     diff = sample_weight * (p - Y)
1.23 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
1.23 logistic.py(348):     grad[:, :n_features] += alpha * w
1.23 logistic.py(349):     if fit_intercept:
1.23 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
1.23 logistic.py(351):     return loss, grad.ravel(), p
1.23 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
1.23 logistic.py(339):     n_classes = Y.shape[1]
1.23 logistic.py(340):     n_features = X.shape[1]
1.23 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
1.23 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
1.23 logistic.py(343):                     dtype=X.dtype)
1.23 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
1.23 logistic.py(282):     n_classes = Y.shape[1]
1.23 logistic.py(283):     n_features = X.shape[1]
1.23 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
1.23 logistic.py(285):     w = w.reshape(n_classes, -1)
1.23 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
1.23 logistic.py(287):     if fit_intercept:
1.23 logistic.py(288):         intercept = w[:, -1]
1.23 logistic.py(289):         w = w[:, :-1]
1.23 logistic.py(292):     p = safe_sparse_dot(X, w.T)
1.23 logistic.py(293):     p += intercept
1.23 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
1.23 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
1.23 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
1.23 logistic.py(297):     p = np.exp(p, p)
1.23 logistic.py(298):     return loss, p, w
1.23 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
1.23 logistic.py(346):     diff = sample_weight * (p - Y)
1.23 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
1.23 logistic.py(348):     grad[:, :n_features] += alpha * w
1.23 logistic.py(349):     if fit_intercept:
1.23 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
1.23 logistic.py(351):     return loss, grad.ravel(), p
1.23 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
1.23 logistic.py(339):     n_classes = Y.shape[1]
1.23 logistic.py(340):     n_features = X.shape[1]
1.23 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
1.23 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
1.23 logistic.py(343):                     dtype=X.dtype)
1.23 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
1.23 logistic.py(282):     n_classes = Y.shape[1]
1.23 logistic.py(283):     n_features = X.shape[1]
1.23 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
1.23 logistic.py(285):     w = w.reshape(n_classes, -1)
1.23 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
1.23 logistic.py(287):     if fit_intercept:
1.23 logistic.py(288):         intercept = w[:, -1]
1.23 logistic.py(289):         w = w[:, :-1]
1.23 logistic.py(292):     p = safe_sparse_dot(X, w.T)
1.23 logistic.py(293):     p += intercept
1.23 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
1.23 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
1.23 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
1.23 logistic.py(297):     p = np.exp(p, p)
1.23 logistic.py(298):     return loss, p, w
1.23 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
1.23 logistic.py(346):     diff = sample_weight * (p - Y)
1.23 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
1.23 logistic.py(348):     grad[:, :n_features] += alpha * w
1.23 logistic.py(349):     if fit_intercept:
1.23 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
1.23 logistic.py(351):     return loss, grad.ravel(), p
1.23 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
1.23 logistic.py(339):     n_classes = Y.shape[1]
1.23 logistic.py(340):     n_features = X.shape[1]
1.23 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
1.23 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
1.23 logistic.py(343):                     dtype=X.dtype)
1.23 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
1.23 logistic.py(282):     n_classes = Y.shape[1]
1.23 logistic.py(283):     n_features = X.shape[1]
1.23 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
1.23 logistic.py(285):     w = w.reshape(n_classes, -1)
1.23 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
1.23 logistic.py(287):     if fit_intercept:
1.23 logistic.py(288):         intercept = w[:, -1]
1.23 logistic.py(289):         w = w[:, :-1]
1.23 logistic.py(292):     p = safe_sparse_dot(X, w.T)
1.23 logistic.py(293):     p += intercept
1.23 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
1.23 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
1.23 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
1.23 logistic.py(297):     p = np.exp(p, p)
1.23 logistic.py(298):     return loss, p, w
1.23 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
1.23 logistic.py(346):     diff = sample_weight * (p - Y)
1.23 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
1.23 logistic.py(348):     grad[:, :n_features] += alpha * w
1.23 logistic.py(349):     if fit_intercept:
1.23 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
1.23 logistic.py(351):     return loss, grad.ravel(), p
1.23 logistic.py(718):             if info["warnflag"] == 1:
1.23 logistic.py(723):             n_iter_i = min(info['nit'], max_iter)
1.23 logistic.py(760):         if multi_class == 'multinomial':
1.23 logistic.py(761):             multi_w0 = np.reshape(w0, (classes.size, -1))
1.23 logistic.py(762):             if classes.size == 2:
1.23 logistic.py(764):             coefs.append(multi_w0)
1.23 logistic.py(768):         n_iter[i] = n_iter_i
1.23 logistic.py(712):     for i, C in enumerate(Cs):
1.23 logistic.py(713):         if solver == 'lbfgs':
1.23 logistic.py(714):             w0, loss, info = optimize.fmin_l_bfgs_b(
1.23 logistic.py(715):                 func, w0, fprime=None,
1.23 logistic.py(716):                 args=(X, target, 1. / C, sample_weight),
1.23 logistic.py(717):                 iprint=(verbose > 0) - 1, pgtol=tol, maxiter=max_iter)
1.23 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
1.23 logistic.py(339):     n_classes = Y.shape[1]
1.23 logistic.py(340):     n_features = X.shape[1]
1.23 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
1.23 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
1.23 logistic.py(343):                     dtype=X.dtype)
1.23 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
1.23 logistic.py(282):     n_classes = Y.shape[1]
1.23 logistic.py(283):     n_features = X.shape[1]
1.23 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
1.23 logistic.py(285):     w = w.reshape(n_classes, -1)
1.23 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
1.23 logistic.py(287):     if fit_intercept:
1.23 logistic.py(288):         intercept = w[:, -1]
1.23 logistic.py(289):         w = w[:, :-1]
1.23 logistic.py(292):     p = safe_sparse_dot(X, w.T)
1.23 logistic.py(293):     p += intercept
1.23 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
1.23 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
1.23 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
1.23 logistic.py(297):     p = np.exp(p, p)
1.23 logistic.py(298):     return loss, p, w
1.23 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
1.23 logistic.py(346):     diff = sample_weight * (p - Y)
1.23 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
1.23 logistic.py(348):     grad[:, :n_features] += alpha * w
1.23 logistic.py(349):     if fit_intercept:
1.23 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
1.23 logistic.py(351):     return loss, grad.ravel(), p
1.23 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
1.23 logistic.py(339):     n_classes = Y.shape[1]
1.23 logistic.py(340):     n_features = X.shape[1]
1.23 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
1.23 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
1.23 logistic.py(343):                     dtype=X.dtype)
1.23 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
1.23 logistic.py(282):     n_classes = Y.shape[1]
1.23 logistic.py(283):     n_features = X.shape[1]
1.23 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
1.23 logistic.py(285):     w = w.reshape(n_classes, -1)
1.23 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
1.23 logistic.py(287):     if fit_intercept:
1.23 logistic.py(288):         intercept = w[:, -1]
1.23 logistic.py(289):         w = w[:, :-1]
1.23 logistic.py(292):     p = safe_sparse_dot(X, w.T)
1.23 logistic.py(293):     p += intercept
1.23 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
1.23 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
1.23 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
1.23 logistic.py(297):     p = np.exp(p, p)
1.23 logistic.py(298):     return loss, p, w
1.23 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
1.23 logistic.py(346):     diff = sample_weight * (p - Y)
1.23 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
1.23 logistic.py(348):     grad[:, :n_features] += alpha * w
1.23 logistic.py(349):     if fit_intercept:
1.23 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
1.23 logistic.py(351):     return loss, grad.ravel(), p
1.23 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
1.23 logistic.py(339):     n_classes = Y.shape[1]
1.23 logistic.py(340):     n_features = X.shape[1]
1.23 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
1.23 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
1.23 logistic.py(343):                     dtype=X.dtype)
1.23 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
1.23 logistic.py(282):     n_classes = Y.shape[1]
1.23 logistic.py(283):     n_features = X.shape[1]
1.23 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
1.23 logistic.py(285):     w = w.reshape(n_classes, -1)
1.23 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
1.23 logistic.py(287):     if fit_intercept:
1.23 logistic.py(288):         intercept = w[:, -1]
1.23 logistic.py(289):         w = w[:, :-1]
1.23 logistic.py(292):     p = safe_sparse_dot(X, w.T)
1.23 logistic.py(293):     p += intercept
1.23 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
1.23 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
1.23 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
1.23 logistic.py(297):     p = np.exp(p, p)
1.23 logistic.py(298):     return loss, p, w
1.23 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
1.23 logistic.py(346):     diff = sample_weight * (p - Y)
1.23 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
1.23 logistic.py(348):     grad[:, :n_features] += alpha * w
1.23 logistic.py(349):     if fit_intercept:
1.23 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
1.23 logistic.py(351):     return loss, grad.ravel(), p
1.23 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
1.23 logistic.py(339):     n_classes = Y.shape[1]
1.23 logistic.py(340):     n_features = X.shape[1]
1.23 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
1.23 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
1.23 logistic.py(343):                     dtype=X.dtype)
1.23 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
1.23 logistic.py(282):     n_classes = Y.shape[1]
1.23 logistic.py(283):     n_features = X.shape[1]
1.23 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
1.23 logistic.py(285):     w = w.reshape(n_classes, -1)
1.23 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
1.23 logistic.py(287):     if fit_intercept:
1.23 logistic.py(288):         intercept = w[:, -1]
1.23 logistic.py(289):         w = w[:, :-1]
1.23 logistic.py(292):     p = safe_sparse_dot(X, w.T)
1.23 logistic.py(293):     p += intercept
1.23 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
1.23 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
1.23 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
1.23 logistic.py(297):     p = np.exp(p, p)
1.23 logistic.py(298):     return loss, p, w
1.23 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
1.23 logistic.py(346):     diff = sample_weight * (p - Y)
1.23 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
1.23 logistic.py(348):     grad[:, :n_features] += alpha * w
1.23 logistic.py(349):     if fit_intercept:
1.23 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
1.23 logistic.py(351):     return loss, grad.ravel(), p
1.23 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
1.23 logistic.py(339):     n_classes = Y.shape[1]
1.23 logistic.py(340):     n_features = X.shape[1]
1.23 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
1.23 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
1.23 logistic.py(343):                     dtype=X.dtype)
1.23 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
1.23 logistic.py(282):     n_classes = Y.shape[1]
1.23 logistic.py(283):     n_features = X.shape[1]
1.23 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
1.23 logistic.py(285):     w = w.reshape(n_classes, -1)
1.23 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
1.23 logistic.py(287):     if fit_intercept:
1.23 logistic.py(288):         intercept = w[:, -1]
1.23 logistic.py(289):         w = w[:, :-1]
1.23 logistic.py(292):     p = safe_sparse_dot(X, w.T)
1.23 logistic.py(293):     p += intercept
1.23 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
1.23 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
1.23 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
1.23 logistic.py(297):     p = np.exp(p, p)
1.23 logistic.py(298):     return loss, p, w
1.23 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
1.23 logistic.py(346):     diff = sample_weight * (p - Y)
1.23 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
1.23 logistic.py(348):     grad[:, :n_features] += alpha * w
1.23 logistic.py(349):     if fit_intercept:
1.23 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
1.23 logistic.py(351):     return loss, grad.ravel(), p
1.23 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
1.23 logistic.py(339):     n_classes = Y.shape[1]
1.23 logistic.py(340):     n_features = X.shape[1]
1.23 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
1.23 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
1.23 logistic.py(343):                     dtype=X.dtype)
1.23 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
1.23 logistic.py(282):     n_classes = Y.shape[1]
1.23 logistic.py(283):     n_features = X.shape[1]
1.23 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
1.23 logistic.py(285):     w = w.reshape(n_classes, -1)
1.23 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
1.23 logistic.py(287):     if fit_intercept:
1.23 logistic.py(288):         intercept = w[:, -1]
1.23 logistic.py(289):         w = w[:, :-1]
1.23 logistic.py(292):     p = safe_sparse_dot(X, w.T)
1.23 logistic.py(293):     p += intercept
1.23 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
1.23 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
1.23 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
1.23 logistic.py(297):     p = np.exp(p, p)
1.23 logistic.py(298):     return loss, p, w
1.23 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
1.23 logistic.py(346):     diff = sample_weight * (p - Y)
1.23 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
1.23 logistic.py(348):     grad[:, :n_features] += alpha * w
1.23 logistic.py(349):     if fit_intercept:
1.23 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
1.23 logistic.py(351):     return loss, grad.ravel(), p
1.23 logistic.py(718):             if info["warnflag"] == 1:
1.23 logistic.py(723):             n_iter_i = min(info['nit'], max_iter)
1.23 logistic.py(760):         if multi_class == 'multinomial':
1.23 logistic.py(761):             multi_w0 = np.reshape(w0, (classes.size, -1))
1.23 logistic.py(762):             if classes.size == 2:
1.23 logistic.py(764):             coefs.append(multi_w0)
1.23 logistic.py(768):         n_iter[i] = n_iter_i
1.23 logistic.py(712):     for i, C in enumerate(Cs):
1.23 logistic.py(713):         if solver == 'lbfgs':
1.23 logistic.py(714):             w0, loss, info = optimize.fmin_l_bfgs_b(
1.23 logistic.py(715):                 func, w0, fprime=None,
1.23 logistic.py(716):                 args=(X, target, 1. / C, sample_weight),
1.23 logistic.py(717):                 iprint=(verbose > 0) - 1, pgtol=tol, maxiter=max_iter)
1.23 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
1.23 logistic.py(339):     n_classes = Y.shape[1]
1.23 logistic.py(340):     n_features = X.shape[1]
1.23 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
1.23 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
1.23 logistic.py(343):                     dtype=X.dtype)
1.23 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
1.23 logistic.py(282):     n_classes = Y.shape[1]
1.23 logistic.py(283):     n_features = X.shape[1]
1.23 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
1.23 logistic.py(285):     w = w.reshape(n_classes, -1)
1.23 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
1.23 logistic.py(287):     if fit_intercept:
1.23 logistic.py(288):         intercept = w[:, -1]
1.23 logistic.py(289):         w = w[:, :-1]
1.23 logistic.py(292):     p = safe_sparse_dot(X, w.T)
1.23 logistic.py(293):     p += intercept
1.23 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
1.23 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
1.23 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
1.23 logistic.py(297):     p = np.exp(p, p)
1.23 logistic.py(298):     return loss, p, w
1.23 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
1.23 logistic.py(346):     diff = sample_weight * (p - Y)
1.23 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
1.23 logistic.py(348):     grad[:, :n_features] += alpha * w
1.23 logistic.py(349):     if fit_intercept:
1.23 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
1.23 logistic.py(351):     return loss, grad.ravel(), p
1.24 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
1.24 logistic.py(339):     n_classes = Y.shape[1]
1.24 logistic.py(340):     n_features = X.shape[1]
1.24 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
1.24 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
1.24 logistic.py(343):                     dtype=X.dtype)
1.24 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
1.24 logistic.py(282):     n_classes = Y.shape[1]
1.24 logistic.py(283):     n_features = X.shape[1]
1.24 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
1.24 logistic.py(285):     w = w.reshape(n_classes, -1)
1.24 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
1.24 logistic.py(287):     if fit_intercept:
1.24 logistic.py(288):         intercept = w[:, -1]
1.24 logistic.py(289):         w = w[:, :-1]
1.24 logistic.py(292):     p = safe_sparse_dot(X, w.T)
1.24 logistic.py(293):     p += intercept
1.24 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
1.24 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
1.24 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
1.24 logistic.py(297):     p = np.exp(p, p)
1.24 logistic.py(298):     return loss, p, w
1.24 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
1.24 logistic.py(346):     diff = sample_weight * (p - Y)
1.24 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
1.24 logistic.py(348):     grad[:, :n_features] += alpha * w
1.24 logistic.py(349):     if fit_intercept:
1.24 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
1.24 logistic.py(351):     return loss, grad.ravel(), p
1.24 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
1.24 logistic.py(339):     n_classes = Y.shape[1]
1.24 logistic.py(340):     n_features = X.shape[1]
1.24 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
1.24 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
1.24 logistic.py(343):                     dtype=X.dtype)
1.24 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
1.24 logistic.py(282):     n_classes = Y.shape[1]
1.24 logistic.py(283):     n_features = X.shape[1]
1.24 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
1.24 logistic.py(285):     w = w.reshape(n_classes, -1)
1.24 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
1.24 logistic.py(287):     if fit_intercept:
1.24 logistic.py(288):         intercept = w[:, -1]
1.24 logistic.py(289):         w = w[:, :-1]
1.24 logistic.py(292):     p = safe_sparse_dot(X, w.T)
1.24 logistic.py(293):     p += intercept
1.24 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
1.24 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
1.24 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
1.24 logistic.py(297):     p = np.exp(p, p)
1.24 logistic.py(298):     return loss, p, w
1.24 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
1.24 logistic.py(346):     diff = sample_weight * (p - Y)
1.24 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
1.24 logistic.py(348):     grad[:, :n_features] += alpha * w
1.24 logistic.py(349):     if fit_intercept:
1.24 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
1.24 logistic.py(351):     return loss, grad.ravel(), p
1.24 logistic.py(718):             if info["warnflag"] == 1:
1.24 logistic.py(723):             n_iter_i = min(info['nit'], max_iter)
1.24 logistic.py(760):         if multi_class == 'multinomial':
1.24 logistic.py(761):             multi_w0 = np.reshape(w0, (classes.size, -1))
1.24 logistic.py(762):             if classes.size == 2:
1.24 logistic.py(764):             coefs.append(multi_w0)
1.24 logistic.py(768):         n_iter[i] = n_iter_i
1.24 logistic.py(712):     for i, C in enumerate(Cs):
1.24 logistic.py(770):     return coefs, np.array(Cs), n_iter
1.24 logistic.py(925):     log_reg = LogisticRegression(multi_class=multi_class)
1.24 logistic.py(1171):         self.penalty = penalty
1.24 logistic.py(1172):         self.dual = dual
1.24 logistic.py(1173):         self.tol = tol
1.24 logistic.py(1174):         self.C = C
1.24 logistic.py(1175):         self.fit_intercept = fit_intercept
1.24 logistic.py(1176):         self.intercept_scaling = intercept_scaling
1.24 logistic.py(1177):         self.class_weight = class_weight
1.24 logistic.py(1178):         self.random_state = random_state
1.24 logistic.py(1179):         self.solver = solver
1.24 logistic.py(1180):         self.max_iter = max_iter
1.24 logistic.py(1181):         self.multi_class = multi_class
1.24 logistic.py(1182):         self.verbose = verbose
1.24 logistic.py(1183):         self.warm_start = warm_start
1.24 logistic.py(1184):         self.n_jobs = n_jobs
1.24 logistic.py(928):     if multi_class == 'ovr':
1.24 logistic.py(930):     elif multi_class == 'multinomial':
1.24 logistic.py(931):         log_reg.classes_ = np.unique(y_train)
1.24 logistic.py(936):     if pos_class is not None:
1.24 logistic.py(941):     scores = list()
1.24 logistic.py(943):     if isinstance(scoring, six.string_types):
1.24 logistic.py(945):     for w in coefs:
1.24 logistic.py(946):         if multi_class == 'ovr':
1.24 logistic.py(948):         if fit_intercept:
1.24 logistic.py(949):             log_reg.coef_ = w[:, :-1]
1.24 logistic.py(950):             log_reg.intercept_ = w[:, -1]
1.24 logistic.py(955):         if scoring is None:
1.24 logistic.py(956):             scores.append(log_reg.score(X_test, y_test))
1.24 logistic.py(945):     for w in coefs:
1.24 logistic.py(946):         if multi_class == 'ovr':
1.24 logistic.py(948):         if fit_intercept:
1.24 logistic.py(949):             log_reg.coef_ = w[:, :-1]
1.24 logistic.py(950):             log_reg.intercept_ = w[:, -1]
1.24 logistic.py(955):         if scoring is None:
1.24 logistic.py(956):             scores.append(log_reg.score(X_test, y_test))
1.24 logistic.py(945):     for w in coefs:
1.24 logistic.py(946):         if multi_class == 'ovr':
1.24 logistic.py(948):         if fit_intercept:
1.24 logistic.py(949):             log_reg.coef_ = w[:, :-1]
1.24 logistic.py(950):             log_reg.intercept_ = w[:, -1]
1.24 logistic.py(955):         if scoring is None:
1.24 logistic.py(956):             scores.append(log_reg.score(X_test, y_test))
1.24 logistic.py(945):     for w in coefs:
1.24 logistic.py(946):         if multi_class == 'ovr':
1.24 logistic.py(948):         if fit_intercept:
1.24 logistic.py(949):             log_reg.coef_ = w[:, :-1]
1.24 logistic.py(950):             log_reg.intercept_ = w[:, -1]
1.24 logistic.py(955):         if scoring is None:
1.24 logistic.py(956):             scores.append(log_reg.score(X_test, y_test))
1.24 logistic.py(945):     for w in coefs:
1.24 logistic.py(946):         if multi_class == 'ovr':
1.24 logistic.py(948):         if fit_intercept:
1.24 logistic.py(949):             log_reg.coef_ = w[:, :-1]
1.24 logistic.py(950):             log_reg.intercept_ = w[:, -1]
1.24 logistic.py(955):         if scoring is None:
1.24 logistic.py(956):             scores.append(log_reg.score(X_test, y_test))
1.24 logistic.py(945):     for w in coefs:
1.24 logistic.py(946):         if multi_class == 'ovr':
1.24 logistic.py(948):         if fit_intercept:
1.24 logistic.py(949):             log_reg.coef_ = w[:, :-1]
1.24 logistic.py(950):             log_reg.intercept_ = w[:, -1]
1.24 logistic.py(955):         if scoring is None:
1.24 logistic.py(956):             scores.append(log_reg.score(X_test, y_test))
1.24 logistic.py(945):     for w in coefs:
1.24 logistic.py(946):         if multi_class == 'ovr':
1.24 logistic.py(948):         if fit_intercept:
1.24 logistic.py(949):             log_reg.coef_ = w[:, :-1]
1.24 logistic.py(950):             log_reg.intercept_ = w[:, -1]
1.24 logistic.py(955):         if scoring is None:
1.24 logistic.py(956):             scores.append(log_reg.score(X_test, y_test))
1.24 logistic.py(945):     for w in coefs:
1.24 logistic.py(946):         if multi_class == 'ovr':
1.24 logistic.py(948):         if fit_intercept:
1.24 logistic.py(949):             log_reg.coef_ = w[:, :-1]
1.24 logistic.py(950):             log_reg.intercept_ = w[:, -1]
1.24 logistic.py(955):         if scoring is None:
1.24 logistic.py(956):             scores.append(log_reg.score(X_test, y_test))
1.24 logistic.py(945):     for w in coefs:
1.24 logistic.py(946):         if multi_class == 'ovr':
1.24 logistic.py(948):         if fit_intercept:
1.24 logistic.py(949):             log_reg.coef_ = w[:, :-1]
1.24 logistic.py(950):             log_reg.intercept_ = w[:, -1]
1.24 logistic.py(955):         if scoring is None:
1.24 logistic.py(956):             scores.append(log_reg.score(X_test, y_test))
1.24 logistic.py(945):     for w in coefs:
1.24 logistic.py(946):         if multi_class == 'ovr':
1.24 logistic.py(948):         if fit_intercept:
1.24 logistic.py(949):             log_reg.coef_ = w[:, :-1]
1.24 logistic.py(950):             log_reg.intercept_ = w[:, -1]
1.24 logistic.py(955):         if scoring is None:
1.24 logistic.py(956):             scores.append(log_reg.score(X_test, y_test))
1.24 logistic.py(945):     for w in coefs:
1.24 logistic.py(959):     return coefs, Cs, np.array(scores), n_iter
1.24 logistic.py(1703):             for train, test in folds)
1.24 logistic.py(903):     _check_solver_option(solver, multi_class, penalty, dual)
1.24 logistic.py(428):     if solver not in ['liblinear', 'newton-cg', 'lbfgs', 'sag', 'saga']:
1.24 logistic.py(433):     if multi_class not in ['multinomial', 'ovr']:
1.24 logistic.py(437):     if multi_class == 'multinomial' and solver == 'liblinear':
1.24 logistic.py(441):     if solver not in ['liblinear', 'saga']:
1.24 logistic.py(442):         if penalty != 'l2':
1.24 logistic.py(445):     if solver != 'liblinear':
1.24 logistic.py(446):         if dual:
1.24 logistic.py(905):     X_train = X[train]
1.24 logistic.py(906):     X_test = X[test]
1.24 logistic.py(907):     y_train = y[train]
1.24 logistic.py(908):     y_test = y[test]
1.24 logistic.py(910):     if sample_weight is not None:
1.24 logistic.py(916):     coefs, Cs, n_iter = logistic_regression_path(
1.24 logistic.py(917):         X_train, y_train, Cs=Cs, fit_intercept=fit_intercept,
1.24 logistic.py(918):         solver=solver, max_iter=max_iter, class_weight=class_weight,
1.24 logistic.py(919):         pos_class=pos_class, multi_class=multi_class,
1.24 logistic.py(920):         tol=tol, verbose=verbose, dual=dual, penalty=penalty,
1.24 logistic.py(921):         intercept_scaling=intercept_scaling, random_state=random_state,
1.24 logistic.py(922):         check_input=False, max_squared_sum=max_squared_sum,
1.24 logistic.py(923):         sample_weight=sample_weight)
1.24 logistic.py(591):     if isinstance(Cs, numbers.Integral):
1.24 logistic.py(592):         Cs = np.logspace(-4, 4, Cs)
1.24 logistic.py(594):     _check_solver_option(solver, multi_class, penalty, dual)
1.24 logistic.py(428):     if solver not in ['liblinear', 'newton-cg', 'lbfgs', 'sag', 'saga']:
1.24 logistic.py(433):     if multi_class not in ['multinomial', 'ovr']:
1.24 logistic.py(437):     if multi_class == 'multinomial' and solver == 'liblinear':
1.24 logistic.py(441):     if solver not in ['liblinear', 'saga']:
1.24 logistic.py(442):         if penalty != 'l2':
1.24 logistic.py(445):     if solver != 'liblinear':
1.24 logistic.py(446):         if dual:
1.24 logistic.py(597):     if check_input:
1.24 logistic.py(602):     _, n_features = X.shape
1.24 logistic.py(603):     classes = np.unique(y)
1.24 logistic.py(604):     random_state = check_random_state(random_state)
1.24 logistic.py(606):     if pos_class is None and multi_class != 'multinomial':
1.24 logistic.py(615):     if sample_weight is not None:
1.24 logistic.py(619):         sample_weight = np.ones(X.shape[0], dtype=X.dtype)
1.24 logistic.py(624):     le = LabelEncoder()
1.24 logistic.py(625):     if isinstance(class_weight, dict) or multi_class == 'multinomial':
1.24 logistic.py(626):         class_weight_ = compute_class_weight(class_weight, classes, y)
1.24 logistic.py(627):         sample_weight *= class_weight_[le.fit_transform(y)]
1.24 logistic.py(631):     if multi_class == 'ovr':
1.24 logistic.py(645):         if solver not in ['sag', 'saga']:
1.24 logistic.py(646):             lbin = LabelBinarizer()
1.24 logistic.py(647):             Y_multi = lbin.fit_transform(y)
1.24 logistic.py(648):             if Y_multi.shape[1] == 1:
1.24 logistic.py(655):         w0 = np.zeros((classes.size, n_features + int(fit_intercept)),
1.24 logistic.py(656):                       order='F', dtype=X.dtype)
1.24 logistic.py(658):     if coef is not None:
1.24 logistic.py(688):     if multi_class == 'multinomial':
1.24 logistic.py(690):         if solver in ['lbfgs', 'newton-cg']:
1.24 logistic.py(691):             w0 = w0.ravel()
1.24 logistic.py(692):         target = Y_multi
1.24 logistic.py(693):         if solver == 'lbfgs':
1.24 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
1.24 logistic.py(699):         warm_start_sag = {'coef': w0.T}
1.24 logistic.py(710):     coefs = list()
1.24 logistic.py(711):     n_iter = np.zeros(len(Cs), dtype=np.int32)
1.24 logistic.py(712):     for i, C in enumerate(Cs):
1.24 logistic.py(713):         if solver == 'lbfgs':
1.24 logistic.py(714):             w0, loss, info = optimize.fmin_l_bfgs_b(
1.24 logistic.py(715):                 func, w0, fprime=None,
1.24 logistic.py(716):                 args=(X, target, 1. / C, sample_weight),
1.24 logistic.py(717):                 iprint=(verbose > 0) - 1, pgtol=tol, maxiter=max_iter)
1.24 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
1.24 logistic.py(339):     n_classes = Y.shape[1]
1.24 logistic.py(340):     n_features = X.shape[1]
1.24 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
1.24 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
1.24 logistic.py(343):                     dtype=X.dtype)
1.24 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
1.24 logistic.py(282):     n_classes = Y.shape[1]
1.24 logistic.py(283):     n_features = X.shape[1]
1.24 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
1.24 logistic.py(285):     w = w.reshape(n_classes, -1)
1.24 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
1.24 logistic.py(287):     if fit_intercept:
1.24 logistic.py(288):         intercept = w[:, -1]
1.24 logistic.py(289):         w = w[:, :-1]
1.24 logistic.py(292):     p = safe_sparse_dot(X, w.T)
1.24 logistic.py(293):     p += intercept
1.24 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
1.24 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
1.24 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
1.24 logistic.py(297):     p = np.exp(p, p)
1.24 logistic.py(298):     return loss, p, w
1.24 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
1.24 logistic.py(346):     diff = sample_weight * (p - Y)
1.24 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
1.24 logistic.py(348):     grad[:, :n_features] += alpha * w
1.24 logistic.py(349):     if fit_intercept:
1.24 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
1.24 logistic.py(351):     return loss, grad.ravel(), p
1.24 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
1.24 logistic.py(339):     n_classes = Y.shape[1]
1.24 logistic.py(340):     n_features = X.shape[1]
1.24 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
1.24 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
1.24 logistic.py(343):                     dtype=X.dtype)
1.24 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
1.24 logistic.py(282):     n_classes = Y.shape[1]
1.24 logistic.py(283):     n_features = X.shape[1]
1.24 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
1.24 logistic.py(285):     w = w.reshape(n_classes, -1)
1.24 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
1.24 logistic.py(287):     if fit_intercept:
1.24 logistic.py(288):         intercept = w[:, -1]
1.24 logistic.py(289):         w = w[:, :-1]
1.24 logistic.py(292):     p = safe_sparse_dot(X, w.T)
1.24 logistic.py(293):     p += intercept
1.24 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
1.24 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
1.24 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
1.24 logistic.py(297):     p = np.exp(p, p)
1.24 logistic.py(298):     return loss, p, w
1.24 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
1.24 logistic.py(346):     diff = sample_weight * (p - Y)
1.24 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
1.24 logistic.py(348):     grad[:, :n_features] += alpha * w
1.24 logistic.py(349):     if fit_intercept:
1.24 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
1.24 logistic.py(351):     return loss, grad.ravel(), p
1.24 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
1.24 logistic.py(339):     n_classes = Y.shape[1]
1.24 logistic.py(340):     n_features = X.shape[1]
1.24 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
1.24 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
1.24 logistic.py(343):                     dtype=X.dtype)
1.24 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
1.24 logistic.py(282):     n_classes = Y.shape[1]
1.24 logistic.py(283):     n_features = X.shape[1]
1.24 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
1.24 logistic.py(285):     w = w.reshape(n_classes, -1)
1.24 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
1.24 logistic.py(287):     if fit_intercept:
1.24 logistic.py(288):         intercept = w[:, -1]
1.24 logistic.py(289):         w = w[:, :-1]
1.24 logistic.py(292):     p = safe_sparse_dot(X, w.T)
1.24 logistic.py(293):     p += intercept
1.24 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
1.24 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
1.24 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
1.24 logistic.py(297):     p = np.exp(p, p)
1.24 logistic.py(298):     return loss, p, w
1.24 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
1.24 logistic.py(346):     diff = sample_weight * (p - Y)
1.24 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
1.24 logistic.py(348):     grad[:, :n_features] += alpha * w
1.24 logistic.py(349):     if fit_intercept:
1.24 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
1.24 logistic.py(351):     return loss, grad.ravel(), p
1.24 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
1.24 logistic.py(339):     n_classes = Y.shape[1]
1.24 logistic.py(340):     n_features = X.shape[1]
1.24 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
1.24 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
1.24 logistic.py(343):                     dtype=X.dtype)
1.24 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
1.24 logistic.py(282):     n_classes = Y.shape[1]
1.24 logistic.py(283):     n_features = X.shape[1]
1.24 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
1.24 logistic.py(285):     w = w.reshape(n_classes, -1)
1.24 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
1.24 logistic.py(287):     if fit_intercept:
1.24 logistic.py(288):         intercept = w[:, -1]
1.24 logistic.py(289):         w = w[:, :-1]
1.24 logistic.py(292):     p = safe_sparse_dot(X, w.T)
1.24 logistic.py(293):     p += intercept
1.24 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
1.24 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
1.24 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
1.24 logistic.py(297):     p = np.exp(p, p)
1.24 logistic.py(298):     return loss, p, w
1.24 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
1.24 logistic.py(346):     diff = sample_weight * (p - Y)
1.24 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
1.24 logistic.py(348):     grad[:, :n_features] += alpha * w
1.24 logistic.py(349):     if fit_intercept:
1.24 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
1.24 logistic.py(351):     return loss, grad.ravel(), p
1.24 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
1.24 logistic.py(339):     n_classes = Y.shape[1]
1.24 logistic.py(340):     n_features = X.shape[1]
1.24 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
1.24 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
1.24 logistic.py(343):                     dtype=X.dtype)
1.24 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
1.24 logistic.py(282):     n_classes = Y.shape[1]
1.24 logistic.py(283):     n_features = X.shape[1]
1.24 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
1.24 logistic.py(285):     w = w.reshape(n_classes, -1)
1.24 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
1.24 logistic.py(287):     if fit_intercept:
1.24 logistic.py(288):         intercept = w[:, -1]
1.24 logistic.py(289):         w = w[:, :-1]
1.24 logistic.py(292):     p = safe_sparse_dot(X, w.T)
1.24 logistic.py(293):     p += intercept
1.24 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
1.24 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
1.24 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
1.24 logistic.py(297):     p = np.exp(p, p)
1.24 logistic.py(298):     return loss, p, w
1.24 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
1.24 logistic.py(346):     diff = sample_weight * (p - Y)
1.24 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
1.24 logistic.py(348):     grad[:, :n_features] += alpha * w
1.24 logistic.py(349):     if fit_intercept:
1.24 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
1.24 logistic.py(351):     return loss, grad.ravel(), p
1.24 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
1.24 logistic.py(339):     n_classes = Y.shape[1]
1.24 logistic.py(340):     n_features = X.shape[1]
1.24 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
1.24 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
1.24 logistic.py(343):                     dtype=X.dtype)
1.24 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
1.24 logistic.py(282):     n_classes = Y.shape[1]
1.24 logistic.py(283):     n_features = X.shape[1]
1.24 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
1.24 logistic.py(285):     w = w.reshape(n_classes, -1)
1.24 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
1.24 logistic.py(287):     if fit_intercept:
1.24 logistic.py(288):         intercept = w[:, -1]
1.24 logistic.py(289):         w = w[:, :-1]
1.24 logistic.py(292):     p = safe_sparse_dot(X, w.T)
1.24 logistic.py(293):     p += intercept
1.24 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
1.24 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
1.24 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
1.24 logistic.py(297):     p = np.exp(p, p)
1.24 logistic.py(298):     return loss, p, w
1.24 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
1.24 logistic.py(346):     diff = sample_weight * (p - Y)
1.24 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
1.24 logistic.py(348):     grad[:, :n_features] += alpha * w
1.24 logistic.py(349):     if fit_intercept:
1.24 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
1.24 logistic.py(351):     return loss, grad.ravel(), p
1.25 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
1.25 logistic.py(339):     n_classes = Y.shape[1]
1.25 logistic.py(340):     n_features = X.shape[1]
1.25 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
1.25 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
1.25 logistic.py(343):                     dtype=X.dtype)
1.25 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
1.25 logistic.py(282):     n_classes = Y.shape[1]
1.25 logistic.py(283):     n_features = X.shape[1]
1.25 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
1.25 logistic.py(285):     w = w.reshape(n_classes, -1)
1.25 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
1.25 logistic.py(287):     if fit_intercept:
1.25 logistic.py(288):         intercept = w[:, -1]
1.25 logistic.py(289):         w = w[:, :-1]
1.25 logistic.py(292):     p = safe_sparse_dot(X, w.T)
1.25 logistic.py(293):     p += intercept
1.25 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
1.25 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
1.25 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
1.25 logistic.py(297):     p = np.exp(p, p)
1.25 logistic.py(298):     return loss, p, w
1.25 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
1.25 logistic.py(346):     diff = sample_weight * (p - Y)
1.25 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
1.25 logistic.py(348):     grad[:, :n_features] += alpha * w
1.25 logistic.py(349):     if fit_intercept:
1.25 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
1.25 logistic.py(351):     return loss, grad.ravel(), p
1.25 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
1.25 logistic.py(339):     n_classes = Y.shape[1]
1.25 logistic.py(340):     n_features = X.shape[1]
1.25 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
1.25 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
1.25 logistic.py(343):                     dtype=X.dtype)
1.25 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
1.25 logistic.py(282):     n_classes = Y.shape[1]
1.25 logistic.py(283):     n_features = X.shape[1]
1.25 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
1.25 logistic.py(285):     w = w.reshape(n_classes, -1)
1.25 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
1.25 logistic.py(287):     if fit_intercept:
1.25 logistic.py(288):         intercept = w[:, -1]
1.25 logistic.py(289):         w = w[:, :-1]
1.25 logistic.py(292):     p = safe_sparse_dot(X, w.T)
1.25 logistic.py(293):     p += intercept
1.25 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
1.25 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
1.25 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
1.25 logistic.py(297):     p = np.exp(p, p)
1.25 logistic.py(298):     return loss, p, w
1.25 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
1.25 logistic.py(346):     diff = sample_weight * (p - Y)
1.25 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
1.25 logistic.py(348):     grad[:, :n_features] += alpha * w
1.25 logistic.py(349):     if fit_intercept:
1.25 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
1.25 logistic.py(351):     return loss, grad.ravel(), p
1.25 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
1.25 logistic.py(339):     n_classes = Y.shape[1]
1.25 logistic.py(340):     n_features = X.shape[1]
1.25 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
1.25 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
1.25 logistic.py(343):                     dtype=X.dtype)
1.25 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
1.25 logistic.py(282):     n_classes = Y.shape[1]
1.25 logistic.py(283):     n_features = X.shape[1]
1.25 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
1.25 logistic.py(285):     w = w.reshape(n_classes, -1)
1.25 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
1.25 logistic.py(287):     if fit_intercept:
1.25 logistic.py(288):         intercept = w[:, -1]
1.25 logistic.py(289):         w = w[:, :-1]
1.25 logistic.py(292):     p = safe_sparse_dot(X, w.T)
1.25 logistic.py(293):     p += intercept
1.25 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
1.25 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
1.25 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
1.25 logistic.py(297):     p = np.exp(p, p)
1.25 logistic.py(298):     return loss, p, w
1.25 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
1.25 logistic.py(346):     diff = sample_weight * (p - Y)
1.25 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
1.25 logistic.py(348):     grad[:, :n_features] += alpha * w
1.25 logistic.py(349):     if fit_intercept:
1.25 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
1.25 logistic.py(351):     return loss, grad.ravel(), p
1.25 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
1.25 logistic.py(339):     n_classes = Y.shape[1]
1.25 logistic.py(340):     n_features = X.shape[1]
1.25 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
1.25 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
1.25 logistic.py(343):                     dtype=X.dtype)
1.25 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
1.25 logistic.py(282):     n_classes = Y.shape[1]
1.25 logistic.py(283):     n_features = X.shape[1]
1.25 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
1.25 logistic.py(285):     w = w.reshape(n_classes, -1)
1.25 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
1.25 logistic.py(287):     if fit_intercept:
1.25 logistic.py(288):         intercept = w[:, -1]
1.25 logistic.py(289):         w = w[:, :-1]
1.25 logistic.py(292):     p = safe_sparse_dot(X, w.T)
1.25 logistic.py(293):     p += intercept
1.25 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
1.25 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
1.25 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
1.25 logistic.py(297):     p = np.exp(p, p)
1.25 logistic.py(298):     return loss, p, w
1.25 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
1.25 logistic.py(346):     diff = sample_weight * (p - Y)
1.25 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
1.25 logistic.py(348):     grad[:, :n_features] += alpha * w
1.25 logistic.py(349):     if fit_intercept:
1.25 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
1.25 logistic.py(351):     return loss, grad.ravel(), p
1.25 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
1.25 logistic.py(339):     n_classes = Y.shape[1]
1.25 logistic.py(340):     n_features = X.shape[1]
1.25 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
1.25 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
1.25 logistic.py(343):                     dtype=X.dtype)
1.25 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
1.25 logistic.py(282):     n_classes = Y.shape[1]
1.25 logistic.py(283):     n_features = X.shape[1]
1.25 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
1.25 logistic.py(285):     w = w.reshape(n_classes, -1)
1.25 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
1.25 logistic.py(287):     if fit_intercept:
1.25 logistic.py(288):         intercept = w[:, -1]
1.25 logistic.py(289):         w = w[:, :-1]
1.25 logistic.py(292):     p = safe_sparse_dot(X, w.T)
1.25 logistic.py(293):     p += intercept
1.25 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
1.25 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
1.25 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
1.25 logistic.py(297):     p = np.exp(p, p)
1.25 logistic.py(298):     return loss, p, w
1.25 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
1.25 logistic.py(346):     diff = sample_weight * (p - Y)
1.25 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
1.25 logistic.py(348):     grad[:, :n_features] += alpha * w
1.25 logistic.py(349):     if fit_intercept:
1.25 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
1.25 logistic.py(351):     return loss, grad.ravel(), p
1.25 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
1.25 logistic.py(339):     n_classes = Y.shape[1]
1.25 logistic.py(340):     n_features = X.shape[1]
1.25 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
1.25 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
1.25 logistic.py(343):                     dtype=X.dtype)
1.25 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
1.25 logistic.py(282):     n_classes = Y.shape[1]
1.25 logistic.py(283):     n_features = X.shape[1]
1.25 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
1.25 logistic.py(285):     w = w.reshape(n_classes, -1)
1.25 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
1.25 logistic.py(287):     if fit_intercept:
1.25 logistic.py(288):         intercept = w[:, -1]
1.25 logistic.py(289):         w = w[:, :-1]
1.25 logistic.py(292):     p = safe_sparse_dot(X, w.T)
1.25 logistic.py(293):     p += intercept
1.25 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
1.25 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
1.25 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
1.25 logistic.py(297):     p = np.exp(p, p)
1.25 logistic.py(298):     return loss, p, w
1.25 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
1.25 logistic.py(346):     diff = sample_weight * (p - Y)
1.25 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
1.25 logistic.py(348):     grad[:, :n_features] += alpha * w
1.25 logistic.py(349):     if fit_intercept:
1.25 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
1.25 logistic.py(351):     return loss, grad.ravel(), p
1.25 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
1.25 logistic.py(339):     n_classes = Y.shape[1]
1.25 logistic.py(340):     n_features = X.shape[1]
1.25 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
1.25 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
1.25 logistic.py(343):                     dtype=X.dtype)
1.25 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
1.25 logistic.py(282):     n_classes = Y.shape[1]
1.25 logistic.py(283):     n_features = X.shape[1]
1.25 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
1.25 logistic.py(285):     w = w.reshape(n_classes, -1)
1.25 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
1.25 logistic.py(287):     if fit_intercept:
1.25 logistic.py(288):         intercept = w[:, -1]
1.25 logistic.py(289):         w = w[:, :-1]
1.25 logistic.py(292):     p = safe_sparse_dot(X, w.T)
1.25 logistic.py(293):     p += intercept
1.25 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
1.25 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
1.25 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
1.25 logistic.py(297):     p = np.exp(p, p)
1.25 logistic.py(298):     return loss, p, w
1.25 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
1.25 logistic.py(346):     diff = sample_weight * (p - Y)
1.25 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
1.25 logistic.py(348):     grad[:, :n_features] += alpha * w
1.25 logistic.py(349):     if fit_intercept:
1.25 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
1.25 logistic.py(351):     return loss, grad.ravel(), p
1.25 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
1.25 logistic.py(339):     n_classes = Y.shape[1]
1.25 logistic.py(340):     n_features = X.shape[1]
1.25 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
1.25 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
1.25 logistic.py(343):                     dtype=X.dtype)
1.25 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
1.25 logistic.py(282):     n_classes = Y.shape[1]
1.25 logistic.py(283):     n_features = X.shape[1]
1.25 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
1.25 logistic.py(285):     w = w.reshape(n_classes, -1)
1.25 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
1.25 logistic.py(287):     if fit_intercept:
1.25 logistic.py(288):         intercept = w[:, -1]
1.25 logistic.py(289):         w = w[:, :-1]
1.25 logistic.py(292):     p = safe_sparse_dot(X, w.T)
1.25 logistic.py(293):     p += intercept
1.25 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
1.25 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
1.25 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
1.25 logistic.py(297):     p = np.exp(p, p)
1.25 logistic.py(298):     return loss, p, w
1.25 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
1.25 logistic.py(346):     diff = sample_weight * (p - Y)
1.25 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
1.25 logistic.py(348):     grad[:, :n_features] += alpha * w
1.25 logistic.py(349):     if fit_intercept:
1.25 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
1.25 logistic.py(351):     return loss, grad.ravel(), p
1.25 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
1.25 logistic.py(339):     n_classes = Y.shape[1]
1.25 logistic.py(340):     n_features = X.shape[1]
1.25 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
1.25 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
1.25 logistic.py(343):                     dtype=X.dtype)
1.25 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
1.25 logistic.py(282):     n_classes = Y.shape[1]
1.25 logistic.py(283):     n_features = X.shape[1]
1.25 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
1.25 logistic.py(285):     w = w.reshape(n_classes, -1)
1.25 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
1.25 logistic.py(287):     if fit_intercept:
1.25 logistic.py(288):         intercept = w[:, -1]
1.25 logistic.py(289):         w = w[:, :-1]
1.25 logistic.py(292):     p = safe_sparse_dot(X, w.T)
1.25 logistic.py(293):     p += intercept
1.25 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
1.25 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
1.25 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
1.25 logistic.py(297):     p = np.exp(p, p)
1.25 logistic.py(298):     return loss, p, w
1.25 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
1.25 logistic.py(346):     diff = sample_weight * (p - Y)
1.25 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
1.25 logistic.py(348):     grad[:, :n_features] += alpha * w
1.25 logistic.py(349):     if fit_intercept:
1.25 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
1.25 logistic.py(351):     return loss, grad.ravel(), p
1.25 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
1.25 logistic.py(339):     n_classes = Y.shape[1]
1.25 logistic.py(340):     n_features = X.shape[1]
1.25 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
1.25 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
1.25 logistic.py(343):                     dtype=X.dtype)
1.25 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
1.25 logistic.py(282):     n_classes = Y.shape[1]
1.25 logistic.py(283):     n_features = X.shape[1]
1.25 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
1.25 logistic.py(285):     w = w.reshape(n_classes, -1)
1.25 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
1.25 logistic.py(287):     if fit_intercept:
1.25 logistic.py(288):         intercept = w[:, -1]
1.25 logistic.py(289):         w = w[:, :-1]
1.25 logistic.py(292):     p = safe_sparse_dot(X, w.T)
1.25 logistic.py(293):     p += intercept
1.25 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
1.25 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
1.25 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
1.25 logistic.py(297):     p = np.exp(p, p)
1.25 logistic.py(298):     return loss, p, w
1.25 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
1.25 logistic.py(346):     diff = sample_weight * (p - Y)
1.25 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
1.25 logistic.py(348):     grad[:, :n_features] += alpha * w
1.25 logistic.py(349):     if fit_intercept:
1.25 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
1.25 logistic.py(351):     return loss, grad.ravel(), p
1.25 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
1.25 logistic.py(339):     n_classes = Y.shape[1]
1.25 logistic.py(340):     n_features = X.shape[1]
1.25 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
1.25 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
1.25 logistic.py(343):                     dtype=X.dtype)
1.25 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
1.25 logistic.py(282):     n_classes = Y.shape[1]
1.25 logistic.py(283):     n_features = X.shape[1]
1.25 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
1.25 logistic.py(285):     w = w.reshape(n_classes, -1)
1.25 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
1.25 logistic.py(287):     if fit_intercept:
1.25 logistic.py(288):         intercept = w[:, -1]
1.25 logistic.py(289):         w = w[:, :-1]
1.25 logistic.py(292):     p = safe_sparse_dot(X, w.T)
1.25 logistic.py(293):     p += intercept
1.25 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
1.25 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
1.25 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
1.25 logistic.py(297):     p = np.exp(p, p)
1.25 logistic.py(298):     return loss, p, w
1.25 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
1.25 logistic.py(346):     diff = sample_weight * (p - Y)
1.25 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
1.25 logistic.py(348):     grad[:, :n_features] += alpha * w
1.25 logistic.py(349):     if fit_intercept:
1.25 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
1.25 logistic.py(351):     return loss, grad.ravel(), p
1.25 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
1.25 logistic.py(339):     n_classes = Y.shape[1]
1.25 logistic.py(340):     n_features = X.shape[1]
1.25 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
1.25 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
1.25 logistic.py(343):                     dtype=X.dtype)
1.25 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
1.25 logistic.py(282):     n_classes = Y.shape[1]
1.25 logistic.py(283):     n_features = X.shape[1]
1.25 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
1.25 logistic.py(285):     w = w.reshape(n_classes, -1)
1.25 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
1.25 logistic.py(287):     if fit_intercept:
1.25 logistic.py(288):         intercept = w[:, -1]
1.25 logistic.py(289):         w = w[:, :-1]
1.25 logistic.py(292):     p = safe_sparse_dot(X, w.T)
1.25 logistic.py(293):     p += intercept
1.25 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
1.25 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
1.25 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
1.25 logistic.py(297):     p = np.exp(p, p)
1.25 logistic.py(298):     return loss, p, w
1.25 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
1.25 logistic.py(346):     diff = sample_weight * (p - Y)
1.25 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
1.25 logistic.py(348):     grad[:, :n_features] += alpha * w
1.25 logistic.py(349):     if fit_intercept:
1.25 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
1.25 logistic.py(351):     return loss, grad.ravel(), p
1.25 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
1.25 logistic.py(339):     n_classes = Y.shape[1]
1.25 logistic.py(340):     n_features = X.shape[1]
1.25 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
1.25 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
1.25 logistic.py(343):                     dtype=X.dtype)
1.25 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
1.25 logistic.py(282):     n_classes = Y.shape[1]
1.25 logistic.py(283):     n_features = X.shape[1]
1.25 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
1.25 logistic.py(285):     w = w.reshape(n_classes, -1)
1.25 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
1.25 logistic.py(287):     if fit_intercept:
1.25 logistic.py(288):         intercept = w[:, -1]
1.25 logistic.py(289):         w = w[:, :-1]
1.25 logistic.py(292):     p = safe_sparse_dot(X, w.T)
1.25 logistic.py(293):     p += intercept
1.25 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
1.25 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
1.25 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
1.25 logistic.py(297):     p = np.exp(p, p)
1.25 logistic.py(298):     return loss, p, w
1.25 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
1.25 logistic.py(346):     diff = sample_weight * (p - Y)
1.25 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
1.25 logistic.py(348):     grad[:, :n_features] += alpha * w
1.25 logistic.py(349):     if fit_intercept:
1.25 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
1.25 logistic.py(351):     return loss, grad.ravel(), p
1.25 logistic.py(718):             if info["warnflag"] == 1:
1.25 logistic.py(723):             n_iter_i = min(info['nit'], max_iter)
1.25 logistic.py(760):         if multi_class == 'multinomial':
1.25 logistic.py(761):             multi_w0 = np.reshape(w0, (classes.size, -1))
1.25 logistic.py(762):             if classes.size == 2:
1.25 logistic.py(764):             coefs.append(multi_w0)
1.25 logistic.py(768):         n_iter[i] = n_iter_i
1.25 logistic.py(712):     for i, C in enumerate(Cs):
1.25 logistic.py(713):         if solver == 'lbfgs':
1.25 logistic.py(714):             w0, loss, info = optimize.fmin_l_bfgs_b(
1.25 logistic.py(715):                 func, w0, fprime=None,
1.25 logistic.py(716):                 args=(X, target, 1. / C, sample_weight),
1.25 logistic.py(717):                 iprint=(verbose > 0) - 1, pgtol=tol, maxiter=max_iter)
1.25 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
1.25 logistic.py(339):     n_classes = Y.shape[1]
1.25 logistic.py(340):     n_features = X.shape[1]
1.25 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
1.25 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
1.25 logistic.py(343):                     dtype=X.dtype)
1.25 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
1.25 logistic.py(282):     n_classes = Y.shape[1]
1.25 logistic.py(283):     n_features = X.shape[1]
1.25 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
1.25 logistic.py(285):     w = w.reshape(n_classes, -1)
1.25 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
1.25 logistic.py(287):     if fit_intercept:
1.25 logistic.py(288):         intercept = w[:, -1]
1.25 logistic.py(289):         w = w[:, :-1]
1.25 logistic.py(292):     p = safe_sparse_dot(X, w.T)
1.25 logistic.py(293):     p += intercept
1.25 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
1.25 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
1.25 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
1.25 logistic.py(297):     p = np.exp(p, p)
1.25 logistic.py(298):     return loss, p, w
1.25 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
1.25 logistic.py(346):     diff = sample_weight * (p - Y)
1.25 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
1.25 logistic.py(348):     grad[:, :n_features] += alpha * w
1.25 logistic.py(349):     if fit_intercept:
1.25 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
1.25 logistic.py(351):     return loss, grad.ravel(), p
1.25 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
1.25 logistic.py(339):     n_classes = Y.shape[1]
1.25 logistic.py(340):     n_features = X.shape[1]
1.25 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
1.25 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
1.25 logistic.py(343):                     dtype=X.dtype)
1.25 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
1.25 logistic.py(282):     n_classes = Y.shape[1]
1.25 logistic.py(283):     n_features = X.shape[1]
1.25 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
1.25 logistic.py(285):     w = w.reshape(n_classes, -1)
1.25 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
1.25 logistic.py(287):     if fit_intercept:
1.25 logistic.py(288):         intercept = w[:, -1]
1.25 logistic.py(289):         w = w[:, :-1]
1.25 logistic.py(292):     p = safe_sparse_dot(X, w.T)
1.25 logistic.py(293):     p += intercept
1.25 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
1.25 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
1.25 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
1.25 logistic.py(297):     p = np.exp(p, p)
1.25 logistic.py(298):     return loss, p, w
1.25 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
1.25 logistic.py(346):     diff = sample_weight * (p - Y)
1.25 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
1.25 logistic.py(348):     grad[:, :n_features] += alpha * w
1.25 logistic.py(349):     if fit_intercept:
1.25 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
1.25 logistic.py(351):     return loss, grad.ravel(), p
1.25 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
1.25 logistic.py(339):     n_classes = Y.shape[1]
1.25 logistic.py(340):     n_features = X.shape[1]
1.25 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
1.25 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
1.25 logistic.py(343):                     dtype=X.dtype)
1.25 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
1.25 logistic.py(282):     n_classes = Y.shape[1]
1.25 logistic.py(283):     n_features = X.shape[1]
1.25 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
1.25 logistic.py(285):     w = w.reshape(n_classes, -1)
1.25 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
1.25 logistic.py(287):     if fit_intercept:
1.25 logistic.py(288):         intercept = w[:, -1]
1.26 logistic.py(289):         w = w[:, :-1]
1.26 logistic.py(292):     p = safe_sparse_dot(X, w.T)
1.26 logistic.py(293):     p += intercept
1.26 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
1.26 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
1.26 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
1.26 logistic.py(297):     p = np.exp(p, p)
1.26 logistic.py(298):     return loss, p, w
1.26 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
1.26 logistic.py(346):     diff = sample_weight * (p - Y)
1.26 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
1.26 logistic.py(348):     grad[:, :n_features] += alpha * w
1.26 logistic.py(349):     if fit_intercept:
1.26 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
1.26 logistic.py(351):     return loss, grad.ravel(), p
1.26 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
1.26 logistic.py(339):     n_classes = Y.shape[1]
1.26 logistic.py(340):     n_features = X.shape[1]
1.26 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
1.26 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
1.26 logistic.py(343):                     dtype=X.dtype)
1.26 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
1.26 logistic.py(282):     n_classes = Y.shape[1]
1.26 logistic.py(283):     n_features = X.shape[1]
1.26 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
1.26 logistic.py(285):     w = w.reshape(n_classes, -1)
1.26 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
1.26 logistic.py(287):     if fit_intercept:
1.26 logistic.py(288):         intercept = w[:, -1]
1.26 logistic.py(289):         w = w[:, :-1]
1.26 logistic.py(292):     p = safe_sparse_dot(X, w.T)
1.26 logistic.py(293):     p += intercept
1.26 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
1.26 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
1.26 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
1.26 logistic.py(297):     p = np.exp(p, p)
1.26 logistic.py(298):     return loss, p, w
1.26 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
1.26 logistic.py(346):     diff = sample_weight * (p - Y)
1.26 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
1.26 logistic.py(348):     grad[:, :n_features] += alpha * w
1.26 logistic.py(349):     if fit_intercept:
1.26 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
1.26 logistic.py(351):     return loss, grad.ravel(), p
1.26 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
1.26 logistic.py(339):     n_classes = Y.shape[1]
1.26 logistic.py(340):     n_features = X.shape[1]
1.26 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
1.26 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
1.26 logistic.py(343):                     dtype=X.dtype)
1.26 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
1.26 logistic.py(282):     n_classes = Y.shape[1]
1.26 logistic.py(283):     n_features = X.shape[1]
1.26 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
1.26 logistic.py(285):     w = w.reshape(n_classes, -1)
1.26 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
1.26 logistic.py(287):     if fit_intercept:
1.26 logistic.py(288):         intercept = w[:, -1]
1.26 logistic.py(289):         w = w[:, :-1]
1.26 logistic.py(292):     p = safe_sparse_dot(X, w.T)
1.26 logistic.py(293):     p += intercept
1.26 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
1.26 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
1.26 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
1.26 logistic.py(297):     p = np.exp(p, p)
1.26 logistic.py(298):     return loss, p, w
1.26 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
1.26 logistic.py(346):     diff = sample_weight * (p - Y)
1.26 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
1.26 logistic.py(348):     grad[:, :n_features] += alpha * w
1.26 logistic.py(349):     if fit_intercept:
1.26 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
1.26 logistic.py(351):     return loss, grad.ravel(), p
1.26 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
1.26 logistic.py(339):     n_classes = Y.shape[1]
1.26 logistic.py(340):     n_features = X.shape[1]
1.26 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
1.26 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
1.26 logistic.py(343):                     dtype=X.dtype)
1.26 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
1.26 logistic.py(282):     n_classes = Y.shape[1]
1.26 logistic.py(283):     n_features = X.shape[1]
1.26 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
1.26 logistic.py(285):     w = w.reshape(n_classes, -1)
1.26 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
1.26 logistic.py(287):     if fit_intercept:
1.26 logistic.py(288):         intercept = w[:, -1]
1.26 logistic.py(289):         w = w[:, :-1]
1.26 logistic.py(292):     p = safe_sparse_dot(X, w.T)
1.26 logistic.py(293):     p += intercept
1.26 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
1.26 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
1.26 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
1.26 logistic.py(297):     p = np.exp(p, p)
1.26 logistic.py(298):     return loss, p, w
1.26 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
1.26 logistic.py(346):     diff = sample_weight * (p - Y)
1.26 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
1.26 logistic.py(348):     grad[:, :n_features] += alpha * w
1.26 logistic.py(349):     if fit_intercept:
1.26 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
1.26 logistic.py(351):     return loss, grad.ravel(), p
1.26 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
1.26 logistic.py(339):     n_classes = Y.shape[1]
1.26 logistic.py(340):     n_features = X.shape[1]
1.26 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
1.26 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
1.26 logistic.py(343):                     dtype=X.dtype)
1.26 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
1.26 logistic.py(282):     n_classes = Y.shape[1]
1.26 logistic.py(283):     n_features = X.shape[1]
1.26 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
1.26 logistic.py(285):     w = w.reshape(n_classes, -1)
1.26 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
1.26 logistic.py(287):     if fit_intercept:
1.26 logistic.py(288):         intercept = w[:, -1]
1.26 logistic.py(289):         w = w[:, :-1]
1.26 logistic.py(292):     p = safe_sparse_dot(X, w.T)
1.26 logistic.py(293):     p += intercept
1.26 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
1.26 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
1.26 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
1.26 logistic.py(297):     p = np.exp(p, p)
1.26 logistic.py(298):     return loss, p, w
1.26 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
1.26 logistic.py(346):     diff = sample_weight * (p - Y)
1.26 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
1.26 logistic.py(348):     grad[:, :n_features] += alpha * w
1.26 logistic.py(349):     if fit_intercept:
1.26 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
1.26 logistic.py(351):     return loss, grad.ravel(), p
1.26 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
1.26 logistic.py(339):     n_classes = Y.shape[1]
1.26 logistic.py(340):     n_features = X.shape[1]
1.26 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
1.26 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
1.26 logistic.py(343):                     dtype=X.dtype)
1.26 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
1.26 logistic.py(282):     n_classes = Y.shape[1]
1.26 logistic.py(283):     n_features = X.shape[1]
1.26 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
1.26 logistic.py(285):     w = w.reshape(n_classes, -1)
1.26 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
1.26 logistic.py(287):     if fit_intercept:
1.26 logistic.py(288):         intercept = w[:, -1]
1.26 logistic.py(289):         w = w[:, :-1]
1.26 logistic.py(292):     p = safe_sparse_dot(X, w.T)
1.26 logistic.py(293):     p += intercept
1.26 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
1.26 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
1.26 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
1.26 logistic.py(297):     p = np.exp(p, p)
1.26 logistic.py(298):     return loss, p, w
1.26 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
1.26 logistic.py(346):     diff = sample_weight * (p - Y)
1.26 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
1.26 logistic.py(348):     grad[:, :n_features] += alpha * w
1.26 logistic.py(349):     if fit_intercept:
1.26 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
1.26 logistic.py(351):     return loss, grad.ravel(), p
1.26 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
1.26 logistic.py(339):     n_classes = Y.shape[1]
1.26 logistic.py(340):     n_features = X.shape[1]
1.26 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
1.26 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
1.26 logistic.py(343):                     dtype=X.dtype)
1.26 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
1.26 logistic.py(282):     n_classes = Y.shape[1]
1.26 logistic.py(283):     n_features = X.shape[1]
1.26 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
1.26 logistic.py(285):     w = w.reshape(n_classes, -1)
1.26 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
1.26 logistic.py(287):     if fit_intercept:
1.26 logistic.py(288):         intercept = w[:, -1]
1.26 logistic.py(289):         w = w[:, :-1]
1.26 logistic.py(292):     p = safe_sparse_dot(X, w.T)
1.26 logistic.py(293):     p += intercept
1.26 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
1.26 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
1.26 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
1.26 logistic.py(297):     p = np.exp(p, p)
1.26 logistic.py(298):     return loss, p, w
1.26 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
1.26 logistic.py(346):     diff = sample_weight * (p - Y)
1.26 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
1.26 logistic.py(348):     grad[:, :n_features] += alpha * w
1.26 logistic.py(349):     if fit_intercept:
1.26 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
1.26 logistic.py(351):     return loss, grad.ravel(), p
1.26 logistic.py(718):             if info["warnflag"] == 1:
1.26 logistic.py(723):             n_iter_i = min(info['nit'], max_iter)
1.26 logistic.py(760):         if multi_class == 'multinomial':
1.26 logistic.py(761):             multi_w0 = np.reshape(w0, (classes.size, -1))
1.26 logistic.py(762):             if classes.size == 2:
1.26 logistic.py(764):             coefs.append(multi_w0)
1.26 logistic.py(768):         n_iter[i] = n_iter_i
1.26 logistic.py(712):     for i, C in enumerate(Cs):
1.26 logistic.py(713):         if solver == 'lbfgs':
1.26 logistic.py(714):             w0, loss, info = optimize.fmin_l_bfgs_b(
1.26 logistic.py(715):                 func, w0, fprime=None,
1.26 logistic.py(716):                 args=(X, target, 1. / C, sample_weight),
1.26 logistic.py(717):                 iprint=(verbose > 0) - 1, pgtol=tol, maxiter=max_iter)
1.26 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
1.26 logistic.py(339):     n_classes = Y.shape[1]
1.26 logistic.py(340):     n_features = X.shape[1]
1.26 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
1.26 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
1.26 logistic.py(343):                     dtype=X.dtype)
1.26 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
1.26 logistic.py(282):     n_classes = Y.shape[1]
1.26 logistic.py(283):     n_features = X.shape[1]
1.26 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
1.26 logistic.py(285):     w = w.reshape(n_classes, -1)
1.26 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
1.26 logistic.py(287):     if fit_intercept:
1.26 logistic.py(288):         intercept = w[:, -1]
1.26 logistic.py(289):         w = w[:, :-1]
1.26 logistic.py(292):     p = safe_sparse_dot(X, w.T)
1.26 logistic.py(293):     p += intercept
1.26 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
1.26 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
1.26 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
1.26 logistic.py(297):     p = np.exp(p, p)
1.26 logistic.py(298):     return loss, p, w
1.26 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
1.26 logistic.py(346):     diff = sample_weight * (p - Y)
1.26 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
1.26 logistic.py(348):     grad[:, :n_features] += alpha * w
1.26 logistic.py(349):     if fit_intercept:
1.26 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
1.26 logistic.py(351):     return loss, grad.ravel(), p
1.26 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
1.26 logistic.py(339):     n_classes = Y.shape[1]
1.26 logistic.py(340):     n_features = X.shape[1]
1.26 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
1.26 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
1.26 logistic.py(343):                     dtype=X.dtype)
1.26 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
1.26 logistic.py(282):     n_classes = Y.shape[1]
1.26 logistic.py(283):     n_features = X.shape[1]
1.26 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
1.26 logistic.py(285):     w = w.reshape(n_classes, -1)
1.26 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
1.26 logistic.py(287):     if fit_intercept:
1.26 logistic.py(288):         intercept = w[:, -1]
1.26 logistic.py(289):         w = w[:, :-1]
1.26 logistic.py(292):     p = safe_sparse_dot(X, w.T)
1.26 logistic.py(293):     p += intercept
1.26 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
1.26 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
1.26 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
1.26 logistic.py(297):     p = np.exp(p, p)
1.26 logistic.py(298):     return loss, p, w
1.26 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
1.26 logistic.py(346):     diff = sample_weight * (p - Y)
1.26 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
1.26 logistic.py(348):     grad[:, :n_features] += alpha * w
1.26 logistic.py(349):     if fit_intercept:
1.26 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
1.26 logistic.py(351):     return loss, grad.ravel(), p
1.26 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
1.26 logistic.py(339):     n_classes = Y.shape[1]
1.26 logistic.py(340):     n_features = X.shape[1]
1.26 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
1.26 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
1.26 logistic.py(343):                     dtype=X.dtype)
1.26 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
1.26 logistic.py(282):     n_classes = Y.shape[1]
1.26 logistic.py(283):     n_features = X.shape[1]
1.26 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
1.26 logistic.py(285):     w = w.reshape(n_classes, -1)
1.26 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
1.26 logistic.py(287):     if fit_intercept:
1.26 logistic.py(288):         intercept = w[:, -1]
1.26 logistic.py(289):         w = w[:, :-1]
1.26 logistic.py(292):     p = safe_sparse_dot(X, w.T)
1.26 logistic.py(293):     p += intercept
1.26 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
1.26 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
1.26 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
1.26 logistic.py(297):     p = np.exp(p, p)
1.26 logistic.py(298):     return loss, p, w
1.26 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
1.26 logistic.py(346):     diff = sample_weight * (p - Y)
1.26 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
1.26 logistic.py(348):     grad[:, :n_features] += alpha * w
1.26 logistic.py(349):     if fit_intercept:
1.26 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
1.26 logistic.py(351):     return loss, grad.ravel(), p
1.26 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
1.26 logistic.py(339):     n_classes = Y.shape[1]
1.26 logistic.py(340):     n_features = X.shape[1]
1.26 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
1.26 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
1.26 logistic.py(343):                     dtype=X.dtype)
1.26 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
1.26 logistic.py(282):     n_classes = Y.shape[1]
1.26 logistic.py(283):     n_features = X.shape[1]
1.26 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
1.26 logistic.py(285):     w = w.reshape(n_classes, -1)
1.26 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
1.26 logistic.py(287):     if fit_intercept:
1.26 logistic.py(288):         intercept = w[:, -1]
1.26 logistic.py(289):         w = w[:, :-1]
1.26 logistic.py(292):     p = safe_sparse_dot(X, w.T)
1.26 logistic.py(293):     p += intercept
1.26 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
1.26 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
1.26 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
1.26 logistic.py(297):     p = np.exp(p, p)
1.26 logistic.py(298):     return loss, p, w
1.26 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
1.26 logistic.py(346):     diff = sample_weight * (p - Y)
1.26 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
1.26 logistic.py(348):     grad[:, :n_features] += alpha * w
1.26 logistic.py(349):     if fit_intercept:
1.26 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
1.26 logistic.py(351):     return loss, grad.ravel(), p
1.26 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
1.26 logistic.py(339):     n_classes = Y.shape[1]
1.26 logistic.py(340):     n_features = X.shape[1]
1.26 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
1.26 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
1.26 logistic.py(343):                     dtype=X.dtype)
1.26 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
1.26 logistic.py(282):     n_classes = Y.shape[1]
1.26 logistic.py(283):     n_features = X.shape[1]
1.26 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
1.26 logistic.py(285):     w = w.reshape(n_classes, -1)
1.26 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
1.26 logistic.py(287):     if fit_intercept:
1.26 logistic.py(288):         intercept = w[:, -1]
1.26 logistic.py(289):         w = w[:, :-1]
1.26 logistic.py(292):     p = safe_sparse_dot(X, w.T)
1.26 logistic.py(293):     p += intercept
1.26 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
1.26 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
1.26 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
1.26 logistic.py(297):     p = np.exp(p, p)
1.26 logistic.py(298):     return loss, p, w
1.26 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
1.26 logistic.py(346):     diff = sample_weight * (p - Y)
1.26 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
1.26 logistic.py(348):     grad[:, :n_features] += alpha * w
1.26 logistic.py(349):     if fit_intercept:
1.26 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
1.26 logistic.py(351):     return loss, grad.ravel(), p
1.26 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
1.26 logistic.py(339):     n_classes = Y.shape[1]
1.26 logistic.py(340):     n_features = X.shape[1]
1.26 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
1.26 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
1.26 logistic.py(343):                     dtype=X.dtype)
1.26 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
1.26 logistic.py(282):     n_classes = Y.shape[1]
1.26 logistic.py(283):     n_features = X.shape[1]
1.26 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
1.26 logistic.py(285):     w = w.reshape(n_classes, -1)
1.26 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
1.26 logistic.py(287):     if fit_intercept:
1.26 logistic.py(288):         intercept = w[:, -1]
1.26 logistic.py(289):         w = w[:, :-1]
1.26 logistic.py(292):     p = safe_sparse_dot(X, w.T)
1.26 logistic.py(293):     p += intercept
1.27 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
1.27 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
1.27 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
1.27 logistic.py(297):     p = np.exp(p, p)
1.27 logistic.py(298):     return loss, p, w
1.27 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
1.27 logistic.py(346):     diff = sample_weight * (p - Y)
1.27 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
1.27 logistic.py(348):     grad[:, :n_features] += alpha * w
1.27 logistic.py(349):     if fit_intercept:
1.27 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
1.27 logistic.py(351):     return loss, grad.ravel(), p
1.27 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
1.27 logistic.py(339):     n_classes = Y.shape[1]
1.27 logistic.py(340):     n_features = X.shape[1]
1.27 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
1.27 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
1.27 logistic.py(343):                     dtype=X.dtype)
1.27 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
1.27 logistic.py(282):     n_classes = Y.shape[1]
1.27 logistic.py(283):     n_features = X.shape[1]
1.27 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
1.27 logistic.py(285):     w = w.reshape(n_classes, -1)
1.27 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
1.27 logistic.py(287):     if fit_intercept:
1.27 logistic.py(288):         intercept = w[:, -1]
1.27 logistic.py(289):         w = w[:, :-1]
1.27 logistic.py(292):     p = safe_sparse_dot(X, w.T)
1.27 logistic.py(293):     p += intercept
1.27 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
1.27 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
1.27 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
1.27 logistic.py(297):     p = np.exp(p, p)
1.27 logistic.py(298):     return loss, p, w
1.27 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
1.27 logistic.py(346):     diff = sample_weight * (p - Y)
1.27 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
1.27 logistic.py(348):     grad[:, :n_features] += alpha * w
1.27 logistic.py(349):     if fit_intercept:
1.27 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
1.27 logistic.py(351):     return loss, grad.ravel(), p
1.27 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
1.27 logistic.py(339):     n_classes = Y.shape[1]
1.27 logistic.py(340):     n_features = X.shape[1]
1.27 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
1.27 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
1.27 logistic.py(343):                     dtype=X.dtype)
1.27 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
1.27 logistic.py(282):     n_classes = Y.shape[1]
1.27 logistic.py(283):     n_features = X.shape[1]
1.27 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
1.27 logistic.py(285):     w = w.reshape(n_classes, -1)
1.27 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
1.27 logistic.py(287):     if fit_intercept:
1.27 logistic.py(288):         intercept = w[:, -1]
1.27 logistic.py(289):         w = w[:, :-1]
1.27 logistic.py(292):     p = safe_sparse_dot(X, w.T)
1.27 logistic.py(293):     p += intercept
1.27 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
1.27 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
1.27 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
1.27 logistic.py(297):     p = np.exp(p, p)
1.27 logistic.py(298):     return loss, p, w
1.27 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
1.27 logistic.py(346):     diff = sample_weight * (p - Y)
1.27 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
1.27 logistic.py(348):     grad[:, :n_features] += alpha * w
1.27 logistic.py(349):     if fit_intercept:
1.27 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
1.27 logistic.py(351):     return loss, grad.ravel(), p
1.27 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
1.27 logistic.py(339):     n_classes = Y.shape[1]
1.27 logistic.py(340):     n_features = X.shape[1]
1.27 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
1.27 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
1.27 logistic.py(343):                     dtype=X.dtype)
1.27 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
1.27 logistic.py(282):     n_classes = Y.shape[1]
1.27 logistic.py(283):     n_features = X.shape[1]
1.27 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
1.27 logistic.py(285):     w = w.reshape(n_classes, -1)
1.27 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
1.27 logistic.py(287):     if fit_intercept:
1.27 logistic.py(288):         intercept = w[:, -1]
1.27 logistic.py(289):         w = w[:, :-1]
1.27 logistic.py(292):     p = safe_sparse_dot(X, w.T)
1.27 logistic.py(293):     p += intercept
1.27 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
1.27 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
1.27 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
1.27 logistic.py(297):     p = np.exp(p, p)
1.27 logistic.py(298):     return loss, p, w
1.27 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
1.27 logistic.py(346):     diff = sample_weight * (p - Y)
1.27 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
1.27 logistic.py(348):     grad[:, :n_features] += alpha * w
1.27 logistic.py(349):     if fit_intercept:
1.27 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
1.27 logistic.py(351):     return loss, grad.ravel(), p
1.27 logistic.py(718):             if info["warnflag"] == 1:
1.27 logistic.py(723):             n_iter_i = min(info['nit'], max_iter)
1.27 logistic.py(760):         if multi_class == 'multinomial':
1.27 logistic.py(761):             multi_w0 = np.reshape(w0, (classes.size, -1))
1.27 logistic.py(762):             if classes.size == 2:
1.27 logistic.py(764):             coefs.append(multi_w0)
1.27 logistic.py(768):         n_iter[i] = n_iter_i
1.27 logistic.py(712):     for i, C in enumerate(Cs):
1.27 logistic.py(713):         if solver == 'lbfgs':
1.27 logistic.py(714):             w0, loss, info = optimize.fmin_l_bfgs_b(
1.27 logistic.py(715):                 func, w0, fprime=None,
1.27 logistic.py(716):                 args=(X, target, 1. / C, sample_weight),
1.27 logistic.py(717):                 iprint=(verbose > 0) - 1, pgtol=tol, maxiter=max_iter)
1.27 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
1.27 logistic.py(339):     n_classes = Y.shape[1]
1.27 logistic.py(340):     n_features = X.shape[1]
1.27 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
1.27 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
1.27 logistic.py(343):                     dtype=X.dtype)
1.27 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
1.27 logistic.py(282):     n_classes = Y.shape[1]
1.27 logistic.py(283):     n_features = X.shape[1]
1.27 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
1.27 logistic.py(285):     w = w.reshape(n_classes, -1)
1.27 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
1.27 logistic.py(287):     if fit_intercept:
1.27 logistic.py(288):         intercept = w[:, -1]
1.27 logistic.py(289):         w = w[:, :-1]
1.27 logistic.py(292):     p = safe_sparse_dot(X, w.T)
1.27 logistic.py(293):     p += intercept
1.27 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
1.27 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
1.27 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
1.27 logistic.py(297):     p = np.exp(p, p)
1.27 logistic.py(298):     return loss, p, w
1.27 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
1.27 logistic.py(346):     diff = sample_weight * (p - Y)
1.27 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
1.27 logistic.py(348):     grad[:, :n_features] += alpha * w
1.27 logistic.py(349):     if fit_intercept:
1.27 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
1.27 logistic.py(351):     return loss, grad.ravel(), p
1.27 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
1.27 logistic.py(339):     n_classes = Y.shape[1]
1.27 logistic.py(340):     n_features = X.shape[1]
1.27 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
1.27 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
1.27 logistic.py(343):                     dtype=X.dtype)
1.27 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
1.27 logistic.py(282):     n_classes = Y.shape[1]
1.27 logistic.py(283):     n_features = X.shape[1]
1.27 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
1.27 logistic.py(285):     w = w.reshape(n_classes, -1)
1.27 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
1.27 logistic.py(287):     if fit_intercept:
1.27 logistic.py(288):         intercept = w[:, -1]
1.27 logistic.py(289):         w = w[:, :-1]
1.27 logistic.py(292):     p = safe_sparse_dot(X, w.T)
1.27 logistic.py(293):     p += intercept
1.27 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
1.27 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
1.27 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
1.27 logistic.py(297):     p = np.exp(p, p)
1.27 logistic.py(298):     return loss, p, w
1.27 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
1.27 logistic.py(346):     diff = sample_weight * (p - Y)
1.27 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
1.27 logistic.py(348):     grad[:, :n_features] += alpha * w
1.27 logistic.py(349):     if fit_intercept:
1.27 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
1.27 logistic.py(351):     return loss, grad.ravel(), p
1.27 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
1.27 logistic.py(339):     n_classes = Y.shape[1]
1.27 logistic.py(340):     n_features = X.shape[1]
1.27 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
1.27 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
1.27 logistic.py(343):                     dtype=X.dtype)
1.27 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
1.27 logistic.py(282):     n_classes = Y.shape[1]
1.27 logistic.py(283):     n_features = X.shape[1]
1.27 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
1.27 logistic.py(285):     w = w.reshape(n_classes, -1)
1.27 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
1.27 logistic.py(287):     if fit_intercept:
1.27 logistic.py(288):         intercept = w[:, -1]
1.27 logistic.py(289):         w = w[:, :-1]
1.27 logistic.py(292):     p = safe_sparse_dot(X, w.T)
1.27 logistic.py(293):     p += intercept
1.27 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
1.27 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
1.27 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
1.27 logistic.py(297):     p = np.exp(p, p)
1.27 logistic.py(298):     return loss, p, w
1.27 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
1.27 logistic.py(346):     diff = sample_weight * (p - Y)
1.27 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
1.27 logistic.py(348):     grad[:, :n_features] += alpha * w
1.27 logistic.py(349):     if fit_intercept:
1.27 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
1.27 logistic.py(351):     return loss, grad.ravel(), p
1.27 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
1.27 logistic.py(339):     n_classes = Y.shape[1]
1.27 logistic.py(340):     n_features = X.shape[1]
1.27 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
1.27 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
1.27 logistic.py(343):                     dtype=X.dtype)
1.27 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
1.27 logistic.py(282):     n_classes = Y.shape[1]
1.27 logistic.py(283):     n_features = X.shape[1]
1.27 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
1.27 logistic.py(285):     w = w.reshape(n_classes, -1)
1.27 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
1.27 logistic.py(287):     if fit_intercept:
1.27 logistic.py(288):         intercept = w[:, -1]
1.27 logistic.py(289):         w = w[:, :-1]
1.27 logistic.py(292):     p = safe_sparse_dot(X, w.T)
1.27 logistic.py(293):     p += intercept
1.27 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
1.27 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
1.27 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
1.27 logistic.py(297):     p = np.exp(p, p)
1.27 logistic.py(298):     return loss, p, w
1.27 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
1.27 logistic.py(346):     diff = sample_weight * (p - Y)
1.27 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
1.27 logistic.py(348):     grad[:, :n_features] += alpha * w
1.27 logistic.py(349):     if fit_intercept:
1.27 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
1.27 logistic.py(351):     return loss, grad.ravel(), p
1.27 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
1.27 logistic.py(339):     n_classes = Y.shape[1]
1.27 logistic.py(340):     n_features = X.shape[1]
1.27 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
1.27 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
1.27 logistic.py(343):                     dtype=X.dtype)
1.27 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
1.27 logistic.py(282):     n_classes = Y.shape[1]
1.27 logistic.py(283):     n_features = X.shape[1]
1.27 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
1.27 logistic.py(285):     w = w.reshape(n_classes, -1)
1.27 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
1.27 logistic.py(287):     if fit_intercept:
1.27 logistic.py(288):         intercept = w[:, -1]
1.27 logistic.py(289):         w = w[:, :-1]
1.27 logistic.py(292):     p = safe_sparse_dot(X, w.T)
1.27 logistic.py(293):     p += intercept
1.27 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
1.27 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
1.27 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
1.27 logistic.py(297):     p = np.exp(p, p)
1.27 logistic.py(298):     return loss, p, w
1.27 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
1.27 logistic.py(346):     diff = sample_weight * (p - Y)
1.27 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
1.27 logistic.py(348):     grad[:, :n_features] += alpha * w
1.27 logistic.py(349):     if fit_intercept:
1.27 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
1.27 logistic.py(351):     return loss, grad.ravel(), p
1.27 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
1.27 logistic.py(339):     n_classes = Y.shape[1]
1.27 logistic.py(340):     n_features = X.shape[1]
1.27 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
1.27 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
1.27 logistic.py(343):                     dtype=X.dtype)
1.27 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
1.27 logistic.py(282):     n_classes = Y.shape[1]
1.27 logistic.py(283):     n_features = X.shape[1]
1.27 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
1.27 logistic.py(285):     w = w.reshape(n_classes, -1)
1.27 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
1.27 logistic.py(287):     if fit_intercept:
1.27 logistic.py(288):         intercept = w[:, -1]
1.27 logistic.py(289):         w = w[:, :-1]
1.27 logistic.py(292):     p = safe_sparse_dot(X, w.T)
1.27 logistic.py(293):     p += intercept
1.27 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
1.27 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
1.27 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
1.27 logistic.py(297):     p = np.exp(p, p)
1.27 logistic.py(298):     return loss, p, w
1.27 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
1.27 logistic.py(346):     diff = sample_weight * (p - Y)
1.27 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
1.27 logistic.py(348):     grad[:, :n_features] += alpha * w
1.27 logistic.py(349):     if fit_intercept:
1.27 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
1.27 logistic.py(351):     return loss, grad.ravel(), p
1.27 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
1.27 logistic.py(339):     n_classes = Y.shape[1]
1.27 logistic.py(340):     n_features = X.shape[1]
1.27 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
1.27 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
1.27 logistic.py(343):                     dtype=X.dtype)
1.27 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
1.27 logistic.py(282):     n_classes = Y.shape[1]
1.27 logistic.py(283):     n_features = X.shape[1]
1.27 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
1.27 logistic.py(285):     w = w.reshape(n_classes, -1)
1.27 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
1.27 logistic.py(287):     if fit_intercept:
1.27 logistic.py(288):         intercept = w[:, -1]
1.27 logistic.py(289):         w = w[:, :-1]
1.27 logistic.py(292):     p = safe_sparse_dot(X, w.T)
1.27 logistic.py(293):     p += intercept
1.27 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
1.27 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
1.27 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
1.27 logistic.py(297):     p = np.exp(p, p)
1.27 logistic.py(298):     return loss, p, w
1.27 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
1.27 logistic.py(346):     diff = sample_weight * (p - Y)
1.27 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
1.27 logistic.py(348):     grad[:, :n_features] += alpha * w
1.27 logistic.py(349):     if fit_intercept:
1.27 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
1.27 logistic.py(351):     return loss, grad.ravel(), p
1.27 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
1.27 logistic.py(339):     n_classes = Y.shape[1]
1.27 logistic.py(340):     n_features = X.shape[1]
1.27 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
1.27 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
1.27 logistic.py(343):                     dtype=X.dtype)
1.27 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
1.27 logistic.py(282):     n_classes = Y.shape[1]
1.27 logistic.py(283):     n_features = X.shape[1]
1.27 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
1.27 logistic.py(285):     w = w.reshape(n_classes, -1)
1.27 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
1.27 logistic.py(287):     if fit_intercept:
1.27 logistic.py(288):         intercept = w[:, -1]
1.27 logistic.py(289):         w = w[:, :-1]
1.27 logistic.py(292):     p = safe_sparse_dot(X, w.T)
1.27 logistic.py(293):     p += intercept
1.27 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
1.27 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
1.27 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
1.27 logistic.py(297):     p = np.exp(p, p)
1.27 logistic.py(298):     return loss, p, w
1.27 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
1.27 logistic.py(346):     diff = sample_weight * (p - Y)
1.27 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
1.27 logistic.py(348):     grad[:, :n_features] += alpha * w
1.27 logistic.py(349):     if fit_intercept:
1.27 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
1.27 logistic.py(351):     return loss, grad.ravel(), p
1.27 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
1.27 logistic.py(339):     n_classes = Y.shape[1]
1.27 logistic.py(340):     n_features = X.shape[1]
1.27 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
1.27 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
1.27 logistic.py(343):                     dtype=X.dtype)
1.27 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
1.27 logistic.py(282):     n_classes = Y.shape[1]
1.27 logistic.py(283):     n_features = X.shape[1]
1.27 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
1.27 logistic.py(285):     w = w.reshape(n_classes, -1)
1.27 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
1.27 logistic.py(287):     if fit_intercept:
1.27 logistic.py(288):         intercept = w[:, -1]
1.27 logistic.py(289):         w = w[:, :-1]
1.27 logistic.py(292):     p = safe_sparse_dot(X, w.T)
1.27 logistic.py(293):     p += intercept
1.27 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
1.27 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
1.28 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
1.28 logistic.py(297):     p = np.exp(p, p)
1.28 logistic.py(298):     return loss, p, w
1.28 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
1.28 logistic.py(346):     diff = sample_weight * (p - Y)
1.28 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
1.28 logistic.py(348):     grad[:, :n_features] += alpha * w
1.28 logistic.py(349):     if fit_intercept:
1.28 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
1.28 logistic.py(351):     return loss, grad.ravel(), p
1.28 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
1.28 logistic.py(339):     n_classes = Y.shape[1]
1.28 logistic.py(340):     n_features = X.shape[1]
1.28 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
1.28 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
1.28 logistic.py(343):                     dtype=X.dtype)
1.28 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
1.28 logistic.py(282):     n_classes = Y.shape[1]
1.28 logistic.py(283):     n_features = X.shape[1]
1.28 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
1.28 logistic.py(285):     w = w.reshape(n_classes, -1)
1.28 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
1.28 logistic.py(287):     if fit_intercept:
1.28 logistic.py(288):         intercept = w[:, -1]
1.28 logistic.py(289):         w = w[:, :-1]
1.28 logistic.py(292):     p = safe_sparse_dot(X, w.T)
1.28 logistic.py(293):     p += intercept
1.28 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
1.28 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
1.28 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
1.28 logistic.py(297):     p = np.exp(p, p)
1.28 logistic.py(298):     return loss, p, w
1.28 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
1.28 logistic.py(346):     diff = sample_weight * (p - Y)
1.28 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
1.28 logistic.py(348):     grad[:, :n_features] += alpha * w
1.28 logistic.py(349):     if fit_intercept:
1.28 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
1.28 logistic.py(351):     return loss, grad.ravel(), p
1.28 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
1.28 logistic.py(339):     n_classes = Y.shape[1]
1.28 logistic.py(340):     n_features = X.shape[1]
1.28 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
1.28 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
1.28 logistic.py(343):                     dtype=X.dtype)
1.28 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
1.28 logistic.py(282):     n_classes = Y.shape[1]
1.28 logistic.py(283):     n_features = X.shape[1]
1.28 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
1.28 logistic.py(285):     w = w.reshape(n_classes, -1)
1.28 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
1.28 logistic.py(287):     if fit_intercept:
1.28 logistic.py(288):         intercept = w[:, -1]
1.28 logistic.py(289):         w = w[:, :-1]
1.28 logistic.py(292):     p = safe_sparse_dot(X, w.T)
1.28 logistic.py(293):     p += intercept
1.28 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
1.28 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
1.28 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
1.28 logistic.py(297):     p = np.exp(p, p)
1.28 logistic.py(298):     return loss, p, w
1.28 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
1.28 logistic.py(346):     diff = sample_weight * (p - Y)
1.28 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
1.28 logistic.py(348):     grad[:, :n_features] += alpha * w
1.28 logistic.py(349):     if fit_intercept:
1.28 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
1.28 logistic.py(351):     return loss, grad.ravel(), p
1.28 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
1.28 logistic.py(339):     n_classes = Y.shape[1]
1.28 logistic.py(340):     n_features = X.shape[1]
1.28 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
1.28 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
1.28 logistic.py(343):                     dtype=X.dtype)
1.28 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
1.28 logistic.py(282):     n_classes = Y.shape[1]
1.28 logistic.py(283):     n_features = X.shape[1]
1.28 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
1.28 logistic.py(285):     w = w.reshape(n_classes, -1)
1.28 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
1.28 logistic.py(287):     if fit_intercept:
1.28 logistic.py(288):         intercept = w[:, -1]
1.28 logistic.py(289):         w = w[:, :-1]
1.28 logistic.py(292):     p = safe_sparse_dot(X, w.T)
1.28 logistic.py(293):     p += intercept
1.28 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
1.28 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
1.28 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
1.28 logistic.py(297):     p = np.exp(p, p)
1.28 logistic.py(298):     return loss, p, w
1.28 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
1.28 logistic.py(346):     diff = sample_weight * (p - Y)
1.28 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
1.28 logistic.py(348):     grad[:, :n_features] += alpha * w
1.28 logistic.py(349):     if fit_intercept:
1.28 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
1.28 logistic.py(351):     return loss, grad.ravel(), p
1.28 logistic.py(718):             if info["warnflag"] == 1:
1.28 logistic.py(723):             n_iter_i = min(info['nit'], max_iter)
1.28 logistic.py(760):         if multi_class == 'multinomial':
1.28 logistic.py(761):             multi_w0 = np.reshape(w0, (classes.size, -1))
1.28 logistic.py(762):             if classes.size == 2:
1.28 logistic.py(764):             coefs.append(multi_w0)
1.28 logistic.py(768):         n_iter[i] = n_iter_i
1.28 logistic.py(712):     for i, C in enumerate(Cs):
1.28 logistic.py(713):         if solver == 'lbfgs':
1.28 logistic.py(714):             w0, loss, info = optimize.fmin_l_bfgs_b(
1.28 logistic.py(715):                 func, w0, fprime=None,
1.28 logistic.py(716):                 args=(X, target, 1. / C, sample_weight),
1.28 logistic.py(717):                 iprint=(verbose > 0) - 1, pgtol=tol, maxiter=max_iter)
1.28 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
1.28 logistic.py(339):     n_classes = Y.shape[1]
1.28 logistic.py(340):     n_features = X.shape[1]
1.28 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
1.28 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
1.28 logistic.py(343):                     dtype=X.dtype)
1.28 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
1.28 logistic.py(282):     n_classes = Y.shape[1]
1.28 logistic.py(283):     n_features = X.shape[1]
1.28 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
1.28 logistic.py(285):     w = w.reshape(n_classes, -1)
1.28 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
1.28 logistic.py(287):     if fit_intercept:
1.28 logistic.py(288):         intercept = w[:, -1]
1.28 logistic.py(289):         w = w[:, :-1]
1.28 logistic.py(292):     p = safe_sparse_dot(X, w.T)
1.28 logistic.py(293):     p += intercept
1.28 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
1.28 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
1.28 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
1.28 logistic.py(297):     p = np.exp(p, p)
1.28 logistic.py(298):     return loss, p, w
1.28 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
1.28 logistic.py(346):     diff = sample_weight * (p - Y)
1.28 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
1.28 logistic.py(348):     grad[:, :n_features] += alpha * w
1.28 logistic.py(349):     if fit_intercept:
1.28 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
1.28 logistic.py(351):     return loss, grad.ravel(), p
1.28 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
1.28 logistic.py(339):     n_classes = Y.shape[1]
1.28 logistic.py(340):     n_features = X.shape[1]
1.28 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
1.28 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
1.28 logistic.py(343):                     dtype=X.dtype)
1.28 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
1.28 logistic.py(282):     n_classes = Y.shape[1]
1.28 logistic.py(283):     n_features = X.shape[1]
1.28 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
1.28 logistic.py(285):     w = w.reshape(n_classes, -1)
1.28 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
1.28 logistic.py(287):     if fit_intercept:
1.28 logistic.py(288):         intercept = w[:, -1]
1.28 logistic.py(289):         w = w[:, :-1]
1.28 logistic.py(292):     p = safe_sparse_dot(X, w.T)
1.28 logistic.py(293):     p += intercept
1.28 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
1.28 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
1.28 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
1.28 logistic.py(297):     p = np.exp(p, p)
1.28 logistic.py(298):     return loss, p, w
1.28 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
1.28 logistic.py(346):     diff = sample_weight * (p - Y)
1.28 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
1.28 logistic.py(348):     grad[:, :n_features] += alpha * w
1.28 logistic.py(349):     if fit_intercept:
1.28 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
1.28 logistic.py(351):     return loss, grad.ravel(), p
1.28 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
1.28 logistic.py(339):     n_classes = Y.shape[1]
1.28 logistic.py(340):     n_features = X.shape[1]
1.28 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
1.28 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
1.28 logistic.py(343):                     dtype=X.dtype)
1.28 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
1.28 logistic.py(282):     n_classes = Y.shape[1]
1.28 logistic.py(283):     n_features = X.shape[1]
1.28 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
1.28 logistic.py(285):     w = w.reshape(n_classes, -1)
1.28 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
1.28 logistic.py(287):     if fit_intercept:
1.28 logistic.py(288):         intercept = w[:, -1]
1.28 logistic.py(289):         w = w[:, :-1]
1.28 logistic.py(292):     p = safe_sparse_dot(X, w.T)
1.28 logistic.py(293):     p += intercept
1.28 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
1.28 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
1.28 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
1.28 logistic.py(297):     p = np.exp(p, p)
1.28 logistic.py(298):     return loss, p, w
1.28 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
1.28 logistic.py(346):     diff = sample_weight * (p - Y)
1.28 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
1.28 logistic.py(348):     grad[:, :n_features] += alpha * w
1.28 logistic.py(349):     if fit_intercept:
1.28 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
1.28 logistic.py(351):     return loss, grad.ravel(), p
1.28 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
1.28 logistic.py(339):     n_classes = Y.shape[1]
1.28 logistic.py(340):     n_features = X.shape[1]
1.28 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
1.28 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
1.28 logistic.py(343):                     dtype=X.dtype)
1.28 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
1.28 logistic.py(282):     n_classes = Y.shape[1]
1.28 logistic.py(283):     n_features = X.shape[1]
1.28 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
1.28 logistic.py(285):     w = w.reshape(n_classes, -1)
1.28 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
1.28 logistic.py(287):     if fit_intercept:
1.28 logistic.py(288):         intercept = w[:, -1]
1.28 logistic.py(289):         w = w[:, :-1]
1.28 logistic.py(292):     p = safe_sparse_dot(X, w.T)
1.28 logistic.py(293):     p += intercept
1.28 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
1.28 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
1.28 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
1.28 logistic.py(297):     p = np.exp(p, p)
1.28 logistic.py(298):     return loss, p, w
1.28 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
1.28 logistic.py(346):     diff = sample_weight * (p - Y)
1.28 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
1.28 logistic.py(348):     grad[:, :n_features] += alpha * w
1.28 logistic.py(349):     if fit_intercept:
1.28 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
1.28 logistic.py(351):     return loss, grad.ravel(), p
1.28 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
1.28 logistic.py(339):     n_classes = Y.shape[1]
1.28 logistic.py(340):     n_features = X.shape[1]
1.28 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
1.28 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
1.28 logistic.py(343):                     dtype=X.dtype)
1.28 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
1.28 logistic.py(282):     n_classes = Y.shape[1]
1.28 logistic.py(283):     n_features = X.shape[1]
1.28 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
1.28 logistic.py(285):     w = w.reshape(n_classes, -1)
1.28 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
1.28 logistic.py(287):     if fit_intercept:
1.28 logistic.py(288):         intercept = w[:, -1]
1.28 logistic.py(289):         w = w[:, :-1]
1.28 logistic.py(292):     p = safe_sparse_dot(X, w.T)
1.28 logistic.py(293):     p += intercept
1.28 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
1.28 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
1.28 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
1.28 logistic.py(297):     p = np.exp(p, p)
1.28 logistic.py(298):     return loss, p, w
1.28 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
1.28 logistic.py(346):     diff = sample_weight * (p - Y)
1.28 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
1.28 logistic.py(348):     grad[:, :n_features] += alpha * w
1.28 logistic.py(349):     if fit_intercept:
1.28 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
1.28 logistic.py(351):     return loss, grad.ravel(), p
1.28 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
1.28 logistic.py(339):     n_classes = Y.shape[1]
1.28 logistic.py(340):     n_features = X.shape[1]
1.28 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
1.28 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
1.28 logistic.py(343):                     dtype=X.dtype)
1.28 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
1.28 logistic.py(282):     n_classes = Y.shape[1]
1.28 logistic.py(283):     n_features = X.shape[1]
1.28 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
1.28 logistic.py(285):     w = w.reshape(n_classes, -1)
1.28 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
1.28 logistic.py(287):     if fit_intercept:
1.28 logistic.py(288):         intercept = w[:, -1]
1.28 logistic.py(289):         w = w[:, :-1]
1.28 logistic.py(292):     p = safe_sparse_dot(X, w.T)
1.28 logistic.py(293):     p += intercept
1.28 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
1.28 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
1.28 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
1.28 logistic.py(297):     p = np.exp(p, p)
1.28 logistic.py(298):     return loss, p, w
1.28 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
1.28 logistic.py(346):     diff = sample_weight * (p - Y)
1.28 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
1.28 logistic.py(348):     grad[:, :n_features] += alpha * w
1.28 logistic.py(349):     if fit_intercept:
1.28 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
1.28 logistic.py(351):     return loss, grad.ravel(), p
1.28 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
1.28 logistic.py(339):     n_classes = Y.shape[1]
1.28 logistic.py(340):     n_features = X.shape[1]
1.28 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
1.28 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
1.28 logistic.py(343):                     dtype=X.dtype)
1.28 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
1.28 logistic.py(282):     n_classes = Y.shape[1]
1.28 logistic.py(283):     n_features = X.shape[1]
1.28 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
1.28 logistic.py(285):     w = w.reshape(n_classes, -1)
1.28 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
1.28 logistic.py(287):     if fit_intercept:
1.28 logistic.py(288):         intercept = w[:, -1]
1.28 logistic.py(289):         w = w[:, :-1]
1.28 logistic.py(292):     p = safe_sparse_dot(X, w.T)
1.28 logistic.py(293):     p += intercept
1.28 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
1.28 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
1.28 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
1.28 logistic.py(297):     p = np.exp(p, p)
1.28 logistic.py(298):     return loss, p, w
1.28 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
1.28 logistic.py(346):     diff = sample_weight * (p - Y)
1.28 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
1.28 logistic.py(348):     grad[:, :n_features] += alpha * w
1.28 logistic.py(349):     if fit_intercept:
1.28 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
1.28 logistic.py(351):     return loss, grad.ravel(), p
1.28 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
1.28 logistic.py(339):     n_classes = Y.shape[1]
1.28 logistic.py(340):     n_features = X.shape[1]
1.28 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
1.28 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
1.28 logistic.py(343):                     dtype=X.dtype)
1.28 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
1.28 logistic.py(282):     n_classes = Y.shape[1]
1.28 logistic.py(283):     n_features = X.shape[1]
1.28 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
1.28 logistic.py(285):     w = w.reshape(n_classes, -1)
1.28 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
1.28 logistic.py(287):     if fit_intercept:
1.28 logistic.py(288):         intercept = w[:, -1]
1.28 logistic.py(289):         w = w[:, :-1]
1.28 logistic.py(292):     p = safe_sparse_dot(X, w.T)
1.28 logistic.py(293):     p += intercept
1.28 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
1.28 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
1.28 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
1.28 logistic.py(297):     p = np.exp(p, p)
1.28 logistic.py(298):     return loss, p, w
1.28 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
1.28 logistic.py(346):     diff = sample_weight * (p - Y)
1.28 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
1.28 logistic.py(348):     grad[:, :n_features] += alpha * w
1.28 logistic.py(349):     if fit_intercept:
1.28 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
1.28 logistic.py(351):     return loss, grad.ravel(), p
1.28 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
1.28 logistic.py(339):     n_classes = Y.shape[1]
1.28 logistic.py(340):     n_features = X.shape[1]
1.28 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
1.28 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
1.28 logistic.py(343):                     dtype=X.dtype)
1.28 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
1.28 logistic.py(282):     n_classes = Y.shape[1]
1.28 logistic.py(283):     n_features = X.shape[1]
1.28 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
1.28 logistic.py(285):     w = w.reshape(n_classes, -1)
1.28 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
1.28 logistic.py(287):     if fit_intercept:
1.28 logistic.py(288):         intercept = w[:, -1]
1.28 logistic.py(289):         w = w[:, :-1]
1.28 logistic.py(292):     p = safe_sparse_dot(X, w.T)
1.28 logistic.py(293):     p += intercept
1.29 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
1.29 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
1.29 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
1.29 logistic.py(297):     p = np.exp(p, p)
1.29 logistic.py(298):     return loss, p, w
1.29 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
1.29 logistic.py(346):     diff = sample_weight * (p - Y)
1.29 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
1.29 logistic.py(348):     grad[:, :n_features] += alpha * w
1.29 logistic.py(349):     if fit_intercept:
1.29 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
1.29 logistic.py(351):     return loss, grad.ravel(), p
1.29 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
1.29 logistic.py(339):     n_classes = Y.shape[1]
1.29 logistic.py(340):     n_features = X.shape[1]
1.29 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
1.29 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
1.29 logistic.py(343):                     dtype=X.dtype)
1.29 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
1.29 logistic.py(282):     n_classes = Y.shape[1]
1.29 logistic.py(283):     n_features = X.shape[1]
1.29 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
1.29 logistic.py(285):     w = w.reshape(n_classes, -1)
1.29 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
1.29 logistic.py(287):     if fit_intercept:
1.29 logistic.py(288):         intercept = w[:, -1]
1.29 logistic.py(289):         w = w[:, :-1]
1.29 logistic.py(292):     p = safe_sparse_dot(X, w.T)
1.29 logistic.py(293):     p += intercept
1.29 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
1.29 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
1.29 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
1.29 logistic.py(297):     p = np.exp(p, p)
1.29 logistic.py(298):     return loss, p, w
1.29 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
1.29 logistic.py(346):     diff = sample_weight * (p - Y)
1.29 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
1.29 logistic.py(348):     grad[:, :n_features] += alpha * w
1.29 logistic.py(349):     if fit_intercept:
1.29 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
1.29 logistic.py(351):     return loss, grad.ravel(), p
1.29 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
1.29 logistic.py(339):     n_classes = Y.shape[1]
1.29 logistic.py(340):     n_features = X.shape[1]
1.29 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
1.29 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
1.29 logistic.py(343):                     dtype=X.dtype)
1.29 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
1.29 logistic.py(282):     n_classes = Y.shape[1]
1.29 logistic.py(283):     n_features = X.shape[1]
1.29 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
1.29 logistic.py(285):     w = w.reshape(n_classes, -1)
1.29 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
1.29 logistic.py(287):     if fit_intercept:
1.29 logistic.py(288):         intercept = w[:, -1]
1.29 logistic.py(289):         w = w[:, :-1]
1.29 logistic.py(292):     p = safe_sparse_dot(X, w.T)
1.29 logistic.py(293):     p += intercept
1.29 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
1.29 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
1.29 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
1.29 logistic.py(297):     p = np.exp(p, p)
1.29 logistic.py(298):     return loss, p, w
1.29 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
1.29 logistic.py(346):     diff = sample_weight * (p - Y)
1.29 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
1.29 logistic.py(348):     grad[:, :n_features] += alpha * w
1.29 logistic.py(349):     if fit_intercept:
1.29 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
1.29 logistic.py(351):     return loss, grad.ravel(), p
1.29 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
1.29 logistic.py(339):     n_classes = Y.shape[1]
1.29 logistic.py(340):     n_features = X.shape[1]
1.29 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
1.29 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
1.29 logistic.py(343):                     dtype=X.dtype)
1.29 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
1.29 logistic.py(282):     n_classes = Y.shape[1]
1.29 logistic.py(283):     n_features = X.shape[1]
1.29 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
1.29 logistic.py(285):     w = w.reshape(n_classes, -1)
1.29 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
1.29 logistic.py(287):     if fit_intercept:
1.29 logistic.py(288):         intercept = w[:, -1]
1.29 logistic.py(289):         w = w[:, :-1]
1.29 logistic.py(292):     p = safe_sparse_dot(X, w.T)
1.29 logistic.py(293):     p += intercept
1.29 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
1.29 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
1.29 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
1.29 logistic.py(297):     p = np.exp(p, p)
1.29 logistic.py(298):     return loss, p, w
1.29 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
1.29 logistic.py(346):     diff = sample_weight * (p - Y)
1.29 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
1.29 logistic.py(348):     grad[:, :n_features] += alpha * w
1.29 logistic.py(349):     if fit_intercept:
1.29 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
1.29 logistic.py(351):     return loss, grad.ravel(), p
1.29 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
1.29 logistic.py(339):     n_classes = Y.shape[1]
1.29 logistic.py(340):     n_features = X.shape[1]
1.29 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
1.29 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
1.29 logistic.py(343):                     dtype=X.dtype)
1.29 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
1.29 logistic.py(282):     n_classes = Y.shape[1]
1.29 logistic.py(283):     n_features = X.shape[1]
1.29 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
1.29 logistic.py(285):     w = w.reshape(n_classes, -1)
1.29 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
1.29 logistic.py(287):     if fit_intercept:
1.29 logistic.py(288):         intercept = w[:, -1]
1.29 logistic.py(289):         w = w[:, :-1]
1.29 logistic.py(292):     p = safe_sparse_dot(X, w.T)
1.29 logistic.py(293):     p += intercept
1.29 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
1.29 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
1.29 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
1.29 logistic.py(297):     p = np.exp(p, p)
1.29 logistic.py(298):     return loss, p, w
1.29 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
1.29 logistic.py(346):     diff = sample_weight * (p - Y)
1.29 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
1.29 logistic.py(348):     grad[:, :n_features] += alpha * w
1.29 logistic.py(349):     if fit_intercept:
1.29 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
1.29 logistic.py(351):     return loss, grad.ravel(), p
1.29 logistic.py(718):             if info["warnflag"] == 1:
1.29 logistic.py(723):             n_iter_i = min(info['nit'], max_iter)
1.29 logistic.py(760):         if multi_class == 'multinomial':
1.29 logistic.py(761):             multi_w0 = np.reshape(w0, (classes.size, -1))
1.29 logistic.py(762):             if classes.size == 2:
1.29 logistic.py(764):             coefs.append(multi_w0)
1.29 logistic.py(768):         n_iter[i] = n_iter_i
1.29 logistic.py(712):     for i, C in enumerate(Cs):
1.29 logistic.py(713):         if solver == 'lbfgs':
1.29 logistic.py(714):             w0, loss, info = optimize.fmin_l_bfgs_b(
1.29 logistic.py(715):                 func, w0, fprime=None,
1.29 logistic.py(716):                 args=(X, target, 1. / C, sample_weight),
1.29 logistic.py(717):                 iprint=(verbose > 0) - 1, pgtol=tol, maxiter=max_iter)
1.29 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
1.29 logistic.py(339):     n_classes = Y.shape[1]
1.29 logistic.py(340):     n_features = X.shape[1]
1.29 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
1.29 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
1.29 logistic.py(343):                     dtype=X.dtype)
1.29 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
1.29 logistic.py(282):     n_classes = Y.shape[1]
1.29 logistic.py(283):     n_features = X.shape[1]
1.29 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
1.29 logistic.py(285):     w = w.reshape(n_classes, -1)
1.29 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
1.29 logistic.py(287):     if fit_intercept:
1.29 logistic.py(288):         intercept = w[:, -1]
1.29 logistic.py(289):         w = w[:, :-1]
1.29 logistic.py(292):     p = safe_sparse_dot(X, w.T)
1.29 logistic.py(293):     p += intercept
1.29 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
1.29 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
1.29 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
1.29 logistic.py(297):     p = np.exp(p, p)
1.29 logistic.py(298):     return loss, p, w
1.29 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
1.29 logistic.py(346):     diff = sample_weight * (p - Y)
1.29 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
1.29 logistic.py(348):     grad[:, :n_features] += alpha * w
1.29 logistic.py(349):     if fit_intercept:
1.29 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
1.29 logistic.py(351):     return loss, grad.ravel(), p
1.29 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
1.29 logistic.py(339):     n_classes = Y.shape[1]
1.29 logistic.py(340):     n_features = X.shape[1]
1.29 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
1.29 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
1.29 logistic.py(343):                     dtype=X.dtype)
1.29 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
1.29 logistic.py(282):     n_classes = Y.shape[1]
1.29 logistic.py(283):     n_features = X.shape[1]
1.29 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
1.29 logistic.py(285):     w = w.reshape(n_classes, -1)
1.29 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
1.29 logistic.py(287):     if fit_intercept:
1.29 logistic.py(288):         intercept = w[:, -1]
1.29 logistic.py(289):         w = w[:, :-1]
1.29 logistic.py(292):     p = safe_sparse_dot(X, w.T)
1.29 logistic.py(293):     p += intercept
1.29 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
1.29 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
1.29 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
1.29 logistic.py(297):     p = np.exp(p, p)
1.29 logistic.py(298):     return loss, p, w
1.29 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
1.29 logistic.py(346):     diff = sample_weight * (p - Y)
1.29 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
1.29 logistic.py(348):     grad[:, :n_features] += alpha * w
1.29 logistic.py(349):     if fit_intercept:
1.29 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
1.29 logistic.py(351):     return loss, grad.ravel(), p
1.29 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
1.29 logistic.py(339):     n_classes = Y.shape[1]
1.29 logistic.py(340):     n_features = X.shape[1]
1.29 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
1.29 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
1.29 logistic.py(343):                     dtype=X.dtype)
1.29 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
1.29 logistic.py(282):     n_classes = Y.shape[1]
1.29 logistic.py(283):     n_features = X.shape[1]
1.29 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
1.29 logistic.py(285):     w = w.reshape(n_classes, -1)
1.29 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
1.29 logistic.py(287):     if fit_intercept:
1.29 logistic.py(288):         intercept = w[:, -1]
1.29 logistic.py(289):         w = w[:, :-1]
1.29 logistic.py(292):     p = safe_sparse_dot(X, w.T)
1.29 logistic.py(293):     p += intercept
1.29 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
1.29 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
1.29 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
1.29 logistic.py(297):     p = np.exp(p, p)
1.29 logistic.py(298):     return loss, p, w
1.29 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
1.29 logistic.py(346):     diff = sample_weight * (p - Y)
1.29 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
1.29 logistic.py(348):     grad[:, :n_features] += alpha * w
1.29 logistic.py(349):     if fit_intercept:
1.29 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
1.29 logistic.py(351):     return loss, grad.ravel(), p
1.29 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
1.29 logistic.py(339):     n_classes = Y.shape[1]
1.29 logistic.py(340):     n_features = X.shape[1]
1.29 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
1.29 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
1.29 logistic.py(343):                     dtype=X.dtype)
1.29 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
1.29 logistic.py(282):     n_classes = Y.shape[1]
1.29 logistic.py(283):     n_features = X.shape[1]
1.29 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
1.29 logistic.py(285):     w = w.reshape(n_classes, -1)
1.29 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
1.29 logistic.py(287):     if fit_intercept:
1.29 logistic.py(288):         intercept = w[:, -1]
1.29 logistic.py(289):         w = w[:, :-1]
1.29 logistic.py(292):     p = safe_sparse_dot(X, w.T)
1.29 logistic.py(293):     p += intercept
1.29 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
1.29 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
1.29 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
1.29 logistic.py(297):     p = np.exp(p, p)
1.29 logistic.py(298):     return loss, p, w
1.29 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
1.29 logistic.py(346):     diff = sample_weight * (p - Y)
1.29 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
1.29 logistic.py(348):     grad[:, :n_features] += alpha * w
1.29 logistic.py(349):     if fit_intercept:
1.29 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
1.29 logistic.py(351):     return loss, grad.ravel(), p
1.29 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
1.29 logistic.py(339):     n_classes = Y.shape[1]
1.29 logistic.py(340):     n_features = X.shape[1]
1.29 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
1.29 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
1.29 logistic.py(343):                     dtype=X.dtype)
1.29 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
1.29 logistic.py(282):     n_classes = Y.shape[1]
1.29 logistic.py(283):     n_features = X.shape[1]
1.29 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
1.29 logistic.py(285):     w = w.reshape(n_classes, -1)
1.29 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
1.29 logistic.py(287):     if fit_intercept:
1.29 logistic.py(288):         intercept = w[:, -1]
1.29 logistic.py(289):         w = w[:, :-1]
1.29 logistic.py(292):     p = safe_sparse_dot(X, w.T)
1.29 logistic.py(293):     p += intercept
1.29 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
1.29 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
1.29 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
1.29 logistic.py(297):     p = np.exp(p, p)
1.29 logistic.py(298):     return loss, p, w
1.29 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
1.29 logistic.py(346):     diff = sample_weight * (p - Y)
1.29 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
1.29 logistic.py(348):     grad[:, :n_features] += alpha * w
1.29 logistic.py(349):     if fit_intercept:
1.29 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
1.29 logistic.py(351):     return loss, grad.ravel(), p
1.29 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
1.29 logistic.py(339):     n_classes = Y.shape[1]
1.29 logistic.py(340):     n_features = X.shape[1]
1.29 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
1.29 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
1.29 logistic.py(343):                     dtype=X.dtype)
1.29 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
1.29 logistic.py(282):     n_classes = Y.shape[1]
1.29 logistic.py(283):     n_features = X.shape[1]
1.29 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
1.29 logistic.py(285):     w = w.reshape(n_classes, -1)
1.29 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
1.29 logistic.py(287):     if fit_intercept:
1.29 logistic.py(288):         intercept = w[:, -1]
1.29 logistic.py(289):         w = w[:, :-1]
1.29 logistic.py(292):     p = safe_sparse_dot(X, w.T)
1.29 logistic.py(293):     p += intercept
1.29 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
1.29 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
1.29 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
1.29 logistic.py(297):     p = np.exp(p, p)
1.29 logistic.py(298):     return loss, p, w
1.29 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
1.29 logistic.py(346):     diff = sample_weight * (p - Y)
1.29 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
1.29 logistic.py(348):     grad[:, :n_features] += alpha * w
1.29 logistic.py(349):     if fit_intercept:
1.29 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
1.29 logistic.py(351):     return loss, grad.ravel(), p
1.29 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
1.29 logistic.py(339):     n_classes = Y.shape[1]
1.29 logistic.py(340):     n_features = X.shape[1]
1.29 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
1.29 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
1.29 logistic.py(343):                     dtype=X.dtype)
1.29 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
1.29 logistic.py(282):     n_classes = Y.shape[1]
1.29 logistic.py(283):     n_features = X.shape[1]
1.29 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
1.29 logistic.py(285):     w = w.reshape(n_classes, -1)
1.29 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
1.29 logistic.py(287):     if fit_intercept:
1.29 logistic.py(288):         intercept = w[:, -1]
1.29 logistic.py(289):         w = w[:, :-1]
1.29 logistic.py(292):     p = safe_sparse_dot(X, w.T)
1.29 logistic.py(293):     p += intercept
1.29 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
1.29 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
1.29 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
1.29 logistic.py(297):     p = np.exp(p, p)
1.29 logistic.py(298):     return loss, p, w
1.29 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
1.29 logistic.py(346):     diff = sample_weight * (p - Y)
1.29 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
1.29 logistic.py(348):     grad[:, :n_features] += alpha * w
1.29 logistic.py(349):     if fit_intercept:
1.29 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
1.29 logistic.py(351):     return loss, grad.ravel(), p
1.29 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
1.29 logistic.py(339):     n_classes = Y.shape[1]
1.29 logistic.py(340):     n_features = X.shape[1]
1.29 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
1.29 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
1.29 logistic.py(343):                     dtype=X.dtype)
1.29 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
1.29 logistic.py(282):     n_classes = Y.shape[1]
1.29 logistic.py(283):     n_features = X.shape[1]
1.30 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
1.30 logistic.py(285):     w = w.reshape(n_classes, -1)
1.30 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
1.30 logistic.py(287):     if fit_intercept:
1.30 logistic.py(288):         intercept = w[:, -1]
1.30 logistic.py(289):         w = w[:, :-1]
1.30 logistic.py(292):     p = safe_sparse_dot(X, w.T)
1.30 logistic.py(293):     p += intercept
1.30 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
1.30 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
1.30 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
1.30 logistic.py(297):     p = np.exp(p, p)
1.30 logistic.py(298):     return loss, p, w
1.30 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
1.30 logistic.py(346):     diff = sample_weight * (p - Y)
1.30 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
1.30 logistic.py(348):     grad[:, :n_features] += alpha * w
1.30 logistic.py(349):     if fit_intercept:
1.30 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
1.30 logistic.py(351):     return loss, grad.ravel(), p
1.30 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
1.30 logistic.py(339):     n_classes = Y.shape[1]
1.30 logistic.py(340):     n_features = X.shape[1]
1.30 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
1.30 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
1.30 logistic.py(343):                     dtype=X.dtype)
1.30 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
1.30 logistic.py(282):     n_classes = Y.shape[1]
1.30 logistic.py(283):     n_features = X.shape[1]
1.30 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
1.30 logistic.py(285):     w = w.reshape(n_classes, -1)
1.30 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
1.30 logistic.py(287):     if fit_intercept:
1.30 logistic.py(288):         intercept = w[:, -1]
1.30 logistic.py(289):         w = w[:, :-1]
1.30 logistic.py(292):     p = safe_sparse_dot(X, w.T)
1.30 logistic.py(293):     p += intercept
1.30 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
1.30 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
1.30 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
1.30 logistic.py(297):     p = np.exp(p, p)
1.30 logistic.py(298):     return loss, p, w
1.30 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
1.30 logistic.py(346):     diff = sample_weight * (p - Y)
1.30 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
1.30 logistic.py(348):     grad[:, :n_features] += alpha * w
1.30 logistic.py(349):     if fit_intercept:
1.30 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
1.30 logistic.py(351):     return loss, grad.ravel(), p
1.30 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
1.30 logistic.py(339):     n_classes = Y.shape[1]
1.30 logistic.py(340):     n_features = X.shape[1]
1.30 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
1.30 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
1.30 logistic.py(343):                     dtype=X.dtype)
1.30 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
1.30 logistic.py(282):     n_classes = Y.shape[1]
1.30 logistic.py(283):     n_features = X.shape[1]
1.30 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
1.30 logistic.py(285):     w = w.reshape(n_classes, -1)
1.30 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
1.30 logistic.py(287):     if fit_intercept:
1.30 logistic.py(288):         intercept = w[:, -1]
1.30 logistic.py(289):         w = w[:, :-1]
1.30 logistic.py(292):     p = safe_sparse_dot(X, w.T)
1.30 logistic.py(293):     p += intercept
1.30 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
1.30 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
1.30 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
1.30 logistic.py(297):     p = np.exp(p, p)
1.30 logistic.py(298):     return loss, p, w
1.30 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
1.30 logistic.py(346):     diff = sample_weight * (p - Y)
1.30 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
1.30 logistic.py(348):     grad[:, :n_features] += alpha * w
1.30 logistic.py(349):     if fit_intercept:
1.30 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
1.30 logistic.py(351):     return loss, grad.ravel(), p
1.30 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
1.30 logistic.py(339):     n_classes = Y.shape[1]
1.30 logistic.py(340):     n_features = X.shape[1]
1.30 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
1.30 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
1.30 logistic.py(343):                     dtype=X.dtype)
1.30 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
1.30 logistic.py(282):     n_classes = Y.shape[1]
1.30 logistic.py(283):     n_features = X.shape[1]
1.30 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
1.30 logistic.py(285):     w = w.reshape(n_classes, -1)
1.30 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
1.30 logistic.py(287):     if fit_intercept:
1.30 logistic.py(288):         intercept = w[:, -1]
1.30 logistic.py(289):         w = w[:, :-1]
1.30 logistic.py(292):     p = safe_sparse_dot(X, w.T)
1.30 logistic.py(293):     p += intercept
1.30 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
1.30 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
1.30 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
1.30 logistic.py(297):     p = np.exp(p, p)
1.30 logistic.py(298):     return loss, p, w
1.30 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
1.30 logistic.py(346):     diff = sample_weight * (p - Y)
1.30 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
1.30 logistic.py(348):     grad[:, :n_features] += alpha * w
1.30 logistic.py(349):     if fit_intercept:
1.30 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
1.30 logistic.py(351):     return loss, grad.ravel(), p
1.30 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
1.30 logistic.py(339):     n_classes = Y.shape[1]
1.30 logistic.py(340):     n_features = X.shape[1]
1.30 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
1.30 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
1.30 logistic.py(343):                     dtype=X.dtype)
1.30 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
1.30 logistic.py(282):     n_classes = Y.shape[1]
1.30 logistic.py(283):     n_features = X.shape[1]
1.30 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
1.30 logistic.py(285):     w = w.reshape(n_classes, -1)
1.30 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
1.30 logistic.py(287):     if fit_intercept:
1.30 logistic.py(288):         intercept = w[:, -1]
1.30 logistic.py(289):         w = w[:, :-1]
1.30 logistic.py(292):     p = safe_sparse_dot(X, w.T)
1.30 logistic.py(293):     p += intercept
1.30 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
1.30 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
1.30 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
1.30 logistic.py(297):     p = np.exp(p, p)
1.30 logistic.py(298):     return loss, p, w
1.30 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
1.30 logistic.py(346):     diff = sample_weight * (p - Y)
1.30 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
1.30 logistic.py(348):     grad[:, :n_features] += alpha * w
1.30 logistic.py(349):     if fit_intercept:
1.30 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
1.30 logistic.py(351):     return loss, grad.ravel(), p
1.30 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
1.30 logistic.py(339):     n_classes = Y.shape[1]
1.30 logistic.py(340):     n_features = X.shape[1]
1.30 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
1.30 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
1.30 logistic.py(343):                     dtype=X.dtype)
1.30 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
1.30 logistic.py(282):     n_classes = Y.shape[1]
1.30 logistic.py(283):     n_features = X.shape[1]
1.30 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
1.30 logistic.py(285):     w = w.reshape(n_classes, -1)
1.30 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
1.30 logistic.py(287):     if fit_intercept:
1.30 logistic.py(288):         intercept = w[:, -1]
1.30 logistic.py(289):         w = w[:, :-1]
1.30 logistic.py(292):     p = safe_sparse_dot(X, w.T)
1.30 logistic.py(293):     p += intercept
1.30 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
1.30 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
1.30 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
1.30 logistic.py(297):     p = np.exp(p, p)
1.30 logistic.py(298):     return loss, p, w
1.30 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
1.30 logistic.py(346):     diff = sample_weight * (p - Y)
1.30 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
1.30 logistic.py(348):     grad[:, :n_features] += alpha * w
1.30 logistic.py(349):     if fit_intercept:
1.30 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
1.30 logistic.py(351):     return loss, grad.ravel(), p
1.30 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
1.30 logistic.py(339):     n_classes = Y.shape[1]
1.30 logistic.py(340):     n_features = X.shape[1]
1.30 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
1.30 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
1.30 logistic.py(343):                     dtype=X.dtype)
1.30 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
1.30 logistic.py(282):     n_classes = Y.shape[1]
1.30 logistic.py(283):     n_features = X.shape[1]
1.30 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
1.30 logistic.py(285):     w = w.reshape(n_classes, -1)
1.30 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
1.30 logistic.py(287):     if fit_intercept:
1.30 logistic.py(288):         intercept = w[:, -1]
1.30 logistic.py(289):         w = w[:, :-1]
1.30 logistic.py(292):     p = safe_sparse_dot(X, w.T)
1.30 logistic.py(293):     p += intercept
1.30 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
1.30 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
1.30 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
1.30 logistic.py(297):     p = np.exp(p, p)
1.30 logistic.py(298):     return loss, p, w
1.30 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
1.30 logistic.py(346):     diff = sample_weight * (p - Y)
1.30 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
1.30 logistic.py(348):     grad[:, :n_features] += alpha * w
1.30 logistic.py(349):     if fit_intercept:
1.30 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
1.30 logistic.py(351):     return loss, grad.ravel(), p
1.30 logistic.py(718):             if info["warnflag"] == 1:
1.30 logistic.py(723):             n_iter_i = min(info['nit'], max_iter)
1.30 logistic.py(760):         if multi_class == 'multinomial':
1.30 logistic.py(761):             multi_w0 = np.reshape(w0, (classes.size, -1))
1.30 logistic.py(762):             if classes.size == 2:
1.30 logistic.py(764):             coefs.append(multi_w0)
1.30 logistic.py(768):         n_iter[i] = n_iter_i
1.30 logistic.py(712):     for i, C in enumerate(Cs):
1.30 logistic.py(713):         if solver == 'lbfgs':
1.30 logistic.py(714):             w0, loss, info = optimize.fmin_l_bfgs_b(
1.30 logistic.py(715):                 func, w0, fprime=None,
1.30 logistic.py(716):                 args=(X, target, 1. / C, sample_weight),
1.30 logistic.py(717):                 iprint=(verbose > 0) - 1, pgtol=tol, maxiter=max_iter)
1.30 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
1.30 logistic.py(339):     n_classes = Y.shape[1]
1.30 logistic.py(340):     n_features = X.shape[1]
1.30 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
1.30 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
1.30 logistic.py(343):                     dtype=X.dtype)
1.30 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
1.30 logistic.py(282):     n_classes = Y.shape[1]
1.30 logistic.py(283):     n_features = X.shape[1]
1.30 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
1.30 logistic.py(285):     w = w.reshape(n_classes, -1)
1.30 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
1.30 logistic.py(287):     if fit_intercept:
1.30 logistic.py(288):         intercept = w[:, -1]
1.30 logistic.py(289):         w = w[:, :-1]
1.30 logistic.py(292):     p = safe_sparse_dot(X, w.T)
1.30 logistic.py(293):     p += intercept
1.30 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
1.30 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
1.30 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
1.30 logistic.py(297):     p = np.exp(p, p)
1.30 logistic.py(298):     return loss, p, w
1.30 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
1.30 logistic.py(346):     diff = sample_weight * (p - Y)
1.30 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
1.30 logistic.py(348):     grad[:, :n_features] += alpha * w
1.30 logistic.py(349):     if fit_intercept:
1.30 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
1.30 logistic.py(351):     return loss, grad.ravel(), p
1.30 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
1.30 logistic.py(339):     n_classes = Y.shape[1]
1.30 logistic.py(340):     n_features = X.shape[1]
1.30 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
1.30 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
1.30 logistic.py(343):                     dtype=X.dtype)
1.30 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
1.30 logistic.py(282):     n_classes = Y.shape[1]
1.30 logistic.py(283):     n_features = X.shape[1]
1.30 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
1.30 logistic.py(285):     w = w.reshape(n_classes, -1)
1.30 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
1.30 logistic.py(287):     if fit_intercept:
1.30 logistic.py(288):         intercept = w[:, -1]
1.30 logistic.py(289):         w = w[:, :-1]
1.30 logistic.py(292):     p = safe_sparse_dot(X, w.T)
1.30 logistic.py(293):     p += intercept
1.30 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
1.30 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
1.30 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
1.30 logistic.py(297):     p = np.exp(p, p)
1.30 logistic.py(298):     return loss, p, w
1.30 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
1.30 logistic.py(346):     diff = sample_weight * (p - Y)
1.30 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
1.30 logistic.py(348):     grad[:, :n_features] += alpha * w
1.30 logistic.py(349):     if fit_intercept:
1.30 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
1.30 logistic.py(351):     return loss, grad.ravel(), p
1.30 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
1.30 logistic.py(339):     n_classes = Y.shape[1]
1.30 logistic.py(340):     n_features = X.shape[1]
1.30 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
1.30 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
1.30 logistic.py(343):                     dtype=X.dtype)
1.30 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
1.30 logistic.py(282):     n_classes = Y.shape[1]
1.30 logistic.py(283):     n_features = X.shape[1]
1.30 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
1.30 logistic.py(285):     w = w.reshape(n_classes, -1)
1.30 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
1.30 logistic.py(287):     if fit_intercept:
1.30 logistic.py(288):         intercept = w[:, -1]
1.30 logistic.py(289):         w = w[:, :-1]
1.30 logistic.py(292):     p = safe_sparse_dot(X, w.T)
1.30 logistic.py(293):     p += intercept
1.30 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
1.30 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
1.30 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
1.30 logistic.py(297):     p = np.exp(p, p)
1.30 logistic.py(298):     return loss, p, w
1.30 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
1.30 logistic.py(346):     diff = sample_weight * (p - Y)
1.30 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
1.30 logistic.py(348):     grad[:, :n_features] += alpha * w
1.30 logistic.py(349):     if fit_intercept:
1.30 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
1.30 logistic.py(351):     return loss, grad.ravel(), p
1.30 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
1.30 logistic.py(339):     n_classes = Y.shape[1]
1.30 logistic.py(340):     n_features = X.shape[1]
1.30 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
1.30 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
1.30 logistic.py(343):                     dtype=X.dtype)
1.30 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
1.30 logistic.py(282):     n_classes = Y.shape[1]
1.30 logistic.py(283):     n_features = X.shape[1]
1.30 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
1.30 logistic.py(285):     w = w.reshape(n_classes, -1)
1.30 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
1.30 logistic.py(287):     if fit_intercept:
1.30 logistic.py(288):         intercept = w[:, -1]
1.30 logistic.py(289):         w = w[:, :-1]
1.30 logistic.py(292):     p = safe_sparse_dot(X, w.T)
1.30 logistic.py(293):     p += intercept
1.30 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
1.30 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
1.30 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
1.30 logistic.py(297):     p = np.exp(p, p)
1.30 logistic.py(298):     return loss, p, w
1.30 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
1.30 logistic.py(346):     diff = sample_weight * (p - Y)
1.30 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
1.30 logistic.py(348):     grad[:, :n_features] += alpha * w
1.30 logistic.py(349):     if fit_intercept:
1.30 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
1.30 logistic.py(351):     return loss, grad.ravel(), p
1.30 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
1.30 logistic.py(339):     n_classes = Y.shape[1]
1.30 logistic.py(340):     n_features = X.shape[1]
1.30 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
1.30 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
1.30 logistic.py(343):                     dtype=X.dtype)
1.30 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
1.30 logistic.py(282):     n_classes = Y.shape[1]
1.30 logistic.py(283):     n_features = X.shape[1]
1.30 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
1.30 logistic.py(285):     w = w.reshape(n_classes, -1)
1.30 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
1.30 logistic.py(287):     if fit_intercept:
1.30 logistic.py(288):         intercept = w[:, -1]
1.30 logistic.py(289):         w = w[:, :-1]
1.30 logistic.py(292):     p = safe_sparse_dot(X, w.T)
1.30 logistic.py(293):     p += intercept
1.30 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
1.30 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
1.30 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
1.30 logistic.py(297):     p = np.exp(p, p)
1.30 logistic.py(298):     return loss, p, w
1.30 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
1.30 logistic.py(346):     diff = sample_weight * (p - Y)
1.30 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
1.30 logistic.py(348):     grad[:, :n_features] += alpha * w
1.30 logistic.py(349):     if fit_intercept:
1.30 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
1.30 logistic.py(351):     return loss, grad.ravel(), p
1.30 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
1.31 logistic.py(339):     n_classes = Y.shape[1]
1.31 logistic.py(340):     n_features = X.shape[1]
1.31 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
1.31 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
1.31 logistic.py(343):                     dtype=X.dtype)
1.31 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
1.31 logistic.py(282):     n_classes = Y.shape[1]
1.31 logistic.py(283):     n_features = X.shape[1]
1.31 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
1.31 logistic.py(285):     w = w.reshape(n_classes, -1)
1.31 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
1.31 logistic.py(287):     if fit_intercept:
1.31 logistic.py(288):         intercept = w[:, -1]
1.31 logistic.py(289):         w = w[:, :-1]
1.31 logistic.py(292):     p = safe_sparse_dot(X, w.T)
1.31 logistic.py(293):     p += intercept
1.31 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
1.31 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
1.31 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
1.31 logistic.py(297):     p = np.exp(p, p)
1.31 logistic.py(298):     return loss, p, w
1.31 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
1.31 logistic.py(346):     diff = sample_weight * (p - Y)
1.31 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
1.31 logistic.py(348):     grad[:, :n_features] += alpha * w
1.31 logistic.py(349):     if fit_intercept:
1.31 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
1.31 logistic.py(351):     return loss, grad.ravel(), p
1.31 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
1.31 logistic.py(339):     n_classes = Y.shape[1]
1.31 logistic.py(340):     n_features = X.shape[1]
1.31 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
1.31 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
1.31 logistic.py(343):                     dtype=X.dtype)
1.31 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
1.31 logistic.py(282):     n_classes = Y.shape[1]
1.31 logistic.py(283):     n_features = X.shape[1]
1.31 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
1.31 logistic.py(285):     w = w.reshape(n_classes, -1)
1.31 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
1.31 logistic.py(287):     if fit_intercept:
1.31 logistic.py(288):         intercept = w[:, -1]
1.31 logistic.py(289):         w = w[:, :-1]
1.31 logistic.py(292):     p = safe_sparse_dot(X, w.T)
1.31 logistic.py(293):     p += intercept
1.31 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
1.31 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
1.31 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
1.31 logistic.py(297):     p = np.exp(p, p)
1.31 logistic.py(298):     return loss, p, w
1.31 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
1.31 logistic.py(346):     diff = sample_weight * (p - Y)
1.31 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
1.31 logistic.py(348):     grad[:, :n_features] += alpha * w
1.31 logistic.py(349):     if fit_intercept:
1.31 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
1.31 logistic.py(351):     return loss, grad.ravel(), p
1.31 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
1.31 logistic.py(339):     n_classes = Y.shape[1]
1.31 logistic.py(340):     n_features = X.shape[1]
1.31 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
1.31 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
1.31 logistic.py(343):                     dtype=X.dtype)
1.31 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
1.31 logistic.py(282):     n_classes = Y.shape[1]
1.31 logistic.py(283):     n_features = X.shape[1]
1.31 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
1.31 logistic.py(285):     w = w.reshape(n_classes, -1)
1.31 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
1.31 logistic.py(287):     if fit_intercept:
1.31 logistic.py(288):         intercept = w[:, -1]
1.31 logistic.py(289):         w = w[:, :-1]
1.31 logistic.py(292):     p = safe_sparse_dot(X, w.T)
1.31 logistic.py(293):     p += intercept
1.31 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
1.31 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
1.31 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
1.31 logistic.py(297):     p = np.exp(p, p)
1.31 logistic.py(298):     return loss, p, w
1.31 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
1.31 logistic.py(346):     diff = sample_weight * (p - Y)
1.31 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
1.31 logistic.py(348):     grad[:, :n_features] += alpha * w
1.31 logistic.py(349):     if fit_intercept:
1.31 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
1.31 logistic.py(351):     return loss, grad.ravel(), p
1.31 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
1.31 logistic.py(339):     n_classes = Y.shape[1]
1.31 logistic.py(340):     n_features = X.shape[1]
1.31 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
1.31 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
1.31 logistic.py(343):                     dtype=X.dtype)
1.31 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
1.31 logistic.py(282):     n_classes = Y.shape[1]
1.31 logistic.py(283):     n_features = X.shape[1]
1.31 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
1.31 logistic.py(285):     w = w.reshape(n_classes, -1)
1.31 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
1.31 logistic.py(287):     if fit_intercept:
1.31 logistic.py(288):         intercept = w[:, -1]
1.31 logistic.py(289):         w = w[:, :-1]
1.31 logistic.py(292):     p = safe_sparse_dot(X, w.T)
1.31 logistic.py(293):     p += intercept
1.31 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
1.31 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
1.31 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
1.31 logistic.py(297):     p = np.exp(p, p)
1.31 logistic.py(298):     return loss, p, w
1.31 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
1.31 logistic.py(346):     diff = sample_weight * (p - Y)
1.31 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
1.31 logistic.py(348):     grad[:, :n_features] += alpha * w
1.31 logistic.py(349):     if fit_intercept:
1.31 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
1.31 logistic.py(351):     return loss, grad.ravel(), p
1.31 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
1.31 logistic.py(339):     n_classes = Y.shape[1]
1.31 logistic.py(340):     n_features = X.shape[1]
1.31 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
1.31 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
1.31 logistic.py(343):                     dtype=X.dtype)
1.31 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
1.31 logistic.py(282):     n_classes = Y.shape[1]
1.31 logistic.py(283):     n_features = X.shape[1]
1.31 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
1.31 logistic.py(285):     w = w.reshape(n_classes, -1)
1.31 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
1.31 logistic.py(287):     if fit_intercept:
1.31 logistic.py(288):         intercept = w[:, -1]
1.31 logistic.py(289):         w = w[:, :-1]
1.31 logistic.py(292):     p = safe_sparse_dot(X, w.T)
1.31 logistic.py(293):     p += intercept
1.31 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
1.31 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
1.31 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
1.31 logistic.py(297):     p = np.exp(p, p)
1.31 logistic.py(298):     return loss, p, w
1.31 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
1.31 logistic.py(346):     diff = sample_weight * (p - Y)
1.31 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
1.31 logistic.py(348):     grad[:, :n_features] += alpha * w
1.31 logistic.py(349):     if fit_intercept:
1.31 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
1.31 logistic.py(351):     return loss, grad.ravel(), p
1.31 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
1.31 logistic.py(339):     n_classes = Y.shape[1]
1.31 logistic.py(340):     n_features = X.shape[1]
1.31 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
1.31 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
1.31 logistic.py(343):                     dtype=X.dtype)
1.31 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
1.31 logistic.py(282):     n_classes = Y.shape[1]
1.31 logistic.py(283):     n_features = X.shape[1]
1.31 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
1.31 logistic.py(285):     w = w.reshape(n_classes, -1)
1.31 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
1.31 logistic.py(287):     if fit_intercept:
1.31 logistic.py(288):         intercept = w[:, -1]
1.31 logistic.py(289):         w = w[:, :-1]
1.31 logistic.py(292):     p = safe_sparse_dot(X, w.T)
1.31 logistic.py(293):     p += intercept
1.31 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
1.31 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
1.31 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
1.31 logistic.py(297):     p = np.exp(p, p)
1.31 logistic.py(298):     return loss, p, w
1.31 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
1.31 logistic.py(346):     diff = sample_weight * (p - Y)
1.31 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
1.31 logistic.py(348):     grad[:, :n_features] += alpha * w
1.31 logistic.py(349):     if fit_intercept:
1.31 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
1.31 logistic.py(351):     return loss, grad.ravel(), p
1.31 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
1.31 logistic.py(339):     n_classes = Y.shape[1]
1.31 logistic.py(340):     n_features = X.shape[1]
1.31 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
1.31 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
1.31 logistic.py(343):                     dtype=X.dtype)
1.31 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
1.31 logistic.py(282):     n_classes = Y.shape[1]
1.31 logistic.py(283):     n_features = X.shape[1]
1.31 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
1.31 logistic.py(285):     w = w.reshape(n_classes, -1)
1.31 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
1.31 logistic.py(287):     if fit_intercept:
1.31 logistic.py(288):         intercept = w[:, -1]
1.31 logistic.py(289):         w = w[:, :-1]
1.31 logistic.py(292):     p = safe_sparse_dot(X, w.T)
1.31 logistic.py(293):     p += intercept
1.31 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
1.31 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
1.31 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
1.31 logistic.py(297):     p = np.exp(p, p)
1.31 logistic.py(298):     return loss, p, w
1.31 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
1.31 logistic.py(346):     diff = sample_weight * (p - Y)
1.31 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
1.31 logistic.py(348):     grad[:, :n_features] += alpha * w
1.31 logistic.py(349):     if fit_intercept:
1.31 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
1.31 logistic.py(351):     return loss, grad.ravel(), p
1.31 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
1.31 logistic.py(339):     n_classes = Y.shape[1]
1.31 logistic.py(340):     n_features = X.shape[1]
1.31 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
1.31 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
1.31 logistic.py(343):                     dtype=X.dtype)
1.31 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
1.31 logistic.py(282):     n_classes = Y.shape[1]
1.31 logistic.py(283):     n_features = X.shape[1]
1.31 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
1.31 logistic.py(285):     w = w.reshape(n_classes, -1)
1.31 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
1.31 logistic.py(287):     if fit_intercept:
1.31 logistic.py(288):         intercept = w[:, -1]
1.31 logistic.py(289):         w = w[:, :-1]
1.31 logistic.py(292):     p = safe_sparse_dot(X, w.T)
1.31 logistic.py(293):     p += intercept
1.31 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
1.31 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
1.31 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
1.31 logistic.py(297):     p = np.exp(p, p)
1.31 logistic.py(298):     return loss, p, w
1.31 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
1.31 logistic.py(346):     diff = sample_weight * (p - Y)
1.31 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
1.31 logistic.py(348):     grad[:, :n_features] += alpha * w
1.31 logistic.py(349):     if fit_intercept:
1.31 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
1.31 logistic.py(351):     return loss, grad.ravel(), p
1.31 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
1.31 logistic.py(339):     n_classes = Y.shape[1]
1.31 logistic.py(340):     n_features = X.shape[1]
1.31 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
1.31 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
1.31 logistic.py(343):                     dtype=X.dtype)
1.31 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
1.31 logistic.py(282):     n_classes = Y.shape[1]
1.31 logistic.py(283):     n_features = X.shape[1]
1.31 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
1.31 logistic.py(285):     w = w.reshape(n_classes, -1)
1.31 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
1.31 logistic.py(287):     if fit_intercept:
1.31 logistic.py(288):         intercept = w[:, -1]
1.31 logistic.py(289):         w = w[:, :-1]
1.31 logistic.py(292):     p = safe_sparse_dot(X, w.T)
1.31 logistic.py(293):     p += intercept
1.31 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
1.31 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
1.31 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
1.31 logistic.py(297):     p = np.exp(p, p)
1.31 logistic.py(298):     return loss, p, w
1.31 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
1.31 logistic.py(346):     diff = sample_weight * (p - Y)
1.31 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
1.31 logistic.py(348):     grad[:, :n_features] += alpha * w
1.31 logistic.py(349):     if fit_intercept:
1.31 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
1.31 logistic.py(351):     return loss, grad.ravel(), p
1.31 logistic.py(718):             if info["warnflag"] == 1:
1.31 logistic.py(723):             n_iter_i = min(info['nit'], max_iter)
1.31 logistic.py(760):         if multi_class == 'multinomial':
1.31 logistic.py(761):             multi_w0 = np.reshape(w0, (classes.size, -1))
1.31 logistic.py(762):             if classes.size == 2:
1.31 logistic.py(764):             coefs.append(multi_w0)
1.31 logistic.py(768):         n_iter[i] = n_iter_i
1.31 logistic.py(712):     for i, C in enumerate(Cs):
1.31 logistic.py(713):         if solver == 'lbfgs':
1.31 logistic.py(714):             w0, loss, info = optimize.fmin_l_bfgs_b(
1.31 logistic.py(715):                 func, w0, fprime=None,
1.31 logistic.py(716):                 args=(X, target, 1. / C, sample_weight),
1.31 logistic.py(717):                 iprint=(verbose > 0) - 1, pgtol=tol, maxiter=max_iter)
1.31 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
1.31 logistic.py(339):     n_classes = Y.shape[1]
1.31 logistic.py(340):     n_features = X.shape[1]
1.31 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
1.31 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
1.31 logistic.py(343):                     dtype=X.dtype)
1.31 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
1.31 logistic.py(282):     n_classes = Y.shape[1]
1.31 logistic.py(283):     n_features = X.shape[1]
1.31 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
1.31 logistic.py(285):     w = w.reshape(n_classes, -1)
1.31 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
1.31 logistic.py(287):     if fit_intercept:
1.31 logistic.py(288):         intercept = w[:, -1]
1.31 logistic.py(289):         w = w[:, :-1]
1.31 logistic.py(292):     p = safe_sparse_dot(X, w.T)
1.31 logistic.py(293):     p += intercept
1.31 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
1.31 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
1.31 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
1.31 logistic.py(297):     p = np.exp(p, p)
1.31 logistic.py(298):     return loss, p, w
1.31 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
1.31 logistic.py(346):     diff = sample_weight * (p - Y)
1.31 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
1.31 logistic.py(348):     grad[:, :n_features] += alpha * w
1.31 logistic.py(349):     if fit_intercept:
1.31 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
1.31 logistic.py(351):     return loss, grad.ravel(), p
1.31 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
1.31 logistic.py(339):     n_classes = Y.shape[1]
1.31 logistic.py(340):     n_features = X.shape[1]
1.31 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
1.31 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
1.31 logistic.py(343):                     dtype=X.dtype)
1.31 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
1.31 logistic.py(282):     n_classes = Y.shape[1]
1.31 logistic.py(283):     n_features = X.shape[1]
1.31 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
1.31 logistic.py(285):     w = w.reshape(n_classes, -1)
1.31 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
1.31 logistic.py(287):     if fit_intercept:
1.31 logistic.py(288):         intercept = w[:, -1]
1.31 logistic.py(289):         w = w[:, :-1]
1.31 logistic.py(292):     p = safe_sparse_dot(X, w.T)
1.31 logistic.py(293):     p += intercept
1.31 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
1.31 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
1.31 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
1.31 logistic.py(297):     p = np.exp(p, p)
1.31 logistic.py(298):     return loss, p, w
1.31 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
1.31 logistic.py(346):     diff = sample_weight * (p - Y)
1.31 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
1.31 logistic.py(348):     grad[:, :n_features] += alpha * w
1.31 logistic.py(349):     if fit_intercept:
1.31 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
1.31 logistic.py(351):     return loss, grad.ravel(), p
1.31 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
1.31 logistic.py(339):     n_classes = Y.shape[1]
1.31 logistic.py(340):     n_features = X.shape[1]
1.31 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
1.31 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
1.31 logistic.py(343):                     dtype=X.dtype)
1.31 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
1.31 logistic.py(282):     n_classes = Y.shape[1]
1.31 logistic.py(283):     n_features = X.shape[1]
1.31 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
1.31 logistic.py(285):     w = w.reshape(n_classes, -1)
1.31 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
1.31 logistic.py(287):     if fit_intercept:
1.31 logistic.py(288):         intercept = w[:, -1]
1.31 logistic.py(289):         w = w[:, :-1]
1.31 logistic.py(292):     p = safe_sparse_dot(X, w.T)
1.31 logistic.py(293):     p += intercept
1.31 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
1.31 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
1.31 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
1.31 logistic.py(297):     p = np.exp(p, p)
1.31 logistic.py(298):     return loss, p, w
1.31 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
1.31 logistic.py(346):     diff = sample_weight * (p - Y)
1.31 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
1.31 logistic.py(348):     grad[:, :n_features] += alpha * w
1.31 logistic.py(349):     if fit_intercept:
1.31 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
1.31 logistic.py(351):     return loss, grad.ravel(), p
1.32 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
1.32 logistic.py(339):     n_classes = Y.shape[1]
1.32 logistic.py(340):     n_features = X.shape[1]
1.32 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
1.32 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
1.32 logistic.py(343):                     dtype=X.dtype)
1.32 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
1.32 logistic.py(282):     n_classes = Y.shape[1]
1.32 logistic.py(283):     n_features = X.shape[1]
1.32 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
1.32 logistic.py(285):     w = w.reshape(n_classes, -1)
1.32 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
1.32 logistic.py(287):     if fit_intercept:
1.32 logistic.py(288):         intercept = w[:, -1]
1.32 logistic.py(289):         w = w[:, :-1]
1.32 logistic.py(292):     p = safe_sparse_dot(X, w.T)
1.32 logistic.py(293):     p += intercept
1.32 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
1.32 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
1.32 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
1.32 logistic.py(297):     p = np.exp(p, p)
1.32 logistic.py(298):     return loss, p, w
1.32 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
1.32 logistic.py(346):     diff = sample_weight * (p - Y)
1.32 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
1.32 logistic.py(348):     grad[:, :n_features] += alpha * w
1.32 logistic.py(349):     if fit_intercept:
1.32 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
1.32 logistic.py(351):     return loss, grad.ravel(), p
1.32 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
1.32 logistic.py(339):     n_classes = Y.shape[1]
1.32 logistic.py(340):     n_features = X.shape[1]
1.32 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
1.32 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
1.32 logistic.py(343):                     dtype=X.dtype)
1.32 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
1.32 logistic.py(282):     n_classes = Y.shape[1]
1.32 logistic.py(283):     n_features = X.shape[1]
1.32 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
1.32 logistic.py(285):     w = w.reshape(n_classes, -1)
1.32 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
1.32 logistic.py(287):     if fit_intercept:
1.32 logistic.py(288):         intercept = w[:, -1]
1.32 logistic.py(289):         w = w[:, :-1]
1.32 logistic.py(292):     p = safe_sparse_dot(X, w.T)
1.32 logistic.py(293):     p += intercept
1.32 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
1.32 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
1.32 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
1.32 logistic.py(297):     p = np.exp(p, p)
1.32 logistic.py(298):     return loss, p, w
1.32 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
1.32 logistic.py(346):     diff = sample_weight * (p - Y)
1.32 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
1.32 logistic.py(348):     grad[:, :n_features] += alpha * w
1.32 logistic.py(349):     if fit_intercept:
1.32 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
1.32 logistic.py(351):     return loss, grad.ravel(), p
1.32 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
1.32 logistic.py(339):     n_classes = Y.shape[1]
1.32 logistic.py(340):     n_features = X.shape[1]
1.32 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
1.32 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
1.32 logistic.py(343):                     dtype=X.dtype)
1.32 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
1.32 logistic.py(282):     n_classes = Y.shape[1]
1.32 logistic.py(283):     n_features = X.shape[1]
1.32 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
1.32 logistic.py(285):     w = w.reshape(n_classes, -1)
1.32 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
1.32 logistic.py(287):     if fit_intercept:
1.32 logistic.py(288):         intercept = w[:, -1]
1.32 logistic.py(289):         w = w[:, :-1]
1.32 logistic.py(292):     p = safe_sparse_dot(X, w.T)
1.32 logistic.py(293):     p += intercept
1.32 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
1.32 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
1.32 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
1.32 logistic.py(297):     p = np.exp(p, p)
1.32 logistic.py(298):     return loss, p, w
1.32 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
1.32 logistic.py(346):     diff = sample_weight * (p - Y)
1.32 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
1.32 logistic.py(348):     grad[:, :n_features] += alpha * w
1.32 logistic.py(349):     if fit_intercept:
1.32 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
1.32 logistic.py(351):     return loss, grad.ravel(), p
1.32 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
1.32 logistic.py(339):     n_classes = Y.shape[1]
1.32 logistic.py(340):     n_features = X.shape[1]
1.32 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
1.32 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
1.32 logistic.py(343):                     dtype=X.dtype)
1.32 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
1.32 logistic.py(282):     n_classes = Y.shape[1]
1.32 logistic.py(283):     n_features = X.shape[1]
1.32 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
1.32 logistic.py(285):     w = w.reshape(n_classes, -1)
1.32 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
1.32 logistic.py(287):     if fit_intercept:
1.32 logistic.py(288):         intercept = w[:, -1]
1.32 logistic.py(289):         w = w[:, :-1]
1.32 logistic.py(292):     p = safe_sparse_dot(X, w.T)
1.32 logistic.py(293):     p += intercept
1.32 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
1.32 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
1.32 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
1.32 logistic.py(297):     p = np.exp(p, p)
1.32 logistic.py(298):     return loss, p, w
1.32 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
1.32 logistic.py(346):     diff = sample_weight * (p - Y)
1.32 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
1.32 logistic.py(348):     grad[:, :n_features] += alpha * w
1.32 logistic.py(349):     if fit_intercept:
1.32 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
1.32 logistic.py(351):     return loss, grad.ravel(), p
1.32 logistic.py(718):             if info["warnflag"] == 1:
1.32 logistic.py(723):             n_iter_i = min(info['nit'], max_iter)
1.32 logistic.py(760):         if multi_class == 'multinomial':
1.32 logistic.py(761):             multi_w0 = np.reshape(w0, (classes.size, -1))
1.32 logistic.py(762):             if classes.size == 2:
1.32 logistic.py(764):             coefs.append(multi_w0)
1.32 logistic.py(768):         n_iter[i] = n_iter_i
1.32 logistic.py(712):     for i, C in enumerate(Cs):
1.32 logistic.py(713):         if solver == 'lbfgs':
1.32 logistic.py(714):             w0, loss, info = optimize.fmin_l_bfgs_b(
1.32 logistic.py(715):                 func, w0, fprime=None,
1.32 logistic.py(716):                 args=(X, target, 1. / C, sample_weight),
1.32 logistic.py(717):                 iprint=(verbose > 0) - 1, pgtol=tol, maxiter=max_iter)
1.32 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
1.32 logistic.py(339):     n_classes = Y.shape[1]
1.32 logistic.py(340):     n_features = X.shape[1]
1.32 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
1.32 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
1.32 logistic.py(343):                     dtype=X.dtype)
1.32 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
1.32 logistic.py(282):     n_classes = Y.shape[1]
1.32 logistic.py(283):     n_features = X.shape[1]
1.32 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
1.32 logistic.py(285):     w = w.reshape(n_classes, -1)
1.32 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
1.32 logistic.py(287):     if fit_intercept:
1.32 logistic.py(288):         intercept = w[:, -1]
1.32 logistic.py(289):         w = w[:, :-1]
1.32 logistic.py(292):     p = safe_sparse_dot(X, w.T)
1.32 logistic.py(293):     p += intercept
1.32 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
1.32 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
1.32 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
1.32 logistic.py(297):     p = np.exp(p, p)
1.32 logistic.py(298):     return loss, p, w
1.32 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
1.32 logistic.py(346):     diff = sample_weight * (p - Y)
1.32 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
1.32 logistic.py(348):     grad[:, :n_features] += alpha * w
1.32 logistic.py(349):     if fit_intercept:
1.32 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
1.32 logistic.py(351):     return loss, grad.ravel(), p
1.32 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
1.32 logistic.py(339):     n_classes = Y.shape[1]
1.32 logistic.py(340):     n_features = X.shape[1]
1.32 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
1.32 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
1.32 logistic.py(343):                     dtype=X.dtype)
1.32 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
1.32 logistic.py(282):     n_classes = Y.shape[1]
1.32 logistic.py(283):     n_features = X.shape[1]
1.32 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
1.32 logistic.py(285):     w = w.reshape(n_classes, -1)
1.32 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
1.32 logistic.py(287):     if fit_intercept:
1.32 logistic.py(288):         intercept = w[:, -1]
1.32 logistic.py(289):         w = w[:, :-1]
1.32 logistic.py(292):     p = safe_sparse_dot(X, w.T)
1.32 logistic.py(293):     p += intercept
1.32 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
1.32 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
1.32 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
1.32 logistic.py(297):     p = np.exp(p, p)
1.32 logistic.py(298):     return loss, p, w
1.32 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
1.32 logistic.py(346):     diff = sample_weight * (p - Y)
1.32 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
1.32 logistic.py(348):     grad[:, :n_features] += alpha * w
1.32 logistic.py(349):     if fit_intercept:
1.32 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
1.32 logistic.py(351):     return loss, grad.ravel(), p
1.32 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
1.32 logistic.py(339):     n_classes = Y.shape[1]
1.32 logistic.py(340):     n_features = X.shape[1]
1.32 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
1.32 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
1.32 logistic.py(343):                     dtype=X.dtype)
1.32 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
1.32 logistic.py(282):     n_classes = Y.shape[1]
1.32 logistic.py(283):     n_features = X.shape[1]
1.32 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
1.32 logistic.py(285):     w = w.reshape(n_classes, -1)
1.32 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
1.32 logistic.py(287):     if fit_intercept:
1.32 logistic.py(288):         intercept = w[:, -1]
1.32 logistic.py(289):         w = w[:, :-1]
1.32 logistic.py(292):     p = safe_sparse_dot(X, w.T)
1.32 logistic.py(293):     p += intercept
1.32 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
1.32 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
1.32 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
1.32 logistic.py(297):     p = np.exp(p, p)
1.32 logistic.py(298):     return loss, p, w
1.32 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
1.32 logistic.py(346):     diff = sample_weight * (p - Y)
1.32 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
1.32 logistic.py(348):     grad[:, :n_features] += alpha * w
1.32 logistic.py(349):     if fit_intercept:
1.32 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
1.32 logistic.py(351):     return loss, grad.ravel(), p
1.32 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
1.32 logistic.py(339):     n_classes = Y.shape[1]
1.32 logistic.py(340):     n_features = X.shape[1]
1.32 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
1.32 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
1.32 logistic.py(343):                     dtype=X.dtype)
1.32 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
1.32 logistic.py(282):     n_classes = Y.shape[1]
1.32 logistic.py(283):     n_features = X.shape[1]
1.32 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
1.32 logistic.py(285):     w = w.reshape(n_classes, -1)
1.32 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
1.32 logistic.py(287):     if fit_intercept:
1.32 logistic.py(288):         intercept = w[:, -1]
1.32 logistic.py(289):         w = w[:, :-1]
1.32 logistic.py(292):     p = safe_sparse_dot(X, w.T)
1.32 logistic.py(293):     p += intercept
1.32 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
1.32 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
1.32 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
1.32 logistic.py(297):     p = np.exp(p, p)
1.32 logistic.py(298):     return loss, p, w
1.32 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
1.32 logistic.py(346):     diff = sample_weight * (p - Y)
1.32 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
1.32 logistic.py(348):     grad[:, :n_features] += alpha * w
1.32 logistic.py(349):     if fit_intercept:
1.32 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
1.32 logistic.py(351):     return loss, grad.ravel(), p
1.32 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
1.32 logistic.py(339):     n_classes = Y.shape[1]
1.32 logistic.py(340):     n_features = X.shape[1]
1.32 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
1.32 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
1.32 logistic.py(343):                     dtype=X.dtype)
1.32 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
1.32 logistic.py(282):     n_classes = Y.shape[1]
1.32 logistic.py(283):     n_features = X.shape[1]
1.32 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
1.32 logistic.py(285):     w = w.reshape(n_classes, -1)
1.32 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
1.32 logistic.py(287):     if fit_intercept:
1.32 logistic.py(288):         intercept = w[:, -1]
1.32 logistic.py(289):         w = w[:, :-1]
1.32 logistic.py(292):     p = safe_sparse_dot(X, w.T)
1.32 logistic.py(293):     p += intercept
1.32 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
1.32 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
1.32 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
1.32 logistic.py(297):     p = np.exp(p, p)
1.32 logistic.py(298):     return loss, p, w
1.32 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
1.32 logistic.py(346):     diff = sample_weight * (p - Y)
1.32 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
1.32 logistic.py(348):     grad[:, :n_features] += alpha * w
1.32 logistic.py(349):     if fit_intercept:
1.32 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
1.32 logistic.py(351):     return loss, grad.ravel(), p
1.32 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
1.32 logistic.py(339):     n_classes = Y.shape[1]
1.32 logistic.py(340):     n_features = X.shape[1]
1.32 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
1.32 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
1.32 logistic.py(343):                     dtype=X.dtype)
1.32 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
1.32 logistic.py(282):     n_classes = Y.shape[1]
1.32 logistic.py(283):     n_features = X.shape[1]
1.32 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
1.32 logistic.py(285):     w = w.reshape(n_classes, -1)
1.32 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
1.32 logistic.py(287):     if fit_intercept:
1.32 logistic.py(288):         intercept = w[:, -1]
1.32 logistic.py(289):         w = w[:, :-1]
1.32 logistic.py(292):     p = safe_sparse_dot(X, w.T)
1.32 logistic.py(293):     p += intercept
1.32 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
1.32 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
1.32 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
1.32 logistic.py(297):     p = np.exp(p, p)
1.32 logistic.py(298):     return loss, p, w
1.32 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
1.32 logistic.py(346):     diff = sample_weight * (p - Y)
1.32 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
1.32 logistic.py(348):     grad[:, :n_features] += alpha * w
1.32 logistic.py(349):     if fit_intercept:
1.32 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
1.32 logistic.py(351):     return loss, grad.ravel(), p
1.32 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
1.32 logistic.py(339):     n_classes = Y.shape[1]
1.32 logistic.py(340):     n_features = X.shape[1]
1.32 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
1.32 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
1.32 logistic.py(343):                     dtype=X.dtype)
1.32 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
1.32 logistic.py(282):     n_classes = Y.shape[1]
1.32 logistic.py(283):     n_features = X.shape[1]
1.32 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
1.32 logistic.py(285):     w = w.reshape(n_classes, -1)
1.32 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
1.32 logistic.py(287):     if fit_intercept:
1.32 logistic.py(288):         intercept = w[:, -1]
1.32 logistic.py(289):         w = w[:, :-1]
1.32 logistic.py(292):     p = safe_sparse_dot(X, w.T)
1.32 logistic.py(293):     p += intercept
1.32 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
1.32 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
1.32 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
1.32 logistic.py(297):     p = np.exp(p, p)
1.32 logistic.py(298):     return loss, p, w
1.32 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
1.32 logistic.py(346):     diff = sample_weight * (p - Y)
1.32 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
1.32 logistic.py(348):     grad[:, :n_features] += alpha * w
1.32 logistic.py(349):     if fit_intercept:
1.32 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
1.32 logistic.py(351):     return loss, grad.ravel(), p
1.32 logistic.py(718):             if info["warnflag"] == 1:
1.32 logistic.py(723):             n_iter_i = min(info['nit'], max_iter)
1.32 logistic.py(760):         if multi_class == 'multinomial':
1.32 logistic.py(761):             multi_w0 = np.reshape(w0, (classes.size, -1))
1.32 logistic.py(762):             if classes.size == 2:
1.32 logistic.py(764):             coefs.append(multi_w0)
1.32 logistic.py(768):         n_iter[i] = n_iter_i
1.32 logistic.py(712):     for i, C in enumerate(Cs):
1.32 logistic.py(713):         if solver == 'lbfgs':
1.32 logistic.py(714):             w0, loss, info = optimize.fmin_l_bfgs_b(
1.32 logistic.py(715):                 func, w0, fprime=None,
1.32 logistic.py(716):                 args=(X, target, 1. / C, sample_weight),
1.32 logistic.py(717):                 iprint=(verbose > 0) - 1, pgtol=tol, maxiter=max_iter)
1.32 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
1.32 logistic.py(339):     n_classes = Y.shape[1]
1.32 logistic.py(340):     n_features = X.shape[1]
1.32 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
1.33 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
1.33 logistic.py(343):                     dtype=X.dtype)
1.33 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
1.33 logistic.py(282):     n_classes = Y.shape[1]
1.33 logistic.py(283):     n_features = X.shape[1]
1.33 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
1.33 logistic.py(285):     w = w.reshape(n_classes, -1)
1.33 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
1.33 logistic.py(287):     if fit_intercept:
1.33 logistic.py(288):         intercept = w[:, -1]
1.33 logistic.py(289):         w = w[:, :-1]
1.33 logistic.py(292):     p = safe_sparse_dot(X, w.T)
1.33 logistic.py(293):     p += intercept
1.33 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
1.33 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
1.33 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
1.33 logistic.py(297):     p = np.exp(p, p)
1.33 logistic.py(298):     return loss, p, w
1.33 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
1.33 logistic.py(346):     diff = sample_weight * (p - Y)
1.33 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
1.33 logistic.py(348):     grad[:, :n_features] += alpha * w
1.33 logistic.py(349):     if fit_intercept:
1.33 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
1.33 logistic.py(351):     return loss, grad.ravel(), p
1.33 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
1.33 logistic.py(339):     n_classes = Y.shape[1]
1.33 logistic.py(340):     n_features = X.shape[1]
1.33 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
1.33 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
1.33 logistic.py(343):                     dtype=X.dtype)
1.33 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
1.33 logistic.py(282):     n_classes = Y.shape[1]
1.33 logistic.py(283):     n_features = X.shape[1]
1.33 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
1.33 logistic.py(285):     w = w.reshape(n_classes, -1)
1.33 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
1.33 logistic.py(287):     if fit_intercept:
1.33 logistic.py(288):         intercept = w[:, -1]
1.33 logistic.py(289):         w = w[:, :-1]
1.33 logistic.py(292):     p = safe_sparse_dot(X, w.T)
1.33 logistic.py(293):     p += intercept
1.33 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
1.33 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
1.33 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
1.33 logistic.py(297):     p = np.exp(p, p)
1.33 logistic.py(298):     return loss, p, w
1.33 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
1.33 logistic.py(346):     diff = sample_weight * (p - Y)
1.33 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
1.33 logistic.py(348):     grad[:, :n_features] += alpha * w
1.33 logistic.py(349):     if fit_intercept:
1.33 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
1.33 logistic.py(351):     return loss, grad.ravel(), p
1.33 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
1.33 logistic.py(339):     n_classes = Y.shape[1]
1.33 logistic.py(340):     n_features = X.shape[1]
1.33 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
1.33 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
1.33 logistic.py(343):                     dtype=X.dtype)
1.33 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
1.33 logistic.py(282):     n_classes = Y.shape[1]
1.33 logistic.py(283):     n_features = X.shape[1]
1.33 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
1.33 logistic.py(285):     w = w.reshape(n_classes, -1)
1.33 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
1.33 logistic.py(287):     if fit_intercept:
1.33 logistic.py(288):         intercept = w[:, -1]
1.33 logistic.py(289):         w = w[:, :-1]
1.33 logistic.py(292):     p = safe_sparse_dot(X, w.T)
1.33 logistic.py(293):     p += intercept
1.33 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
1.33 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
1.33 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
1.33 logistic.py(297):     p = np.exp(p, p)
1.33 logistic.py(298):     return loss, p, w
1.33 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
1.33 logistic.py(346):     diff = sample_weight * (p - Y)
1.33 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
1.33 logistic.py(348):     grad[:, :n_features] += alpha * w
1.33 logistic.py(349):     if fit_intercept:
1.33 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
1.33 logistic.py(351):     return loss, grad.ravel(), p
1.33 logistic.py(718):             if info["warnflag"] == 1:
1.33 logistic.py(723):             n_iter_i = min(info['nit'], max_iter)
1.33 logistic.py(760):         if multi_class == 'multinomial':
1.33 logistic.py(761):             multi_w0 = np.reshape(w0, (classes.size, -1))
1.33 logistic.py(762):             if classes.size == 2:
1.33 logistic.py(764):             coefs.append(multi_w0)
1.33 logistic.py(768):         n_iter[i] = n_iter_i
1.33 logistic.py(712):     for i, C in enumerate(Cs):
1.33 logistic.py(770):     return coefs, np.array(Cs), n_iter
1.33 logistic.py(925):     log_reg = LogisticRegression(multi_class=multi_class)
1.33 logistic.py(1171):         self.penalty = penalty
1.33 logistic.py(1172):         self.dual = dual
1.33 logistic.py(1173):         self.tol = tol
1.33 logistic.py(1174):         self.C = C
1.33 logistic.py(1175):         self.fit_intercept = fit_intercept
1.33 logistic.py(1176):         self.intercept_scaling = intercept_scaling
1.33 logistic.py(1177):         self.class_weight = class_weight
1.33 logistic.py(1178):         self.random_state = random_state
1.33 logistic.py(1179):         self.solver = solver
1.33 logistic.py(1180):         self.max_iter = max_iter
1.33 logistic.py(1181):         self.multi_class = multi_class
1.33 logistic.py(1182):         self.verbose = verbose
1.33 logistic.py(1183):         self.warm_start = warm_start
1.33 logistic.py(1184):         self.n_jobs = n_jobs
1.33 logistic.py(928):     if multi_class == 'ovr':
1.33 logistic.py(930):     elif multi_class == 'multinomial':
1.33 logistic.py(931):         log_reg.classes_ = np.unique(y_train)
1.33 logistic.py(936):     if pos_class is not None:
1.33 logistic.py(941):     scores = list()
1.33 logistic.py(943):     if isinstance(scoring, six.string_types):
1.33 logistic.py(945):     for w in coefs:
1.33 logistic.py(946):         if multi_class == 'ovr':
1.33 logistic.py(948):         if fit_intercept:
1.33 logistic.py(949):             log_reg.coef_ = w[:, :-1]
1.33 logistic.py(950):             log_reg.intercept_ = w[:, -1]
1.33 logistic.py(955):         if scoring is None:
1.33 logistic.py(956):             scores.append(log_reg.score(X_test, y_test))
1.33 logistic.py(945):     for w in coefs:
1.33 logistic.py(946):         if multi_class == 'ovr':
1.33 logistic.py(948):         if fit_intercept:
1.33 logistic.py(949):             log_reg.coef_ = w[:, :-1]
1.33 logistic.py(950):             log_reg.intercept_ = w[:, -1]
1.33 logistic.py(955):         if scoring is None:
1.33 logistic.py(956):             scores.append(log_reg.score(X_test, y_test))
1.33 logistic.py(945):     for w in coefs:
1.33 logistic.py(946):         if multi_class == 'ovr':
1.33 logistic.py(948):         if fit_intercept:
1.33 logistic.py(949):             log_reg.coef_ = w[:, :-1]
1.33 logistic.py(950):             log_reg.intercept_ = w[:, -1]
1.33 logistic.py(955):         if scoring is None:
1.33 logistic.py(956):             scores.append(log_reg.score(X_test, y_test))
1.33 logistic.py(945):     for w in coefs:
1.33 logistic.py(946):         if multi_class == 'ovr':
1.33 logistic.py(948):         if fit_intercept:
1.33 logistic.py(949):             log_reg.coef_ = w[:, :-1]
1.33 logistic.py(950):             log_reg.intercept_ = w[:, -1]
1.33 logistic.py(955):         if scoring is None:
1.33 logistic.py(956):             scores.append(log_reg.score(X_test, y_test))
1.33 logistic.py(945):     for w in coefs:
1.33 logistic.py(946):         if multi_class == 'ovr':
1.33 logistic.py(948):         if fit_intercept:
1.33 logistic.py(949):             log_reg.coef_ = w[:, :-1]
1.33 logistic.py(950):             log_reg.intercept_ = w[:, -1]
1.33 logistic.py(955):         if scoring is None:
1.33 logistic.py(956):             scores.append(log_reg.score(X_test, y_test))
1.33 logistic.py(945):     for w in coefs:
1.33 logistic.py(946):         if multi_class == 'ovr':
1.33 logistic.py(948):         if fit_intercept:
1.33 logistic.py(949):             log_reg.coef_ = w[:, :-1]
1.33 logistic.py(950):             log_reg.intercept_ = w[:, -1]
1.33 logistic.py(955):         if scoring is None:
1.33 logistic.py(956):             scores.append(log_reg.score(X_test, y_test))
1.33 logistic.py(945):     for w in coefs:
1.33 logistic.py(946):         if multi_class == 'ovr':
1.33 logistic.py(948):         if fit_intercept:
1.33 logistic.py(949):             log_reg.coef_ = w[:, :-1]
1.33 logistic.py(950):             log_reg.intercept_ = w[:, -1]
1.33 logistic.py(955):         if scoring is None:
1.33 logistic.py(956):             scores.append(log_reg.score(X_test, y_test))
1.33 logistic.py(945):     for w in coefs:
1.33 logistic.py(946):         if multi_class == 'ovr':
1.33 logistic.py(948):         if fit_intercept:
1.33 logistic.py(949):             log_reg.coef_ = w[:, :-1]
1.33 logistic.py(950):             log_reg.intercept_ = w[:, -1]
1.33 logistic.py(955):         if scoring is None:
1.33 logistic.py(956):             scores.append(log_reg.score(X_test, y_test))
1.33 logistic.py(945):     for w in coefs:
1.33 logistic.py(946):         if multi_class == 'ovr':
1.33 logistic.py(948):         if fit_intercept:
1.33 logistic.py(949):             log_reg.coef_ = w[:, :-1]
1.33 logistic.py(950):             log_reg.intercept_ = w[:, -1]
1.33 logistic.py(955):         if scoring is None:
1.33 logistic.py(956):             scores.append(log_reg.score(X_test, y_test))
1.33 logistic.py(945):     for w in coefs:
1.33 logistic.py(946):         if multi_class == 'ovr':
1.33 logistic.py(948):         if fit_intercept:
1.33 logistic.py(949):             log_reg.coef_ = w[:, :-1]
1.33 logistic.py(950):             log_reg.intercept_ = w[:, -1]
1.33 logistic.py(955):         if scoring is None:
1.33 logistic.py(956):             scores.append(log_reg.score(X_test, y_test))
1.33 logistic.py(945):     for w in coefs:
1.33 logistic.py(959):     return coefs, Cs, np.array(scores), n_iter
1.33 logistic.py(1703):             for train, test in folds)
1.33 logistic.py(903):     _check_solver_option(solver, multi_class, penalty, dual)
1.33 logistic.py(428):     if solver not in ['liblinear', 'newton-cg', 'lbfgs', 'sag', 'saga']:
1.33 logistic.py(433):     if multi_class not in ['multinomial', 'ovr']:
1.33 logistic.py(437):     if multi_class == 'multinomial' and solver == 'liblinear':
1.33 logistic.py(441):     if solver not in ['liblinear', 'saga']:
1.33 logistic.py(442):         if penalty != 'l2':
1.33 logistic.py(445):     if solver != 'liblinear':
1.33 logistic.py(446):         if dual:
1.33 logistic.py(905):     X_train = X[train]
1.33 logistic.py(906):     X_test = X[test]
1.33 logistic.py(907):     y_train = y[train]
1.33 logistic.py(908):     y_test = y[test]
1.33 logistic.py(910):     if sample_weight is not None:
1.33 logistic.py(916):     coefs, Cs, n_iter = logistic_regression_path(
1.33 logistic.py(917):         X_train, y_train, Cs=Cs, fit_intercept=fit_intercept,
1.33 logistic.py(918):         solver=solver, max_iter=max_iter, class_weight=class_weight,
1.33 logistic.py(919):         pos_class=pos_class, multi_class=multi_class,
1.33 logistic.py(920):         tol=tol, verbose=verbose, dual=dual, penalty=penalty,
1.33 logistic.py(921):         intercept_scaling=intercept_scaling, random_state=random_state,
1.33 logistic.py(922):         check_input=False, max_squared_sum=max_squared_sum,
1.33 logistic.py(923):         sample_weight=sample_weight)
1.33 logistic.py(591):     if isinstance(Cs, numbers.Integral):
1.33 logistic.py(592):         Cs = np.logspace(-4, 4, Cs)
1.33 logistic.py(594):     _check_solver_option(solver, multi_class, penalty, dual)
1.33 logistic.py(428):     if solver not in ['liblinear', 'newton-cg', 'lbfgs', 'sag', 'saga']:
1.33 logistic.py(433):     if multi_class not in ['multinomial', 'ovr']:
1.33 logistic.py(437):     if multi_class == 'multinomial' and solver == 'liblinear':
1.33 logistic.py(441):     if solver not in ['liblinear', 'saga']:
1.33 logistic.py(442):         if penalty != 'l2':
1.33 logistic.py(445):     if solver != 'liblinear':
1.33 logistic.py(446):         if dual:
1.33 logistic.py(597):     if check_input:
1.33 logistic.py(602):     _, n_features = X.shape
1.33 logistic.py(603):     classes = np.unique(y)
1.33 logistic.py(604):     random_state = check_random_state(random_state)
1.33 logistic.py(606):     if pos_class is None and multi_class != 'multinomial':
1.33 logistic.py(615):     if sample_weight is not None:
1.33 logistic.py(619):         sample_weight = np.ones(X.shape[0], dtype=X.dtype)
1.33 logistic.py(624):     le = LabelEncoder()
1.33 logistic.py(625):     if isinstance(class_weight, dict) or multi_class == 'multinomial':
1.33 logistic.py(626):         class_weight_ = compute_class_weight(class_weight, classes, y)
1.33 logistic.py(627):         sample_weight *= class_weight_[le.fit_transform(y)]
1.33 logistic.py(631):     if multi_class == 'ovr':
1.33 logistic.py(645):         if solver not in ['sag', 'saga']:
1.33 logistic.py(646):             lbin = LabelBinarizer()
1.33 logistic.py(647):             Y_multi = lbin.fit_transform(y)
1.33 logistic.py(648):             if Y_multi.shape[1] == 1:
1.33 logistic.py(655):         w0 = np.zeros((classes.size, n_features + int(fit_intercept)),
1.33 logistic.py(656):                       order='F', dtype=X.dtype)
1.33 logistic.py(658):     if coef is not None:
1.33 logistic.py(688):     if multi_class == 'multinomial':
1.33 logistic.py(690):         if solver in ['lbfgs', 'newton-cg']:
1.33 logistic.py(691):             w0 = w0.ravel()
1.33 logistic.py(692):         target = Y_multi
1.33 logistic.py(693):         if solver == 'lbfgs':
1.33 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
1.33 logistic.py(699):         warm_start_sag = {'coef': w0.T}
1.33 logistic.py(710):     coefs = list()
1.33 logistic.py(711):     n_iter = np.zeros(len(Cs), dtype=np.int32)
1.33 logistic.py(712):     for i, C in enumerate(Cs):
1.33 logistic.py(713):         if solver == 'lbfgs':
1.33 logistic.py(714):             w0, loss, info = optimize.fmin_l_bfgs_b(
1.33 logistic.py(715):                 func, w0, fprime=None,
1.33 logistic.py(716):                 args=(X, target, 1. / C, sample_weight),
1.33 logistic.py(717):                 iprint=(verbose > 0) - 1, pgtol=tol, maxiter=max_iter)
1.33 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
1.33 logistic.py(339):     n_classes = Y.shape[1]
1.33 logistic.py(340):     n_features = X.shape[1]
1.33 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
1.33 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
1.33 logistic.py(343):                     dtype=X.dtype)
1.33 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
1.33 logistic.py(282):     n_classes = Y.shape[1]
1.33 logistic.py(283):     n_features = X.shape[1]
1.33 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
1.33 logistic.py(285):     w = w.reshape(n_classes, -1)
1.33 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
1.33 logistic.py(287):     if fit_intercept:
1.33 logistic.py(288):         intercept = w[:, -1]
1.33 logistic.py(289):         w = w[:, :-1]
1.33 logistic.py(292):     p = safe_sparse_dot(X, w.T)
1.33 logistic.py(293):     p += intercept
1.33 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
1.33 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
1.33 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
1.33 logistic.py(297):     p = np.exp(p, p)
1.33 logistic.py(298):     return loss, p, w
1.33 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
1.33 logistic.py(346):     diff = sample_weight * (p - Y)
1.33 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
1.33 logistic.py(348):     grad[:, :n_features] += alpha * w
1.34 logistic.py(349):     if fit_intercept:
1.34 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
1.34 logistic.py(351):     return loss, grad.ravel(), p
1.34 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
1.34 logistic.py(339):     n_classes = Y.shape[1]
1.34 logistic.py(340):     n_features = X.shape[1]
1.34 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
1.34 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
1.34 logistic.py(343):                     dtype=X.dtype)
1.34 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
1.34 logistic.py(282):     n_classes = Y.shape[1]
1.34 logistic.py(283):     n_features = X.shape[1]
1.34 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
1.34 logistic.py(285):     w = w.reshape(n_classes, -1)
1.34 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
1.34 logistic.py(287):     if fit_intercept:
1.34 logistic.py(288):         intercept = w[:, -1]
1.34 logistic.py(289):         w = w[:, :-1]
1.34 logistic.py(292):     p = safe_sparse_dot(X, w.T)
1.34 logistic.py(293):     p += intercept
1.34 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
1.34 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
1.34 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
1.34 logistic.py(297):     p = np.exp(p, p)
1.34 logistic.py(298):     return loss, p, w
1.34 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
1.34 logistic.py(346):     diff = sample_weight * (p - Y)
1.34 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
1.34 logistic.py(348):     grad[:, :n_features] += alpha * w
1.34 logistic.py(349):     if fit_intercept:
1.34 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
1.34 logistic.py(351):     return loss, grad.ravel(), p
1.34 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
1.34 logistic.py(339):     n_classes = Y.shape[1]
1.34 logistic.py(340):     n_features = X.shape[1]
1.34 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
1.34 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
1.34 logistic.py(343):                     dtype=X.dtype)
1.34 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
1.34 logistic.py(282):     n_classes = Y.shape[1]
1.34 logistic.py(283):     n_features = X.shape[1]
1.34 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
1.34 logistic.py(285):     w = w.reshape(n_classes, -1)
1.34 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
1.34 logistic.py(287):     if fit_intercept:
1.34 logistic.py(288):         intercept = w[:, -1]
1.34 logistic.py(289):         w = w[:, :-1]
1.34 logistic.py(292):     p = safe_sparse_dot(X, w.T)
1.34 logistic.py(293):     p += intercept
1.34 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
1.34 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
1.34 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
1.34 logistic.py(297):     p = np.exp(p, p)
1.34 logistic.py(298):     return loss, p, w
1.34 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
1.34 logistic.py(346):     diff = sample_weight * (p - Y)
1.34 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
1.34 logistic.py(348):     grad[:, :n_features] += alpha * w
1.34 logistic.py(349):     if fit_intercept:
1.34 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
1.34 logistic.py(351):     return loss, grad.ravel(), p
1.34 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
1.34 logistic.py(339):     n_classes = Y.shape[1]
1.34 logistic.py(340):     n_features = X.shape[1]
1.34 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
1.34 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
1.34 logistic.py(343):                     dtype=X.dtype)
1.34 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
1.34 logistic.py(282):     n_classes = Y.shape[1]
1.34 logistic.py(283):     n_features = X.shape[1]
1.34 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
1.34 logistic.py(285):     w = w.reshape(n_classes, -1)
1.34 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
1.34 logistic.py(287):     if fit_intercept:
1.34 logistic.py(288):         intercept = w[:, -1]
1.34 logistic.py(289):         w = w[:, :-1]
1.34 logistic.py(292):     p = safe_sparse_dot(X, w.T)
1.34 logistic.py(293):     p += intercept
1.34 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
1.34 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
1.34 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
1.34 logistic.py(297):     p = np.exp(p, p)
1.34 logistic.py(298):     return loss, p, w
1.34 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
1.34 logistic.py(346):     diff = sample_weight * (p - Y)
1.34 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
1.34 logistic.py(348):     grad[:, :n_features] += alpha * w
1.34 logistic.py(349):     if fit_intercept:
1.34 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
1.34 logistic.py(351):     return loss, grad.ravel(), p
1.34 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
1.34 logistic.py(339):     n_classes = Y.shape[1]
1.34 logistic.py(340):     n_features = X.shape[1]
1.34 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
1.34 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
1.34 logistic.py(343):                     dtype=X.dtype)
1.34 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
1.34 logistic.py(282):     n_classes = Y.shape[1]
1.34 logistic.py(283):     n_features = X.shape[1]
1.34 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
1.34 logistic.py(285):     w = w.reshape(n_classes, -1)
1.34 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
1.34 logistic.py(287):     if fit_intercept:
1.34 logistic.py(288):         intercept = w[:, -1]
1.34 logistic.py(289):         w = w[:, :-1]
1.34 logistic.py(292):     p = safe_sparse_dot(X, w.T)
1.34 logistic.py(293):     p += intercept
1.34 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
1.34 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
1.34 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
1.34 logistic.py(297):     p = np.exp(p, p)
1.34 logistic.py(298):     return loss, p, w
1.34 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
1.34 logistic.py(346):     diff = sample_weight * (p - Y)
1.34 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
1.34 logistic.py(348):     grad[:, :n_features] += alpha * w
1.34 logistic.py(349):     if fit_intercept:
1.34 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
1.34 logistic.py(351):     return loss, grad.ravel(), p
1.34 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
1.34 logistic.py(339):     n_classes = Y.shape[1]
1.34 logistic.py(340):     n_features = X.shape[1]
1.34 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
1.34 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
1.34 logistic.py(343):                     dtype=X.dtype)
1.34 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
1.34 logistic.py(282):     n_classes = Y.shape[1]
1.34 logistic.py(283):     n_features = X.shape[1]
1.34 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
1.34 logistic.py(285):     w = w.reshape(n_classes, -1)
1.34 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
1.34 logistic.py(287):     if fit_intercept:
1.34 logistic.py(288):         intercept = w[:, -1]
1.34 logistic.py(289):         w = w[:, :-1]
1.34 logistic.py(292):     p = safe_sparse_dot(X, w.T)
1.34 logistic.py(293):     p += intercept
1.34 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
1.34 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
1.34 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
1.34 logistic.py(297):     p = np.exp(p, p)
1.34 logistic.py(298):     return loss, p, w
1.34 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
1.34 logistic.py(346):     diff = sample_weight * (p - Y)
1.34 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
1.34 logistic.py(348):     grad[:, :n_features] += alpha * w
1.34 logistic.py(349):     if fit_intercept:
1.34 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
1.34 logistic.py(351):     return loss, grad.ravel(), p
1.34 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
1.34 logistic.py(339):     n_classes = Y.shape[1]
1.34 logistic.py(340):     n_features = X.shape[1]
1.34 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
1.34 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
1.34 logistic.py(343):                     dtype=X.dtype)
1.34 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
1.34 logistic.py(282):     n_classes = Y.shape[1]
1.34 logistic.py(283):     n_features = X.shape[1]
1.34 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
1.34 logistic.py(285):     w = w.reshape(n_classes, -1)
1.34 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
1.34 logistic.py(287):     if fit_intercept:
1.34 logistic.py(288):         intercept = w[:, -1]
1.34 logistic.py(289):         w = w[:, :-1]
1.34 logistic.py(292):     p = safe_sparse_dot(X, w.T)
1.34 logistic.py(293):     p += intercept
1.34 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
1.34 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
1.34 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
1.34 logistic.py(297):     p = np.exp(p, p)
1.34 logistic.py(298):     return loss, p, w
1.34 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
1.34 logistic.py(346):     diff = sample_weight * (p - Y)
1.34 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
1.34 logistic.py(348):     grad[:, :n_features] += alpha * w
1.34 logistic.py(349):     if fit_intercept:
1.34 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
1.34 logistic.py(351):     return loss, grad.ravel(), p
1.34 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
1.34 logistic.py(339):     n_classes = Y.shape[1]
1.34 logistic.py(340):     n_features = X.shape[1]
1.34 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
1.34 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
1.34 logistic.py(343):                     dtype=X.dtype)
1.34 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
1.34 logistic.py(282):     n_classes = Y.shape[1]
1.34 logistic.py(283):     n_features = X.shape[1]
1.34 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
1.34 logistic.py(285):     w = w.reshape(n_classes, -1)
1.34 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
1.34 logistic.py(287):     if fit_intercept:
1.34 logistic.py(288):         intercept = w[:, -1]
1.34 logistic.py(289):         w = w[:, :-1]
1.34 logistic.py(292):     p = safe_sparse_dot(X, w.T)
1.34 logistic.py(293):     p += intercept
1.34 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
1.34 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
1.34 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
1.34 logistic.py(297):     p = np.exp(p, p)
1.34 logistic.py(298):     return loss, p, w
1.34 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
1.34 logistic.py(346):     diff = sample_weight * (p - Y)
1.34 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
1.34 logistic.py(348):     grad[:, :n_features] += alpha * w
1.34 logistic.py(349):     if fit_intercept:
1.34 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
1.34 logistic.py(351):     return loss, grad.ravel(), p
1.34 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
1.34 logistic.py(339):     n_classes = Y.shape[1]
1.34 logistic.py(340):     n_features = X.shape[1]
1.34 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
1.34 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
1.34 logistic.py(343):                     dtype=X.dtype)
1.34 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
1.34 logistic.py(282):     n_classes = Y.shape[1]
1.34 logistic.py(283):     n_features = X.shape[1]
1.34 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
1.34 logistic.py(285):     w = w.reshape(n_classes, -1)
1.34 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
1.34 logistic.py(287):     if fit_intercept:
1.34 logistic.py(288):         intercept = w[:, -1]
1.34 logistic.py(289):         w = w[:, :-1]
1.34 logistic.py(292):     p = safe_sparse_dot(X, w.T)
1.34 logistic.py(293):     p += intercept
1.34 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
1.34 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
1.34 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
1.34 logistic.py(297):     p = np.exp(p, p)
1.34 logistic.py(298):     return loss, p, w
1.34 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
1.34 logistic.py(346):     diff = sample_weight * (p - Y)
1.34 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
1.34 logistic.py(348):     grad[:, :n_features] += alpha * w
1.34 logistic.py(349):     if fit_intercept:
1.34 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
1.34 logistic.py(351):     return loss, grad.ravel(), p
1.34 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
1.34 logistic.py(339):     n_classes = Y.shape[1]
1.34 logistic.py(340):     n_features = X.shape[1]
1.34 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
1.34 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
1.34 logistic.py(343):                     dtype=X.dtype)
1.34 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
1.34 logistic.py(282):     n_classes = Y.shape[1]
1.34 logistic.py(283):     n_features = X.shape[1]
1.34 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
1.34 logistic.py(285):     w = w.reshape(n_classes, -1)
1.34 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
1.34 logistic.py(287):     if fit_intercept:
1.34 logistic.py(288):         intercept = w[:, -1]
1.34 logistic.py(289):         w = w[:, :-1]
1.34 logistic.py(292):     p = safe_sparse_dot(X, w.T)
1.34 logistic.py(293):     p += intercept
1.34 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
1.34 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
1.34 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
1.34 logistic.py(297):     p = np.exp(p, p)
1.34 logistic.py(298):     return loss, p, w
1.34 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
1.34 logistic.py(346):     diff = sample_weight * (p - Y)
1.34 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
1.34 logistic.py(348):     grad[:, :n_features] += alpha * w
1.34 logistic.py(349):     if fit_intercept:
1.34 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
1.34 logistic.py(351):     return loss, grad.ravel(), p
1.34 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
1.34 logistic.py(339):     n_classes = Y.shape[1]
1.34 logistic.py(340):     n_features = X.shape[1]
1.34 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
1.34 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
1.34 logistic.py(343):                     dtype=X.dtype)
1.34 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
1.34 logistic.py(282):     n_classes = Y.shape[1]
1.34 logistic.py(283):     n_features = X.shape[1]
1.34 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
1.34 logistic.py(285):     w = w.reshape(n_classes, -1)
1.34 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
1.34 logistic.py(287):     if fit_intercept:
1.34 logistic.py(288):         intercept = w[:, -1]
1.34 logistic.py(289):         w = w[:, :-1]
1.34 logistic.py(292):     p = safe_sparse_dot(X, w.T)
1.34 logistic.py(293):     p += intercept
1.34 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
1.34 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
1.34 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
1.34 logistic.py(297):     p = np.exp(p, p)
1.34 logistic.py(298):     return loss, p, w
1.34 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
1.34 logistic.py(346):     diff = sample_weight * (p - Y)
1.34 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
1.34 logistic.py(348):     grad[:, :n_features] += alpha * w
1.34 logistic.py(349):     if fit_intercept:
1.34 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
1.34 logistic.py(351):     return loss, grad.ravel(), p
1.34 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
1.34 logistic.py(339):     n_classes = Y.shape[1]
1.34 logistic.py(340):     n_features = X.shape[1]
1.34 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
1.34 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
1.34 logistic.py(343):                     dtype=X.dtype)
1.34 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
1.34 logistic.py(282):     n_classes = Y.shape[1]
1.34 logistic.py(283):     n_features = X.shape[1]
1.34 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
1.34 logistic.py(285):     w = w.reshape(n_classes, -1)
1.34 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
1.34 logistic.py(287):     if fit_intercept:
1.34 logistic.py(288):         intercept = w[:, -1]
1.34 logistic.py(289):         w = w[:, :-1]
1.34 logistic.py(292):     p = safe_sparse_dot(X, w.T)
1.34 logistic.py(293):     p += intercept
1.34 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
1.34 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
1.34 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
1.34 logistic.py(297):     p = np.exp(p, p)
1.34 logistic.py(298):     return loss, p, w
1.34 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
1.34 logistic.py(346):     diff = sample_weight * (p - Y)
1.34 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
1.34 logistic.py(348):     grad[:, :n_features] += alpha * w
1.34 logistic.py(349):     if fit_intercept:
1.34 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
1.34 logistic.py(351):     return loss, grad.ravel(), p
1.34 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
1.34 logistic.py(339):     n_classes = Y.shape[1]
1.34 logistic.py(340):     n_features = X.shape[1]
1.34 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
1.34 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
1.34 logistic.py(343):                     dtype=X.dtype)
1.34 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
1.34 logistic.py(282):     n_classes = Y.shape[1]
1.34 logistic.py(283):     n_features = X.shape[1]
1.34 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
1.34 logistic.py(285):     w = w.reshape(n_classes, -1)
1.34 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
1.34 logistic.py(287):     if fit_intercept:
1.34 logistic.py(288):         intercept = w[:, -1]
1.34 logistic.py(289):         w = w[:, :-1]
1.34 logistic.py(292):     p = safe_sparse_dot(X, w.T)
1.34 logistic.py(293):     p += intercept
1.34 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
1.34 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
1.34 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
1.34 logistic.py(297):     p = np.exp(p, p)
1.34 logistic.py(298):     return loss, p, w
1.34 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
1.34 logistic.py(346):     diff = sample_weight * (p - Y)
1.34 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
1.34 logistic.py(348):     grad[:, :n_features] += alpha * w
1.34 logistic.py(349):     if fit_intercept:
1.34 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
1.34 logistic.py(351):     return loss, grad.ravel(), p
1.34 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
1.34 logistic.py(339):     n_classes = Y.shape[1]
1.34 logistic.py(340):     n_features = X.shape[1]
1.34 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
1.34 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
1.34 logistic.py(343):                     dtype=X.dtype)
1.34 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
1.34 logistic.py(282):     n_classes = Y.shape[1]
1.34 logistic.py(283):     n_features = X.shape[1]
1.34 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
1.34 logistic.py(285):     w = w.reshape(n_classes, -1)
1.34 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
1.34 logistic.py(287):     if fit_intercept:
1.34 logistic.py(288):         intercept = w[:, -1]
1.34 logistic.py(289):         w = w[:, :-1]
1.34 logistic.py(292):     p = safe_sparse_dot(X, w.T)
1.34 logistic.py(293):     p += intercept
1.34 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
1.34 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
1.34 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
1.34 logistic.py(297):     p = np.exp(p, p)
1.34 logistic.py(298):     return loss, p, w
1.34 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
1.34 logistic.py(346):     diff = sample_weight * (p - Y)
1.34 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
1.34 logistic.py(348):     grad[:, :n_features] += alpha * w
1.34 logistic.py(349):     if fit_intercept:
1.34 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
1.34 logistic.py(351):     return loss, grad.ravel(), p
1.34 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
1.34 logistic.py(339):     n_classes = Y.shape[1]
1.34 logistic.py(340):     n_features = X.shape[1]
1.34 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
1.34 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
1.34 logistic.py(343):                     dtype=X.dtype)
1.34 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
1.34 logistic.py(282):     n_classes = Y.shape[1]
1.34 logistic.py(283):     n_features = X.shape[1]
1.34 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
1.34 logistic.py(285):     w = w.reshape(n_classes, -1)
1.34 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
1.34 logistic.py(287):     if fit_intercept:
1.34 logistic.py(288):         intercept = w[:, -1]
1.34 logistic.py(289):         w = w[:, :-1]
1.34 logistic.py(292):     p = safe_sparse_dot(X, w.T)
1.34 logistic.py(293):     p += intercept
1.34 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
1.34 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
1.34 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
1.34 logistic.py(297):     p = np.exp(p, p)
1.34 logistic.py(298):     return loss, p, w
1.34 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
1.34 logistic.py(346):     diff = sample_weight * (p - Y)
1.34 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
1.34 logistic.py(348):     grad[:, :n_features] += alpha * w
1.34 logistic.py(349):     if fit_intercept:
1.34 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
1.34 logistic.py(351):     return loss, grad.ravel(), p
1.34 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
1.34 logistic.py(339):     n_classes = Y.shape[1]
1.34 logistic.py(340):     n_features = X.shape[1]
1.34 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
1.34 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
1.34 logistic.py(343):                     dtype=X.dtype)
1.34 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
1.34 logistic.py(282):     n_classes = Y.shape[1]
1.34 logistic.py(283):     n_features = X.shape[1]
1.34 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
1.34 logistic.py(285):     w = w.reshape(n_classes, -1)
1.34 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
1.34 logistic.py(287):     if fit_intercept:
1.34 logistic.py(288):         intercept = w[:, -1]
1.35 logistic.py(289):         w = w[:, :-1]
1.35 logistic.py(292):     p = safe_sparse_dot(X, w.T)
1.35 logistic.py(293):     p += intercept
1.35 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
1.35 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
1.35 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
1.35 logistic.py(297):     p = np.exp(p, p)
1.35 logistic.py(298):     return loss, p, w
1.35 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
1.35 logistic.py(346):     diff = sample_weight * (p - Y)
1.35 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
1.35 logistic.py(348):     grad[:, :n_features] += alpha * w
1.35 logistic.py(349):     if fit_intercept:
1.35 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
1.35 logistic.py(351):     return loss, grad.ravel(), p
1.35 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
1.35 logistic.py(339):     n_classes = Y.shape[1]
1.35 logistic.py(340):     n_features = X.shape[1]
1.35 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
1.35 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
1.35 logistic.py(343):                     dtype=X.dtype)
1.35 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
1.35 logistic.py(282):     n_classes = Y.shape[1]
1.35 logistic.py(283):     n_features = X.shape[1]
1.35 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
1.35 logistic.py(285):     w = w.reshape(n_classes, -1)
1.35 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
1.35 logistic.py(287):     if fit_intercept:
1.35 logistic.py(288):         intercept = w[:, -1]
1.35 logistic.py(289):         w = w[:, :-1]
1.35 logistic.py(292):     p = safe_sparse_dot(X, w.T)
1.35 logistic.py(293):     p += intercept
1.35 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
1.35 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
1.35 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
1.35 logistic.py(297):     p = np.exp(p, p)
1.35 logistic.py(298):     return loss, p, w
1.35 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
1.35 logistic.py(346):     diff = sample_weight * (p - Y)
1.35 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
1.35 logistic.py(348):     grad[:, :n_features] += alpha * w
1.35 logistic.py(349):     if fit_intercept:
1.35 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
1.35 logistic.py(351):     return loss, grad.ravel(), p
1.35 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
1.35 logistic.py(339):     n_classes = Y.shape[1]
1.35 logistic.py(340):     n_features = X.shape[1]
1.35 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
1.35 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
1.35 logistic.py(343):                     dtype=X.dtype)
1.35 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
1.35 logistic.py(282):     n_classes = Y.shape[1]
1.35 logistic.py(283):     n_features = X.shape[1]
1.35 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
1.35 logistic.py(285):     w = w.reshape(n_classes, -1)
1.35 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
1.35 logistic.py(287):     if fit_intercept:
1.35 logistic.py(288):         intercept = w[:, -1]
1.35 logistic.py(289):         w = w[:, :-1]
1.35 logistic.py(292):     p = safe_sparse_dot(X, w.T)
1.35 logistic.py(293):     p += intercept
1.35 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
1.35 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
1.35 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
1.35 logistic.py(297):     p = np.exp(p, p)
1.35 logistic.py(298):     return loss, p, w
1.35 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
1.35 logistic.py(346):     diff = sample_weight * (p - Y)
1.35 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
1.35 logistic.py(348):     grad[:, :n_features] += alpha * w
1.35 logistic.py(349):     if fit_intercept:
1.35 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
1.35 logistic.py(351):     return loss, grad.ravel(), p
1.35 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
1.35 logistic.py(339):     n_classes = Y.shape[1]
1.35 logistic.py(340):     n_features = X.shape[1]
1.35 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
1.35 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
1.35 logistic.py(343):                     dtype=X.dtype)
1.35 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
1.35 logistic.py(282):     n_classes = Y.shape[1]
1.35 logistic.py(283):     n_features = X.shape[1]
1.35 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
1.35 logistic.py(285):     w = w.reshape(n_classes, -1)
1.35 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
1.35 logistic.py(287):     if fit_intercept:
1.35 logistic.py(288):         intercept = w[:, -1]
1.35 logistic.py(289):         w = w[:, :-1]
1.35 logistic.py(292):     p = safe_sparse_dot(X, w.T)
1.35 logistic.py(293):     p += intercept
1.35 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
1.35 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
1.35 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
1.35 logistic.py(297):     p = np.exp(p, p)
1.35 logistic.py(298):     return loss, p, w
1.35 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
1.35 logistic.py(346):     diff = sample_weight * (p - Y)
1.35 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
1.35 logistic.py(348):     grad[:, :n_features] += alpha * w
1.35 logistic.py(349):     if fit_intercept:
1.35 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
1.35 logistic.py(351):     return loss, grad.ravel(), p
1.35 logistic.py(718):             if info["warnflag"] == 1:
1.35 logistic.py(723):             n_iter_i = min(info['nit'], max_iter)
1.35 logistic.py(760):         if multi_class == 'multinomial':
1.35 logistic.py(761):             multi_w0 = np.reshape(w0, (classes.size, -1))
1.35 logistic.py(762):             if classes.size == 2:
1.35 logistic.py(764):             coefs.append(multi_w0)
1.35 logistic.py(768):         n_iter[i] = n_iter_i
1.35 logistic.py(712):     for i, C in enumerate(Cs):
1.35 logistic.py(713):         if solver == 'lbfgs':
1.35 logistic.py(714):             w0, loss, info = optimize.fmin_l_bfgs_b(
1.35 logistic.py(715):                 func, w0, fprime=None,
1.35 logistic.py(716):                 args=(X, target, 1. / C, sample_weight),
1.35 logistic.py(717):                 iprint=(verbose > 0) - 1, pgtol=tol, maxiter=max_iter)
1.35 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
1.35 logistic.py(339):     n_classes = Y.shape[1]
1.35 logistic.py(340):     n_features = X.shape[1]
1.35 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
1.35 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
1.35 logistic.py(343):                     dtype=X.dtype)
1.35 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
1.35 logistic.py(282):     n_classes = Y.shape[1]
1.35 logistic.py(283):     n_features = X.shape[1]
1.35 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
1.35 logistic.py(285):     w = w.reshape(n_classes, -1)
1.35 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
1.35 logistic.py(287):     if fit_intercept:
1.35 logistic.py(288):         intercept = w[:, -1]
1.35 logistic.py(289):         w = w[:, :-1]
1.35 logistic.py(292):     p = safe_sparse_dot(X, w.T)
1.35 logistic.py(293):     p += intercept
1.35 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
1.35 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
1.35 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
1.35 logistic.py(297):     p = np.exp(p, p)
1.35 logistic.py(298):     return loss, p, w
1.35 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
1.35 logistic.py(346):     diff = sample_weight * (p - Y)
1.35 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
1.35 logistic.py(348):     grad[:, :n_features] += alpha * w
1.35 logistic.py(349):     if fit_intercept:
1.35 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
1.35 logistic.py(351):     return loss, grad.ravel(), p
1.35 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
1.35 logistic.py(339):     n_classes = Y.shape[1]
1.35 logistic.py(340):     n_features = X.shape[1]
1.35 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
1.35 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
1.35 logistic.py(343):                     dtype=X.dtype)
1.35 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
1.35 logistic.py(282):     n_classes = Y.shape[1]
1.35 logistic.py(283):     n_features = X.shape[1]
1.35 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
1.35 logistic.py(285):     w = w.reshape(n_classes, -1)
1.35 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
1.35 logistic.py(287):     if fit_intercept:
1.35 logistic.py(288):         intercept = w[:, -1]
1.35 logistic.py(289):         w = w[:, :-1]
1.35 logistic.py(292):     p = safe_sparse_dot(X, w.T)
1.35 logistic.py(293):     p += intercept
1.35 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
1.35 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
1.35 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
1.35 logistic.py(297):     p = np.exp(p, p)
1.35 logistic.py(298):     return loss, p, w
1.35 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
1.35 logistic.py(346):     diff = sample_weight * (p - Y)
1.35 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
1.35 logistic.py(348):     grad[:, :n_features] += alpha * w
1.35 logistic.py(349):     if fit_intercept:
1.35 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
1.35 logistic.py(351):     return loss, grad.ravel(), p
1.35 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
1.35 logistic.py(339):     n_classes = Y.shape[1]
1.35 logistic.py(340):     n_features = X.shape[1]
1.35 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
1.35 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
1.35 logistic.py(343):                     dtype=X.dtype)
1.35 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
1.35 logistic.py(282):     n_classes = Y.shape[1]
1.35 logistic.py(283):     n_features = X.shape[1]
1.35 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
1.35 logistic.py(285):     w = w.reshape(n_classes, -1)
1.35 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
1.35 logistic.py(287):     if fit_intercept:
1.35 logistic.py(288):         intercept = w[:, -1]
1.35 logistic.py(289):         w = w[:, :-1]
1.35 logistic.py(292):     p = safe_sparse_dot(X, w.T)
1.35 logistic.py(293):     p += intercept
1.35 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
1.35 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
1.35 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
1.35 logistic.py(297):     p = np.exp(p, p)
1.35 logistic.py(298):     return loss, p, w
1.35 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
1.35 logistic.py(346):     diff = sample_weight * (p - Y)
1.35 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
1.35 logistic.py(348):     grad[:, :n_features] += alpha * w
1.35 logistic.py(349):     if fit_intercept:
1.35 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
1.35 logistic.py(351):     return loss, grad.ravel(), p
1.35 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
1.35 logistic.py(339):     n_classes = Y.shape[1]
1.35 logistic.py(340):     n_features = X.shape[1]
1.35 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
1.35 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
1.35 logistic.py(343):                     dtype=X.dtype)
1.35 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
1.35 logistic.py(282):     n_classes = Y.shape[1]
1.35 logistic.py(283):     n_features = X.shape[1]
1.35 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
1.35 logistic.py(285):     w = w.reshape(n_classes, -1)
1.35 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
1.35 logistic.py(287):     if fit_intercept:
1.35 logistic.py(288):         intercept = w[:, -1]
1.35 logistic.py(289):         w = w[:, :-1]
1.35 logistic.py(292):     p = safe_sparse_dot(X, w.T)
1.35 logistic.py(293):     p += intercept
1.35 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
1.35 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
1.35 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
1.35 logistic.py(297):     p = np.exp(p, p)
1.35 logistic.py(298):     return loss, p, w
1.35 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
1.35 logistic.py(346):     diff = sample_weight * (p - Y)
1.35 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
1.35 logistic.py(348):     grad[:, :n_features] += alpha * w
1.35 logistic.py(349):     if fit_intercept:
1.35 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
1.35 logistic.py(351):     return loss, grad.ravel(), p
1.35 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
1.35 logistic.py(339):     n_classes = Y.shape[1]
1.35 logistic.py(340):     n_features = X.shape[1]
1.35 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
1.35 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
1.35 logistic.py(343):                     dtype=X.dtype)
1.35 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
1.35 logistic.py(282):     n_classes = Y.shape[1]
1.35 logistic.py(283):     n_features = X.shape[1]
1.35 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
1.35 logistic.py(285):     w = w.reshape(n_classes, -1)
1.35 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
1.35 logistic.py(287):     if fit_intercept:
1.35 logistic.py(288):         intercept = w[:, -1]
1.35 logistic.py(289):         w = w[:, :-1]
1.35 logistic.py(292):     p = safe_sparse_dot(X, w.T)
1.35 logistic.py(293):     p += intercept
1.35 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
1.35 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
1.35 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
1.35 logistic.py(297):     p = np.exp(p, p)
1.35 logistic.py(298):     return loss, p, w
1.35 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
1.35 logistic.py(346):     diff = sample_weight * (p - Y)
1.35 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
1.35 logistic.py(348):     grad[:, :n_features] += alpha * w
1.35 logistic.py(349):     if fit_intercept:
1.35 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
1.35 logistic.py(351):     return loss, grad.ravel(), p
1.35 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
1.35 logistic.py(339):     n_classes = Y.shape[1]
1.35 logistic.py(340):     n_features = X.shape[1]
1.35 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
1.35 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
1.35 logistic.py(343):                     dtype=X.dtype)
1.35 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
1.35 logistic.py(282):     n_classes = Y.shape[1]
1.35 logistic.py(283):     n_features = X.shape[1]
1.35 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
1.35 logistic.py(285):     w = w.reshape(n_classes, -1)
1.35 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
1.35 logistic.py(287):     if fit_intercept:
1.35 logistic.py(288):         intercept = w[:, -1]
1.35 logistic.py(289):         w = w[:, :-1]
1.35 logistic.py(292):     p = safe_sparse_dot(X, w.T)
1.35 logistic.py(293):     p += intercept
1.35 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
1.35 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
1.35 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
1.35 logistic.py(297):     p = np.exp(p, p)
1.35 logistic.py(298):     return loss, p, w
1.35 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
1.35 logistic.py(346):     diff = sample_weight * (p - Y)
1.35 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
1.35 logistic.py(348):     grad[:, :n_features] += alpha * w
1.35 logistic.py(349):     if fit_intercept:
1.35 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
1.35 logistic.py(351):     return loss, grad.ravel(), p
1.35 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
1.35 logistic.py(339):     n_classes = Y.shape[1]
1.35 logistic.py(340):     n_features = X.shape[1]
1.35 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
1.35 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
1.35 logistic.py(343):                     dtype=X.dtype)
1.35 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
1.35 logistic.py(282):     n_classes = Y.shape[1]
1.35 logistic.py(283):     n_features = X.shape[1]
1.35 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
1.35 logistic.py(285):     w = w.reshape(n_classes, -1)
1.35 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
1.35 logistic.py(287):     if fit_intercept:
1.35 logistic.py(288):         intercept = w[:, -1]
1.35 logistic.py(289):         w = w[:, :-1]
1.35 logistic.py(292):     p = safe_sparse_dot(X, w.T)
1.35 logistic.py(293):     p += intercept
1.35 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
1.35 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
1.35 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
1.35 logistic.py(297):     p = np.exp(p, p)
1.35 logistic.py(298):     return loss, p, w
1.35 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
1.35 logistic.py(346):     diff = sample_weight * (p - Y)
1.35 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
1.35 logistic.py(348):     grad[:, :n_features] += alpha * w
1.35 logistic.py(349):     if fit_intercept:
1.35 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
1.35 logistic.py(351):     return loss, grad.ravel(), p
1.35 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
1.35 logistic.py(339):     n_classes = Y.shape[1]
1.35 logistic.py(340):     n_features = X.shape[1]
1.35 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
1.35 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
1.35 logistic.py(343):                     dtype=X.dtype)
1.35 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
1.35 logistic.py(282):     n_classes = Y.shape[1]
1.35 logistic.py(283):     n_features = X.shape[1]
1.35 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
1.35 logistic.py(285):     w = w.reshape(n_classes, -1)
1.35 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
1.35 logistic.py(287):     if fit_intercept:
1.35 logistic.py(288):         intercept = w[:, -1]
1.35 logistic.py(289):         w = w[:, :-1]
1.35 logistic.py(292):     p = safe_sparse_dot(X, w.T)
1.35 logistic.py(293):     p += intercept
1.35 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
1.35 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
1.35 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
1.35 logistic.py(297):     p = np.exp(p, p)
1.35 logistic.py(298):     return loss, p, w
1.35 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
1.35 logistic.py(346):     diff = sample_weight * (p - Y)
1.35 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
1.35 logistic.py(348):     grad[:, :n_features] += alpha * w
1.35 logistic.py(349):     if fit_intercept:
1.35 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
1.35 logistic.py(351):     return loss, grad.ravel(), p
1.35 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
1.35 logistic.py(339):     n_classes = Y.shape[1]
1.35 logistic.py(340):     n_features = X.shape[1]
1.35 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
1.35 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
1.35 logistic.py(343):                     dtype=X.dtype)
1.35 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
1.35 logistic.py(282):     n_classes = Y.shape[1]
1.35 logistic.py(283):     n_features = X.shape[1]
1.35 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
1.35 logistic.py(285):     w = w.reshape(n_classes, -1)
1.35 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
1.35 logistic.py(287):     if fit_intercept:
1.35 logistic.py(288):         intercept = w[:, -1]
1.35 logistic.py(289):         w = w[:, :-1]
1.35 logistic.py(292):     p = safe_sparse_dot(X, w.T)
1.35 logistic.py(293):     p += intercept
1.35 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
1.35 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
1.35 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
1.35 logistic.py(297):     p = np.exp(p, p)
1.35 logistic.py(298):     return loss, p, w
1.35 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
1.35 logistic.py(346):     diff = sample_weight * (p - Y)
1.35 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
1.35 logistic.py(348):     grad[:, :n_features] += alpha * w
1.35 logistic.py(349):     if fit_intercept:
1.35 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
1.35 logistic.py(351):     return loss, grad.ravel(), p
1.35 logistic.py(718):             if info["warnflag"] == 1:
1.35 logistic.py(723):             n_iter_i = min(info['nit'], max_iter)
1.35 logistic.py(760):         if multi_class == 'multinomial':
1.35 logistic.py(761):             multi_w0 = np.reshape(w0, (classes.size, -1))
1.35 logistic.py(762):             if classes.size == 2:
1.35 logistic.py(764):             coefs.append(multi_w0)
1.35 logistic.py(768):         n_iter[i] = n_iter_i
1.35 logistic.py(712):     for i, C in enumerate(Cs):
1.35 logistic.py(713):         if solver == 'lbfgs':
1.35 logistic.py(714):             w0, loss, info = optimize.fmin_l_bfgs_b(
1.35 logistic.py(715):                 func, w0, fprime=None,
1.35 logistic.py(716):                 args=(X, target, 1. / C, sample_weight),
1.35 logistic.py(717):                 iprint=(verbose > 0) - 1, pgtol=tol, maxiter=max_iter)
1.35 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
1.35 logistic.py(339):     n_classes = Y.shape[1]
1.35 logistic.py(340):     n_features = X.shape[1]
1.35 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
1.35 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
1.35 logistic.py(343):                     dtype=X.dtype)
1.35 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
1.35 logistic.py(282):     n_classes = Y.shape[1]
1.35 logistic.py(283):     n_features = X.shape[1]
1.35 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
1.35 logistic.py(285):     w = w.reshape(n_classes, -1)
1.35 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
1.35 logistic.py(287):     if fit_intercept:
1.35 logistic.py(288):         intercept = w[:, -1]
1.35 logistic.py(289):         w = w[:, :-1]
1.35 logistic.py(292):     p = safe_sparse_dot(X, w.T)
1.35 logistic.py(293):     p += intercept
1.35 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
1.35 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
1.35 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
1.35 logistic.py(297):     p = np.exp(p, p)
1.35 logistic.py(298):     return loss, p, w
1.35 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
1.35 logistic.py(346):     diff = sample_weight * (p - Y)
1.35 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
1.35 logistic.py(348):     grad[:, :n_features] += alpha * w
1.35 logistic.py(349):     if fit_intercept:
1.35 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
1.35 logistic.py(351):     return loss, grad.ravel(), p
1.35 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
1.35 logistic.py(339):     n_classes = Y.shape[1]
1.35 logistic.py(340):     n_features = X.shape[1]
1.35 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
1.35 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
1.35 logistic.py(343):                     dtype=X.dtype)
1.35 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
1.35 logistic.py(282):     n_classes = Y.shape[1]
1.35 logistic.py(283):     n_features = X.shape[1]
1.35 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
1.35 logistic.py(285):     w = w.reshape(n_classes, -1)
1.35 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
1.35 logistic.py(287):     if fit_intercept:
1.35 logistic.py(288):         intercept = w[:, -1]
1.35 logistic.py(289):         w = w[:, :-1]
1.35 logistic.py(292):     p = safe_sparse_dot(X, w.T)
1.35 logistic.py(293):     p += intercept
1.35 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
1.35 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
1.35 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
1.35 logistic.py(297):     p = np.exp(p, p)
1.35 logistic.py(298):     return loss, p, w
1.35 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
1.35 logistic.py(346):     diff = sample_weight * (p - Y)
1.35 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
1.35 logistic.py(348):     grad[:, :n_features] += alpha * w
1.35 logistic.py(349):     if fit_intercept:
1.35 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
1.35 logistic.py(351):     return loss, grad.ravel(), p
1.35 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
1.35 logistic.py(339):     n_classes = Y.shape[1]
1.35 logistic.py(340):     n_features = X.shape[1]
1.35 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
1.35 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
1.36 logistic.py(343):                     dtype=X.dtype)
1.36 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
1.36 logistic.py(282):     n_classes = Y.shape[1]
1.36 logistic.py(283):     n_features = X.shape[1]
1.36 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
1.36 logistic.py(285):     w = w.reshape(n_classes, -1)
1.36 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
1.36 logistic.py(287):     if fit_intercept:
1.36 logistic.py(288):         intercept = w[:, -1]
1.36 logistic.py(289):         w = w[:, :-1]
1.36 logistic.py(292):     p = safe_sparse_dot(X, w.T)
1.36 logistic.py(293):     p += intercept
1.36 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
1.36 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
1.36 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
1.36 logistic.py(297):     p = np.exp(p, p)
1.36 logistic.py(298):     return loss, p, w
1.36 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
1.36 logistic.py(346):     diff = sample_weight * (p - Y)
1.36 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
1.36 logistic.py(348):     grad[:, :n_features] += alpha * w
1.36 logistic.py(349):     if fit_intercept:
1.36 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
1.36 logistic.py(351):     return loss, grad.ravel(), p
1.36 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
1.36 logistic.py(339):     n_classes = Y.shape[1]
1.36 logistic.py(340):     n_features = X.shape[1]
1.36 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
1.36 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
1.36 logistic.py(343):                     dtype=X.dtype)
1.36 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
1.36 logistic.py(282):     n_classes = Y.shape[1]
1.36 logistic.py(283):     n_features = X.shape[1]
1.36 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
1.36 logistic.py(285):     w = w.reshape(n_classes, -1)
1.36 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
1.36 logistic.py(287):     if fit_intercept:
1.36 logistic.py(288):         intercept = w[:, -1]
1.36 logistic.py(289):         w = w[:, :-1]
1.36 logistic.py(292):     p = safe_sparse_dot(X, w.T)
1.36 logistic.py(293):     p += intercept
1.36 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
1.36 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
1.36 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
1.36 logistic.py(297):     p = np.exp(p, p)
1.36 logistic.py(298):     return loss, p, w
1.36 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
1.36 logistic.py(346):     diff = sample_weight * (p - Y)
1.36 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
1.36 logistic.py(348):     grad[:, :n_features] += alpha * w
1.36 logistic.py(349):     if fit_intercept:
1.36 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
1.36 logistic.py(351):     return loss, grad.ravel(), p
1.36 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
1.36 logistic.py(339):     n_classes = Y.shape[1]
1.36 logistic.py(340):     n_features = X.shape[1]
1.36 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
1.36 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
1.36 logistic.py(343):                     dtype=X.dtype)
1.36 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
1.36 logistic.py(282):     n_classes = Y.shape[1]
1.36 logistic.py(283):     n_features = X.shape[1]
1.36 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
1.36 logistic.py(285):     w = w.reshape(n_classes, -1)
1.36 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
1.36 logistic.py(287):     if fit_intercept:
1.36 logistic.py(288):         intercept = w[:, -1]
1.36 logistic.py(289):         w = w[:, :-1]
1.36 logistic.py(292):     p = safe_sparse_dot(X, w.T)
1.36 logistic.py(293):     p += intercept
1.36 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
1.36 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
1.36 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
1.36 logistic.py(297):     p = np.exp(p, p)
1.36 logistic.py(298):     return loss, p, w
1.36 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
1.36 logistic.py(346):     diff = sample_weight * (p - Y)
1.36 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
1.36 logistic.py(348):     grad[:, :n_features] += alpha * w
1.36 logistic.py(349):     if fit_intercept:
1.36 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
1.36 logistic.py(351):     return loss, grad.ravel(), p
1.36 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
1.36 logistic.py(339):     n_classes = Y.shape[1]
1.36 logistic.py(340):     n_features = X.shape[1]
1.36 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
1.36 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
1.36 logistic.py(343):                     dtype=X.dtype)
1.36 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
1.36 logistic.py(282):     n_classes = Y.shape[1]
1.36 logistic.py(283):     n_features = X.shape[1]
1.36 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
1.36 logistic.py(285):     w = w.reshape(n_classes, -1)
1.36 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
1.36 logistic.py(287):     if fit_intercept:
1.36 logistic.py(288):         intercept = w[:, -1]
1.36 logistic.py(289):         w = w[:, :-1]
1.36 logistic.py(292):     p = safe_sparse_dot(X, w.T)
1.36 logistic.py(293):     p += intercept
1.36 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
1.36 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
1.36 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
1.36 logistic.py(297):     p = np.exp(p, p)
1.36 logistic.py(298):     return loss, p, w
1.36 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
1.36 logistic.py(346):     diff = sample_weight * (p - Y)
1.36 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
1.36 logistic.py(348):     grad[:, :n_features] += alpha * w
1.36 logistic.py(349):     if fit_intercept:
1.36 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
1.36 logistic.py(351):     return loss, grad.ravel(), p
1.36 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
1.36 logistic.py(339):     n_classes = Y.shape[1]
1.36 logistic.py(340):     n_features = X.shape[1]
1.36 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
1.36 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
1.36 logistic.py(343):                     dtype=X.dtype)
1.36 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
1.36 logistic.py(282):     n_classes = Y.shape[1]
1.36 logistic.py(283):     n_features = X.shape[1]
1.36 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
1.36 logistic.py(285):     w = w.reshape(n_classes, -1)
1.36 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
1.36 logistic.py(287):     if fit_intercept:
1.36 logistic.py(288):         intercept = w[:, -1]
1.36 logistic.py(289):         w = w[:, :-1]
1.36 logistic.py(292):     p = safe_sparse_dot(X, w.T)
1.36 logistic.py(293):     p += intercept
1.36 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
1.36 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
1.36 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
1.36 logistic.py(297):     p = np.exp(p, p)
1.36 logistic.py(298):     return loss, p, w
1.36 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
1.36 logistic.py(346):     diff = sample_weight * (p - Y)
1.36 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
1.36 logistic.py(348):     grad[:, :n_features] += alpha * w
1.36 logistic.py(349):     if fit_intercept:
1.36 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
1.36 logistic.py(351):     return loss, grad.ravel(), p
1.36 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
1.36 logistic.py(339):     n_classes = Y.shape[1]
1.36 logistic.py(340):     n_features = X.shape[1]
1.36 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
1.36 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
1.36 logistic.py(343):                     dtype=X.dtype)
1.36 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
1.36 logistic.py(282):     n_classes = Y.shape[1]
1.36 logistic.py(283):     n_features = X.shape[1]
1.36 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
1.36 logistic.py(285):     w = w.reshape(n_classes, -1)
1.36 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
1.36 logistic.py(287):     if fit_intercept:
1.36 logistic.py(288):         intercept = w[:, -1]
1.36 logistic.py(289):         w = w[:, :-1]
1.36 logistic.py(292):     p = safe_sparse_dot(X, w.T)
1.36 logistic.py(293):     p += intercept
1.36 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
1.36 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
1.36 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
1.36 logistic.py(297):     p = np.exp(p, p)
1.36 logistic.py(298):     return loss, p, w
1.36 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
1.36 logistic.py(346):     diff = sample_weight * (p - Y)
1.36 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
1.36 logistic.py(348):     grad[:, :n_features] += alpha * w
1.36 logistic.py(349):     if fit_intercept:
1.36 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
1.36 logistic.py(351):     return loss, grad.ravel(), p
1.36 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
1.36 logistic.py(339):     n_classes = Y.shape[1]
1.36 logistic.py(340):     n_features = X.shape[1]
1.36 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
1.36 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
1.36 logistic.py(343):                     dtype=X.dtype)
1.36 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
1.36 logistic.py(282):     n_classes = Y.shape[1]
1.36 logistic.py(283):     n_features = X.shape[1]
1.36 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
1.36 logistic.py(285):     w = w.reshape(n_classes, -1)
1.36 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
1.36 logistic.py(287):     if fit_intercept:
1.36 logistic.py(288):         intercept = w[:, -1]
1.36 logistic.py(289):         w = w[:, :-1]
1.36 logistic.py(292):     p = safe_sparse_dot(X, w.T)
1.36 logistic.py(293):     p += intercept
1.36 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
1.36 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
1.36 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
1.36 logistic.py(297):     p = np.exp(p, p)
1.36 logistic.py(298):     return loss, p, w
1.36 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
1.36 logistic.py(346):     diff = sample_weight * (p - Y)
1.36 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
1.36 logistic.py(348):     grad[:, :n_features] += alpha * w
1.36 logistic.py(349):     if fit_intercept:
1.36 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
1.36 logistic.py(351):     return loss, grad.ravel(), p
1.36 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
1.36 logistic.py(339):     n_classes = Y.shape[1]
1.36 logistic.py(340):     n_features = X.shape[1]
1.36 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
1.36 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
1.36 logistic.py(343):                     dtype=X.dtype)
1.36 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
1.36 logistic.py(282):     n_classes = Y.shape[1]
1.36 logistic.py(283):     n_features = X.shape[1]
1.36 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
1.36 logistic.py(285):     w = w.reshape(n_classes, -1)
1.36 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
1.36 logistic.py(287):     if fit_intercept:
1.36 logistic.py(288):         intercept = w[:, -1]
1.36 logistic.py(289):         w = w[:, :-1]
1.36 logistic.py(292):     p = safe_sparse_dot(X, w.T)
1.36 logistic.py(293):     p += intercept
1.36 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
1.36 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
1.36 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
1.36 logistic.py(297):     p = np.exp(p, p)
1.36 logistic.py(298):     return loss, p, w
1.36 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
1.36 logistic.py(346):     diff = sample_weight * (p - Y)
1.36 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
1.36 logistic.py(348):     grad[:, :n_features] += alpha * w
1.36 logistic.py(349):     if fit_intercept:
1.36 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
1.36 logistic.py(351):     return loss, grad.ravel(), p
1.36 logistic.py(718):             if info["warnflag"] == 1:
1.36 logistic.py(723):             n_iter_i = min(info['nit'], max_iter)
1.36 logistic.py(760):         if multi_class == 'multinomial':
1.36 logistic.py(761):             multi_w0 = np.reshape(w0, (classes.size, -1))
1.36 logistic.py(762):             if classes.size == 2:
1.36 logistic.py(764):             coefs.append(multi_w0)
1.36 logistic.py(768):         n_iter[i] = n_iter_i
1.36 logistic.py(712):     for i, C in enumerate(Cs):
1.36 logistic.py(713):         if solver == 'lbfgs':
1.36 logistic.py(714):             w0, loss, info = optimize.fmin_l_bfgs_b(
1.36 logistic.py(715):                 func, w0, fprime=None,
1.36 logistic.py(716):                 args=(X, target, 1. / C, sample_weight),
1.36 logistic.py(717):                 iprint=(verbose > 0) - 1, pgtol=tol, maxiter=max_iter)
1.36 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
1.36 logistic.py(339):     n_classes = Y.shape[1]
1.36 logistic.py(340):     n_features = X.shape[1]
1.36 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
1.36 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
1.36 logistic.py(343):                     dtype=X.dtype)
1.36 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
1.36 logistic.py(282):     n_classes = Y.shape[1]
1.36 logistic.py(283):     n_features = X.shape[1]
1.36 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
1.36 logistic.py(285):     w = w.reshape(n_classes, -1)
1.36 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
1.36 logistic.py(287):     if fit_intercept:
1.36 logistic.py(288):         intercept = w[:, -1]
1.36 logistic.py(289):         w = w[:, :-1]
1.36 logistic.py(292):     p = safe_sparse_dot(X, w.T)
1.36 logistic.py(293):     p += intercept
1.36 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
1.36 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
1.36 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
1.36 logistic.py(297):     p = np.exp(p, p)
1.36 logistic.py(298):     return loss, p, w
1.36 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
1.36 logistic.py(346):     diff = sample_weight * (p - Y)
1.36 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
1.36 logistic.py(348):     grad[:, :n_features] += alpha * w
1.36 logistic.py(349):     if fit_intercept:
1.36 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
1.36 logistic.py(351):     return loss, grad.ravel(), p
1.36 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
1.36 logistic.py(339):     n_classes = Y.shape[1]
1.36 logistic.py(340):     n_features = X.shape[1]
1.36 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
1.36 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
1.36 logistic.py(343):                     dtype=X.dtype)
1.36 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
1.36 logistic.py(282):     n_classes = Y.shape[1]
1.36 logistic.py(283):     n_features = X.shape[1]
1.36 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
1.36 logistic.py(285):     w = w.reshape(n_classes, -1)
1.36 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
1.36 logistic.py(287):     if fit_intercept:
1.36 logistic.py(288):         intercept = w[:, -1]
1.36 logistic.py(289):         w = w[:, :-1]
1.36 logistic.py(292):     p = safe_sparse_dot(X, w.T)
1.36 logistic.py(293):     p += intercept
1.36 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
1.36 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
1.36 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
1.36 logistic.py(297):     p = np.exp(p, p)
1.36 logistic.py(298):     return loss, p, w
1.36 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
1.36 logistic.py(346):     diff = sample_weight * (p - Y)
1.36 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
1.36 logistic.py(348):     grad[:, :n_features] += alpha * w
1.36 logistic.py(349):     if fit_intercept:
1.36 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
1.36 logistic.py(351):     return loss, grad.ravel(), p
1.36 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
1.36 logistic.py(339):     n_classes = Y.shape[1]
1.36 logistic.py(340):     n_features = X.shape[1]
1.36 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
1.36 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
1.36 logistic.py(343):                     dtype=X.dtype)
1.36 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
1.36 logistic.py(282):     n_classes = Y.shape[1]
1.36 logistic.py(283):     n_features = X.shape[1]
1.36 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
1.36 logistic.py(285):     w = w.reshape(n_classes, -1)
1.36 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
1.36 logistic.py(287):     if fit_intercept:
1.36 logistic.py(288):         intercept = w[:, -1]
1.36 logistic.py(289):         w = w[:, :-1]
1.36 logistic.py(292):     p = safe_sparse_dot(X, w.T)
1.36 logistic.py(293):     p += intercept
1.36 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
1.36 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
1.36 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
1.36 logistic.py(297):     p = np.exp(p, p)
1.36 logistic.py(298):     return loss, p, w
1.36 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
1.36 logistic.py(346):     diff = sample_weight * (p - Y)
1.36 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
1.36 logistic.py(348):     grad[:, :n_features] += alpha * w
1.36 logistic.py(349):     if fit_intercept:
1.36 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
1.36 logistic.py(351):     return loss, grad.ravel(), p
1.36 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
1.36 logistic.py(339):     n_classes = Y.shape[1]
1.36 logistic.py(340):     n_features = X.shape[1]
1.36 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
1.36 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
1.36 logistic.py(343):                     dtype=X.dtype)
1.36 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
1.36 logistic.py(282):     n_classes = Y.shape[1]
1.36 logistic.py(283):     n_features = X.shape[1]
1.36 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
1.36 logistic.py(285):     w = w.reshape(n_classes, -1)
1.36 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
1.36 logistic.py(287):     if fit_intercept:
1.36 logistic.py(288):         intercept = w[:, -1]
1.36 logistic.py(289):         w = w[:, :-1]
1.36 logistic.py(292):     p = safe_sparse_dot(X, w.T)
1.36 logistic.py(293):     p += intercept
1.36 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
1.36 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
1.36 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
1.36 logistic.py(297):     p = np.exp(p, p)
1.36 logistic.py(298):     return loss, p, w
1.36 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
1.36 logistic.py(346):     diff = sample_weight * (p - Y)
1.36 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
1.36 logistic.py(348):     grad[:, :n_features] += alpha * w
1.36 logistic.py(349):     if fit_intercept:
1.36 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
1.36 logistic.py(351):     return loss, grad.ravel(), p
1.36 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
1.36 logistic.py(339):     n_classes = Y.shape[1]
1.36 logistic.py(340):     n_features = X.shape[1]
1.36 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
1.36 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
1.36 logistic.py(343):                     dtype=X.dtype)
1.36 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
1.36 logistic.py(282):     n_classes = Y.shape[1]
1.36 logistic.py(283):     n_features = X.shape[1]
1.36 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
1.36 logistic.py(285):     w = w.reshape(n_classes, -1)
1.36 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
1.36 logistic.py(287):     if fit_intercept:
1.36 logistic.py(288):         intercept = w[:, -1]
1.36 logistic.py(289):         w = w[:, :-1]
1.36 logistic.py(292):     p = safe_sparse_dot(X, w.T)
1.36 logistic.py(293):     p += intercept
1.36 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
1.36 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
1.36 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
1.36 logistic.py(297):     p = np.exp(p, p)
1.36 logistic.py(298):     return loss, p, w
1.36 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
1.36 logistic.py(346):     diff = sample_weight * (p - Y)
1.36 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
1.36 logistic.py(348):     grad[:, :n_features] += alpha * w
1.36 logistic.py(349):     if fit_intercept:
1.36 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
1.36 logistic.py(351):     return loss, grad.ravel(), p
1.36 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
1.36 logistic.py(339):     n_classes = Y.shape[1]
1.36 logistic.py(340):     n_features = X.shape[1]
1.36 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
1.36 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
1.36 logistic.py(343):                     dtype=X.dtype)
1.36 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
1.36 logistic.py(282):     n_classes = Y.shape[1]
1.36 logistic.py(283):     n_features = X.shape[1]
1.36 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
1.36 logistic.py(285):     w = w.reshape(n_classes, -1)
1.36 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
1.36 logistic.py(287):     if fit_intercept:
1.36 logistic.py(288):         intercept = w[:, -1]
1.36 logistic.py(289):         w = w[:, :-1]
1.36 logistic.py(292):     p = safe_sparse_dot(X, w.T)
1.36 logistic.py(293):     p += intercept
1.36 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
1.36 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
1.36 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
1.36 logistic.py(297):     p = np.exp(p, p)
1.36 logistic.py(298):     return loss, p, w
1.36 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
1.36 logistic.py(346):     diff = sample_weight * (p - Y)
1.36 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
1.36 logistic.py(348):     grad[:, :n_features] += alpha * w
1.36 logistic.py(349):     if fit_intercept:
1.36 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
1.36 logistic.py(351):     return loss, grad.ravel(), p
1.36 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
1.36 logistic.py(339):     n_classes = Y.shape[1]
1.36 logistic.py(340):     n_features = X.shape[1]
1.36 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
1.36 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
1.36 logistic.py(343):                     dtype=X.dtype)
1.36 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
1.36 logistic.py(282):     n_classes = Y.shape[1]
1.36 logistic.py(283):     n_features = X.shape[1]
1.36 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
1.36 logistic.py(285):     w = w.reshape(n_classes, -1)
1.36 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
1.36 logistic.py(287):     if fit_intercept:
1.36 logistic.py(288):         intercept = w[:, -1]
1.36 logistic.py(289):         w = w[:, :-1]
1.36 logistic.py(292):     p = safe_sparse_dot(X, w.T)
1.36 logistic.py(293):     p += intercept
1.36 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
1.36 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
1.36 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
1.36 logistic.py(297):     p = np.exp(p, p)
1.36 logistic.py(298):     return loss, p, w
1.36 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
1.36 logistic.py(346):     diff = sample_weight * (p - Y)
1.36 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
1.36 logistic.py(348):     grad[:, :n_features] += alpha * w
1.36 logistic.py(349):     if fit_intercept:
1.36 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
1.36 logistic.py(351):     return loss, grad.ravel(), p
1.36 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
1.36 logistic.py(339):     n_classes = Y.shape[1]
1.36 logistic.py(340):     n_features = X.shape[1]
1.36 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
1.36 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
1.36 logistic.py(343):                     dtype=X.dtype)
1.36 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
1.36 logistic.py(282):     n_classes = Y.shape[1]
1.36 logistic.py(283):     n_features = X.shape[1]
1.36 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
1.36 logistic.py(285):     w = w.reshape(n_classes, -1)
1.36 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
1.36 logistic.py(287):     if fit_intercept:
1.36 logistic.py(288):         intercept = w[:, -1]
1.36 logistic.py(289):         w = w[:, :-1]
1.36 logistic.py(292):     p = safe_sparse_dot(X, w.T)
1.36 logistic.py(293):     p += intercept
1.36 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
1.36 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
1.36 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
1.36 logistic.py(297):     p = np.exp(p, p)
1.36 logistic.py(298):     return loss, p, w
1.36 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
1.36 logistic.py(346):     diff = sample_weight * (p - Y)
1.36 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
1.36 logistic.py(348):     grad[:, :n_features] += alpha * w
1.36 logistic.py(349):     if fit_intercept:
1.36 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
1.36 logistic.py(351):     return loss, grad.ravel(), p
1.36 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
1.36 logistic.py(339):     n_classes = Y.shape[1]
1.36 logistic.py(340):     n_features = X.shape[1]
1.36 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
1.36 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
1.36 logistic.py(343):                     dtype=X.dtype)
1.36 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
1.36 logistic.py(282):     n_classes = Y.shape[1]
1.36 logistic.py(283):     n_features = X.shape[1]
1.36 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
1.36 logistic.py(285):     w = w.reshape(n_classes, -1)
1.36 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
1.36 logistic.py(287):     if fit_intercept:
1.36 logistic.py(288):         intercept = w[:, -1]
1.37 logistic.py(289):         w = w[:, :-1]
1.37 logistic.py(292):     p = safe_sparse_dot(X, w.T)
1.37 logistic.py(293):     p += intercept
1.37 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
1.37 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
1.37 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
1.37 logistic.py(297):     p = np.exp(p, p)
1.37 logistic.py(298):     return loss, p, w
1.37 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
1.37 logistic.py(346):     diff = sample_weight * (p - Y)
1.37 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
1.37 logistic.py(348):     grad[:, :n_features] += alpha * w
1.37 logistic.py(349):     if fit_intercept:
1.37 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
1.37 logistic.py(351):     return loss, grad.ravel(), p
1.37 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
1.37 logistic.py(339):     n_classes = Y.shape[1]
1.37 logistic.py(340):     n_features = X.shape[1]
1.37 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
1.37 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
1.37 logistic.py(343):                     dtype=X.dtype)
1.37 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
1.37 logistic.py(282):     n_classes = Y.shape[1]
1.37 logistic.py(283):     n_features = X.shape[1]
1.37 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
1.37 logistic.py(285):     w = w.reshape(n_classes, -1)
1.37 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
1.37 logistic.py(287):     if fit_intercept:
1.37 logistic.py(288):         intercept = w[:, -1]
1.37 logistic.py(289):         w = w[:, :-1]
1.37 logistic.py(292):     p = safe_sparse_dot(X, w.T)
1.37 logistic.py(293):     p += intercept
1.37 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
1.37 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
1.37 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
1.37 logistic.py(297):     p = np.exp(p, p)
1.37 logistic.py(298):     return loss, p, w
1.37 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
1.37 logistic.py(346):     diff = sample_weight * (p - Y)
1.37 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
1.37 logistic.py(348):     grad[:, :n_features] += alpha * w
1.37 logistic.py(349):     if fit_intercept:
1.37 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
1.37 logistic.py(351):     return loss, grad.ravel(), p
1.37 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
1.37 logistic.py(339):     n_classes = Y.shape[1]
1.37 logistic.py(340):     n_features = X.shape[1]
1.37 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
1.37 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
1.37 logistic.py(343):                     dtype=X.dtype)
1.37 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
1.37 logistic.py(282):     n_classes = Y.shape[1]
1.37 logistic.py(283):     n_features = X.shape[1]
1.37 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
1.37 logistic.py(285):     w = w.reshape(n_classes, -1)
1.37 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
1.37 logistic.py(287):     if fit_intercept:
1.37 logistic.py(288):         intercept = w[:, -1]
1.37 logistic.py(289):         w = w[:, :-1]
1.37 logistic.py(292):     p = safe_sparse_dot(X, w.T)
1.37 logistic.py(293):     p += intercept
1.37 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
1.37 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
1.37 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
1.37 logistic.py(297):     p = np.exp(p, p)
1.37 logistic.py(298):     return loss, p, w
1.37 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
1.37 logistic.py(346):     diff = sample_weight * (p - Y)
1.37 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
1.37 logistic.py(348):     grad[:, :n_features] += alpha * w
1.37 logistic.py(349):     if fit_intercept:
1.37 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
1.37 logistic.py(351):     return loss, grad.ravel(), p
1.37 logistic.py(718):             if info["warnflag"] == 1:
1.37 logistic.py(723):             n_iter_i = min(info['nit'], max_iter)
1.37 logistic.py(760):         if multi_class == 'multinomial':
1.37 logistic.py(761):             multi_w0 = np.reshape(w0, (classes.size, -1))
1.37 logistic.py(762):             if classes.size == 2:
1.37 logistic.py(764):             coefs.append(multi_w0)
1.37 logistic.py(768):         n_iter[i] = n_iter_i
1.37 logistic.py(712):     for i, C in enumerate(Cs):
1.37 logistic.py(713):         if solver == 'lbfgs':
1.37 logistic.py(714):             w0, loss, info = optimize.fmin_l_bfgs_b(
1.37 logistic.py(715):                 func, w0, fprime=None,
1.37 logistic.py(716):                 args=(X, target, 1. / C, sample_weight),
1.37 logistic.py(717):                 iprint=(verbose > 0) - 1, pgtol=tol, maxiter=max_iter)
1.37 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
1.37 logistic.py(339):     n_classes = Y.shape[1]
1.37 logistic.py(340):     n_features = X.shape[1]
1.37 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
1.37 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
1.37 logistic.py(343):                     dtype=X.dtype)
1.37 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
1.37 logistic.py(282):     n_classes = Y.shape[1]
1.37 logistic.py(283):     n_features = X.shape[1]
1.37 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
1.37 logistic.py(285):     w = w.reshape(n_classes, -1)
1.37 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
1.37 logistic.py(287):     if fit_intercept:
1.37 logistic.py(288):         intercept = w[:, -1]
1.37 logistic.py(289):         w = w[:, :-1]
1.37 logistic.py(292):     p = safe_sparse_dot(X, w.T)
1.37 logistic.py(293):     p += intercept
1.37 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
1.37 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
1.37 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
1.37 logistic.py(297):     p = np.exp(p, p)
1.37 logistic.py(298):     return loss, p, w
1.37 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
1.37 logistic.py(346):     diff = sample_weight * (p - Y)
1.37 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
1.37 logistic.py(348):     grad[:, :n_features] += alpha * w
1.37 logistic.py(349):     if fit_intercept:
1.37 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
1.37 logistic.py(351):     return loss, grad.ravel(), p
1.37 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
1.37 logistic.py(339):     n_classes = Y.shape[1]
1.37 logistic.py(340):     n_features = X.shape[1]
1.37 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
1.37 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
1.37 logistic.py(343):                     dtype=X.dtype)
1.37 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
1.37 logistic.py(282):     n_classes = Y.shape[1]
1.37 logistic.py(283):     n_features = X.shape[1]
1.37 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
1.37 logistic.py(285):     w = w.reshape(n_classes, -1)
1.37 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
1.37 logistic.py(287):     if fit_intercept:
1.37 logistic.py(288):         intercept = w[:, -1]
1.37 logistic.py(289):         w = w[:, :-1]
1.37 logistic.py(292):     p = safe_sparse_dot(X, w.T)
1.37 logistic.py(293):     p += intercept
1.37 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
1.37 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
1.37 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
1.37 logistic.py(297):     p = np.exp(p, p)
1.37 logistic.py(298):     return loss, p, w
1.37 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
1.37 logistic.py(346):     diff = sample_weight * (p - Y)
1.37 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
1.37 logistic.py(348):     grad[:, :n_features] += alpha * w
1.37 logistic.py(349):     if fit_intercept:
1.37 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
1.37 logistic.py(351):     return loss, grad.ravel(), p
1.37 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
1.37 logistic.py(339):     n_classes = Y.shape[1]
1.37 logistic.py(340):     n_features = X.shape[1]
1.37 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
1.37 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
1.37 logistic.py(343):                     dtype=X.dtype)
1.37 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
1.37 logistic.py(282):     n_classes = Y.shape[1]
1.37 logistic.py(283):     n_features = X.shape[1]
1.37 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
1.37 logistic.py(285):     w = w.reshape(n_classes, -1)
1.37 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
1.37 logistic.py(287):     if fit_intercept:
1.37 logistic.py(288):         intercept = w[:, -1]
1.37 logistic.py(289):         w = w[:, :-1]
1.37 logistic.py(292):     p = safe_sparse_dot(X, w.T)
1.37 logistic.py(293):     p += intercept
1.37 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
1.37 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
1.37 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
1.37 logistic.py(297):     p = np.exp(p, p)
1.37 logistic.py(298):     return loss, p, w
1.37 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
1.37 logistic.py(346):     diff = sample_weight * (p - Y)
1.37 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
1.37 logistic.py(348):     grad[:, :n_features] += alpha * w
1.37 logistic.py(349):     if fit_intercept:
1.37 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
1.37 logistic.py(351):     return loss, grad.ravel(), p
1.37 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
1.37 logistic.py(339):     n_classes = Y.shape[1]
1.37 logistic.py(340):     n_features = X.shape[1]
1.37 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
1.37 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
1.37 logistic.py(343):                     dtype=X.dtype)
1.37 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
1.37 logistic.py(282):     n_classes = Y.shape[1]
1.37 logistic.py(283):     n_features = X.shape[1]
1.37 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
1.37 logistic.py(285):     w = w.reshape(n_classes, -1)
1.37 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
1.37 logistic.py(287):     if fit_intercept:
1.37 logistic.py(288):         intercept = w[:, -1]
1.37 logistic.py(289):         w = w[:, :-1]
1.37 logistic.py(292):     p = safe_sparse_dot(X, w.T)
1.37 logistic.py(293):     p += intercept
1.37 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
1.37 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
1.37 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
1.37 logistic.py(297):     p = np.exp(p, p)
1.37 logistic.py(298):     return loss, p, w
1.37 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
1.37 logistic.py(346):     diff = sample_weight * (p - Y)
1.37 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
1.37 logistic.py(348):     grad[:, :n_features] += alpha * w
1.37 logistic.py(349):     if fit_intercept:
1.37 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
1.37 logistic.py(351):     return loss, grad.ravel(), p
1.37 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
1.37 logistic.py(339):     n_classes = Y.shape[1]
1.37 logistic.py(340):     n_features = X.shape[1]
1.37 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
1.37 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
1.37 logistic.py(343):                     dtype=X.dtype)
1.37 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
1.37 logistic.py(282):     n_classes = Y.shape[1]
1.37 logistic.py(283):     n_features = X.shape[1]
1.37 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
1.37 logistic.py(285):     w = w.reshape(n_classes, -1)
1.37 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
1.37 logistic.py(287):     if fit_intercept:
1.37 logistic.py(288):         intercept = w[:, -1]
1.37 logistic.py(289):         w = w[:, :-1]
1.37 logistic.py(292):     p = safe_sparse_dot(X, w.T)
1.37 logistic.py(293):     p += intercept
1.37 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
1.37 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
1.37 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
1.37 logistic.py(297):     p = np.exp(p, p)
1.37 logistic.py(298):     return loss, p, w
1.37 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
1.37 logistic.py(346):     diff = sample_weight * (p - Y)
1.37 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
1.37 logistic.py(348):     grad[:, :n_features] += alpha * w
1.37 logistic.py(349):     if fit_intercept:
1.37 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
1.37 logistic.py(351):     return loss, grad.ravel(), p
1.37 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
1.37 logistic.py(339):     n_classes = Y.shape[1]
1.37 logistic.py(340):     n_features = X.shape[1]
1.37 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
1.37 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
1.37 logistic.py(343):                     dtype=X.dtype)
1.37 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
1.37 logistic.py(282):     n_classes = Y.shape[1]
1.37 logistic.py(283):     n_features = X.shape[1]
1.37 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
1.37 logistic.py(285):     w = w.reshape(n_classes, -1)
1.37 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
1.37 logistic.py(287):     if fit_intercept:
1.37 logistic.py(288):         intercept = w[:, -1]
1.37 logistic.py(289):         w = w[:, :-1]
1.37 logistic.py(292):     p = safe_sparse_dot(X, w.T)
1.37 logistic.py(293):     p += intercept
1.37 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
1.37 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
1.37 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
1.37 logistic.py(297):     p = np.exp(p, p)
1.37 logistic.py(298):     return loss, p, w
1.37 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
1.37 logistic.py(346):     diff = sample_weight * (p - Y)
1.37 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
1.37 logistic.py(348):     grad[:, :n_features] += alpha * w
1.37 logistic.py(349):     if fit_intercept:
1.37 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
1.37 logistic.py(351):     return loss, grad.ravel(), p
1.37 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
1.37 logistic.py(339):     n_classes = Y.shape[1]
1.37 logistic.py(340):     n_features = X.shape[1]
1.37 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
1.37 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
1.37 logistic.py(343):                     dtype=X.dtype)
1.37 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
1.37 logistic.py(282):     n_classes = Y.shape[1]
1.37 logistic.py(283):     n_features = X.shape[1]
1.37 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
1.37 logistic.py(285):     w = w.reshape(n_classes, -1)
1.37 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
1.37 logistic.py(287):     if fit_intercept:
1.37 logistic.py(288):         intercept = w[:, -1]
1.37 logistic.py(289):         w = w[:, :-1]
1.37 logistic.py(292):     p = safe_sparse_dot(X, w.T)
1.37 logistic.py(293):     p += intercept
1.37 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
1.37 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
1.37 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
1.37 logistic.py(297):     p = np.exp(p, p)
1.37 logistic.py(298):     return loss, p, w
1.37 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
1.37 logistic.py(346):     diff = sample_weight * (p - Y)
1.37 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
1.37 logistic.py(348):     grad[:, :n_features] += alpha * w
1.37 logistic.py(349):     if fit_intercept:
1.37 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
1.37 logistic.py(351):     return loss, grad.ravel(), p
1.37 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
1.37 logistic.py(339):     n_classes = Y.shape[1]
1.37 logistic.py(340):     n_features = X.shape[1]
1.37 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
1.37 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
1.37 logistic.py(343):                     dtype=X.dtype)
1.37 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
1.37 logistic.py(282):     n_classes = Y.shape[1]
1.37 logistic.py(283):     n_features = X.shape[1]
1.37 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
1.37 logistic.py(285):     w = w.reshape(n_classes, -1)
1.37 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
1.37 logistic.py(287):     if fit_intercept:
1.37 logistic.py(288):         intercept = w[:, -1]
1.37 logistic.py(289):         w = w[:, :-1]
1.37 logistic.py(292):     p = safe_sparse_dot(X, w.T)
1.37 logistic.py(293):     p += intercept
1.37 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
1.37 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
1.37 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
1.37 logistic.py(297):     p = np.exp(p, p)
1.37 logistic.py(298):     return loss, p, w
1.37 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
1.37 logistic.py(346):     diff = sample_weight * (p - Y)
1.37 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
1.37 logistic.py(348):     grad[:, :n_features] += alpha * w
1.37 logistic.py(349):     if fit_intercept:
1.37 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
1.37 logistic.py(351):     return loss, grad.ravel(), p
1.37 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
1.37 logistic.py(339):     n_classes = Y.shape[1]
1.37 logistic.py(340):     n_features = X.shape[1]
1.37 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
1.37 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
1.37 logistic.py(343):                     dtype=X.dtype)
1.37 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
1.37 logistic.py(282):     n_classes = Y.shape[1]
1.37 logistic.py(283):     n_features = X.shape[1]
1.37 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
1.37 logistic.py(285):     w = w.reshape(n_classes, -1)
1.37 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
1.37 logistic.py(287):     if fit_intercept:
1.37 logistic.py(288):         intercept = w[:, -1]
1.37 logistic.py(289):         w = w[:, :-1]
1.37 logistic.py(292):     p = safe_sparse_dot(X, w.T)
1.37 logistic.py(293):     p += intercept
1.37 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
1.37 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
1.37 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
1.37 logistic.py(297):     p = np.exp(p, p)
1.37 logistic.py(298):     return loss, p, w
1.37 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
1.37 logistic.py(346):     diff = sample_weight * (p - Y)
1.37 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
1.37 logistic.py(348):     grad[:, :n_features] += alpha * w
1.37 logistic.py(349):     if fit_intercept:
1.37 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
1.37 logistic.py(351):     return loss, grad.ravel(), p
1.37 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
1.37 logistic.py(339):     n_classes = Y.shape[1]
1.37 logistic.py(340):     n_features = X.shape[1]
1.37 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
1.37 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
1.37 logistic.py(343):                     dtype=X.dtype)
1.37 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
1.37 logistic.py(282):     n_classes = Y.shape[1]
1.37 logistic.py(283):     n_features = X.shape[1]
1.37 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
1.37 logistic.py(285):     w = w.reshape(n_classes, -1)
1.37 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
1.37 logistic.py(287):     if fit_intercept:
1.37 logistic.py(288):         intercept = w[:, -1]
1.37 logistic.py(289):         w = w[:, :-1]
1.37 logistic.py(292):     p = safe_sparse_dot(X, w.T)
1.37 logistic.py(293):     p += intercept
1.37 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
1.37 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
1.37 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
1.37 logistic.py(297):     p = np.exp(p, p)
1.37 logistic.py(298):     return loss, p, w
1.37 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
1.37 logistic.py(346):     diff = sample_weight * (p - Y)
1.37 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
1.37 logistic.py(348):     grad[:, :n_features] += alpha * w
1.37 logistic.py(349):     if fit_intercept:
1.37 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
1.37 logistic.py(351):     return loss, grad.ravel(), p
1.37 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
1.37 logistic.py(339):     n_classes = Y.shape[1]
1.37 logistic.py(340):     n_features = X.shape[1]
1.37 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
1.37 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
1.37 logistic.py(343):                     dtype=X.dtype)
1.37 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
1.37 logistic.py(282):     n_classes = Y.shape[1]
1.37 logistic.py(283):     n_features = X.shape[1]
1.37 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
1.37 logistic.py(285):     w = w.reshape(n_classes, -1)
1.37 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
1.37 logistic.py(287):     if fit_intercept:
1.37 logistic.py(288):         intercept = w[:, -1]
1.37 logistic.py(289):         w = w[:, :-1]
1.37 logistic.py(292):     p = safe_sparse_dot(X, w.T)
1.37 logistic.py(293):     p += intercept
1.37 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
1.37 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
1.37 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
1.37 logistic.py(297):     p = np.exp(p, p)
1.37 logistic.py(298):     return loss, p, w
1.37 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
1.37 logistic.py(346):     diff = sample_weight * (p - Y)
1.37 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
1.37 logistic.py(348):     grad[:, :n_features] += alpha * w
1.37 logistic.py(349):     if fit_intercept:
1.37 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
1.37 logistic.py(351):     return loss, grad.ravel(), p
1.37 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
1.37 logistic.py(339):     n_classes = Y.shape[1]
1.37 logistic.py(340):     n_features = X.shape[1]
1.37 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
1.37 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
1.37 logistic.py(343):                     dtype=X.dtype)
1.37 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
1.37 logistic.py(282):     n_classes = Y.shape[1]
1.37 logistic.py(283):     n_features = X.shape[1]
1.37 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
1.37 logistic.py(285):     w = w.reshape(n_classes, -1)
1.37 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
1.37 logistic.py(287):     if fit_intercept:
1.37 logistic.py(288):         intercept = w[:, -1]
1.37 logistic.py(289):         w = w[:, :-1]
1.37 logistic.py(292):     p = safe_sparse_dot(X, w.T)
1.37 logistic.py(293):     p += intercept
1.37 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
1.37 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
1.37 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
1.37 logistic.py(297):     p = np.exp(p, p)
1.37 logistic.py(298):     return loss, p, w
1.37 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
1.37 logistic.py(346):     diff = sample_weight * (p - Y)
1.37 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
1.37 logistic.py(348):     grad[:, :n_features] += alpha * w
1.37 logistic.py(349):     if fit_intercept:
1.37 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
1.37 logistic.py(351):     return loss, grad.ravel(), p
1.37 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
1.37 logistic.py(339):     n_classes = Y.shape[1]
1.37 logistic.py(340):     n_features = X.shape[1]
1.37 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
1.37 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
1.37 logistic.py(343):                     dtype=X.dtype)
1.37 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
1.37 logistic.py(282):     n_classes = Y.shape[1]
1.37 logistic.py(283):     n_features = X.shape[1]
1.37 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
1.37 logistic.py(285):     w = w.reshape(n_classes, -1)
1.37 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
1.37 logistic.py(287):     if fit_intercept:
1.37 logistic.py(288):         intercept = w[:, -1]
1.37 logistic.py(289):         w = w[:, :-1]
1.37 logistic.py(292):     p = safe_sparse_dot(X, w.T)
1.37 logistic.py(293):     p += intercept
1.37 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
1.37 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
1.37 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
1.37 logistic.py(297):     p = np.exp(p, p)
1.37 logistic.py(298):     return loss, p, w
1.37 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
1.37 logistic.py(346):     diff = sample_weight * (p - Y)
1.37 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
1.37 logistic.py(348):     grad[:, :n_features] += alpha * w
1.37 logistic.py(349):     if fit_intercept:
1.37 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
1.37 logistic.py(351):     return loss, grad.ravel(), p
1.37 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
1.37 logistic.py(339):     n_classes = Y.shape[1]
1.37 logistic.py(340):     n_features = X.shape[1]
1.37 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
1.37 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
1.37 logistic.py(343):                     dtype=X.dtype)
1.37 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
1.37 logistic.py(282):     n_classes = Y.shape[1]
1.37 logistic.py(283):     n_features = X.shape[1]
1.37 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
1.37 logistic.py(285):     w = w.reshape(n_classes, -1)
1.37 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
1.37 logistic.py(287):     if fit_intercept:
1.37 logistic.py(288):         intercept = w[:, -1]
1.37 logistic.py(289):         w = w[:, :-1]
1.37 logistic.py(292):     p = safe_sparse_dot(X, w.T)
1.37 logistic.py(293):     p += intercept
1.37 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
1.37 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
1.38 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
1.38 logistic.py(297):     p = np.exp(p, p)
1.38 logistic.py(298):     return loss, p, w
1.38 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
1.38 logistic.py(346):     diff = sample_weight * (p - Y)
1.38 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
1.38 logistic.py(348):     grad[:, :n_features] += alpha * w
1.38 logistic.py(349):     if fit_intercept:
1.38 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
1.38 logistic.py(351):     return loss, grad.ravel(), p
1.38 logistic.py(718):             if info["warnflag"] == 1:
1.38 logistic.py(723):             n_iter_i = min(info['nit'], max_iter)
1.38 logistic.py(760):         if multi_class == 'multinomial':
1.38 logistic.py(761):             multi_w0 = np.reshape(w0, (classes.size, -1))
1.38 logistic.py(762):             if classes.size == 2:
1.38 logistic.py(764):             coefs.append(multi_w0)
1.38 logistic.py(768):         n_iter[i] = n_iter_i
1.38 logistic.py(712):     for i, C in enumerate(Cs):
1.38 logistic.py(713):         if solver == 'lbfgs':
1.38 logistic.py(714):             w0, loss, info = optimize.fmin_l_bfgs_b(
1.38 logistic.py(715):                 func, w0, fprime=None,
1.38 logistic.py(716):                 args=(X, target, 1. / C, sample_weight),
1.38 logistic.py(717):                 iprint=(verbose > 0) - 1, pgtol=tol, maxiter=max_iter)
1.38 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
1.38 logistic.py(339):     n_classes = Y.shape[1]
1.38 logistic.py(340):     n_features = X.shape[1]
1.38 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
1.38 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
1.38 logistic.py(343):                     dtype=X.dtype)
1.38 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
1.38 logistic.py(282):     n_classes = Y.shape[1]
1.38 logistic.py(283):     n_features = X.shape[1]
1.38 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
1.38 logistic.py(285):     w = w.reshape(n_classes, -1)
1.38 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
1.38 logistic.py(287):     if fit_intercept:
1.38 logistic.py(288):         intercept = w[:, -1]
1.38 logistic.py(289):         w = w[:, :-1]
1.38 logistic.py(292):     p = safe_sparse_dot(X, w.T)
1.38 logistic.py(293):     p += intercept
1.38 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
1.38 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
1.38 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
1.38 logistic.py(297):     p = np.exp(p, p)
1.38 logistic.py(298):     return loss, p, w
1.38 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
1.38 logistic.py(346):     diff = sample_weight * (p - Y)
1.38 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
1.38 logistic.py(348):     grad[:, :n_features] += alpha * w
1.38 logistic.py(349):     if fit_intercept:
1.38 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
1.38 logistic.py(351):     return loss, grad.ravel(), p
1.38 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
1.38 logistic.py(339):     n_classes = Y.shape[1]
1.38 logistic.py(340):     n_features = X.shape[1]
1.38 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
1.38 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
1.38 logistic.py(343):                     dtype=X.dtype)
1.38 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
1.38 logistic.py(282):     n_classes = Y.shape[1]
1.38 logistic.py(283):     n_features = X.shape[1]
1.38 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
1.38 logistic.py(285):     w = w.reshape(n_classes, -1)
1.38 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
1.38 logistic.py(287):     if fit_intercept:
1.38 logistic.py(288):         intercept = w[:, -1]
1.38 logistic.py(289):         w = w[:, :-1]
1.38 logistic.py(292):     p = safe_sparse_dot(X, w.T)
1.38 logistic.py(293):     p += intercept
1.38 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
1.38 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
1.38 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
1.38 logistic.py(297):     p = np.exp(p, p)
1.38 logistic.py(298):     return loss, p, w
1.38 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
1.38 logistic.py(346):     diff = sample_weight * (p - Y)
1.38 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
1.38 logistic.py(348):     grad[:, :n_features] += alpha * w
1.38 logistic.py(349):     if fit_intercept:
1.38 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
1.38 logistic.py(351):     return loss, grad.ravel(), p
1.38 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
1.38 logistic.py(339):     n_classes = Y.shape[1]
1.38 logistic.py(340):     n_features = X.shape[1]
1.38 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
1.38 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
1.38 logistic.py(343):                     dtype=X.dtype)
1.38 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
1.38 logistic.py(282):     n_classes = Y.shape[1]
1.38 logistic.py(283):     n_features = X.shape[1]
1.38 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
1.38 logistic.py(285):     w = w.reshape(n_classes, -1)
1.38 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
1.38 logistic.py(287):     if fit_intercept:
1.38 logistic.py(288):         intercept = w[:, -1]
1.38 logistic.py(289):         w = w[:, :-1]
1.38 logistic.py(292):     p = safe_sparse_dot(X, w.T)
1.38 logistic.py(293):     p += intercept
1.38 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
1.38 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
1.38 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
1.38 logistic.py(297):     p = np.exp(p, p)
1.38 logistic.py(298):     return loss, p, w
1.38 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
1.38 logistic.py(346):     diff = sample_weight * (p - Y)
1.38 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
1.38 logistic.py(348):     grad[:, :n_features] += alpha * w
1.38 logistic.py(349):     if fit_intercept:
1.38 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
1.38 logistic.py(351):     return loss, grad.ravel(), p
1.38 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
1.38 logistic.py(339):     n_classes = Y.shape[1]
1.38 logistic.py(340):     n_features = X.shape[1]
1.38 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
1.38 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
1.38 logistic.py(343):                     dtype=X.dtype)
1.38 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
1.38 logistic.py(282):     n_classes = Y.shape[1]
1.38 logistic.py(283):     n_features = X.shape[1]
1.38 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
1.38 logistic.py(285):     w = w.reshape(n_classes, -1)
1.38 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
1.38 logistic.py(287):     if fit_intercept:
1.38 logistic.py(288):         intercept = w[:, -1]
1.38 logistic.py(289):         w = w[:, :-1]
1.38 logistic.py(292):     p = safe_sparse_dot(X, w.T)
1.38 logistic.py(293):     p += intercept
1.38 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
1.38 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
1.38 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
1.38 logistic.py(297):     p = np.exp(p, p)
1.38 logistic.py(298):     return loss, p, w
1.38 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
1.38 logistic.py(346):     diff = sample_weight * (p - Y)
1.38 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
1.38 logistic.py(348):     grad[:, :n_features] += alpha * w
1.38 logistic.py(349):     if fit_intercept:
1.38 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
1.38 logistic.py(351):     return loss, grad.ravel(), p
1.38 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
1.38 logistic.py(339):     n_classes = Y.shape[1]
1.38 logistic.py(340):     n_features = X.shape[1]
1.38 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
1.38 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
1.38 logistic.py(343):                     dtype=X.dtype)
1.38 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
1.38 logistic.py(282):     n_classes = Y.shape[1]
1.38 logistic.py(283):     n_features = X.shape[1]
1.38 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
1.38 logistic.py(285):     w = w.reshape(n_classes, -1)
1.38 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
1.38 logistic.py(287):     if fit_intercept:
1.38 logistic.py(288):         intercept = w[:, -1]
1.38 logistic.py(289):         w = w[:, :-1]
1.38 logistic.py(292):     p = safe_sparse_dot(X, w.T)
1.38 logistic.py(293):     p += intercept
1.38 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
1.38 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
1.38 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
1.38 logistic.py(297):     p = np.exp(p, p)
1.38 logistic.py(298):     return loss, p, w
1.38 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
1.38 logistic.py(346):     diff = sample_weight * (p - Y)
1.38 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
1.38 logistic.py(348):     grad[:, :n_features] += alpha * w
1.38 logistic.py(349):     if fit_intercept:
1.38 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
1.38 logistic.py(351):     return loss, grad.ravel(), p
1.38 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
1.38 logistic.py(339):     n_classes = Y.shape[1]
1.38 logistic.py(340):     n_features = X.shape[1]
1.38 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
1.38 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
1.38 logistic.py(343):                     dtype=X.dtype)
1.38 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
1.38 logistic.py(282):     n_classes = Y.shape[1]
1.38 logistic.py(283):     n_features = X.shape[1]
1.38 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
1.38 logistic.py(285):     w = w.reshape(n_classes, -1)
1.38 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
1.38 logistic.py(287):     if fit_intercept:
1.38 logistic.py(288):         intercept = w[:, -1]
1.38 logistic.py(289):         w = w[:, :-1]
1.38 logistic.py(292):     p = safe_sparse_dot(X, w.T)
1.38 logistic.py(293):     p += intercept
1.38 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
1.38 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
1.38 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
1.38 logistic.py(297):     p = np.exp(p, p)
1.38 logistic.py(298):     return loss, p, w
1.38 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
1.38 logistic.py(346):     diff = sample_weight * (p - Y)
1.38 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
1.38 logistic.py(348):     grad[:, :n_features] += alpha * w
1.38 logistic.py(349):     if fit_intercept:
1.38 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
1.38 logistic.py(351):     return loss, grad.ravel(), p
1.38 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
1.38 logistic.py(339):     n_classes = Y.shape[1]
1.38 logistic.py(340):     n_features = X.shape[1]
1.38 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
1.38 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
1.38 logistic.py(343):                     dtype=X.dtype)
1.38 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
1.38 logistic.py(282):     n_classes = Y.shape[1]
1.38 logistic.py(283):     n_features = X.shape[1]
1.38 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
1.38 logistic.py(285):     w = w.reshape(n_classes, -1)
1.38 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
1.38 logistic.py(287):     if fit_intercept:
1.38 logistic.py(288):         intercept = w[:, -1]
1.38 logistic.py(289):         w = w[:, :-1]
1.38 logistic.py(292):     p = safe_sparse_dot(X, w.T)
1.38 logistic.py(293):     p += intercept
1.38 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
1.38 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
1.38 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
1.38 logistic.py(297):     p = np.exp(p, p)
1.38 logistic.py(298):     return loss, p, w
1.38 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
1.38 logistic.py(346):     diff = sample_weight * (p - Y)
1.38 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
1.38 logistic.py(348):     grad[:, :n_features] += alpha * w
1.38 logistic.py(349):     if fit_intercept:
1.38 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
1.38 logistic.py(351):     return loss, grad.ravel(), p
1.38 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
1.38 logistic.py(339):     n_classes = Y.shape[1]
1.38 logistic.py(340):     n_features = X.shape[1]
1.38 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
1.38 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
1.38 logistic.py(343):                     dtype=X.dtype)
1.38 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
1.38 logistic.py(282):     n_classes = Y.shape[1]
1.38 logistic.py(283):     n_features = X.shape[1]
1.38 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
1.38 logistic.py(285):     w = w.reshape(n_classes, -1)
1.38 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
1.38 logistic.py(287):     if fit_intercept:
1.38 logistic.py(288):         intercept = w[:, -1]
1.38 logistic.py(289):         w = w[:, :-1]
1.38 logistic.py(292):     p = safe_sparse_dot(X, w.T)
1.38 logistic.py(293):     p += intercept
1.38 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
1.38 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
1.38 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
1.38 logistic.py(297):     p = np.exp(p, p)
1.38 logistic.py(298):     return loss, p, w
1.38 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
1.38 logistic.py(346):     diff = sample_weight * (p - Y)
1.38 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
1.38 logistic.py(348):     grad[:, :n_features] += alpha * w
1.38 logistic.py(349):     if fit_intercept:
1.38 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
1.38 logistic.py(351):     return loss, grad.ravel(), p
1.38 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
1.38 logistic.py(339):     n_classes = Y.shape[1]
1.38 logistic.py(340):     n_features = X.shape[1]
1.38 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
1.38 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
1.38 logistic.py(343):                     dtype=X.dtype)
1.38 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
1.38 logistic.py(282):     n_classes = Y.shape[1]
1.38 logistic.py(283):     n_features = X.shape[1]
1.38 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
1.38 logistic.py(285):     w = w.reshape(n_classes, -1)
1.38 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
1.38 logistic.py(287):     if fit_intercept:
1.38 logistic.py(288):         intercept = w[:, -1]
1.38 logistic.py(289):         w = w[:, :-1]
1.38 logistic.py(292):     p = safe_sparse_dot(X, w.T)
1.38 logistic.py(293):     p += intercept
1.38 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
1.38 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
1.38 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
1.38 logistic.py(297):     p = np.exp(p, p)
1.38 logistic.py(298):     return loss, p, w
1.38 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
1.38 logistic.py(346):     diff = sample_weight * (p - Y)
1.38 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
1.38 logistic.py(348):     grad[:, :n_features] += alpha * w
1.38 logistic.py(349):     if fit_intercept:
1.38 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
1.38 logistic.py(351):     return loss, grad.ravel(), p
1.38 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
1.38 logistic.py(339):     n_classes = Y.shape[1]
1.38 logistic.py(340):     n_features = X.shape[1]
1.38 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
1.38 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
1.38 logistic.py(343):                     dtype=X.dtype)
1.38 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
1.38 logistic.py(282):     n_classes = Y.shape[1]
1.38 logistic.py(283):     n_features = X.shape[1]
1.38 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
1.38 logistic.py(285):     w = w.reshape(n_classes, -1)
1.38 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
1.38 logistic.py(287):     if fit_intercept:
1.38 logistic.py(288):         intercept = w[:, -1]
1.38 logistic.py(289):         w = w[:, :-1]
1.38 logistic.py(292):     p = safe_sparse_dot(X, w.T)
1.38 logistic.py(293):     p += intercept
1.38 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
1.38 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
1.38 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
1.38 logistic.py(297):     p = np.exp(p, p)
1.38 logistic.py(298):     return loss, p, w
1.38 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
1.38 logistic.py(346):     diff = sample_weight * (p - Y)
1.38 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
1.38 logistic.py(348):     grad[:, :n_features] += alpha * w
1.38 logistic.py(349):     if fit_intercept:
1.38 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
1.38 logistic.py(351):     return loss, grad.ravel(), p
1.38 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
1.38 logistic.py(339):     n_classes = Y.shape[1]
1.38 logistic.py(340):     n_features = X.shape[1]
1.38 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
1.38 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
1.38 logistic.py(343):                     dtype=X.dtype)
1.38 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
1.38 logistic.py(282):     n_classes = Y.shape[1]
1.38 logistic.py(283):     n_features = X.shape[1]
1.38 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
1.38 logistic.py(285):     w = w.reshape(n_classes, -1)
1.38 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
1.38 logistic.py(287):     if fit_intercept:
1.38 logistic.py(288):         intercept = w[:, -1]
1.38 logistic.py(289):         w = w[:, :-1]
1.38 logistic.py(292):     p = safe_sparse_dot(X, w.T)
1.38 logistic.py(293):     p += intercept
1.38 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
1.38 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
1.38 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
1.38 logistic.py(297):     p = np.exp(p, p)
1.38 logistic.py(298):     return loss, p, w
1.38 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
1.38 logistic.py(346):     diff = sample_weight * (p - Y)
1.38 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
1.38 logistic.py(348):     grad[:, :n_features] += alpha * w
1.38 logistic.py(349):     if fit_intercept:
1.38 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
1.38 logistic.py(351):     return loss, grad.ravel(), p
1.38 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
1.38 logistic.py(339):     n_classes = Y.shape[1]
1.38 logistic.py(340):     n_features = X.shape[1]
1.38 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
1.38 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
1.38 logistic.py(343):                     dtype=X.dtype)
1.38 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
1.38 logistic.py(282):     n_classes = Y.shape[1]
1.38 logistic.py(283):     n_features = X.shape[1]
1.38 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
1.38 logistic.py(285):     w = w.reshape(n_classes, -1)
1.38 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
1.38 logistic.py(287):     if fit_intercept:
1.38 logistic.py(288):         intercept = w[:, -1]
1.38 logistic.py(289):         w = w[:, :-1]
1.38 logistic.py(292):     p = safe_sparse_dot(X, w.T)
1.38 logistic.py(293):     p += intercept
1.38 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
1.38 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
1.38 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
1.38 logistic.py(297):     p = np.exp(p, p)
1.38 logistic.py(298):     return loss, p, w
1.38 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
1.38 logistic.py(346):     diff = sample_weight * (p - Y)
1.38 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
1.38 logistic.py(348):     grad[:, :n_features] += alpha * w
1.38 logistic.py(349):     if fit_intercept:
1.38 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
1.38 logistic.py(351):     return loss, grad.ravel(), p
1.38 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
1.38 logistic.py(339):     n_classes = Y.shape[1]
1.38 logistic.py(340):     n_features = X.shape[1]
1.38 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
1.38 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
1.38 logistic.py(343):                     dtype=X.dtype)
1.38 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
1.38 logistic.py(282):     n_classes = Y.shape[1]
1.38 logistic.py(283):     n_features = X.shape[1]
1.38 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
1.38 logistic.py(285):     w = w.reshape(n_classes, -1)
1.38 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
1.38 logistic.py(287):     if fit_intercept:
1.38 logistic.py(288):         intercept = w[:, -1]
1.38 logistic.py(289):         w = w[:, :-1]
1.38 logistic.py(292):     p = safe_sparse_dot(X, w.T)
1.38 logistic.py(293):     p += intercept
1.38 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
1.38 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
1.38 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
1.38 logistic.py(297):     p = np.exp(p, p)
1.38 logistic.py(298):     return loss, p, w
1.38 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
1.38 logistic.py(346):     diff = sample_weight * (p - Y)
1.38 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
1.38 logistic.py(348):     grad[:, :n_features] += alpha * w
1.38 logistic.py(349):     if fit_intercept:
1.38 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
1.38 logistic.py(351):     return loss, grad.ravel(), p
1.38 logistic.py(718):             if info["warnflag"] == 1:
1.38 logistic.py(723):             n_iter_i = min(info['nit'], max_iter)
1.38 logistic.py(760):         if multi_class == 'multinomial':
1.38 logistic.py(761):             multi_w0 = np.reshape(w0, (classes.size, -1))
1.38 logistic.py(762):             if classes.size == 2:
1.38 logistic.py(764):             coefs.append(multi_w0)
1.38 logistic.py(768):         n_iter[i] = n_iter_i
1.38 logistic.py(712):     for i, C in enumerate(Cs):
1.38 logistic.py(713):         if solver == 'lbfgs':
1.38 logistic.py(714):             w0, loss, info = optimize.fmin_l_bfgs_b(
1.38 logistic.py(715):                 func, w0, fprime=None,
1.38 logistic.py(716):                 args=(X, target, 1. / C, sample_weight),
1.38 logistic.py(717):                 iprint=(verbose > 0) - 1, pgtol=tol, maxiter=max_iter)
1.38 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
1.38 logistic.py(339):     n_classes = Y.shape[1]
1.38 logistic.py(340):     n_features = X.shape[1]
1.38 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
1.38 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
1.38 logistic.py(343):                     dtype=X.dtype)
1.38 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
1.38 logistic.py(282):     n_classes = Y.shape[1]
1.38 logistic.py(283):     n_features = X.shape[1]
1.38 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
1.38 logistic.py(285):     w = w.reshape(n_classes, -1)
1.38 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
1.38 logistic.py(287):     if fit_intercept:
1.38 logistic.py(288):         intercept = w[:, -1]
1.38 logistic.py(289):         w = w[:, :-1]
1.38 logistic.py(292):     p = safe_sparse_dot(X, w.T)
1.38 logistic.py(293):     p += intercept
1.38 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
1.38 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
1.38 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
1.38 logistic.py(297):     p = np.exp(p, p)
1.38 logistic.py(298):     return loss, p, w
1.38 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
1.38 logistic.py(346):     diff = sample_weight * (p - Y)
1.38 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
1.38 logistic.py(348):     grad[:, :n_features] += alpha * w
1.38 logistic.py(349):     if fit_intercept:
1.38 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
1.38 logistic.py(351):     return loss, grad.ravel(), p
1.38 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
1.38 logistic.py(339):     n_classes = Y.shape[1]
1.38 logistic.py(340):     n_features = X.shape[1]
1.38 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
1.38 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
1.38 logistic.py(343):                     dtype=X.dtype)
1.38 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
1.38 logistic.py(282):     n_classes = Y.shape[1]
1.38 logistic.py(283):     n_features = X.shape[1]
1.38 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
1.38 logistic.py(285):     w = w.reshape(n_classes, -1)
1.38 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
1.38 logistic.py(287):     if fit_intercept:
1.38 logistic.py(288):         intercept = w[:, -1]
1.38 logistic.py(289):         w = w[:, :-1]
1.38 logistic.py(292):     p = safe_sparse_dot(X, w.T)
1.38 logistic.py(293):     p += intercept
1.38 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
1.38 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
1.39 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
1.39 logistic.py(297):     p = np.exp(p, p)
1.39 logistic.py(298):     return loss, p, w
1.39 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
1.39 logistic.py(346):     diff = sample_weight * (p - Y)
1.39 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
1.39 logistic.py(348):     grad[:, :n_features] += alpha * w
1.39 logistic.py(349):     if fit_intercept:
1.39 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
1.39 logistic.py(351):     return loss, grad.ravel(), p
1.39 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
1.39 logistic.py(339):     n_classes = Y.shape[1]
1.39 logistic.py(340):     n_features = X.shape[1]
1.39 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
1.39 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
1.39 logistic.py(343):                     dtype=X.dtype)
1.39 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
1.39 logistic.py(282):     n_classes = Y.shape[1]
1.39 logistic.py(283):     n_features = X.shape[1]
1.39 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
1.39 logistic.py(285):     w = w.reshape(n_classes, -1)
1.39 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
1.39 logistic.py(287):     if fit_intercept:
1.39 logistic.py(288):         intercept = w[:, -1]
1.39 logistic.py(289):         w = w[:, :-1]
1.39 logistic.py(292):     p = safe_sparse_dot(X, w.T)
1.39 logistic.py(293):     p += intercept
1.39 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
1.39 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
1.39 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
1.39 logistic.py(297):     p = np.exp(p, p)
1.39 logistic.py(298):     return loss, p, w
1.39 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
1.39 logistic.py(346):     diff = sample_weight * (p - Y)
1.39 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
1.39 logistic.py(348):     grad[:, :n_features] += alpha * w
1.39 logistic.py(349):     if fit_intercept:
1.39 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
1.39 logistic.py(351):     return loss, grad.ravel(), p
1.39 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
1.39 logistic.py(339):     n_classes = Y.shape[1]
1.39 logistic.py(340):     n_features = X.shape[1]
1.39 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
1.39 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
1.39 logistic.py(343):                     dtype=X.dtype)
1.39 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
1.39 logistic.py(282):     n_classes = Y.shape[1]
1.39 logistic.py(283):     n_features = X.shape[1]
1.39 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
1.39 logistic.py(285):     w = w.reshape(n_classes, -1)
1.39 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
1.39 logistic.py(287):     if fit_intercept:
1.39 logistic.py(288):         intercept = w[:, -1]
1.39 logistic.py(289):         w = w[:, :-1]
1.39 logistic.py(292):     p = safe_sparse_dot(X, w.T)
1.39 logistic.py(293):     p += intercept
1.39 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
1.39 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
1.39 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
1.39 logistic.py(297):     p = np.exp(p, p)
1.39 logistic.py(298):     return loss, p, w
1.39 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
1.39 logistic.py(346):     diff = sample_weight * (p - Y)
1.39 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
1.39 logistic.py(348):     grad[:, :n_features] += alpha * w
1.39 logistic.py(349):     if fit_intercept:
1.39 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
1.39 logistic.py(351):     return loss, grad.ravel(), p
1.39 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
1.39 logistic.py(339):     n_classes = Y.shape[1]
1.39 logistic.py(340):     n_features = X.shape[1]
1.39 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
1.39 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
1.39 logistic.py(343):                     dtype=X.dtype)
1.39 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
1.39 logistic.py(282):     n_classes = Y.shape[1]
1.39 logistic.py(283):     n_features = X.shape[1]
1.39 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
1.39 logistic.py(285):     w = w.reshape(n_classes, -1)
1.39 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
1.39 logistic.py(287):     if fit_intercept:
1.39 logistic.py(288):         intercept = w[:, -1]
1.39 logistic.py(289):         w = w[:, :-1]
1.39 logistic.py(292):     p = safe_sparse_dot(X, w.T)
1.39 logistic.py(293):     p += intercept
1.39 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
1.39 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
1.39 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
1.39 logistic.py(297):     p = np.exp(p, p)
1.39 logistic.py(298):     return loss, p, w
1.39 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
1.39 logistic.py(346):     diff = sample_weight * (p - Y)
1.39 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
1.39 logistic.py(348):     grad[:, :n_features] += alpha * w
1.39 logistic.py(349):     if fit_intercept:
1.39 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
1.39 logistic.py(351):     return loss, grad.ravel(), p
1.39 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
1.39 logistic.py(339):     n_classes = Y.shape[1]
1.39 logistic.py(340):     n_features = X.shape[1]
1.39 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
1.39 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
1.39 logistic.py(343):                     dtype=X.dtype)
1.39 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
1.39 logistic.py(282):     n_classes = Y.shape[1]
1.39 logistic.py(283):     n_features = X.shape[1]
1.39 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
1.39 logistic.py(285):     w = w.reshape(n_classes, -1)
1.39 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
1.39 logistic.py(287):     if fit_intercept:
1.39 logistic.py(288):         intercept = w[:, -1]
1.39 logistic.py(289):         w = w[:, :-1]
1.39 logistic.py(292):     p = safe_sparse_dot(X, w.T)
1.39 logistic.py(293):     p += intercept
1.39 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
1.39 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
1.39 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
1.39 logistic.py(297):     p = np.exp(p, p)
1.39 logistic.py(298):     return loss, p, w
1.39 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
1.39 logistic.py(346):     diff = sample_weight * (p - Y)
1.39 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
1.39 logistic.py(348):     grad[:, :n_features] += alpha * w
1.39 logistic.py(349):     if fit_intercept:
1.39 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
1.39 logistic.py(351):     return loss, grad.ravel(), p
1.39 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
1.39 logistic.py(339):     n_classes = Y.shape[1]
1.39 logistic.py(340):     n_features = X.shape[1]
1.39 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
1.39 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
1.39 logistic.py(343):                     dtype=X.dtype)
1.39 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
1.39 logistic.py(282):     n_classes = Y.shape[1]
1.39 logistic.py(283):     n_features = X.shape[1]
1.39 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
1.39 logistic.py(285):     w = w.reshape(n_classes, -1)
1.39 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
1.39 logistic.py(287):     if fit_intercept:
1.39 logistic.py(288):         intercept = w[:, -1]
1.39 logistic.py(289):         w = w[:, :-1]
1.39 logistic.py(292):     p = safe_sparse_dot(X, w.T)
1.39 logistic.py(293):     p += intercept
1.39 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
1.39 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
1.39 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
1.39 logistic.py(297):     p = np.exp(p, p)
1.39 logistic.py(298):     return loss, p, w
1.39 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
1.39 logistic.py(346):     diff = sample_weight * (p - Y)
1.39 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
1.39 logistic.py(348):     grad[:, :n_features] += alpha * w
1.39 logistic.py(349):     if fit_intercept:
1.39 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
1.39 logistic.py(351):     return loss, grad.ravel(), p
1.39 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
1.39 logistic.py(339):     n_classes = Y.shape[1]
1.39 logistic.py(340):     n_features = X.shape[1]
1.39 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
1.39 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
1.39 logistic.py(343):                     dtype=X.dtype)
1.39 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
1.39 logistic.py(282):     n_classes = Y.shape[1]
1.39 logistic.py(283):     n_features = X.shape[1]
1.39 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
1.39 logistic.py(285):     w = w.reshape(n_classes, -1)
1.39 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
1.39 logistic.py(287):     if fit_intercept:
1.39 logistic.py(288):         intercept = w[:, -1]
1.39 logistic.py(289):         w = w[:, :-1]
1.39 logistic.py(292):     p = safe_sparse_dot(X, w.T)
1.39 logistic.py(293):     p += intercept
1.39 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
1.39 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
1.39 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
1.39 logistic.py(297):     p = np.exp(p, p)
1.39 logistic.py(298):     return loss, p, w
1.39 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
1.39 logistic.py(346):     diff = sample_weight * (p - Y)
1.39 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
1.39 logistic.py(348):     grad[:, :n_features] += alpha * w
1.39 logistic.py(349):     if fit_intercept:
1.39 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
1.39 logistic.py(351):     return loss, grad.ravel(), p
1.39 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
1.39 logistic.py(339):     n_classes = Y.shape[1]
1.39 logistic.py(340):     n_features = X.shape[1]
1.39 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
1.39 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
1.39 logistic.py(343):                     dtype=X.dtype)
1.39 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
1.39 logistic.py(282):     n_classes = Y.shape[1]
1.39 logistic.py(283):     n_features = X.shape[1]
1.39 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
1.39 logistic.py(285):     w = w.reshape(n_classes, -1)
1.39 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
1.39 logistic.py(287):     if fit_intercept:
1.39 logistic.py(288):         intercept = w[:, -1]
1.39 logistic.py(289):         w = w[:, :-1]
1.39 logistic.py(292):     p = safe_sparse_dot(X, w.T)
1.39 logistic.py(293):     p += intercept
1.39 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
1.39 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
1.39 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
1.39 logistic.py(297):     p = np.exp(p, p)
1.39 logistic.py(298):     return loss, p, w
1.39 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
1.39 logistic.py(346):     diff = sample_weight * (p - Y)
1.39 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
1.39 logistic.py(348):     grad[:, :n_features] += alpha * w
1.39 logistic.py(349):     if fit_intercept:
1.39 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
1.39 logistic.py(351):     return loss, grad.ravel(), p
1.39 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
1.39 logistic.py(339):     n_classes = Y.shape[1]
1.39 logistic.py(340):     n_features = X.shape[1]
1.39 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
1.39 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
1.39 logistic.py(343):                     dtype=X.dtype)
1.39 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
1.39 logistic.py(282):     n_classes = Y.shape[1]
1.39 logistic.py(283):     n_features = X.shape[1]
1.39 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
1.39 logistic.py(285):     w = w.reshape(n_classes, -1)
1.39 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
1.39 logistic.py(287):     if fit_intercept:
1.39 logistic.py(288):         intercept = w[:, -1]
1.39 logistic.py(289):         w = w[:, :-1]
1.39 logistic.py(292):     p = safe_sparse_dot(X, w.T)
1.39 logistic.py(293):     p += intercept
1.39 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
1.39 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
1.39 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
1.39 logistic.py(297):     p = np.exp(p, p)
1.39 logistic.py(298):     return loss, p, w
1.39 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
1.39 logistic.py(346):     diff = sample_weight * (p - Y)
1.39 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
1.39 logistic.py(348):     grad[:, :n_features] += alpha * w
1.39 logistic.py(349):     if fit_intercept:
1.39 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
1.39 logistic.py(351):     return loss, grad.ravel(), p
1.39 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
1.39 logistic.py(339):     n_classes = Y.shape[1]
1.39 logistic.py(340):     n_features = X.shape[1]
1.39 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
1.39 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
1.39 logistic.py(343):                     dtype=X.dtype)
1.39 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
1.39 logistic.py(282):     n_classes = Y.shape[1]
1.39 logistic.py(283):     n_features = X.shape[1]
1.39 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
1.39 logistic.py(285):     w = w.reshape(n_classes, -1)
1.39 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
1.39 logistic.py(287):     if fit_intercept:
1.39 logistic.py(288):         intercept = w[:, -1]
1.39 logistic.py(289):         w = w[:, :-1]
1.39 logistic.py(292):     p = safe_sparse_dot(X, w.T)
1.39 logistic.py(293):     p += intercept
1.39 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
1.39 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
1.39 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
1.39 logistic.py(297):     p = np.exp(p, p)
1.39 logistic.py(298):     return loss, p, w
1.39 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
1.39 logistic.py(346):     diff = sample_weight * (p - Y)
1.39 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
1.39 logistic.py(348):     grad[:, :n_features] += alpha * w
1.39 logistic.py(349):     if fit_intercept:
1.39 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
1.39 logistic.py(351):     return loss, grad.ravel(), p
1.39 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
1.39 logistic.py(339):     n_classes = Y.shape[1]
1.39 logistic.py(340):     n_features = X.shape[1]
1.39 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
1.39 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
1.39 logistic.py(343):                     dtype=X.dtype)
1.39 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
1.39 logistic.py(282):     n_classes = Y.shape[1]
1.39 logistic.py(283):     n_features = X.shape[1]
1.39 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
1.39 logistic.py(285):     w = w.reshape(n_classes, -1)
1.39 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
1.39 logistic.py(287):     if fit_intercept:
1.39 logistic.py(288):         intercept = w[:, -1]
1.39 logistic.py(289):         w = w[:, :-1]
1.39 logistic.py(292):     p = safe_sparse_dot(X, w.T)
1.39 logistic.py(293):     p += intercept
1.39 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
1.39 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
1.39 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
1.39 logistic.py(297):     p = np.exp(p, p)
1.39 logistic.py(298):     return loss, p, w
1.39 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
1.39 logistic.py(346):     diff = sample_weight * (p - Y)
1.39 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
1.39 logistic.py(348):     grad[:, :n_features] += alpha * w
1.39 logistic.py(349):     if fit_intercept:
1.39 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
1.39 logistic.py(351):     return loss, grad.ravel(), p
1.39 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
1.39 logistic.py(339):     n_classes = Y.shape[1]
1.39 logistic.py(340):     n_features = X.shape[1]
1.39 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
1.39 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
1.39 logistic.py(343):                     dtype=X.dtype)
1.39 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
1.39 logistic.py(282):     n_classes = Y.shape[1]
1.39 logistic.py(283):     n_features = X.shape[1]
1.39 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
1.39 logistic.py(285):     w = w.reshape(n_classes, -1)
1.39 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
1.39 logistic.py(287):     if fit_intercept:
1.39 logistic.py(288):         intercept = w[:, -1]
1.39 logistic.py(289):         w = w[:, :-1]
1.39 logistic.py(292):     p = safe_sparse_dot(X, w.T)
1.39 logistic.py(293):     p += intercept
1.39 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
1.39 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
1.39 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
1.39 logistic.py(297):     p = np.exp(p, p)
1.39 logistic.py(298):     return loss, p, w
1.39 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
1.39 logistic.py(346):     diff = sample_weight * (p - Y)
1.39 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
1.39 logistic.py(348):     grad[:, :n_features] += alpha * w
1.39 logistic.py(349):     if fit_intercept:
1.39 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
1.39 logistic.py(351):     return loss, grad.ravel(), p
1.39 logistic.py(718):             if info["warnflag"] == 1:
1.39 logistic.py(723):             n_iter_i = min(info['nit'], max_iter)
1.39 logistic.py(760):         if multi_class == 'multinomial':
1.39 logistic.py(761):             multi_w0 = np.reshape(w0, (classes.size, -1))
1.39 logistic.py(762):             if classes.size == 2:
1.39 logistic.py(764):             coefs.append(multi_w0)
1.39 logistic.py(768):         n_iter[i] = n_iter_i
1.39 logistic.py(712):     for i, C in enumerate(Cs):
1.39 logistic.py(713):         if solver == 'lbfgs':
1.39 logistic.py(714):             w0, loss, info = optimize.fmin_l_bfgs_b(
1.39 logistic.py(715):                 func, w0, fprime=None,
1.39 logistic.py(716):                 args=(X, target, 1. / C, sample_weight),
1.39 logistic.py(717):                 iprint=(verbose > 0) - 1, pgtol=tol, maxiter=max_iter)
1.39 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
1.39 logistic.py(339):     n_classes = Y.shape[1]
1.39 logistic.py(340):     n_features = X.shape[1]
1.39 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
1.39 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
1.39 logistic.py(343):                     dtype=X.dtype)
1.39 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
1.39 logistic.py(282):     n_classes = Y.shape[1]
1.39 logistic.py(283):     n_features = X.shape[1]
1.39 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
1.39 logistic.py(285):     w = w.reshape(n_classes, -1)
1.39 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
1.39 logistic.py(287):     if fit_intercept:
1.39 logistic.py(288):         intercept = w[:, -1]
1.39 logistic.py(289):         w = w[:, :-1]
1.39 logistic.py(292):     p = safe_sparse_dot(X, w.T)
1.39 logistic.py(293):     p += intercept
1.39 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
1.39 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
1.39 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
1.39 logistic.py(297):     p = np.exp(p, p)
1.39 logistic.py(298):     return loss, p, w
1.39 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
1.39 logistic.py(346):     diff = sample_weight * (p - Y)
1.39 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
1.39 logistic.py(348):     grad[:, :n_features] += alpha * w
1.39 logistic.py(349):     if fit_intercept:
1.39 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
1.39 logistic.py(351):     return loss, grad.ravel(), p
1.39 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
1.39 logistic.py(339):     n_classes = Y.shape[1]
1.39 logistic.py(340):     n_features = X.shape[1]
1.39 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
1.39 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
1.39 logistic.py(343):                     dtype=X.dtype)
1.39 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
1.39 logistic.py(282):     n_classes = Y.shape[1]
1.39 logistic.py(283):     n_features = X.shape[1]
1.39 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
1.39 logistic.py(285):     w = w.reshape(n_classes, -1)
1.39 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
1.39 logistic.py(287):     if fit_intercept:
1.39 logistic.py(288):         intercept = w[:, -1]
1.39 logistic.py(289):         w = w[:, :-1]
1.39 logistic.py(292):     p = safe_sparse_dot(X, w.T)
1.39 logistic.py(293):     p += intercept
1.39 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
1.39 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
1.39 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
1.39 logistic.py(297):     p = np.exp(p, p)
1.39 logistic.py(298):     return loss, p, w
1.39 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
1.39 logistic.py(346):     diff = sample_weight * (p - Y)
1.39 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
1.39 logistic.py(348):     grad[:, :n_features] += alpha * w
1.39 logistic.py(349):     if fit_intercept:
1.39 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
1.39 logistic.py(351):     return loss, grad.ravel(), p
1.39 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
1.39 logistic.py(339):     n_classes = Y.shape[1]
1.39 logistic.py(340):     n_features = X.shape[1]
1.39 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
1.39 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
1.39 logistic.py(343):                     dtype=X.dtype)
1.39 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
1.39 logistic.py(282):     n_classes = Y.shape[1]
1.39 logistic.py(283):     n_features = X.shape[1]
1.39 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
1.39 logistic.py(285):     w = w.reshape(n_classes, -1)
1.39 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
1.39 logistic.py(287):     if fit_intercept:
1.39 logistic.py(288):         intercept = w[:, -1]
1.39 logistic.py(289):         w = w[:, :-1]
1.39 logistic.py(292):     p = safe_sparse_dot(X, w.T)
1.39 logistic.py(293):     p += intercept
1.39 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
1.39 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
1.39 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
1.39 logistic.py(297):     p = np.exp(p, p)
1.39 logistic.py(298):     return loss, p, w
1.39 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
1.39 logistic.py(346):     diff = sample_weight * (p - Y)
1.39 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
1.39 logistic.py(348):     grad[:, :n_features] += alpha * w
1.39 logistic.py(349):     if fit_intercept:
1.39 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
1.39 logistic.py(351):     return loss, grad.ravel(), p
1.39 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
1.39 logistic.py(339):     n_classes = Y.shape[1]
1.39 logistic.py(340):     n_features = X.shape[1]
1.39 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
1.39 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
1.39 logistic.py(343):                     dtype=X.dtype)
1.39 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
1.39 logistic.py(282):     n_classes = Y.shape[1]
1.39 logistic.py(283):     n_features = X.shape[1]
1.39 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
1.39 logistic.py(285):     w = w.reshape(n_classes, -1)
1.39 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
1.39 logistic.py(287):     if fit_intercept:
1.39 logistic.py(288):         intercept = w[:, -1]
1.39 logistic.py(289):         w = w[:, :-1]
1.39 logistic.py(292):     p = safe_sparse_dot(X, w.T)
1.39 logistic.py(293):     p += intercept
1.39 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
1.39 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
1.39 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
1.39 logistic.py(297):     p = np.exp(p, p)
1.39 logistic.py(298):     return loss, p, w
1.39 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
1.39 logistic.py(346):     diff = sample_weight * (p - Y)
1.39 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
1.39 logistic.py(348):     grad[:, :n_features] += alpha * w
1.39 logistic.py(349):     if fit_intercept:
1.39 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
1.39 logistic.py(351):     return loss, grad.ravel(), p
1.39 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
1.39 logistic.py(339):     n_classes = Y.shape[1]
1.39 logistic.py(340):     n_features = X.shape[1]
1.39 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
1.39 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
1.39 logistic.py(343):                     dtype=X.dtype)
1.39 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
1.39 logistic.py(282):     n_classes = Y.shape[1]
1.39 logistic.py(283):     n_features = X.shape[1]
1.39 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
1.39 logistic.py(285):     w = w.reshape(n_classes, -1)
1.39 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
1.39 logistic.py(287):     if fit_intercept:
1.39 logistic.py(288):         intercept = w[:, -1]
1.39 logistic.py(289):         w = w[:, :-1]
1.39 logistic.py(292):     p = safe_sparse_dot(X, w.T)
1.39 logistic.py(293):     p += intercept
1.39 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
1.39 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
1.40 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
1.40 logistic.py(297):     p = np.exp(p, p)
1.40 logistic.py(298):     return loss, p, w
1.40 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
1.40 logistic.py(346):     diff = sample_weight * (p - Y)
1.40 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
1.40 logistic.py(348):     grad[:, :n_features] += alpha * w
1.40 logistic.py(349):     if fit_intercept:
1.40 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
1.40 logistic.py(351):     return loss, grad.ravel(), p
1.40 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
1.40 logistic.py(339):     n_classes = Y.shape[1]
1.40 logistic.py(340):     n_features = X.shape[1]
1.40 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
1.40 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
1.40 logistic.py(343):                     dtype=X.dtype)
1.40 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
1.40 logistic.py(282):     n_classes = Y.shape[1]
1.40 logistic.py(283):     n_features = X.shape[1]
1.40 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
1.40 logistic.py(285):     w = w.reshape(n_classes, -1)
1.40 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
1.40 logistic.py(287):     if fit_intercept:
1.40 logistic.py(288):         intercept = w[:, -1]
1.40 logistic.py(289):         w = w[:, :-1]
1.40 logistic.py(292):     p = safe_sparse_dot(X, w.T)
1.40 logistic.py(293):     p += intercept
1.40 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
1.40 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
1.40 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
1.40 logistic.py(297):     p = np.exp(p, p)
1.40 logistic.py(298):     return loss, p, w
1.40 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
1.40 logistic.py(346):     diff = sample_weight * (p - Y)
1.40 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
1.40 logistic.py(348):     grad[:, :n_features] += alpha * w
1.40 logistic.py(349):     if fit_intercept:
1.40 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
1.40 logistic.py(351):     return loss, grad.ravel(), p
1.40 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
1.40 logistic.py(339):     n_classes = Y.shape[1]
1.40 logistic.py(340):     n_features = X.shape[1]
1.40 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
1.40 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
1.40 logistic.py(343):                     dtype=X.dtype)
1.40 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
1.40 logistic.py(282):     n_classes = Y.shape[1]
1.40 logistic.py(283):     n_features = X.shape[1]
1.40 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
1.40 logistic.py(285):     w = w.reshape(n_classes, -1)
1.40 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
1.40 logistic.py(287):     if fit_intercept:
1.40 logistic.py(288):         intercept = w[:, -1]
1.40 logistic.py(289):         w = w[:, :-1]
1.40 logistic.py(292):     p = safe_sparse_dot(X, w.T)
1.40 logistic.py(293):     p += intercept
1.40 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
1.40 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
1.40 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
1.40 logistic.py(297):     p = np.exp(p, p)
1.40 logistic.py(298):     return loss, p, w
1.40 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
1.40 logistic.py(346):     diff = sample_weight * (p - Y)
1.40 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
1.40 logistic.py(348):     grad[:, :n_features] += alpha * w
1.40 logistic.py(349):     if fit_intercept:
1.40 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
1.40 logistic.py(351):     return loss, grad.ravel(), p
1.40 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
1.40 logistic.py(339):     n_classes = Y.shape[1]
1.40 logistic.py(340):     n_features = X.shape[1]
1.40 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
1.40 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
1.40 logistic.py(343):                     dtype=X.dtype)
1.40 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
1.40 logistic.py(282):     n_classes = Y.shape[1]
1.40 logistic.py(283):     n_features = X.shape[1]
1.40 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
1.40 logistic.py(285):     w = w.reshape(n_classes, -1)
1.40 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
1.40 logistic.py(287):     if fit_intercept:
1.40 logistic.py(288):         intercept = w[:, -1]
1.40 logistic.py(289):         w = w[:, :-1]
1.40 logistic.py(292):     p = safe_sparse_dot(X, w.T)
1.40 logistic.py(293):     p += intercept
1.40 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
1.40 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
1.40 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
1.40 logistic.py(297):     p = np.exp(p, p)
1.40 logistic.py(298):     return loss, p, w
1.40 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
1.40 logistic.py(346):     diff = sample_weight * (p - Y)
1.40 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
1.40 logistic.py(348):     grad[:, :n_features] += alpha * w
1.40 logistic.py(349):     if fit_intercept:
1.40 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
1.40 logistic.py(351):     return loss, grad.ravel(), p
1.40 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
1.40 logistic.py(339):     n_classes = Y.shape[1]
1.40 logistic.py(340):     n_features = X.shape[1]
1.40 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
1.40 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
1.40 logistic.py(343):                     dtype=X.dtype)
1.40 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
1.40 logistic.py(282):     n_classes = Y.shape[1]
1.40 logistic.py(283):     n_features = X.shape[1]
1.40 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
1.40 logistic.py(285):     w = w.reshape(n_classes, -1)
1.40 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
1.40 logistic.py(287):     if fit_intercept:
1.40 logistic.py(288):         intercept = w[:, -1]
1.40 logistic.py(289):         w = w[:, :-1]
1.40 logistic.py(292):     p = safe_sparse_dot(X, w.T)
1.40 logistic.py(293):     p += intercept
1.40 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
1.40 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
1.40 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
1.40 logistic.py(297):     p = np.exp(p, p)
1.40 logistic.py(298):     return loss, p, w
1.40 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
1.40 logistic.py(346):     diff = sample_weight * (p - Y)
1.40 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
1.40 logistic.py(348):     grad[:, :n_features] += alpha * w
1.40 logistic.py(349):     if fit_intercept:
1.40 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
1.40 logistic.py(351):     return loss, grad.ravel(), p
1.40 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
1.40 logistic.py(339):     n_classes = Y.shape[1]
1.40 logistic.py(340):     n_features = X.shape[1]
1.40 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
1.40 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
1.40 logistic.py(343):                     dtype=X.dtype)
1.40 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
1.40 logistic.py(282):     n_classes = Y.shape[1]
1.40 logistic.py(283):     n_features = X.shape[1]
1.40 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
1.40 logistic.py(285):     w = w.reshape(n_classes, -1)
1.40 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
1.40 logistic.py(287):     if fit_intercept:
1.40 logistic.py(288):         intercept = w[:, -1]
1.40 logistic.py(289):         w = w[:, :-1]
1.40 logistic.py(292):     p = safe_sparse_dot(X, w.T)
1.40 logistic.py(293):     p += intercept
1.40 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
1.40 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
1.40 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
1.40 logistic.py(297):     p = np.exp(p, p)
1.40 logistic.py(298):     return loss, p, w
1.40 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
1.40 logistic.py(346):     diff = sample_weight * (p - Y)
1.40 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
1.40 logistic.py(348):     grad[:, :n_features] += alpha * w
1.40 logistic.py(349):     if fit_intercept:
1.40 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
1.40 logistic.py(351):     return loss, grad.ravel(), p
1.40 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
1.40 logistic.py(339):     n_classes = Y.shape[1]
1.40 logistic.py(340):     n_features = X.shape[1]
1.40 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
1.40 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
1.40 logistic.py(343):                     dtype=X.dtype)
1.40 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
1.40 logistic.py(282):     n_classes = Y.shape[1]
1.40 logistic.py(283):     n_features = X.shape[1]
1.40 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
1.40 logistic.py(285):     w = w.reshape(n_classes, -1)
1.40 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
1.40 logistic.py(287):     if fit_intercept:
1.40 logistic.py(288):         intercept = w[:, -1]
1.40 logistic.py(289):         w = w[:, :-1]
1.40 logistic.py(292):     p = safe_sparse_dot(X, w.T)
1.40 logistic.py(293):     p += intercept
1.40 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
1.40 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
1.40 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
1.40 logistic.py(297):     p = np.exp(p, p)
1.40 logistic.py(298):     return loss, p, w
1.40 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
1.40 logistic.py(346):     diff = sample_weight * (p - Y)
1.40 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
1.40 logistic.py(348):     grad[:, :n_features] += alpha * w
1.40 logistic.py(349):     if fit_intercept:
1.40 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
1.40 logistic.py(351):     return loss, grad.ravel(), p
1.40 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
1.40 logistic.py(339):     n_classes = Y.shape[1]
1.40 logistic.py(340):     n_features = X.shape[1]
1.40 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
1.40 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
1.40 logistic.py(343):                     dtype=X.dtype)
1.40 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
1.40 logistic.py(282):     n_classes = Y.shape[1]
1.40 logistic.py(283):     n_features = X.shape[1]
1.40 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
1.40 logistic.py(285):     w = w.reshape(n_classes, -1)
1.40 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
1.40 logistic.py(287):     if fit_intercept:
1.40 logistic.py(288):         intercept = w[:, -1]
1.40 logistic.py(289):         w = w[:, :-1]
1.40 logistic.py(292):     p = safe_sparse_dot(X, w.T)
1.40 logistic.py(293):     p += intercept
1.40 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
1.40 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
1.40 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
1.40 logistic.py(297):     p = np.exp(p, p)
1.40 logistic.py(298):     return loss, p, w
1.40 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
1.40 logistic.py(346):     diff = sample_weight * (p - Y)
1.40 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
1.40 logistic.py(348):     grad[:, :n_features] += alpha * w
1.40 logistic.py(349):     if fit_intercept:
1.40 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
1.40 logistic.py(351):     return loss, grad.ravel(), p
1.40 logistic.py(718):             if info["warnflag"] == 1:
1.40 logistic.py(723):             n_iter_i = min(info['nit'], max_iter)
1.40 logistic.py(760):         if multi_class == 'multinomial':
1.40 logistic.py(761):             multi_w0 = np.reshape(w0, (classes.size, -1))
1.40 logistic.py(762):             if classes.size == 2:
1.40 logistic.py(764):             coefs.append(multi_w0)
1.40 logistic.py(768):         n_iter[i] = n_iter_i
1.40 logistic.py(712):     for i, C in enumerate(Cs):
1.40 logistic.py(713):         if solver == 'lbfgs':
1.40 logistic.py(714):             w0, loss, info = optimize.fmin_l_bfgs_b(
1.40 logistic.py(715):                 func, w0, fprime=None,
1.40 logistic.py(716):                 args=(X, target, 1. / C, sample_weight),
1.40 logistic.py(717):                 iprint=(verbose > 0) - 1, pgtol=tol, maxiter=max_iter)
1.40 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
1.40 logistic.py(339):     n_classes = Y.shape[1]
1.40 logistic.py(340):     n_features = X.shape[1]
1.40 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
1.40 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
1.40 logistic.py(343):                     dtype=X.dtype)
1.40 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
1.40 logistic.py(282):     n_classes = Y.shape[1]
1.40 logistic.py(283):     n_features = X.shape[1]
1.40 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
1.40 logistic.py(285):     w = w.reshape(n_classes, -1)
1.40 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
1.40 logistic.py(287):     if fit_intercept:
1.40 logistic.py(288):         intercept = w[:, -1]
1.40 logistic.py(289):         w = w[:, :-1]
1.40 logistic.py(292):     p = safe_sparse_dot(X, w.T)
1.40 logistic.py(293):     p += intercept
1.40 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
1.40 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
1.40 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
1.40 logistic.py(297):     p = np.exp(p, p)
1.40 logistic.py(298):     return loss, p, w
1.40 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
1.40 logistic.py(346):     diff = sample_weight * (p - Y)
1.40 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
1.40 logistic.py(348):     grad[:, :n_features] += alpha * w
1.40 logistic.py(349):     if fit_intercept:
1.40 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
1.40 logistic.py(351):     return loss, grad.ravel(), p
1.40 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
1.40 logistic.py(339):     n_classes = Y.shape[1]
1.40 logistic.py(340):     n_features = X.shape[1]
1.40 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
1.40 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
1.40 logistic.py(343):                     dtype=X.dtype)
1.40 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
1.40 logistic.py(282):     n_classes = Y.shape[1]
1.40 logistic.py(283):     n_features = X.shape[1]
1.40 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
1.40 logistic.py(285):     w = w.reshape(n_classes, -1)
1.40 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
1.40 logistic.py(287):     if fit_intercept:
1.40 logistic.py(288):         intercept = w[:, -1]
1.40 logistic.py(289):         w = w[:, :-1]
1.40 logistic.py(292):     p = safe_sparse_dot(X, w.T)
1.40 logistic.py(293):     p += intercept
1.40 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
1.40 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
1.40 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
1.40 logistic.py(297):     p = np.exp(p, p)
1.40 logistic.py(298):     return loss, p, w
1.40 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
1.40 logistic.py(346):     diff = sample_weight * (p - Y)
1.40 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
1.40 logistic.py(348):     grad[:, :n_features] += alpha * w
1.40 logistic.py(349):     if fit_intercept:
1.40 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
1.40 logistic.py(351):     return loss, grad.ravel(), p
1.40 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
1.40 logistic.py(339):     n_classes = Y.shape[1]
1.40 logistic.py(340):     n_features = X.shape[1]
1.40 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
1.40 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
1.40 logistic.py(343):                     dtype=X.dtype)
1.40 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
1.40 logistic.py(282):     n_classes = Y.shape[1]
1.40 logistic.py(283):     n_features = X.shape[1]
1.40 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
1.40 logistic.py(285):     w = w.reshape(n_classes, -1)
1.40 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
1.40 logistic.py(287):     if fit_intercept:
1.40 logistic.py(288):         intercept = w[:, -1]
1.40 logistic.py(289):         w = w[:, :-1]
1.40 logistic.py(292):     p = safe_sparse_dot(X, w.T)
1.40 logistic.py(293):     p += intercept
1.40 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
1.40 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
1.40 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
1.40 logistic.py(297):     p = np.exp(p, p)
1.40 logistic.py(298):     return loss, p, w
1.40 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
1.40 logistic.py(346):     diff = sample_weight * (p - Y)
1.40 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
1.40 logistic.py(348):     grad[:, :n_features] += alpha * w
1.40 logistic.py(349):     if fit_intercept:
1.40 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
1.40 logistic.py(351):     return loss, grad.ravel(), p
1.40 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
1.40 logistic.py(339):     n_classes = Y.shape[1]
1.40 logistic.py(340):     n_features = X.shape[1]
1.40 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
1.40 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
1.40 logistic.py(343):                     dtype=X.dtype)
1.40 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
1.40 logistic.py(282):     n_classes = Y.shape[1]
1.40 logistic.py(283):     n_features = X.shape[1]
1.40 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
1.40 logistic.py(285):     w = w.reshape(n_classes, -1)
1.40 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
1.40 logistic.py(287):     if fit_intercept:
1.40 logistic.py(288):         intercept = w[:, -1]
1.40 logistic.py(289):         w = w[:, :-1]
1.40 logistic.py(292):     p = safe_sparse_dot(X, w.T)
1.40 logistic.py(293):     p += intercept
1.40 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
1.40 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
1.40 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
1.40 logistic.py(297):     p = np.exp(p, p)
1.40 logistic.py(298):     return loss, p, w
1.40 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
1.40 logistic.py(346):     diff = sample_weight * (p - Y)
1.40 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
1.40 logistic.py(348):     grad[:, :n_features] += alpha * w
1.40 logistic.py(349):     if fit_intercept:
1.40 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
1.40 logistic.py(351):     return loss, grad.ravel(), p
1.40 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
1.40 logistic.py(339):     n_classes = Y.shape[1]
1.40 logistic.py(340):     n_features = X.shape[1]
1.40 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
1.40 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
1.40 logistic.py(343):                     dtype=X.dtype)
1.40 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
1.40 logistic.py(282):     n_classes = Y.shape[1]
1.40 logistic.py(283):     n_features = X.shape[1]
1.40 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
1.40 logistic.py(285):     w = w.reshape(n_classes, -1)
1.40 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
1.40 logistic.py(287):     if fit_intercept:
1.40 logistic.py(288):         intercept = w[:, -1]
1.40 logistic.py(289):         w = w[:, :-1]
1.40 logistic.py(292):     p = safe_sparse_dot(X, w.T)
1.40 logistic.py(293):     p += intercept
1.40 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
1.40 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
1.40 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
1.40 logistic.py(297):     p = np.exp(p, p)
1.40 logistic.py(298):     return loss, p, w
1.40 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
1.40 logistic.py(346):     diff = sample_weight * (p - Y)
1.40 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
1.40 logistic.py(348):     grad[:, :n_features] += alpha * w
1.40 logistic.py(349):     if fit_intercept:
1.40 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
1.40 logistic.py(351):     return loss, grad.ravel(), p
1.40 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
1.40 logistic.py(339):     n_classes = Y.shape[1]
1.40 logistic.py(340):     n_features = X.shape[1]
1.40 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
1.40 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
1.40 logistic.py(343):                     dtype=X.dtype)
1.40 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
1.40 logistic.py(282):     n_classes = Y.shape[1]
1.40 logistic.py(283):     n_features = X.shape[1]
1.40 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
1.40 logistic.py(285):     w = w.reshape(n_classes, -1)
1.40 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
1.40 logistic.py(287):     if fit_intercept:
1.40 logistic.py(288):         intercept = w[:, -1]
1.40 logistic.py(289):         w = w[:, :-1]
1.40 logistic.py(292):     p = safe_sparse_dot(X, w.T)
1.40 logistic.py(293):     p += intercept
1.40 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
1.40 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
1.40 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
1.40 logistic.py(297):     p = np.exp(p, p)
1.40 logistic.py(298):     return loss, p, w
1.40 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
1.40 logistic.py(346):     diff = sample_weight * (p - Y)
1.40 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
1.40 logistic.py(348):     grad[:, :n_features] += alpha * w
1.40 logistic.py(349):     if fit_intercept:
1.40 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
1.40 logistic.py(351):     return loss, grad.ravel(), p
1.40 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
1.40 logistic.py(339):     n_classes = Y.shape[1]
1.40 logistic.py(340):     n_features = X.shape[1]
1.40 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
1.40 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
1.40 logistic.py(343):                     dtype=X.dtype)
1.40 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
1.40 logistic.py(282):     n_classes = Y.shape[1]
1.40 logistic.py(283):     n_features = X.shape[1]
1.40 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
1.40 logistic.py(285):     w = w.reshape(n_classes, -1)
1.40 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
1.40 logistic.py(287):     if fit_intercept:
1.40 logistic.py(288):         intercept = w[:, -1]
1.40 logistic.py(289):         w = w[:, :-1]
1.40 logistic.py(292):     p = safe_sparse_dot(X, w.T)
1.40 logistic.py(293):     p += intercept
1.40 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
1.40 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
1.40 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
1.40 logistic.py(297):     p = np.exp(p, p)
1.40 logistic.py(298):     return loss, p, w
1.40 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
1.40 logistic.py(346):     diff = sample_weight * (p - Y)
1.40 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
1.40 logistic.py(348):     grad[:, :n_features] += alpha * w
1.40 logistic.py(349):     if fit_intercept:
1.40 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
1.40 logistic.py(351):     return loss, grad.ravel(), p
1.40 logistic.py(718):             if info["warnflag"] == 1:
1.40 logistic.py(723):             n_iter_i = min(info['nit'], max_iter)
1.40 logistic.py(760):         if multi_class == 'multinomial':
1.40 logistic.py(761):             multi_w0 = np.reshape(w0, (classes.size, -1))
1.40 logistic.py(762):             if classes.size == 2:
1.40 logistic.py(764):             coefs.append(multi_w0)
1.40 logistic.py(768):         n_iter[i] = n_iter_i
1.40 logistic.py(712):     for i, C in enumerate(Cs):
1.40 logistic.py(713):         if solver == 'lbfgs':
1.40 logistic.py(714):             w0, loss, info = optimize.fmin_l_bfgs_b(
1.40 logistic.py(715):                 func, w0, fprime=None,
1.40 logistic.py(716):                 args=(X, target, 1. / C, sample_weight),
1.40 logistic.py(717):                 iprint=(verbose > 0) - 1, pgtol=tol, maxiter=max_iter)
1.40 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
1.40 logistic.py(339):     n_classes = Y.shape[1]
1.40 logistic.py(340):     n_features = X.shape[1]
1.40 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
1.40 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
1.40 logistic.py(343):                     dtype=X.dtype)
1.40 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
1.40 logistic.py(282):     n_classes = Y.shape[1]
1.40 logistic.py(283):     n_features = X.shape[1]
1.40 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
1.40 logistic.py(285):     w = w.reshape(n_classes, -1)
1.40 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
1.40 logistic.py(287):     if fit_intercept:
1.40 logistic.py(288):         intercept = w[:, -1]
1.40 logistic.py(289):         w = w[:, :-1]
1.40 logistic.py(292):     p = safe_sparse_dot(X, w.T)
1.40 logistic.py(293):     p += intercept
1.40 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
1.40 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
1.40 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
1.40 logistic.py(297):     p = np.exp(p, p)
1.40 logistic.py(298):     return loss, p, w
1.40 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
1.40 logistic.py(346):     diff = sample_weight * (p - Y)
1.40 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
1.40 logistic.py(348):     grad[:, :n_features] += alpha * w
1.40 logistic.py(349):     if fit_intercept:
1.40 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
1.40 logistic.py(351):     return loss, grad.ravel(), p
1.40 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
1.40 logistic.py(339):     n_classes = Y.shape[1]
1.41 logistic.py(340):     n_features = X.shape[1]
1.41 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
1.41 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
1.41 logistic.py(343):                     dtype=X.dtype)
1.41 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
1.41 logistic.py(282):     n_classes = Y.shape[1]
1.41 logistic.py(283):     n_features = X.shape[1]
1.41 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
1.41 logistic.py(285):     w = w.reshape(n_classes, -1)
1.41 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
1.41 logistic.py(287):     if fit_intercept:
1.41 logistic.py(288):         intercept = w[:, -1]
1.41 logistic.py(289):         w = w[:, :-1]
1.41 logistic.py(292):     p = safe_sparse_dot(X, w.T)
1.41 logistic.py(293):     p += intercept
1.41 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
1.41 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
1.41 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
1.41 logistic.py(297):     p = np.exp(p, p)
1.41 logistic.py(298):     return loss, p, w
1.41 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
1.41 logistic.py(346):     diff = sample_weight * (p - Y)
1.41 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
1.41 logistic.py(348):     grad[:, :n_features] += alpha * w
1.41 logistic.py(349):     if fit_intercept:
1.41 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
1.41 logistic.py(351):     return loss, grad.ravel(), p
1.41 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
1.41 logistic.py(339):     n_classes = Y.shape[1]
1.41 logistic.py(340):     n_features = X.shape[1]
1.41 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
1.41 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
1.41 logistic.py(343):                     dtype=X.dtype)
1.41 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
1.41 logistic.py(282):     n_classes = Y.shape[1]
1.41 logistic.py(283):     n_features = X.shape[1]
1.41 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
1.41 logistic.py(285):     w = w.reshape(n_classes, -1)
1.41 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
1.41 logistic.py(287):     if fit_intercept:
1.41 logistic.py(288):         intercept = w[:, -1]
1.41 logistic.py(289):         w = w[:, :-1]
1.41 logistic.py(292):     p = safe_sparse_dot(X, w.T)
1.41 logistic.py(293):     p += intercept
1.41 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
1.41 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
1.41 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
1.41 logistic.py(297):     p = np.exp(p, p)
1.41 logistic.py(298):     return loss, p, w
1.41 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
1.41 logistic.py(346):     diff = sample_weight * (p - Y)
1.41 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
1.41 logistic.py(348):     grad[:, :n_features] += alpha * w
1.41 logistic.py(349):     if fit_intercept:
1.41 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
1.41 logistic.py(351):     return loss, grad.ravel(), p
1.41 logistic.py(718):             if info["warnflag"] == 1:
1.41 logistic.py(723):             n_iter_i = min(info['nit'], max_iter)
1.41 logistic.py(760):         if multi_class == 'multinomial':
1.41 logistic.py(761):             multi_w0 = np.reshape(w0, (classes.size, -1))
1.41 logistic.py(762):             if classes.size == 2:
1.41 logistic.py(764):             coefs.append(multi_w0)
1.41 logistic.py(768):         n_iter[i] = n_iter_i
1.41 logistic.py(712):     for i, C in enumerate(Cs):
1.41 logistic.py(770):     return coefs, np.array(Cs), n_iter
1.41 logistic.py(925):     log_reg = LogisticRegression(multi_class=multi_class)
1.41 logistic.py(1171):         self.penalty = penalty
1.41 logistic.py(1172):         self.dual = dual
1.41 logistic.py(1173):         self.tol = tol
1.41 logistic.py(1174):         self.C = C
1.41 logistic.py(1175):         self.fit_intercept = fit_intercept
1.41 logistic.py(1176):         self.intercept_scaling = intercept_scaling
1.41 logistic.py(1177):         self.class_weight = class_weight
1.41 logistic.py(1178):         self.random_state = random_state
1.41 logistic.py(1179):         self.solver = solver
1.41 logistic.py(1180):         self.max_iter = max_iter
1.41 logistic.py(1181):         self.multi_class = multi_class
1.41 logistic.py(1182):         self.verbose = verbose
1.41 logistic.py(1183):         self.warm_start = warm_start
1.41 logistic.py(1184):         self.n_jobs = n_jobs
1.41 logistic.py(928):     if multi_class == 'ovr':
1.41 logistic.py(930):     elif multi_class == 'multinomial':
1.41 logistic.py(931):         log_reg.classes_ = np.unique(y_train)
1.41 logistic.py(936):     if pos_class is not None:
1.41 logistic.py(941):     scores = list()
1.41 logistic.py(943):     if isinstance(scoring, six.string_types):
1.41 logistic.py(945):     for w in coefs:
1.41 logistic.py(946):         if multi_class == 'ovr':
1.41 logistic.py(948):         if fit_intercept:
1.41 logistic.py(949):             log_reg.coef_ = w[:, :-1]
1.41 logistic.py(950):             log_reg.intercept_ = w[:, -1]
1.41 logistic.py(955):         if scoring is None:
1.41 logistic.py(956):             scores.append(log_reg.score(X_test, y_test))
1.41 logistic.py(945):     for w in coefs:
1.41 logistic.py(946):         if multi_class == 'ovr':
1.41 logistic.py(948):         if fit_intercept:
1.41 logistic.py(949):             log_reg.coef_ = w[:, :-1]
1.41 logistic.py(950):             log_reg.intercept_ = w[:, -1]
1.41 logistic.py(955):         if scoring is None:
1.41 logistic.py(956):             scores.append(log_reg.score(X_test, y_test))
1.41 logistic.py(945):     for w in coefs:
1.41 logistic.py(946):         if multi_class == 'ovr':
1.41 logistic.py(948):         if fit_intercept:
1.41 logistic.py(949):             log_reg.coef_ = w[:, :-1]
1.41 logistic.py(950):             log_reg.intercept_ = w[:, -1]
1.41 logistic.py(955):         if scoring is None:
1.41 logistic.py(956):             scores.append(log_reg.score(X_test, y_test))
1.41 logistic.py(945):     for w in coefs:
1.41 logistic.py(946):         if multi_class == 'ovr':
1.41 logistic.py(948):         if fit_intercept:
1.41 logistic.py(949):             log_reg.coef_ = w[:, :-1]
1.41 logistic.py(950):             log_reg.intercept_ = w[:, -1]
1.41 logistic.py(955):         if scoring is None:
1.41 logistic.py(956):             scores.append(log_reg.score(X_test, y_test))
1.41 logistic.py(945):     for w in coefs:
1.41 logistic.py(946):         if multi_class == 'ovr':
1.41 logistic.py(948):         if fit_intercept:
1.41 logistic.py(949):             log_reg.coef_ = w[:, :-1]
1.41 logistic.py(950):             log_reg.intercept_ = w[:, -1]
1.41 logistic.py(955):         if scoring is None:
1.41 logistic.py(956):             scores.append(log_reg.score(X_test, y_test))
1.41 logistic.py(945):     for w in coefs:
1.41 logistic.py(946):         if multi_class == 'ovr':
1.41 logistic.py(948):         if fit_intercept:
1.41 logistic.py(949):             log_reg.coef_ = w[:, :-1]
1.41 logistic.py(950):             log_reg.intercept_ = w[:, -1]
1.41 logistic.py(955):         if scoring is None:
1.41 logistic.py(956):             scores.append(log_reg.score(X_test, y_test))
1.41 logistic.py(945):     for w in coefs:
1.41 logistic.py(946):         if multi_class == 'ovr':
1.41 logistic.py(948):         if fit_intercept:
1.41 logistic.py(949):             log_reg.coef_ = w[:, :-1]
1.41 logistic.py(950):             log_reg.intercept_ = w[:, -1]
1.41 logistic.py(955):         if scoring is None:
1.41 logistic.py(956):             scores.append(log_reg.score(X_test, y_test))
1.41 logistic.py(945):     for w in coefs:
1.41 logistic.py(946):         if multi_class == 'ovr':
1.41 logistic.py(948):         if fit_intercept:
1.41 logistic.py(949):             log_reg.coef_ = w[:, :-1]
1.41 logistic.py(950):             log_reg.intercept_ = w[:, -1]
1.41 logistic.py(955):         if scoring is None:
1.41 logistic.py(956):             scores.append(log_reg.score(X_test, y_test))
1.41 logistic.py(945):     for w in coefs:
1.41 logistic.py(946):         if multi_class == 'ovr':
1.41 logistic.py(948):         if fit_intercept:
1.41 logistic.py(949):             log_reg.coef_ = w[:, :-1]
1.41 logistic.py(950):             log_reg.intercept_ = w[:, -1]
1.41 logistic.py(955):         if scoring is None:
1.41 logistic.py(956):             scores.append(log_reg.score(X_test, y_test))
1.41 logistic.py(945):     for w in coefs:
1.41 logistic.py(946):         if multi_class == 'ovr':
1.41 logistic.py(948):         if fit_intercept:
1.41 logistic.py(949):             log_reg.coef_ = w[:, :-1]
1.41 logistic.py(950):             log_reg.intercept_ = w[:, -1]
1.41 logistic.py(955):         if scoring is None:
1.41 logistic.py(956):             scores.append(log_reg.score(X_test, y_test))
1.41 logistic.py(945):     for w in coefs:
1.41 logistic.py(959):     return coefs, Cs, np.array(scores), n_iter
1.41 logistic.py(1703):             for train, test in folds)
1.41 logistic.py(903):     _check_solver_option(solver, multi_class, penalty, dual)
1.41 logistic.py(428):     if solver not in ['liblinear', 'newton-cg', 'lbfgs', 'sag', 'saga']:
1.41 logistic.py(433):     if multi_class not in ['multinomial', 'ovr']:
1.41 logistic.py(437):     if multi_class == 'multinomial' and solver == 'liblinear':
1.41 logistic.py(441):     if solver not in ['liblinear', 'saga']:
1.41 logistic.py(442):         if penalty != 'l2':
1.41 logistic.py(445):     if solver != 'liblinear':
1.41 logistic.py(446):         if dual:
1.41 logistic.py(905):     X_train = X[train]
1.41 logistic.py(906):     X_test = X[test]
1.41 logistic.py(907):     y_train = y[train]
1.41 logistic.py(908):     y_test = y[test]
1.41 logistic.py(910):     if sample_weight is not None:
1.41 logistic.py(916):     coefs, Cs, n_iter = logistic_regression_path(
1.41 logistic.py(917):         X_train, y_train, Cs=Cs, fit_intercept=fit_intercept,
1.41 logistic.py(918):         solver=solver, max_iter=max_iter, class_weight=class_weight,
1.41 logistic.py(919):         pos_class=pos_class, multi_class=multi_class,
1.41 logistic.py(920):         tol=tol, verbose=verbose, dual=dual, penalty=penalty,
1.41 logistic.py(921):         intercept_scaling=intercept_scaling, random_state=random_state,
1.41 logistic.py(922):         check_input=False, max_squared_sum=max_squared_sum,
1.41 logistic.py(923):         sample_weight=sample_weight)
1.41 logistic.py(591):     if isinstance(Cs, numbers.Integral):
1.41 logistic.py(592):         Cs = np.logspace(-4, 4, Cs)
1.41 logistic.py(594):     _check_solver_option(solver, multi_class, penalty, dual)
1.41 logistic.py(428):     if solver not in ['liblinear', 'newton-cg', 'lbfgs', 'sag', 'saga']:
1.41 logistic.py(433):     if multi_class not in ['multinomial', 'ovr']:
1.41 logistic.py(437):     if multi_class == 'multinomial' and solver == 'liblinear':
1.41 logistic.py(441):     if solver not in ['liblinear', 'saga']:
1.41 logistic.py(442):         if penalty != 'l2':
1.41 logistic.py(445):     if solver != 'liblinear':
1.41 logistic.py(446):         if dual:
1.41 logistic.py(597):     if check_input:
1.41 logistic.py(602):     _, n_features = X.shape
1.41 logistic.py(603):     classes = np.unique(y)
1.41 logistic.py(604):     random_state = check_random_state(random_state)
1.41 logistic.py(606):     if pos_class is None and multi_class != 'multinomial':
1.41 logistic.py(615):     if sample_weight is not None:
1.41 logistic.py(619):         sample_weight = np.ones(X.shape[0], dtype=X.dtype)
1.41 logistic.py(624):     le = LabelEncoder()
1.41 logistic.py(625):     if isinstance(class_weight, dict) or multi_class == 'multinomial':
1.41 logistic.py(626):         class_weight_ = compute_class_weight(class_weight, classes, y)
1.41 logistic.py(627):         sample_weight *= class_weight_[le.fit_transform(y)]
1.41 logistic.py(631):     if multi_class == 'ovr':
1.41 logistic.py(645):         if solver not in ['sag', 'saga']:
1.41 logistic.py(646):             lbin = LabelBinarizer()
1.41 logistic.py(647):             Y_multi = lbin.fit_transform(y)
1.41 logistic.py(648):             if Y_multi.shape[1] == 1:
1.41 logistic.py(655):         w0 = np.zeros((classes.size, n_features + int(fit_intercept)),
1.41 logistic.py(656):                       order='F', dtype=X.dtype)
1.41 logistic.py(658):     if coef is not None:
1.41 logistic.py(688):     if multi_class == 'multinomial':
1.41 logistic.py(690):         if solver in ['lbfgs', 'newton-cg']:
1.41 logistic.py(691):             w0 = w0.ravel()
1.41 logistic.py(692):         target = Y_multi
1.41 logistic.py(693):         if solver == 'lbfgs':
1.41 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
1.41 logistic.py(699):         warm_start_sag = {'coef': w0.T}
1.41 logistic.py(710):     coefs = list()
1.41 logistic.py(711):     n_iter = np.zeros(len(Cs), dtype=np.int32)
1.41 logistic.py(712):     for i, C in enumerate(Cs):
1.41 logistic.py(713):         if solver == 'lbfgs':
1.41 logistic.py(714):             w0, loss, info = optimize.fmin_l_bfgs_b(
1.41 logistic.py(715):                 func, w0, fprime=None,
1.41 logistic.py(716):                 args=(X, target, 1. / C, sample_weight),
1.41 logistic.py(717):                 iprint=(verbose > 0) - 1, pgtol=tol, maxiter=max_iter)
1.41 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
1.41 logistic.py(339):     n_classes = Y.shape[1]
1.41 logistic.py(340):     n_features = X.shape[1]
1.41 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
1.41 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
1.41 logistic.py(343):                     dtype=X.dtype)
1.41 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
1.41 logistic.py(282):     n_classes = Y.shape[1]
1.41 logistic.py(283):     n_features = X.shape[1]
1.41 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
1.41 logistic.py(285):     w = w.reshape(n_classes, -1)
1.41 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
1.41 logistic.py(287):     if fit_intercept:
1.41 logistic.py(288):         intercept = w[:, -1]
1.41 logistic.py(289):         w = w[:, :-1]
1.41 logistic.py(292):     p = safe_sparse_dot(X, w.T)
1.41 logistic.py(293):     p += intercept
1.41 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
1.41 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
1.41 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
1.41 logistic.py(297):     p = np.exp(p, p)
1.41 logistic.py(298):     return loss, p, w
1.41 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
1.41 logistic.py(346):     diff = sample_weight * (p - Y)
1.41 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
1.41 logistic.py(348):     grad[:, :n_features] += alpha * w
1.41 logistic.py(349):     if fit_intercept:
1.41 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
1.41 logistic.py(351):     return loss, grad.ravel(), p
1.41 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
1.41 logistic.py(339):     n_classes = Y.shape[1]
1.41 logistic.py(340):     n_features = X.shape[1]
1.41 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
1.41 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
1.41 logistic.py(343):                     dtype=X.dtype)
1.41 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
1.41 logistic.py(282):     n_classes = Y.shape[1]
1.41 logistic.py(283):     n_features = X.shape[1]
1.41 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
1.41 logistic.py(285):     w = w.reshape(n_classes, -1)
1.41 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
1.41 logistic.py(287):     if fit_intercept:
1.41 logistic.py(288):         intercept = w[:, -1]
1.41 logistic.py(289):         w = w[:, :-1]
1.41 logistic.py(292):     p = safe_sparse_dot(X, w.T)
1.41 logistic.py(293):     p += intercept
1.41 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
1.41 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
1.41 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
1.41 logistic.py(297):     p = np.exp(p, p)
1.41 logistic.py(298):     return loss, p, w
1.41 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
1.41 logistic.py(346):     diff = sample_weight * (p - Y)
1.41 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
1.41 logistic.py(348):     grad[:, :n_features] += alpha * w
1.41 logistic.py(349):     if fit_intercept:
1.41 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
1.41 logistic.py(351):     return loss, grad.ravel(), p
1.41 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
1.41 logistic.py(339):     n_classes = Y.shape[1]
1.41 logistic.py(340):     n_features = X.shape[1]
1.41 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
1.41 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
1.41 logistic.py(343):                     dtype=X.dtype)
1.41 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
1.41 logistic.py(282):     n_classes = Y.shape[1]
1.41 logistic.py(283):     n_features = X.shape[1]
1.41 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
1.41 logistic.py(285):     w = w.reshape(n_classes, -1)
1.41 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
1.41 logistic.py(287):     if fit_intercept:
1.41 logistic.py(288):         intercept = w[:, -1]
1.41 logistic.py(289):         w = w[:, :-1]
1.41 logistic.py(292):     p = safe_sparse_dot(X, w.T)
1.41 logistic.py(293):     p += intercept
1.41 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
1.41 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
1.41 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
1.41 logistic.py(297):     p = np.exp(p, p)
1.41 logistic.py(298):     return loss, p, w
1.41 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
1.41 logistic.py(346):     diff = sample_weight * (p - Y)
1.41 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
1.41 logistic.py(348):     grad[:, :n_features] += alpha * w
1.41 logistic.py(349):     if fit_intercept:
1.41 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
1.41 logistic.py(351):     return loss, grad.ravel(), p
1.41 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
1.41 logistic.py(339):     n_classes = Y.shape[1]
1.41 logistic.py(340):     n_features = X.shape[1]
1.41 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
1.41 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
1.41 logistic.py(343):                     dtype=X.dtype)
1.41 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
1.41 logistic.py(282):     n_classes = Y.shape[1]
1.41 logistic.py(283):     n_features = X.shape[1]
1.41 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
1.41 logistic.py(285):     w = w.reshape(n_classes, -1)
1.41 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
1.41 logistic.py(287):     if fit_intercept:
1.41 logistic.py(288):         intercept = w[:, -1]
1.41 logistic.py(289):         w = w[:, :-1]
1.41 logistic.py(292):     p = safe_sparse_dot(X, w.T)
1.41 logistic.py(293):     p += intercept
1.41 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
1.41 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
1.41 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
1.41 logistic.py(297):     p = np.exp(p, p)
1.41 logistic.py(298):     return loss, p, w
1.41 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
1.41 logistic.py(346):     diff = sample_weight * (p - Y)
1.41 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
1.41 logistic.py(348):     grad[:, :n_features] += alpha * w
1.41 logistic.py(349):     if fit_intercept:
1.41 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
1.41 logistic.py(351):     return loss, grad.ravel(), p
1.41 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
1.41 logistic.py(339):     n_classes = Y.shape[1]
1.41 logistic.py(340):     n_features = X.shape[1]
1.41 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
1.41 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
1.41 logistic.py(343):                     dtype=X.dtype)
1.41 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
1.41 logistic.py(282):     n_classes = Y.shape[1]
1.41 logistic.py(283):     n_features = X.shape[1]
1.41 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
1.41 logistic.py(285):     w = w.reshape(n_classes, -1)
1.41 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
1.41 logistic.py(287):     if fit_intercept:
1.41 logistic.py(288):         intercept = w[:, -1]
1.41 logistic.py(289):         w = w[:, :-1]
1.41 logistic.py(292):     p = safe_sparse_dot(X, w.T)
1.41 logistic.py(293):     p += intercept
1.41 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
1.41 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
1.41 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
1.41 logistic.py(297):     p = np.exp(p, p)
1.41 logistic.py(298):     return loss, p, w
1.41 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
1.41 logistic.py(346):     diff = sample_weight * (p - Y)
1.41 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
1.41 logistic.py(348):     grad[:, :n_features] += alpha * w
1.41 logistic.py(349):     if fit_intercept:
1.41 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
1.41 logistic.py(351):     return loss, grad.ravel(), p
1.41 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
1.41 logistic.py(339):     n_classes = Y.shape[1]
1.41 logistic.py(340):     n_features = X.shape[1]
1.41 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
1.41 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
1.41 logistic.py(343):                     dtype=X.dtype)
1.41 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
1.41 logistic.py(282):     n_classes = Y.shape[1]
1.41 logistic.py(283):     n_features = X.shape[1]
1.41 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
1.41 logistic.py(285):     w = w.reshape(n_classes, -1)
1.41 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
1.41 logistic.py(287):     if fit_intercept:
1.41 logistic.py(288):         intercept = w[:, -1]
1.41 logistic.py(289):         w = w[:, :-1]
1.41 logistic.py(292):     p = safe_sparse_dot(X, w.T)
1.41 logistic.py(293):     p += intercept
1.41 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
1.41 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
1.41 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
1.41 logistic.py(297):     p = np.exp(p, p)
1.41 logistic.py(298):     return loss, p, w
1.41 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
1.41 logistic.py(346):     diff = sample_weight * (p - Y)
1.41 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
1.41 logistic.py(348):     grad[:, :n_features] += alpha * w
1.41 logistic.py(349):     if fit_intercept:
1.42 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
1.42 logistic.py(351):     return loss, grad.ravel(), p
1.42 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
1.42 logistic.py(339):     n_classes = Y.shape[1]
1.42 logistic.py(340):     n_features = X.shape[1]
1.42 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
1.42 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
1.42 logistic.py(343):                     dtype=X.dtype)
1.42 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
1.42 logistic.py(282):     n_classes = Y.shape[1]
1.42 logistic.py(283):     n_features = X.shape[1]
1.42 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
1.42 logistic.py(285):     w = w.reshape(n_classes, -1)
1.42 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
1.42 logistic.py(287):     if fit_intercept:
1.42 logistic.py(288):         intercept = w[:, -1]
1.42 logistic.py(289):         w = w[:, :-1]
1.42 logistic.py(292):     p = safe_sparse_dot(X, w.T)
1.42 logistic.py(293):     p += intercept
1.42 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
1.42 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
1.42 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
1.42 logistic.py(297):     p = np.exp(p, p)
1.42 logistic.py(298):     return loss, p, w
1.42 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
1.42 logistic.py(346):     diff = sample_weight * (p - Y)
1.42 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
1.42 logistic.py(348):     grad[:, :n_features] += alpha * w
1.42 logistic.py(349):     if fit_intercept:
1.42 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
1.42 logistic.py(351):     return loss, grad.ravel(), p
1.42 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
1.42 logistic.py(339):     n_classes = Y.shape[1]
1.42 logistic.py(340):     n_features = X.shape[1]
1.42 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
1.42 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
1.42 logistic.py(343):                     dtype=X.dtype)
1.42 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
1.42 logistic.py(282):     n_classes = Y.shape[1]
1.42 logistic.py(283):     n_features = X.shape[1]
1.42 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
1.42 logistic.py(285):     w = w.reshape(n_classes, -1)
1.42 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
1.42 logistic.py(287):     if fit_intercept:
1.42 logistic.py(288):         intercept = w[:, -1]
1.42 logistic.py(289):         w = w[:, :-1]
1.42 logistic.py(292):     p = safe_sparse_dot(X, w.T)
1.42 logistic.py(293):     p += intercept
1.42 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
1.42 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
1.42 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
1.42 logistic.py(297):     p = np.exp(p, p)
1.42 logistic.py(298):     return loss, p, w
1.42 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
1.42 logistic.py(346):     diff = sample_weight * (p - Y)
1.42 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
1.42 logistic.py(348):     grad[:, :n_features] += alpha * w
1.42 logistic.py(349):     if fit_intercept:
1.42 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
1.42 logistic.py(351):     return loss, grad.ravel(), p
1.42 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
1.42 logistic.py(339):     n_classes = Y.shape[1]
1.42 logistic.py(340):     n_features = X.shape[1]
1.42 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
1.42 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
1.42 logistic.py(343):                     dtype=X.dtype)
1.42 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
1.42 logistic.py(282):     n_classes = Y.shape[1]
1.42 logistic.py(283):     n_features = X.shape[1]
1.42 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
1.42 logistic.py(285):     w = w.reshape(n_classes, -1)
1.42 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
1.42 logistic.py(287):     if fit_intercept:
1.42 logistic.py(288):         intercept = w[:, -1]
1.42 logistic.py(289):         w = w[:, :-1]
1.42 logistic.py(292):     p = safe_sparse_dot(X, w.T)
1.42 logistic.py(293):     p += intercept
1.42 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
1.42 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
1.42 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
1.42 logistic.py(297):     p = np.exp(p, p)
1.42 logistic.py(298):     return loss, p, w
1.42 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
1.42 logistic.py(346):     diff = sample_weight * (p - Y)
1.42 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
1.42 logistic.py(348):     grad[:, :n_features] += alpha * w
1.42 logistic.py(349):     if fit_intercept:
1.42 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
1.42 logistic.py(351):     return loss, grad.ravel(), p
1.42 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
1.42 logistic.py(339):     n_classes = Y.shape[1]
1.42 logistic.py(340):     n_features = X.shape[1]
1.42 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
1.42 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
1.42 logistic.py(343):                     dtype=X.dtype)
1.42 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
1.42 logistic.py(282):     n_classes = Y.shape[1]
1.42 logistic.py(283):     n_features = X.shape[1]
1.42 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
1.42 logistic.py(285):     w = w.reshape(n_classes, -1)
1.42 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
1.42 logistic.py(287):     if fit_intercept:
1.42 logistic.py(288):         intercept = w[:, -1]
1.42 logistic.py(289):         w = w[:, :-1]
1.42 logistic.py(292):     p = safe_sparse_dot(X, w.T)
1.42 logistic.py(293):     p += intercept
1.42 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
1.42 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
1.42 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
1.42 logistic.py(297):     p = np.exp(p, p)
1.42 logistic.py(298):     return loss, p, w
1.42 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
1.42 logistic.py(346):     diff = sample_weight * (p - Y)
1.42 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
1.42 logistic.py(348):     grad[:, :n_features] += alpha * w
1.42 logistic.py(349):     if fit_intercept:
1.42 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
1.42 logistic.py(351):     return loss, grad.ravel(), p
1.42 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
1.42 logistic.py(339):     n_classes = Y.shape[1]
1.42 logistic.py(340):     n_features = X.shape[1]
1.42 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
1.42 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
1.42 logistic.py(343):                     dtype=X.dtype)
1.42 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
1.42 logistic.py(282):     n_classes = Y.shape[1]
1.42 logistic.py(283):     n_features = X.shape[1]
1.42 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
1.42 logistic.py(285):     w = w.reshape(n_classes, -1)
1.42 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
1.42 logistic.py(287):     if fit_intercept:
1.42 logistic.py(288):         intercept = w[:, -1]
1.42 logistic.py(289):         w = w[:, :-1]
1.42 logistic.py(292):     p = safe_sparse_dot(X, w.T)
1.42 logistic.py(293):     p += intercept
1.42 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
1.42 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
1.42 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
1.42 logistic.py(297):     p = np.exp(p, p)
1.42 logistic.py(298):     return loss, p, w
1.42 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
1.42 logistic.py(346):     diff = sample_weight * (p - Y)
1.42 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
1.42 logistic.py(348):     grad[:, :n_features] += alpha * w
1.42 logistic.py(349):     if fit_intercept:
1.42 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
1.42 logistic.py(351):     return loss, grad.ravel(), p
1.42 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
1.42 logistic.py(339):     n_classes = Y.shape[1]
1.42 logistic.py(340):     n_features = X.shape[1]
1.42 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
1.42 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
1.42 logistic.py(343):                     dtype=X.dtype)
1.42 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
1.42 logistic.py(282):     n_classes = Y.shape[1]
1.42 logistic.py(283):     n_features = X.shape[1]
1.42 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
1.42 logistic.py(285):     w = w.reshape(n_classes, -1)
1.42 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
1.42 logistic.py(287):     if fit_intercept:
1.42 logistic.py(288):         intercept = w[:, -1]
1.42 logistic.py(289):         w = w[:, :-1]
1.42 logistic.py(292):     p = safe_sparse_dot(X, w.T)
1.42 logistic.py(293):     p += intercept
1.42 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
1.42 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
1.42 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
1.42 logistic.py(297):     p = np.exp(p, p)
1.42 logistic.py(298):     return loss, p, w
1.42 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
1.42 logistic.py(346):     diff = sample_weight * (p - Y)
1.42 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
1.42 logistic.py(348):     grad[:, :n_features] += alpha * w
1.42 logistic.py(349):     if fit_intercept:
1.42 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
1.42 logistic.py(351):     return loss, grad.ravel(), p
1.42 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
1.42 logistic.py(339):     n_classes = Y.shape[1]
1.42 logistic.py(340):     n_features = X.shape[1]
1.42 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
1.42 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
1.42 logistic.py(343):                     dtype=X.dtype)
1.42 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
1.42 logistic.py(282):     n_classes = Y.shape[1]
1.42 logistic.py(283):     n_features = X.shape[1]
1.42 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
1.42 logistic.py(285):     w = w.reshape(n_classes, -1)
1.42 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
1.42 logistic.py(287):     if fit_intercept:
1.42 logistic.py(288):         intercept = w[:, -1]
1.42 logistic.py(289):         w = w[:, :-1]
1.42 logistic.py(292):     p = safe_sparse_dot(X, w.T)
1.42 logistic.py(293):     p += intercept
1.42 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
1.42 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
1.42 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
1.42 logistic.py(297):     p = np.exp(p, p)
1.42 logistic.py(298):     return loss, p, w
1.42 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
1.42 logistic.py(346):     diff = sample_weight * (p - Y)
1.42 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
1.42 logistic.py(348):     grad[:, :n_features] += alpha * w
1.42 logistic.py(349):     if fit_intercept:
1.42 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
1.42 logistic.py(351):     return loss, grad.ravel(), p
1.42 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
1.42 logistic.py(339):     n_classes = Y.shape[1]
1.42 logistic.py(340):     n_features = X.shape[1]
1.42 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
1.42 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
1.42 logistic.py(343):                     dtype=X.dtype)
1.42 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
1.42 logistic.py(282):     n_classes = Y.shape[1]
1.42 logistic.py(283):     n_features = X.shape[1]
1.42 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
1.42 logistic.py(285):     w = w.reshape(n_classes, -1)
1.42 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
1.42 logistic.py(287):     if fit_intercept:
1.42 logistic.py(288):         intercept = w[:, -1]
1.42 logistic.py(289):         w = w[:, :-1]
1.42 logistic.py(292):     p = safe_sparse_dot(X, w.T)
1.42 logistic.py(293):     p += intercept
1.42 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
1.42 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
1.42 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
1.42 logistic.py(297):     p = np.exp(p, p)
1.42 logistic.py(298):     return loss, p, w
1.42 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
1.42 logistic.py(346):     diff = sample_weight * (p - Y)
1.42 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
1.42 logistic.py(348):     grad[:, :n_features] += alpha * w
1.42 logistic.py(349):     if fit_intercept:
1.42 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
1.42 logistic.py(351):     return loss, grad.ravel(), p
1.42 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
1.42 logistic.py(339):     n_classes = Y.shape[1]
1.42 logistic.py(340):     n_features = X.shape[1]
1.42 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
1.42 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
1.42 logistic.py(343):                     dtype=X.dtype)
1.42 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
1.42 logistic.py(282):     n_classes = Y.shape[1]
1.42 logistic.py(283):     n_features = X.shape[1]
1.42 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
1.42 logistic.py(285):     w = w.reshape(n_classes, -1)
1.42 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
1.42 logistic.py(287):     if fit_intercept:
1.42 logistic.py(288):         intercept = w[:, -1]
1.42 logistic.py(289):         w = w[:, :-1]
1.42 logistic.py(292):     p = safe_sparse_dot(X, w.T)
1.42 logistic.py(293):     p += intercept
1.42 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
1.42 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
1.42 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
1.42 logistic.py(297):     p = np.exp(p, p)
1.42 logistic.py(298):     return loss, p, w
1.42 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
1.42 logistic.py(346):     diff = sample_weight * (p - Y)
1.42 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
1.42 logistic.py(348):     grad[:, :n_features] += alpha * w
1.42 logistic.py(349):     if fit_intercept:
1.42 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
1.42 logistic.py(351):     return loss, grad.ravel(), p
1.42 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
1.42 logistic.py(339):     n_classes = Y.shape[1]
1.42 logistic.py(340):     n_features = X.shape[1]
1.42 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
1.42 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
1.42 logistic.py(343):                     dtype=X.dtype)
1.42 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
1.42 logistic.py(282):     n_classes = Y.shape[1]
1.42 logistic.py(283):     n_features = X.shape[1]
1.42 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
1.42 logistic.py(285):     w = w.reshape(n_classes, -1)
1.42 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
1.42 logistic.py(287):     if fit_intercept:
1.42 logistic.py(288):         intercept = w[:, -1]
1.42 logistic.py(289):         w = w[:, :-1]
1.42 logistic.py(292):     p = safe_sparse_dot(X, w.T)
1.42 logistic.py(293):     p += intercept
1.42 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
1.42 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
1.42 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
1.42 logistic.py(297):     p = np.exp(p, p)
1.42 logistic.py(298):     return loss, p, w
1.42 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
1.42 logistic.py(346):     diff = sample_weight * (p - Y)
1.42 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
1.42 logistic.py(348):     grad[:, :n_features] += alpha * w
1.42 logistic.py(349):     if fit_intercept:
1.42 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
1.42 logistic.py(351):     return loss, grad.ravel(), p
1.42 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
1.42 logistic.py(339):     n_classes = Y.shape[1]
1.42 logistic.py(340):     n_features = X.shape[1]
1.42 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
1.42 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
1.42 logistic.py(343):                     dtype=X.dtype)
1.42 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
1.42 logistic.py(282):     n_classes = Y.shape[1]
1.42 logistic.py(283):     n_features = X.shape[1]
1.42 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
1.42 logistic.py(285):     w = w.reshape(n_classes, -1)
1.42 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
1.42 logistic.py(287):     if fit_intercept:
1.42 logistic.py(288):         intercept = w[:, -1]
1.42 logistic.py(289):         w = w[:, :-1]
1.42 logistic.py(292):     p = safe_sparse_dot(X, w.T)
1.42 logistic.py(293):     p += intercept
1.42 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
1.42 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
1.42 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
1.42 logistic.py(297):     p = np.exp(p, p)
1.42 logistic.py(298):     return loss, p, w
1.42 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
1.42 logistic.py(346):     diff = sample_weight * (p - Y)
1.42 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
1.42 logistic.py(348):     grad[:, :n_features] += alpha * w
1.42 logistic.py(349):     if fit_intercept:
1.42 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
1.42 logistic.py(351):     return loss, grad.ravel(), p
1.42 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
1.42 logistic.py(339):     n_classes = Y.shape[1]
1.42 logistic.py(340):     n_features = X.shape[1]
1.42 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
1.42 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
1.42 logistic.py(343):                     dtype=X.dtype)
1.42 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
1.42 logistic.py(282):     n_classes = Y.shape[1]
1.42 logistic.py(283):     n_features = X.shape[1]
1.42 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
1.42 logistic.py(285):     w = w.reshape(n_classes, -1)
1.42 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
1.42 logistic.py(287):     if fit_intercept:
1.42 logistic.py(288):         intercept = w[:, -1]
1.42 logistic.py(289):         w = w[:, :-1]
1.42 logistic.py(292):     p = safe_sparse_dot(X, w.T)
1.42 logistic.py(293):     p += intercept
1.42 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
1.42 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
1.42 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
1.42 logistic.py(297):     p = np.exp(p, p)
1.42 logistic.py(298):     return loss, p, w
1.42 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
1.42 logistic.py(346):     diff = sample_weight * (p - Y)
1.42 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
1.42 logistic.py(348):     grad[:, :n_features] += alpha * w
1.42 logistic.py(349):     if fit_intercept:
1.42 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
1.42 logistic.py(351):     return loss, grad.ravel(), p
1.42 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
1.42 logistic.py(339):     n_classes = Y.shape[1]
1.42 logistic.py(340):     n_features = X.shape[1]
1.42 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
1.42 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
1.42 logistic.py(343):                     dtype=X.dtype)
1.42 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
1.42 logistic.py(282):     n_classes = Y.shape[1]
1.42 logistic.py(283):     n_features = X.shape[1]
1.42 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
1.42 logistic.py(285):     w = w.reshape(n_classes, -1)
1.42 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
1.42 logistic.py(287):     if fit_intercept:
1.42 logistic.py(288):         intercept = w[:, -1]
1.42 logistic.py(289):         w = w[:, :-1]
1.42 logistic.py(292):     p = safe_sparse_dot(X, w.T)
1.42 logistic.py(293):     p += intercept
1.42 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
1.42 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
1.42 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
1.42 logistic.py(297):     p = np.exp(p, p)
1.42 logistic.py(298):     return loss, p, w
1.42 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
1.42 logistic.py(346):     diff = sample_weight * (p - Y)
1.42 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
1.42 logistic.py(348):     grad[:, :n_features] += alpha * w
1.42 logistic.py(349):     if fit_intercept:
1.42 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
1.42 logistic.py(351):     return loss, grad.ravel(), p
1.42 logistic.py(718):             if info["warnflag"] == 1:
1.42 logistic.py(723):             n_iter_i = min(info['nit'], max_iter)
1.42 logistic.py(760):         if multi_class == 'multinomial':
1.42 logistic.py(761):             multi_w0 = np.reshape(w0, (classes.size, -1))
1.42 logistic.py(762):             if classes.size == 2:
1.42 logistic.py(764):             coefs.append(multi_w0)
1.42 logistic.py(768):         n_iter[i] = n_iter_i
1.42 logistic.py(712):     for i, C in enumerate(Cs):
1.42 logistic.py(713):         if solver == 'lbfgs':
1.42 logistic.py(714):             w0, loss, info = optimize.fmin_l_bfgs_b(
1.42 logistic.py(715):                 func, w0, fprime=None,
1.42 logistic.py(716):                 args=(X, target, 1. / C, sample_weight),
1.42 logistic.py(717):                 iprint=(verbose > 0) - 1, pgtol=tol, maxiter=max_iter)
1.42 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
1.42 logistic.py(339):     n_classes = Y.shape[1]
1.42 logistic.py(340):     n_features = X.shape[1]
1.42 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
1.42 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
1.42 logistic.py(343):                     dtype=X.dtype)
1.42 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
1.42 logistic.py(282):     n_classes = Y.shape[1]
1.42 logistic.py(283):     n_features = X.shape[1]
1.42 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
1.42 logistic.py(285):     w = w.reshape(n_classes, -1)
1.42 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
1.42 logistic.py(287):     if fit_intercept:
1.42 logistic.py(288):         intercept = w[:, -1]
1.42 logistic.py(289):         w = w[:, :-1]
1.42 logistic.py(292):     p = safe_sparse_dot(X, w.T)
1.42 logistic.py(293):     p += intercept
1.42 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
1.42 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
1.42 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
1.42 logistic.py(297):     p = np.exp(p, p)
1.42 logistic.py(298):     return loss, p, w
1.42 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
1.42 logistic.py(346):     diff = sample_weight * (p - Y)
1.42 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
1.42 logistic.py(348):     grad[:, :n_features] += alpha * w
1.42 logistic.py(349):     if fit_intercept:
1.42 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
1.42 logistic.py(351):     return loss, grad.ravel(), p
1.42 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
1.42 logistic.py(339):     n_classes = Y.shape[1]
1.42 logistic.py(340):     n_features = X.shape[1]
1.42 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
1.42 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
1.42 logistic.py(343):                     dtype=X.dtype)
1.42 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
1.42 logistic.py(282):     n_classes = Y.shape[1]
1.42 logistic.py(283):     n_features = X.shape[1]
1.42 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
1.42 logistic.py(285):     w = w.reshape(n_classes, -1)
1.42 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
1.42 logistic.py(287):     if fit_intercept:
1.42 logistic.py(288):         intercept = w[:, -1]
1.42 logistic.py(289):         w = w[:, :-1]
1.42 logistic.py(292):     p = safe_sparse_dot(X, w.T)
1.42 logistic.py(293):     p += intercept
1.42 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
1.42 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
1.42 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
1.42 logistic.py(297):     p = np.exp(p, p)
1.42 logistic.py(298):     return loss, p, w
1.42 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
1.42 logistic.py(346):     diff = sample_weight * (p - Y)
1.42 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
1.42 logistic.py(348):     grad[:, :n_features] += alpha * w
1.42 logistic.py(349):     if fit_intercept:
1.42 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
1.42 logistic.py(351):     return loss, grad.ravel(), p
1.42 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
1.42 logistic.py(339):     n_classes = Y.shape[1]
1.42 logistic.py(340):     n_features = X.shape[1]
1.42 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
1.42 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
1.42 logistic.py(343):                     dtype=X.dtype)
1.42 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
1.42 logistic.py(282):     n_classes = Y.shape[1]
1.42 logistic.py(283):     n_features = X.shape[1]
1.42 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
1.42 logistic.py(285):     w = w.reshape(n_classes, -1)
1.42 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
1.42 logistic.py(287):     if fit_intercept:
1.42 logistic.py(288):         intercept = w[:, -1]
1.42 logistic.py(289):         w = w[:, :-1]
1.42 logistic.py(292):     p = safe_sparse_dot(X, w.T)
1.42 logistic.py(293):     p += intercept
1.42 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
1.42 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
1.42 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
1.42 logistic.py(297):     p = np.exp(p, p)
1.42 logistic.py(298):     return loss, p, w
1.42 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
1.42 logistic.py(346):     diff = sample_weight * (p - Y)
1.42 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
1.42 logistic.py(348):     grad[:, :n_features] += alpha * w
1.42 logistic.py(349):     if fit_intercept:
1.43 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
1.43 logistic.py(351):     return loss, grad.ravel(), p
1.43 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
1.43 logistic.py(339):     n_classes = Y.shape[1]
1.43 logistic.py(340):     n_features = X.shape[1]
1.43 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
1.43 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
1.43 logistic.py(343):                     dtype=X.dtype)
1.43 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
1.43 logistic.py(282):     n_classes = Y.shape[1]
1.43 logistic.py(283):     n_features = X.shape[1]
1.43 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
1.43 logistic.py(285):     w = w.reshape(n_classes, -1)
1.43 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
1.43 logistic.py(287):     if fit_intercept:
1.43 logistic.py(288):         intercept = w[:, -1]
1.43 logistic.py(289):         w = w[:, :-1]
1.43 logistic.py(292):     p = safe_sparse_dot(X, w.T)
1.43 logistic.py(293):     p += intercept
1.43 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
1.43 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
1.43 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
1.43 logistic.py(297):     p = np.exp(p, p)
1.43 logistic.py(298):     return loss, p, w
1.43 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
1.43 logistic.py(346):     diff = sample_weight * (p - Y)
1.43 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
1.43 logistic.py(348):     grad[:, :n_features] += alpha * w
1.43 logistic.py(349):     if fit_intercept:
1.43 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
1.43 logistic.py(351):     return loss, grad.ravel(), p
1.43 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
1.43 logistic.py(339):     n_classes = Y.shape[1]
1.43 logistic.py(340):     n_features = X.shape[1]
1.43 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
1.43 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
1.43 logistic.py(343):                     dtype=X.dtype)
1.43 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
1.43 logistic.py(282):     n_classes = Y.shape[1]
1.43 logistic.py(283):     n_features = X.shape[1]
1.43 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
1.43 logistic.py(285):     w = w.reshape(n_classes, -1)
1.43 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
1.43 logistic.py(287):     if fit_intercept:
1.43 logistic.py(288):         intercept = w[:, -1]
1.43 logistic.py(289):         w = w[:, :-1]
1.43 logistic.py(292):     p = safe_sparse_dot(X, w.T)
1.43 logistic.py(293):     p += intercept
1.43 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
1.43 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
1.43 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
1.43 logistic.py(297):     p = np.exp(p, p)
1.43 logistic.py(298):     return loss, p, w
1.43 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
1.43 logistic.py(346):     diff = sample_weight * (p - Y)
1.43 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
1.43 logistic.py(348):     grad[:, :n_features] += alpha * w
1.43 logistic.py(349):     if fit_intercept:
1.43 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
1.43 logistic.py(351):     return loss, grad.ravel(), p
1.43 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
1.43 logistic.py(339):     n_classes = Y.shape[1]
1.43 logistic.py(340):     n_features = X.shape[1]
1.43 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
1.43 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
1.43 logistic.py(343):                     dtype=X.dtype)
1.43 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
1.43 logistic.py(282):     n_classes = Y.shape[1]
1.43 logistic.py(283):     n_features = X.shape[1]
1.43 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
1.43 logistic.py(285):     w = w.reshape(n_classes, -1)
1.43 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
1.43 logistic.py(287):     if fit_intercept:
1.43 logistic.py(288):         intercept = w[:, -1]
1.43 logistic.py(289):         w = w[:, :-1]
1.43 logistic.py(292):     p = safe_sparse_dot(X, w.T)
1.43 logistic.py(293):     p += intercept
1.43 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
1.43 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
1.43 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
1.43 logistic.py(297):     p = np.exp(p, p)
1.43 logistic.py(298):     return loss, p, w
1.43 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
1.43 logistic.py(346):     diff = sample_weight * (p - Y)
1.43 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
1.43 logistic.py(348):     grad[:, :n_features] += alpha * w
1.43 logistic.py(349):     if fit_intercept:
1.43 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
1.43 logistic.py(351):     return loss, grad.ravel(), p
1.43 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
1.43 logistic.py(339):     n_classes = Y.shape[1]
1.43 logistic.py(340):     n_features = X.shape[1]
1.43 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
1.43 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
1.43 logistic.py(343):                     dtype=X.dtype)
1.43 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
1.43 logistic.py(282):     n_classes = Y.shape[1]
1.43 logistic.py(283):     n_features = X.shape[1]
1.43 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
1.43 logistic.py(285):     w = w.reshape(n_classes, -1)
1.43 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
1.43 logistic.py(287):     if fit_intercept:
1.43 logistic.py(288):         intercept = w[:, -1]
1.43 logistic.py(289):         w = w[:, :-1]
1.43 logistic.py(292):     p = safe_sparse_dot(X, w.T)
1.43 logistic.py(293):     p += intercept
1.43 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
1.43 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
1.43 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
1.43 logistic.py(297):     p = np.exp(p, p)
1.43 logistic.py(298):     return loss, p, w
1.43 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
1.43 logistic.py(346):     diff = sample_weight * (p - Y)
1.43 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
1.43 logistic.py(348):     grad[:, :n_features] += alpha * w
1.43 logistic.py(349):     if fit_intercept:
1.43 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
1.43 logistic.py(351):     return loss, grad.ravel(), p
1.43 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
1.43 logistic.py(339):     n_classes = Y.shape[1]
1.43 logistic.py(340):     n_features = X.shape[1]
1.43 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
1.43 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
1.43 logistic.py(343):                     dtype=X.dtype)
1.43 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
1.43 logistic.py(282):     n_classes = Y.shape[1]
1.43 logistic.py(283):     n_features = X.shape[1]
1.43 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
1.43 logistic.py(285):     w = w.reshape(n_classes, -1)
1.43 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
1.43 logistic.py(287):     if fit_intercept:
1.43 logistic.py(288):         intercept = w[:, -1]
1.43 logistic.py(289):         w = w[:, :-1]
1.43 logistic.py(292):     p = safe_sparse_dot(X, w.T)
1.43 logistic.py(293):     p += intercept
1.43 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
1.43 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
1.43 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
1.43 logistic.py(297):     p = np.exp(p, p)
1.43 logistic.py(298):     return loss, p, w
1.43 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
1.43 logistic.py(346):     diff = sample_weight * (p - Y)
1.43 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
1.43 logistic.py(348):     grad[:, :n_features] += alpha * w
1.43 logistic.py(349):     if fit_intercept:
1.43 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
1.43 logistic.py(351):     return loss, grad.ravel(), p
1.43 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
1.43 logistic.py(339):     n_classes = Y.shape[1]
1.43 logistic.py(340):     n_features = X.shape[1]
1.43 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
1.43 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
1.43 logistic.py(343):                     dtype=X.dtype)
1.43 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
1.43 logistic.py(282):     n_classes = Y.shape[1]
1.43 logistic.py(283):     n_features = X.shape[1]
1.43 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
1.43 logistic.py(285):     w = w.reshape(n_classes, -1)
1.43 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
1.43 logistic.py(287):     if fit_intercept:
1.43 logistic.py(288):         intercept = w[:, -1]
1.43 logistic.py(289):         w = w[:, :-1]
1.43 logistic.py(292):     p = safe_sparse_dot(X, w.T)
1.43 logistic.py(293):     p += intercept
1.43 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
1.43 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
1.43 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
1.43 logistic.py(297):     p = np.exp(p, p)
1.43 logistic.py(298):     return loss, p, w
1.43 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
1.43 logistic.py(346):     diff = sample_weight * (p - Y)
1.43 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
1.43 logistic.py(348):     grad[:, :n_features] += alpha * w
1.43 logistic.py(349):     if fit_intercept:
1.43 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
1.43 logistic.py(351):     return loss, grad.ravel(), p
1.43 logistic.py(718):             if info["warnflag"] == 1:
1.43 logistic.py(723):             n_iter_i = min(info['nit'], max_iter)
1.43 logistic.py(760):         if multi_class == 'multinomial':
1.43 logistic.py(761):             multi_w0 = np.reshape(w0, (classes.size, -1))
1.43 logistic.py(762):             if classes.size == 2:
1.43 logistic.py(764):             coefs.append(multi_w0)
1.43 logistic.py(768):         n_iter[i] = n_iter_i
1.43 logistic.py(712):     for i, C in enumerate(Cs):
1.43 logistic.py(713):         if solver == 'lbfgs':
1.43 logistic.py(714):             w0, loss, info = optimize.fmin_l_bfgs_b(
1.43 logistic.py(715):                 func, w0, fprime=None,
1.43 logistic.py(716):                 args=(X, target, 1. / C, sample_weight),
1.43 logistic.py(717):                 iprint=(verbose > 0) - 1, pgtol=tol, maxiter=max_iter)
1.43 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
1.43 logistic.py(339):     n_classes = Y.shape[1]
1.43 logistic.py(340):     n_features = X.shape[1]
1.43 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
1.43 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
1.43 logistic.py(343):                     dtype=X.dtype)
1.43 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
1.43 logistic.py(282):     n_classes = Y.shape[1]
1.43 logistic.py(283):     n_features = X.shape[1]
1.43 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
1.43 logistic.py(285):     w = w.reshape(n_classes, -1)
1.43 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
1.43 logistic.py(287):     if fit_intercept:
1.43 logistic.py(288):         intercept = w[:, -1]
1.43 logistic.py(289):         w = w[:, :-1]
1.43 logistic.py(292):     p = safe_sparse_dot(X, w.T)
1.43 logistic.py(293):     p += intercept
1.43 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
1.43 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
1.43 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
1.43 logistic.py(297):     p = np.exp(p, p)
1.43 logistic.py(298):     return loss, p, w
1.43 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
1.43 logistic.py(346):     diff = sample_weight * (p - Y)
1.43 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
1.43 logistic.py(348):     grad[:, :n_features] += alpha * w
1.43 logistic.py(349):     if fit_intercept:
1.43 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
1.43 logistic.py(351):     return loss, grad.ravel(), p
1.43 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
1.43 logistic.py(339):     n_classes = Y.shape[1]
1.43 logistic.py(340):     n_features = X.shape[1]
1.43 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
1.43 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
1.43 logistic.py(343):                     dtype=X.dtype)
1.43 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
1.43 logistic.py(282):     n_classes = Y.shape[1]
1.43 logistic.py(283):     n_features = X.shape[1]
1.43 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
1.43 logistic.py(285):     w = w.reshape(n_classes, -1)
1.43 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
1.43 logistic.py(287):     if fit_intercept:
1.43 logistic.py(288):         intercept = w[:, -1]
1.43 logistic.py(289):         w = w[:, :-1]
1.43 logistic.py(292):     p = safe_sparse_dot(X, w.T)
1.43 logistic.py(293):     p += intercept
1.43 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
1.43 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
1.43 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
1.43 logistic.py(297):     p = np.exp(p, p)
1.43 logistic.py(298):     return loss, p, w
1.43 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
1.43 logistic.py(346):     diff = sample_weight * (p - Y)
1.43 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
1.43 logistic.py(348):     grad[:, :n_features] += alpha * w
1.43 logistic.py(349):     if fit_intercept:
1.43 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
1.43 logistic.py(351):     return loss, grad.ravel(), p
1.43 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
1.43 logistic.py(339):     n_classes = Y.shape[1]
1.43 logistic.py(340):     n_features = X.shape[1]
1.43 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
1.43 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
1.43 logistic.py(343):                     dtype=X.dtype)
1.43 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
1.43 logistic.py(282):     n_classes = Y.shape[1]
1.43 logistic.py(283):     n_features = X.shape[1]
1.43 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
1.43 logistic.py(285):     w = w.reshape(n_classes, -1)
1.43 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
1.43 logistic.py(287):     if fit_intercept:
1.43 logistic.py(288):         intercept = w[:, -1]
1.43 logistic.py(289):         w = w[:, :-1]
1.43 logistic.py(292):     p = safe_sparse_dot(X, w.T)
1.43 logistic.py(293):     p += intercept
1.43 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
1.43 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
1.43 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
1.43 logistic.py(297):     p = np.exp(p, p)
1.43 logistic.py(298):     return loss, p, w
1.43 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
1.43 logistic.py(346):     diff = sample_weight * (p - Y)
1.43 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
1.43 logistic.py(348):     grad[:, :n_features] += alpha * w
1.43 logistic.py(349):     if fit_intercept:
1.43 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
1.43 logistic.py(351):     return loss, grad.ravel(), p
1.43 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
1.43 logistic.py(339):     n_classes = Y.shape[1]
1.43 logistic.py(340):     n_features = X.shape[1]
1.43 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
1.43 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
1.43 logistic.py(343):                     dtype=X.dtype)
1.43 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
1.43 logistic.py(282):     n_classes = Y.shape[1]
1.43 logistic.py(283):     n_features = X.shape[1]
1.43 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
1.43 logistic.py(285):     w = w.reshape(n_classes, -1)
1.43 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
1.43 logistic.py(287):     if fit_intercept:
1.43 logistic.py(288):         intercept = w[:, -1]
1.43 logistic.py(289):         w = w[:, :-1]
1.43 logistic.py(292):     p = safe_sparse_dot(X, w.T)
1.43 logistic.py(293):     p += intercept
1.43 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
1.43 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
1.43 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
1.43 logistic.py(297):     p = np.exp(p, p)
1.43 logistic.py(298):     return loss, p, w
1.43 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
1.43 logistic.py(346):     diff = sample_weight * (p - Y)
1.43 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
1.43 logistic.py(348):     grad[:, :n_features] += alpha * w
1.43 logistic.py(349):     if fit_intercept:
1.43 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
1.43 logistic.py(351):     return loss, grad.ravel(), p
1.43 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
1.43 logistic.py(339):     n_classes = Y.shape[1]
1.43 logistic.py(340):     n_features = X.shape[1]
1.43 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
1.43 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
1.43 logistic.py(343):                     dtype=X.dtype)
1.43 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
1.43 logistic.py(282):     n_classes = Y.shape[1]
1.43 logistic.py(283):     n_features = X.shape[1]
1.43 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
1.43 logistic.py(285):     w = w.reshape(n_classes, -1)
1.43 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
1.43 logistic.py(287):     if fit_intercept:
1.43 logistic.py(288):         intercept = w[:, -1]
1.43 logistic.py(289):         w = w[:, :-1]
1.43 logistic.py(292):     p = safe_sparse_dot(X, w.T)
1.43 logistic.py(293):     p += intercept
1.43 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
1.43 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
1.43 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
1.43 logistic.py(297):     p = np.exp(p, p)
1.43 logistic.py(298):     return loss, p, w
1.43 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
1.43 logistic.py(346):     diff = sample_weight * (p - Y)
1.43 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
1.43 logistic.py(348):     grad[:, :n_features] += alpha * w
1.43 logistic.py(349):     if fit_intercept:
1.43 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
1.43 logistic.py(351):     return loss, grad.ravel(), p
1.43 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
1.43 logistic.py(339):     n_classes = Y.shape[1]
1.43 logistic.py(340):     n_features = X.shape[1]
1.43 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
1.43 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
1.43 logistic.py(343):                     dtype=X.dtype)
1.43 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
1.43 logistic.py(282):     n_classes = Y.shape[1]
1.43 logistic.py(283):     n_features = X.shape[1]
1.43 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
1.43 logistic.py(285):     w = w.reshape(n_classes, -1)
1.43 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
1.43 logistic.py(287):     if fit_intercept:
1.43 logistic.py(288):         intercept = w[:, -1]
1.43 logistic.py(289):         w = w[:, :-1]
1.43 logistic.py(292):     p = safe_sparse_dot(X, w.T)
1.43 logistic.py(293):     p += intercept
1.43 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
1.43 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
1.43 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
1.43 logistic.py(297):     p = np.exp(p, p)
1.43 logistic.py(298):     return loss, p, w
1.43 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
1.43 logistic.py(346):     diff = sample_weight * (p - Y)
1.43 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
1.43 logistic.py(348):     grad[:, :n_features] += alpha * w
1.43 logistic.py(349):     if fit_intercept:
1.43 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
1.43 logistic.py(351):     return loss, grad.ravel(), p
1.43 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
1.43 logistic.py(339):     n_classes = Y.shape[1]
1.43 logistic.py(340):     n_features = X.shape[1]
1.43 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
1.43 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
1.43 logistic.py(343):                     dtype=X.dtype)
1.43 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
1.43 logistic.py(282):     n_classes = Y.shape[1]
1.43 logistic.py(283):     n_features = X.shape[1]
1.43 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
1.43 logistic.py(285):     w = w.reshape(n_classes, -1)
1.43 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
1.43 logistic.py(287):     if fit_intercept:
1.43 logistic.py(288):         intercept = w[:, -1]
1.43 logistic.py(289):         w = w[:, :-1]
1.43 logistic.py(292):     p = safe_sparse_dot(X, w.T)
1.43 logistic.py(293):     p += intercept
1.43 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
1.43 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
1.43 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
1.43 logistic.py(297):     p = np.exp(p, p)
1.43 logistic.py(298):     return loss, p, w
1.43 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
1.43 logistic.py(346):     diff = sample_weight * (p - Y)
1.43 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
1.43 logistic.py(348):     grad[:, :n_features] += alpha * w
1.43 logistic.py(349):     if fit_intercept:
1.43 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
1.43 logistic.py(351):     return loss, grad.ravel(), p
1.43 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
1.43 logistic.py(339):     n_classes = Y.shape[1]
1.43 logistic.py(340):     n_features = X.shape[1]
1.43 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
1.43 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
1.43 logistic.py(343):                     dtype=X.dtype)
1.43 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
1.43 logistic.py(282):     n_classes = Y.shape[1]
1.43 logistic.py(283):     n_features = X.shape[1]
1.43 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
1.43 logistic.py(285):     w = w.reshape(n_classes, -1)
1.43 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
1.43 logistic.py(287):     if fit_intercept:
1.43 logistic.py(288):         intercept = w[:, -1]
1.43 logistic.py(289):         w = w[:, :-1]
1.43 logistic.py(292):     p = safe_sparse_dot(X, w.T)
1.43 logistic.py(293):     p += intercept
1.43 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
1.43 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
1.43 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
1.43 logistic.py(297):     p = np.exp(p, p)
1.43 logistic.py(298):     return loss, p, w
1.43 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
1.43 logistic.py(346):     diff = sample_weight * (p - Y)
1.43 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
1.43 logistic.py(348):     grad[:, :n_features] += alpha * w
1.43 logistic.py(349):     if fit_intercept:
1.43 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
1.43 logistic.py(351):     return loss, grad.ravel(), p
1.43 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
1.43 logistic.py(339):     n_classes = Y.shape[1]
1.43 logistic.py(340):     n_features = X.shape[1]
1.43 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
1.43 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
1.43 logistic.py(343):                     dtype=X.dtype)
1.43 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
1.43 logistic.py(282):     n_classes = Y.shape[1]
1.43 logistic.py(283):     n_features = X.shape[1]
1.43 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
1.43 logistic.py(285):     w = w.reshape(n_classes, -1)
1.43 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
1.43 logistic.py(287):     if fit_intercept:
1.43 logistic.py(288):         intercept = w[:, -1]
1.43 logistic.py(289):         w = w[:, :-1]
1.43 logistic.py(292):     p = safe_sparse_dot(X, w.T)
1.43 logistic.py(293):     p += intercept
1.43 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
1.43 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
1.43 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
1.43 logistic.py(297):     p = np.exp(p, p)
1.43 logistic.py(298):     return loss, p, w
1.43 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
1.43 logistic.py(346):     diff = sample_weight * (p - Y)
1.43 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
1.43 logistic.py(348):     grad[:, :n_features] += alpha * w
1.43 logistic.py(349):     if fit_intercept:
1.43 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
1.43 logistic.py(351):     return loss, grad.ravel(), p
1.43 logistic.py(718):             if info["warnflag"] == 1:
1.43 logistic.py(723):             n_iter_i = min(info['nit'], max_iter)
1.43 logistic.py(760):         if multi_class == 'multinomial':
1.43 logistic.py(761):             multi_w0 = np.reshape(w0, (classes.size, -1))
1.43 logistic.py(762):             if classes.size == 2:
1.43 logistic.py(764):             coefs.append(multi_w0)
1.43 logistic.py(768):         n_iter[i] = n_iter_i
1.43 logistic.py(712):     for i, C in enumerate(Cs):
1.43 logistic.py(713):         if solver == 'lbfgs':
1.43 logistic.py(714):             w0, loss, info = optimize.fmin_l_bfgs_b(
1.43 logistic.py(715):                 func, w0, fprime=None,
1.43 logistic.py(716):                 args=(X, target, 1. / C, sample_weight),
1.43 logistic.py(717):                 iprint=(verbose > 0) - 1, pgtol=tol, maxiter=max_iter)
1.43 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
1.43 logistic.py(339):     n_classes = Y.shape[1]
1.43 logistic.py(340):     n_features = X.shape[1]
1.43 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
1.43 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
1.43 logistic.py(343):                     dtype=X.dtype)
1.43 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
1.43 logistic.py(282):     n_classes = Y.shape[1]
1.44 logistic.py(283):     n_features = X.shape[1]
1.44 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
1.44 logistic.py(285):     w = w.reshape(n_classes, -1)
1.44 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
1.44 logistic.py(287):     if fit_intercept:
1.44 logistic.py(288):         intercept = w[:, -1]
1.44 logistic.py(289):         w = w[:, :-1]
1.44 logistic.py(292):     p = safe_sparse_dot(X, w.T)
1.44 logistic.py(293):     p += intercept
1.44 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
1.44 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
1.44 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
1.44 logistic.py(297):     p = np.exp(p, p)
1.44 logistic.py(298):     return loss, p, w
1.44 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
1.44 logistic.py(346):     diff = sample_weight * (p - Y)
1.44 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
1.44 logistic.py(348):     grad[:, :n_features] += alpha * w
1.44 logistic.py(349):     if fit_intercept:
1.44 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
1.44 logistic.py(351):     return loss, grad.ravel(), p
1.44 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
1.44 logistic.py(339):     n_classes = Y.shape[1]
1.44 logistic.py(340):     n_features = X.shape[1]
1.44 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
1.44 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
1.44 logistic.py(343):                     dtype=X.dtype)
1.44 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
1.44 logistic.py(282):     n_classes = Y.shape[1]
1.44 logistic.py(283):     n_features = X.shape[1]
1.44 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
1.44 logistic.py(285):     w = w.reshape(n_classes, -1)
1.44 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
1.44 logistic.py(287):     if fit_intercept:
1.44 logistic.py(288):         intercept = w[:, -1]
1.44 logistic.py(289):         w = w[:, :-1]
1.44 logistic.py(292):     p = safe_sparse_dot(X, w.T)
1.44 logistic.py(293):     p += intercept
1.44 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
1.44 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
1.44 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
1.44 logistic.py(297):     p = np.exp(p, p)
1.44 logistic.py(298):     return loss, p, w
1.44 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
1.44 logistic.py(346):     diff = sample_weight * (p - Y)
1.44 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
1.44 logistic.py(348):     grad[:, :n_features] += alpha * w
1.44 logistic.py(349):     if fit_intercept:
1.44 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
1.44 logistic.py(351):     return loss, grad.ravel(), p
1.44 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
1.44 logistic.py(339):     n_classes = Y.shape[1]
1.44 logistic.py(340):     n_features = X.shape[1]
1.44 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
1.44 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
1.44 logistic.py(343):                     dtype=X.dtype)
1.44 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
1.44 logistic.py(282):     n_classes = Y.shape[1]
1.44 logistic.py(283):     n_features = X.shape[1]
1.44 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
1.44 logistic.py(285):     w = w.reshape(n_classes, -1)
1.44 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
1.44 logistic.py(287):     if fit_intercept:
1.44 logistic.py(288):         intercept = w[:, -1]
1.44 logistic.py(289):         w = w[:, :-1]
1.44 logistic.py(292):     p = safe_sparse_dot(X, w.T)
1.44 logistic.py(293):     p += intercept
1.44 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
1.44 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
1.44 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
1.44 logistic.py(297):     p = np.exp(p, p)
1.44 logistic.py(298):     return loss, p, w
1.44 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
1.44 logistic.py(346):     diff = sample_weight * (p - Y)
1.44 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
1.44 logistic.py(348):     grad[:, :n_features] += alpha * w
1.44 logistic.py(349):     if fit_intercept:
1.44 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
1.44 logistic.py(351):     return loss, grad.ravel(), p
1.44 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
1.44 logistic.py(339):     n_classes = Y.shape[1]
1.44 logistic.py(340):     n_features = X.shape[1]
1.44 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
1.44 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
1.44 logistic.py(343):                     dtype=X.dtype)
1.44 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
1.44 logistic.py(282):     n_classes = Y.shape[1]
1.44 logistic.py(283):     n_features = X.shape[1]
1.44 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
1.44 logistic.py(285):     w = w.reshape(n_classes, -1)
1.44 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
1.44 logistic.py(287):     if fit_intercept:
1.44 logistic.py(288):         intercept = w[:, -1]
1.44 logistic.py(289):         w = w[:, :-1]
1.44 logistic.py(292):     p = safe_sparse_dot(X, w.T)
1.44 logistic.py(293):     p += intercept
1.44 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
1.44 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
1.44 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
1.44 logistic.py(297):     p = np.exp(p, p)
1.44 logistic.py(298):     return loss, p, w
1.44 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
1.44 logistic.py(346):     diff = sample_weight * (p - Y)
1.44 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
1.44 logistic.py(348):     grad[:, :n_features] += alpha * w
1.44 logistic.py(349):     if fit_intercept:
1.44 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
1.44 logistic.py(351):     return loss, grad.ravel(), p
1.44 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
1.44 logistic.py(339):     n_classes = Y.shape[1]
1.44 logistic.py(340):     n_features = X.shape[1]
1.44 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
1.44 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
1.44 logistic.py(343):                     dtype=X.dtype)
1.44 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
1.44 logistic.py(282):     n_classes = Y.shape[1]
1.44 logistic.py(283):     n_features = X.shape[1]
1.44 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
1.44 logistic.py(285):     w = w.reshape(n_classes, -1)
1.44 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
1.44 logistic.py(287):     if fit_intercept:
1.44 logistic.py(288):         intercept = w[:, -1]
1.44 logistic.py(289):         w = w[:, :-1]
1.44 logistic.py(292):     p = safe_sparse_dot(X, w.T)
1.44 logistic.py(293):     p += intercept
1.44 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
1.44 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
1.44 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
1.44 logistic.py(297):     p = np.exp(p, p)
1.44 logistic.py(298):     return loss, p, w
1.44 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
1.44 logistic.py(346):     diff = sample_weight * (p - Y)
1.44 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
1.44 logistic.py(348):     grad[:, :n_features] += alpha * w
1.44 logistic.py(349):     if fit_intercept:
1.44 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
1.44 logistic.py(351):     return loss, grad.ravel(), p
1.44 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
1.44 logistic.py(339):     n_classes = Y.shape[1]
1.44 logistic.py(340):     n_features = X.shape[1]
1.44 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
1.44 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
1.44 logistic.py(343):                     dtype=X.dtype)
1.44 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
1.44 logistic.py(282):     n_classes = Y.shape[1]
1.44 logistic.py(283):     n_features = X.shape[1]
1.44 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
1.44 logistic.py(285):     w = w.reshape(n_classes, -1)
1.44 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
1.44 logistic.py(287):     if fit_intercept:
1.44 logistic.py(288):         intercept = w[:, -1]
1.44 logistic.py(289):         w = w[:, :-1]
1.44 logistic.py(292):     p = safe_sparse_dot(X, w.T)
1.44 logistic.py(293):     p += intercept
1.44 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
1.44 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
1.44 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
1.44 logistic.py(297):     p = np.exp(p, p)
1.44 logistic.py(298):     return loss, p, w
1.44 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
1.44 logistic.py(346):     diff = sample_weight * (p - Y)
1.44 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
1.44 logistic.py(348):     grad[:, :n_features] += alpha * w
1.44 logistic.py(349):     if fit_intercept:
1.44 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
1.44 logistic.py(351):     return loss, grad.ravel(), p
1.44 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
1.44 logistic.py(339):     n_classes = Y.shape[1]
1.44 logistic.py(340):     n_features = X.shape[1]
1.44 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
1.44 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
1.44 logistic.py(343):                     dtype=X.dtype)
1.44 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
1.44 logistic.py(282):     n_classes = Y.shape[1]
1.44 logistic.py(283):     n_features = X.shape[1]
1.44 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
1.44 logistic.py(285):     w = w.reshape(n_classes, -1)
1.44 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
1.44 logistic.py(287):     if fit_intercept:
1.44 logistic.py(288):         intercept = w[:, -1]
1.44 logistic.py(289):         w = w[:, :-1]
1.44 logistic.py(292):     p = safe_sparse_dot(X, w.T)
1.44 logistic.py(293):     p += intercept
1.44 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
1.44 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
1.44 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
1.44 logistic.py(297):     p = np.exp(p, p)
1.44 logistic.py(298):     return loss, p, w
1.44 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
1.44 logistic.py(346):     diff = sample_weight * (p - Y)
1.44 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
1.44 logistic.py(348):     grad[:, :n_features] += alpha * w
1.44 logistic.py(349):     if fit_intercept:
1.44 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
1.44 logistic.py(351):     return loss, grad.ravel(), p
1.44 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
1.44 logistic.py(339):     n_classes = Y.shape[1]
1.44 logistic.py(340):     n_features = X.shape[1]
1.44 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
1.44 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
1.44 logistic.py(343):                     dtype=X.dtype)
1.44 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
1.44 logistic.py(282):     n_classes = Y.shape[1]
1.44 logistic.py(283):     n_features = X.shape[1]
1.44 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
1.44 logistic.py(285):     w = w.reshape(n_classes, -1)
1.44 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
1.44 logistic.py(287):     if fit_intercept:
1.44 logistic.py(288):         intercept = w[:, -1]
1.44 logistic.py(289):         w = w[:, :-1]
1.44 logistic.py(292):     p = safe_sparse_dot(X, w.T)
1.44 logistic.py(293):     p += intercept
1.44 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
1.44 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
1.44 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
1.44 logistic.py(297):     p = np.exp(p, p)
1.44 logistic.py(298):     return loss, p, w
1.44 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
1.44 logistic.py(346):     diff = sample_weight * (p - Y)
1.44 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
1.44 logistic.py(348):     grad[:, :n_features] += alpha * w
1.44 logistic.py(349):     if fit_intercept:
1.44 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
1.44 logistic.py(351):     return loss, grad.ravel(), p
1.44 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
1.44 logistic.py(339):     n_classes = Y.shape[1]
1.44 logistic.py(340):     n_features = X.shape[1]
1.44 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
1.44 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
1.44 logistic.py(343):                     dtype=X.dtype)
1.44 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
1.44 logistic.py(282):     n_classes = Y.shape[1]
1.44 logistic.py(283):     n_features = X.shape[1]
1.44 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
1.44 logistic.py(285):     w = w.reshape(n_classes, -1)
1.44 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
1.44 logistic.py(287):     if fit_intercept:
1.44 logistic.py(288):         intercept = w[:, -1]
1.44 logistic.py(289):         w = w[:, :-1]
1.44 logistic.py(292):     p = safe_sparse_dot(X, w.T)
1.44 logistic.py(293):     p += intercept
1.44 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
1.44 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
1.44 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
1.44 logistic.py(297):     p = np.exp(p, p)
1.44 logistic.py(298):     return loss, p, w
1.44 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
1.44 logistic.py(346):     diff = sample_weight * (p - Y)
1.44 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
1.44 logistic.py(348):     grad[:, :n_features] += alpha * w
1.44 logistic.py(349):     if fit_intercept:
1.44 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
1.44 logistic.py(351):     return loss, grad.ravel(), p
1.44 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
1.44 logistic.py(339):     n_classes = Y.shape[1]
1.44 logistic.py(340):     n_features = X.shape[1]
1.44 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
1.44 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
1.44 logistic.py(343):                     dtype=X.dtype)
1.44 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
1.44 logistic.py(282):     n_classes = Y.shape[1]
1.44 logistic.py(283):     n_features = X.shape[1]
1.44 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
1.44 logistic.py(285):     w = w.reshape(n_classes, -1)
1.44 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
1.44 logistic.py(287):     if fit_intercept:
1.44 logistic.py(288):         intercept = w[:, -1]
1.44 logistic.py(289):         w = w[:, :-1]
1.44 logistic.py(292):     p = safe_sparse_dot(X, w.T)
1.44 logistic.py(293):     p += intercept
1.44 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
1.44 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
1.44 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
1.44 logistic.py(297):     p = np.exp(p, p)
1.44 logistic.py(298):     return loss, p, w
1.44 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
1.44 logistic.py(346):     diff = sample_weight * (p - Y)
1.44 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
1.44 logistic.py(348):     grad[:, :n_features] += alpha * w
1.44 logistic.py(349):     if fit_intercept:
1.44 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
1.44 logistic.py(351):     return loss, grad.ravel(), p
1.44 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
1.44 logistic.py(339):     n_classes = Y.shape[1]
1.44 logistic.py(340):     n_features = X.shape[1]
1.44 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
1.44 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
1.44 logistic.py(343):                     dtype=X.dtype)
1.44 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
1.44 logistic.py(282):     n_classes = Y.shape[1]
1.44 logistic.py(283):     n_features = X.shape[1]
1.44 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
1.44 logistic.py(285):     w = w.reshape(n_classes, -1)
1.44 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
1.44 logistic.py(287):     if fit_intercept:
1.44 logistic.py(288):         intercept = w[:, -1]
1.44 logistic.py(289):         w = w[:, :-1]
1.44 logistic.py(292):     p = safe_sparse_dot(X, w.T)
1.44 logistic.py(293):     p += intercept
1.44 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
1.44 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
1.44 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
1.44 logistic.py(297):     p = np.exp(p, p)
1.44 logistic.py(298):     return loss, p, w
1.44 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
1.44 logistic.py(346):     diff = sample_weight * (p - Y)
1.44 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
1.44 logistic.py(348):     grad[:, :n_features] += alpha * w
1.44 logistic.py(349):     if fit_intercept:
1.44 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
1.44 logistic.py(351):     return loss, grad.ravel(), p
1.44 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
1.44 logistic.py(339):     n_classes = Y.shape[1]
1.44 logistic.py(340):     n_features = X.shape[1]
1.44 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
1.44 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
1.44 logistic.py(343):                     dtype=X.dtype)
1.44 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
1.44 logistic.py(282):     n_classes = Y.shape[1]
1.44 logistic.py(283):     n_features = X.shape[1]
1.44 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
1.44 logistic.py(285):     w = w.reshape(n_classes, -1)
1.44 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
1.44 logistic.py(287):     if fit_intercept:
1.44 logistic.py(288):         intercept = w[:, -1]
1.44 logistic.py(289):         w = w[:, :-1]
1.44 logistic.py(292):     p = safe_sparse_dot(X, w.T)
1.44 logistic.py(293):     p += intercept
1.44 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
1.44 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
1.44 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
1.44 logistic.py(297):     p = np.exp(p, p)
1.44 logistic.py(298):     return loss, p, w
1.44 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
1.44 logistic.py(346):     diff = sample_weight * (p - Y)
1.44 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
1.44 logistic.py(348):     grad[:, :n_features] += alpha * w
1.44 logistic.py(349):     if fit_intercept:
1.44 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
1.44 logistic.py(351):     return loss, grad.ravel(), p
1.44 logistic.py(718):             if info["warnflag"] == 1:
1.44 logistic.py(723):             n_iter_i = min(info['nit'], max_iter)
1.44 logistic.py(760):         if multi_class == 'multinomial':
1.44 logistic.py(761):             multi_w0 = np.reshape(w0, (classes.size, -1))
1.44 logistic.py(762):             if classes.size == 2:
1.44 logistic.py(764):             coefs.append(multi_w0)
1.44 logistic.py(768):         n_iter[i] = n_iter_i
1.44 logistic.py(712):     for i, C in enumerate(Cs):
1.44 logistic.py(713):         if solver == 'lbfgs':
1.44 logistic.py(714):             w0, loss, info = optimize.fmin_l_bfgs_b(
1.44 logistic.py(715):                 func, w0, fprime=None,
1.44 logistic.py(716):                 args=(X, target, 1. / C, sample_weight),
1.44 logistic.py(717):                 iprint=(verbose > 0) - 1, pgtol=tol, maxiter=max_iter)
1.44 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
1.44 logistic.py(339):     n_classes = Y.shape[1]
1.44 logistic.py(340):     n_features = X.shape[1]
1.44 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
1.44 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
1.44 logistic.py(343):                     dtype=X.dtype)
1.44 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
1.44 logistic.py(282):     n_classes = Y.shape[1]
1.44 logistic.py(283):     n_features = X.shape[1]
1.44 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
1.44 logistic.py(285):     w = w.reshape(n_classes, -1)
1.44 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
1.44 logistic.py(287):     if fit_intercept:
1.44 logistic.py(288):         intercept = w[:, -1]
1.44 logistic.py(289):         w = w[:, :-1]
1.44 logistic.py(292):     p = safe_sparse_dot(X, w.T)
1.44 logistic.py(293):     p += intercept
1.44 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
1.44 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
1.44 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
1.44 logistic.py(297):     p = np.exp(p, p)
1.44 logistic.py(298):     return loss, p, w
1.44 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
1.44 logistic.py(346):     diff = sample_weight * (p - Y)
1.44 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
1.44 logistic.py(348):     grad[:, :n_features] += alpha * w
1.44 logistic.py(349):     if fit_intercept:
1.44 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
1.44 logistic.py(351):     return loss, grad.ravel(), p
1.44 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
1.44 logistic.py(339):     n_classes = Y.shape[1]
1.44 logistic.py(340):     n_features = X.shape[1]
1.44 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
1.44 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
1.44 logistic.py(343):                     dtype=X.dtype)
1.44 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
1.44 logistic.py(282):     n_classes = Y.shape[1]
1.44 logistic.py(283):     n_features = X.shape[1]
1.44 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
1.44 logistic.py(285):     w = w.reshape(n_classes, -1)
1.44 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
1.44 logistic.py(287):     if fit_intercept:
1.44 logistic.py(288):         intercept = w[:, -1]
1.44 logistic.py(289):         w = w[:, :-1]
1.44 logistic.py(292):     p = safe_sparse_dot(X, w.T)
1.44 logistic.py(293):     p += intercept
1.44 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
1.44 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
1.44 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
1.44 logistic.py(297):     p = np.exp(p, p)
1.44 logistic.py(298):     return loss, p, w
1.44 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
1.44 logistic.py(346):     diff = sample_weight * (p - Y)
1.44 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
1.44 logistic.py(348):     grad[:, :n_features] += alpha * w
1.44 logistic.py(349):     if fit_intercept:
1.44 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
1.44 logistic.py(351):     return loss, grad.ravel(), p
1.44 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
1.44 logistic.py(339):     n_classes = Y.shape[1]
1.44 logistic.py(340):     n_features = X.shape[1]
1.44 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
1.44 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
1.44 logistic.py(343):                     dtype=X.dtype)
1.44 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
1.44 logistic.py(282):     n_classes = Y.shape[1]
1.44 logistic.py(283):     n_features = X.shape[1]
1.44 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
1.44 logistic.py(285):     w = w.reshape(n_classes, -1)
1.44 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
1.44 logistic.py(287):     if fit_intercept:
1.44 logistic.py(288):         intercept = w[:, -1]
1.44 logistic.py(289):         w = w[:, :-1]
1.44 logistic.py(292):     p = safe_sparse_dot(X, w.T)
1.44 logistic.py(293):     p += intercept
1.44 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
1.44 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
1.44 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
1.44 logistic.py(297):     p = np.exp(p, p)
1.44 logistic.py(298):     return loss, p, w
1.44 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
1.44 logistic.py(346):     diff = sample_weight * (p - Y)
1.44 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
1.44 logistic.py(348):     grad[:, :n_features] += alpha * w
1.44 logistic.py(349):     if fit_intercept:
1.44 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
1.44 logistic.py(351):     return loss, grad.ravel(), p
1.44 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
1.44 logistic.py(339):     n_classes = Y.shape[1]
1.44 logistic.py(340):     n_features = X.shape[1]
1.44 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
1.44 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
1.44 logistic.py(343):                     dtype=X.dtype)
1.44 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
1.44 logistic.py(282):     n_classes = Y.shape[1]
1.44 logistic.py(283):     n_features = X.shape[1]
1.44 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
1.44 logistic.py(285):     w = w.reshape(n_classes, -1)
1.44 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
1.44 logistic.py(287):     if fit_intercept:
1.44 logistic.py(288):         intercept = w[:, -1]
1.44 logistic.py(289):         w = w[:, :-1]
1.44 logistic.py(292):     p = safe_sparse_dot(X, w.T)
1.44 logistic.py(293):     p += intercept
1.44 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
1.44 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
1.44 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
1.44 logistic.py(297):     p = np.exp(p, p)
1.44 logistic.py(298):     return loss, p, w
1.44 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
1.44 logistic.py(346):     diff = sample_weight * (p - Y)
1.44 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
1.44 logistic.py(348):     grad[:, :n_features] += alpha * w
1.44 logistic.py(349):     if fit_intercept:
1.44 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
1.44 logistic.py(351):     return loss, grad.ravel(), p
1.45 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
1.45 logistic.py(339):     n_classes = Y.shape[1]
1.45 logistic.py(340):     n_features = X.shape[1]
1.45 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
1.45 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
1.45 logistic.py(343):                     dtype=X.dtype)
1.45 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
1.45 logistic.py(282):     n_classes = Y.shape[1]
1.45 logistic.py(283):     n_features = X.shape[1]
1.45 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
1.45 logistic.py(285):     w = w.reshape(n_classes, -1)
1.45 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
1.45 logistic.py(287):     if fit_intercept:
1.45 logistic.py(288):         intercept = w[:, -1]
1.45 logistic.py(289):         w = w[:, :-1]
1.45 logistic.py(292):     p = safe_sparse_dot(X, w.T)
1.45 logistic.py(293):     p += intercept
1.45 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
1.45 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
1.45 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
1.45 logistic.py(297):     p = np.exp(p, p)
1.45 logistic.py(298):     return loss, p, w
1.45 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
1.45 logistic.py(346):     diff = sample_weight * (p - Y)
1.45 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
1.45 logistic.py(348):     grad[:, :n_features] += alpha * w
1.45 logistic.py(349):     if fit_intercept:
1.45 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
1.45 logistic.py(351):     return loss, grad.ravel(), p
1.45 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
1.45 logistic.py(339):     n_classes = Y.shape[1]
1.45 logistic.py(340):     n_features = X.shape[1]
1.45 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
1.45 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
1.45 logistic.py(343):                     dtype=X.dtype)
1.45 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
1.45 logistic.py(282):     n_classes = Y.shape[1]
1.45 logistic.py(283):     n_features = X.shape[1]
1.45 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
1.45 logistic.py(285):     w = w.reshape(n_classes, -1)
1.45 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
1.45 logistic.py(287):     if fit_intercept:
1.45 logistic.py(288):         intercept = w[:, -1]
1.45 logistic.py(289):         w = w[:, :-1]
1.45 logistic.py(292):     p = safe_sparse_dot(X, w.T)
1.45 logistic.py(293):     p += intercept
1.45 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
1.45 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
1.45 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
1.45 logistic.py(297):     p = np.exp(p, p)
1.45 logistic.py(298):     return loss, p, w
1.45 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
1.45 logistic.py(346):     diff = sample_weight * (p - Y)
1.45 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
1.45 logistic.py(348):     grad[:, :n_features] += alpha * w
1.45 logistic.py(349):     if fit_intercept:
1.45 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
1.45 logistic.py(351):     return loss, grad.ravel(), p
1.45 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
1.45 logistic.py(339):     n_classes = Y.shape[1]
1.45 logistic.py(340):     n_features = X.shape[1]
1.45 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
1.45 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
1.45 logistic.py(343):                     dtype=X.dtype)
1.45 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
1.45 logistic.py(282):     n_classes = Y.shape[1]
1.45 logistic.py(283):     n_features = X.shape[1]
1.45 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
1.45 logistic.py(285):     w = w.reshape(n_classes, -1)
1.45 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
1.45 logistic.py(287):     if fit_intercept:
1.45 logistic.py(288):         intercept = w[:, -1]
1.45 logistic.py(289):         w = w[:, :-1]
1.45 logistic.py(292):     p = safe_sparse_dot(X, w.T)
1.45 logistic.py(293):     p += intercept
1.45 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
1.45 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
1.45 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
1.45 logistic.py(297):     p = np.exp(p, p)
1.45 logistic.py(298):     return loss, p, w
1.45 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
1.45 logistic.py(346):     diff = sample_weight * (p - Y)
1.45 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
1.45 logistic.py(348):     grad[:, :n_features] += alpha * w
1.45 logistic.py(349):     if fit_intercept:
1.45 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
1.45 logistic.py(351):     return loss, grad.ravel(), p
1.45 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
1.45 logistic.py(339):     n_classes = Y.shape[1]
1.45 logistic.py(340):     n_features = X.shape[1]
1.45 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
1.45 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
1.45 logistic.py(343):                     dtype=X.dtype)
1.45 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
1.45 logistic.py(282):     n_classes = Y.shape[1]
1.45 logistic.py(283):     n_features = X.shape[1]
1.45 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
1.45 logistic.py(285):     w = w.reshape(n_classes, -1)
1.45 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
1.45 logistic.py(287):     if fit_intercept:
1.45 logistic.py(288):         intercept = w[:, -1]
1.45 logistic.py(289):         w = w[:, :-1]
1.45 logistic.py(292):     p = safe_sparse_dot(X, w.T)
1.45 logistic.py(293):     p += intercept
1.45 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
1.45 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
1.45 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
1.45 logistic.py(297):     p = np.exp(p, p)
1.45 logistic.py(298):     return loss, p, w
1.45 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
1.45 logistic.py(346):     diff = sample_weight * (p - Y)
1.45 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
1.45 logistic.py(348):     grad[:, :n_features] += alpha * w
1.45 logistic.py(349):     if fit_intercept:
1.45 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
1.45 logistic.py(351):     return loss, grad.ravel(), p
1.45 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
1.45 logistic.py(339):     n_classes = Y.shape[1]
1.45 logistic.py(340):     n_features = X.shape[1]
1.45 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
1.45 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
1.45 logistic.py(343):                     dtype=X.dtype)
1.45 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
1.45 logistic.py(282):     n_classes = Y.shape[1]
1.45 logistic.py(283):     n_features = X.shape[1]
1.45 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
1.45 logistic.py(285):     w = w.reshape(n_classes, -1)
1.45 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
1.45 logistic.py(287):     if fit_intercept:
1.45 logistic.py(288):         intercept = w[:, -1]
1.45 logistic.py(289):         w = w[:, :-1]
1.45 logistic.py(292):     p = safe_sparse_dot(X, w.T)
1.45 logistic.py(293):     p += intercept
1.45 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
1.45 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
1.45 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
1.45 logistic.py(297):     p = np.exp(p, p)
1.45 logistic.py(298):     return loss, p, w
1.45 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
1.45 logistic.py(346):     diff = sample_weight * (p - Y)
1.45 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
1.45 logistic.py(348):     grad[:, :n_features] += alpha * w
1.45 logistic.py(349):     if fit_intercept:
1.45 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
1.45 logistic.py(351):     return loss, grad.ravel(), p
1.45 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
1.45 logistic.py(339):     n_classes = Y.shape[1]
1.45 logistic.py(340):     n_features = X.shape[1]
1.45 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
1.45 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
1.45 logistic.py(343):                     dtype=X.dtype)
1.45 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
1.45 logistic.py(282):     n_classes = Y.shape[1]
1.45 logistic.py(283):     n_features = X.shape[1]
1.45 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
1.45 logistic.py(285):     w = w.reshape(n_classes, -1)
1.45 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
1.45 logistic.py(287):     if fit_intercept:
1.45 logistic.py(288):         intercept = w[:, -1]
1.45 logistic.py(289):         w = w[:, :-1]
1.45 logistic.py(292):     p = safe_sparse_dot(X, w.T)
1.45 logistic.py(293):     p += intercept
1.45 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
1.45 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
1.45 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
1.45 logistic.py(297):     p = np.exp(p, p)
1.45 logistic.py(298):     return loss, p, w
1.45 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
1.45 logistic.py(346):     diff = sample_weight * (p - Y)
1.45 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
1.45 logistic.py(348):     grad[:, :n_features] += alpha * w
1.45 logistic.py(349):     if fit_intercept:
1.45 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
1.45 logistic.py(351):     return loss, grad.ravel(), p
1.45 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
1.45 logistic.py(339):     n_classes = Y.shape[1]
1.45 logistic.py(340):     n_features = X.shape[1]
1.45 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
1.45 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
1.45 logistic.py(343):                     dtype=X.dtype)
1.45 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
1.45 logistic.py(282):     n_classes = Y.shape[1]
1.45 logistic.py(283):     n_features = X.shape[1]
1.45 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
1.45 logistic.py(285):     w = w.reshape(n_classes, -1)
1.45 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
1.45 logistic.py(287):     if fit_intercept:
1.45 logistic.py(288):         intercept = w[:, -1]
1.45 logistic.py(289):         w = w[:, :-1]
1.45 logistic.py(292):     p = safe_sparse_dot(X, w.T)
1.45 logistic.py(293):     p += intercept
1.45 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
1.45 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
1.45 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
1.45 logistic.py(297):     p = np.exp(p, p)
1.45 logistic.py(298):     return loss, p, w
1.45 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
1.45 logistic.py(346):     diff = sample_weight * (p - Y)
1.45 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
1.45 logistic.py(348):     grad[:, :n_features] += alpha * w
1.45 logistic.py(349):     if fit_intercept:
1.45 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
1.45 logistic.py(351):     return loss, grad.ravel(), p
1.45 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
1.45 logistic.py(339):     n_classes = Y.shape[1]
1.45 logistic.py(340):     n_features = X.shape[1]
1.45 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
1.45 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
1.45 logistic.py(343):                     dtype=X.dtype)
1.45 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
1.45 logistic.py(282):     n_classes = Y.shape[1]
1.45 logistic.py(283):     n_features = X.shape[1]
1.45 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
1.45 logistic.py(285):     w = w.reshape(n_classes, -1)
1.45 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
1.45 logistic.py(287):     if fit_intercept:
1.45 logistic.py(288):         intercept = w[:, -1]
1.45 logistic.py(289):         w = w[:, :-1]
1.45 logistic.py(292):     p = safe_sparse_dot(X, w.T)
1.45 logistic.py(293):     p += intercept
1.45 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
1.45 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
1.45 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
1.45 logistic.py(297):     p = np.exp(p, p)
1.45 logistic.py(298):     return loss, p, w
1.45 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
1.45 logistic.py(346):     diff = sample_weight * (p - Y)
1.45 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
1.45 logistic.py(348):     grad[:, :n_features] += alpha * w
1.45 logistic.py(349):     if fit_intercept:
1.45 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
1.45 logistic.py(351):     return loss, grad.ravel(), p
1.45 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
1.45 logistic.py(339):     n_classes = Y.shape[1]
1.45 logistic.py(340):     n_features = X.shape[1]
1.45 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
1.45 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
1.45 logistic.py(343):                     dtype=X.dtype)
1.45 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
1.45 logistic.py(282):     n_classes = Y.shape[1]
1.45 logistic.py(283):     n_features = X.shape[1]
1.45 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
1.45 logistic.py(285):     w = w.reshape(n_classes, -1)
1.45 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
1.45 logistic.py(287):     if fit_intercept:
1.45 logistic.py(288):         intercept = w[:, -1]
1.45 logistic.py(289):         w = w[:, :-1]
1.45 logistic.py(292):     p = safe_sparse_dot(X, w.T)
1.45 logistic.py(293):     p += intercept
1.45 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
1.45 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
1.45 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
1.45 logistic.py(297):     p = np.exp(p, p)
1.45 logistic.py(298):     return loss, p, w
1.45 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
1.45 logistic.py(346):     diff = sample_weight * (p - Y)
1.45 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
1.45 logistic.py(348):     grad[:, :n_features] += alpha * w
1.45 logistic.py(349):     if fit_intercept:
1.45 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
1.45 logistic.py(351):     return loss, grad.ravel(), p
1.45 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
1.45 logistic.py(339):     n_classes = Y.shape[1]
1.45 logistic.py(340):     n_features = X.shape[1]
1.45 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
1.45 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
1.45 logistic.py(343):                     dtype=X.dtype)
1.45 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
1.45 logistic.py(282):     n_classes = Y.shape[1]
1.45 logistic.py(283):     n_features = X.shape[1]
1.45 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
1.45 logistic.py(285):     w = w.reshape(n_classes, -1)
1.45 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
1.45 logistic.py(287):     if fit_intercept:
1.45 logistic.py(288):         intercept = w[:, -1]
1.45 logistic.py(289):         w = w[:, :-1]
1.45 logistic.py(292):     p = safe_sparse_dot(X, w.T)
1.45 logistic.py(293):     p += intercept
1.45 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
1.45 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
1.45 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
1.45 logistic.py(297):     p = np.exp(p, p)
1.45 logistic.py(298):     return loss, p, w
1.45 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
1.45 logistic.py(346):     diff = sample_weight * (p - Y)
1.45 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
1.45 logistic.py(348):     grad[:, :n_features] += alpha * w
1.45 logistic.py(349):     if fit_intercept:
1.45 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
1.45 logistic.py(351):     return loss, grad.ravel(), p
1.45 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
1.45 logistic.py(339):     n_classes = Y.shape[1]
1.45 logistic.py(340):     n_features = X.shape[1]
1.45 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
1.45 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
1.45 logistic.py(343):                     dtype=X.dtype)
1.45 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
1.45 logistic.py(282):     n_classes = Y.shape[1]
1.45 logistic.py(283):     n_features = X.shape[1]
1.45 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
1.45 logistic.py(285):     w = w.reshape(n_classes, -1)
1.45 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
1.45 logistic.py(287):     if fit_intercept:
1.45 logistic.py(288):         intercept = w[:, -1]
1.45 logistic.py(289):         w = w[:, :-1]
1.45 logistic.py(292):     p = safe_sparse_dot(X, w.T)
1.45 logistic.py(293):     p += intercept
1.45 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
1.45 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
1.45 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
1.45 logistic.py(297):     p = np.exp(p, p)
1.45 logistic.py(298):     return loss, p, w
1.45 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
1.45 logistic.py(346):     diff = sample_weight * (p - Y)
1.45 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
1.45 logistic.py(348):     grad[:, :n_features] += alpha * w
1.45 logistic.py(349):     if fit_intercept:
1.45 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
1.45 logistic.py(351):     return loss, grad.ravel(), p
1.45 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
1.45 logistic.py(339):     n_classes = Y.shape[1]
1.45 logistic.py(340):     n_features = X.shape[1]
1.45 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
1.45 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
1.45 logistic.py(343):                     dtype=X.dtype)
1.45 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
1.45 logistic.py(282):     n_classes = Y.shape[1]
1.45 logistic.py(283):     n_features = X.shape[1]
1.45 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
1.45 logistic.py(285):     w = w.reshape(n_classes, -1)
1.45 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
1.45 logistic.py(287):     if fit_intercept:
1.45 logistic.py(288):         intercept = w[:, -1]
1.45 logistic.py(289):         w = w[:, :-1]
1.45 logistic.py(292):     p = safe_sparse_dot(X, w.T)
1.45 logistic.py(293):     p += intercept
1.45 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
1.45 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
1.45 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
1.45 logistic.py(297):     p = np.exp(p, p)
1.45 logistic.py(298):     return loss, p, w
1.45 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
1.45 logistic.py(346):     diff = sample_weight * (p - Y)
1.45 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
1.45 logistic.py(348):     grad[:, :n_features] += alpha * w
1.45 logistic.py(349):     if fit_intercept:
1.45 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
1.45 logistic.py(351):     return loss, grad.ravel(), p
1.45 logistic.py(718):             if info["warnflag"] == 1:
1.45 logistic.py(723):             n_iter_i = min(info['nit'], max_iter)
1.45 logistic.py(760):         if multi_class == 'multinomial':
1.45 logistic.py(761):             multi_w0 = np.reshape(w0, (classes.size, -1))
1.45 logistic.py(762):             if classes.size == 2:
1.45 logistic.py(764):             coefs.append(multi_w0)
1.45 logistic.py(768):         n_iter[i] = n_iter_i
1.45 logistic.py(712):     for i, C in enumerate(Cs):
1.45 logistic.py(713):         if solver == 'lbfgs':
1.45 logistic.py(714):             w0, loss, info = optimize.fmin_l_bfgs_b(
1.45 logistic.py(715):                 func, w0, fprime=None,
1.45 logistic.py(716):                 args=(X, target, 1. / C, sample_weight),
1.45 logistic.py(717):                 iprint=(verbose > 0) - 1, pgtol=tol, maxiter=max_iter)
1.45 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
1.45 logistic.py(339):     n_classes = Y.shape[1]
1.45 logistic.py(340):     n_features = X.shape[1]
1.45 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
1.45 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
1.45 logistic.py(343):                     dtype=X.dtype)
1.45 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
1.45 logistic.py(282):     n_classes = Y.shape[1]
1.45 logistic.py(283):     n_features = X.shape[1]
1.45 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
1.45 logistic.py(285):     w = w.reshape(n_classes, -1)
1.45 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
1.45 logistic.py(287):     if fit_intercept:
1.45 logistic.py(288):         intercept = w[:, -1]
1.45 logistic.py(289):         w = w[:, :-1]
1.45 logistic.py(292):     p = safe_sparse_dot(X, w.T)
1.45 logistic.py(293):     p += intercept
1.45 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
1.45 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
1.45 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
1.45 logistic.py(297):     p = np.exp(p, p)
1.45 logistic.py(298):     return loss, p, w
1.45 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
1.45 logistic.py(346):     diff = sample_weight * (p - Y)
1.45 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
1.45 logistic.py(348):     grad[:, :n_features] += alpha * w
1.45 logistic.py(349):     if fit_intercept:
1.45 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
1.45 logistic.py(351):     return loss, grad.ravel(), p
1.45 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
1.45 logistic.py(339):     n_classes = Y.shape[1]
1.45 logistic.py(340):     n_features = X.shape[1]
1.45 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
1.45 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
1.45 logistic.py(343):                     dtype=X.dtype)
1.45 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
1.45 logistic.py(282):     n_classes = Y.shape[1]
1.45 logistic.py(283):     n_features = X.shape[1]
1.45 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
1.45 logistic.py(285):     w = w.reshape(n_classes, -1)
1.45 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
1.45 logistic.py(287):     if fit_intercept:
1.45 logistic.py(288):         intercept = w[:, -1]
1.45 logistic.py(289):         w = w[:, :-1]
1.45 logistic.py(292):     p = safe_sparse_dot(X, w.T)
1.45 logistic.py(293):     p += intercept
1.45 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
1.45 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
1.45 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
1.45 logistic.py(297):     p = np.exp(p, p)
1.45 logistic.py(298):     return loss, p, w
1.45 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
1.45 logistic.py(346):     diff = sample_weight * (p - Y)
1.45 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
1.45 logistic.py(348):     grad[:, :n_features] += alpha * w
1.45 logistic.py(349):     if fit_intercept:
1.45 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
1.45 logistic.py(351):     return loss, grad.ravel(), p
1.45 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
1.45 logistic.py(339):     n_classes = Y.shape[1]
1.45 logistic.py(340):     n_features = X.shape[1]
1.45 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
1.45 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
1.45 logistic.py(343):                     dtype=X.dtype)
1.45 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
1.45 logistic.py(282):     n_classes = Y.shape[1]
1.45 logistic.py(283):     n_features = X.shape[1]
1.45 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
1.45 logistic.py(285):     w = w.reshape(n_classes, -1)
1.45 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
1.45 logistic.py(287):     if fit_intercept:
1.45 logistic.py(288):         intercept = w[:, -1]
1.45 logistic.py(289):         w = w[:, :-1]
1.45 logistic.py(292):     p = safe_sparse_dot(X, w.T)
1.45 logistic.py(293):     p += intercept
1.45 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
1.45 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
1.45 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
1.45 logistic.py(297):     p = np.exp(p, p)
1.45 logistic.py(298):     return loss, p, w
1.45 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
1.45 logistic.py(346):     diff = sample_weight * (p - Y)
1.45 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
1.45 logistic.py(348):     grad[:, :n_features] += alpha * w
1.45 logistic.py(349):     if fit_intercept:
1.45 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
1.45 logistic.py(351):     return loss, grad.ravel(), p
1.45 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
1.45 logistic.py(339):     n_classes = Y.shape[1]
1.45 logistic.py(340):     n_features = X.shape[1]
1.45 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
1.45 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
1.45 logistic.py(343):                     dtype=X.dtype)
1.45 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
1.45 logistic.py(282):     n_classes = Y.shape[1]
1.45 logistic.py(283):     n_features = X.shape[1]
1.45 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
1.45 logistic.py(285):     w = w.reshape(n_classes, -1)
1.45 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
1.45 logistic.py(287):     if fit_intercept:
1.45 logistic.py(288):         intercept = w[:, -1]
1.45 logistic.py(289):         w = w[:, :-1]
1.45 logistic.py(292):     p = safe_sparse_dot(X, w.T)
1.45 logistic.py(293):     p += intercept
1.45 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
1.45 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
1.45 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
1.45 logistic.py(297):     p = np.exp(p, p)
1.45 logistic.py(298):     return loss, p, w
1.45 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
1.45 logistic.py(346):     diff = sample_weight * (p - Y)
1.45 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
1.45 logistic.py(348):     grad[:, :n_features] += alpha * w
1.45 logistic.py(349):     if fit_intercept:
1.45 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
1.45 logistic.py(351):     return loss, grad.ravel(), p
1.45 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
1.45 logistic.py(339):     n_classes = Y.shape[1]
1.45 logistic.py(340):     n_features = X.shape[1]
1.46 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
1.46 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
1.46 logistic.py(343):                     dtype=X.dtype)
1.46 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
1.46 logistic.py(282):     n_classes = Y.shape[1]
1.46 logistic.py(283):     n_features = X.shape[1]
1.46 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
1.46 logistic.py(285):     w = w.reshape(n_classes, -1)
1.46 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
1.46 logistic.py(287):     if fit_intercept:
1.46 logistic.py(288):         intercept = w[:, -1]
1.46 logistic.py(289):         w = w[:, :-1]
1.46 logistic.py(292):     p = safe_sparse_dot(X, w.T)
1.46 logistic.py(293):     p += intercept
1.46 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
1.46 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
1.46 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
1.46 logistic.py(297):     p = np.exp(p, p)
1.46 logistic.py(298):     return loss, p, w
1.46 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
1.46 logistic.py(346):     diff = sample_weight * (p - Y)
1.46 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
1.46 logistic.py(348):     grad[:, :n_features] += alpha * w
1.46 logistic.py(349):     if fit_intercept:
1.46 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
1.46 logistic.py(351):     return loss, grad.ravel(), p
1.46 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
1.46 logistic.py(339):     n_classes = Y.shape[1]
1.46 logistic.py(340):     n_features = X.shape[1]
1.46 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
1.46 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
1.46 logistic.py(343):                     dtype=X.dtype)
1.46 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
1.46 logistic.py(282):     n_classes = Y.shape[1]
1.46 logistic.py(283):     n_features = X.shape[1]
1.46 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
1.46 logistic.py(285):     w = w.reshape(n_classes, -1)
1.46 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
1.46 logistic.py(287):     if fit_intercept:
1.46 logistic.py(288):         intercept = w[:, -1]
1.46 logistic.py(289):         w = w[:, :-1]
1.46 logistic.py(292):     p = safe_sparse_dot(X, w.T)
1.46 logistic.py(293):     p += intercept
1.46 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
1.46 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
1.46 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
1.46 logistic.py(297):     p = np.exp(p, p)
1.46 logistic.py(298):     return loss, p, w
1.46 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
1.46 logistic.py(346):     diff = sample_weight * (p - Y)
1.46 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
1.46 logistic.py(348):     grad[:, :n_features] += alpha * w
1.46 logistic.py(349):     if fit_intercept:
1.46 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
1.46 logistic.py(351):     return loss, grad.ravel(), p
1.46 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
1.46 logistic.py(339):     n_classes = Y.shape[1]
1.46 logistic.py(340):     n_features = X.shape[1]
1.46 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
1.46 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
1.46 logistic.py(343):                     dtype=X.dtype)
1.46 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
1.46 logistic.py(282):     n_classes = Y.shape[1]
1.46 logistic.py(283):     n_features = X.shape[1]
1.46 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
1.46 logistic.py(285):     w = w.reshape(n_classes, -1)
1.46 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
1.46 logistic.py(287):     if fit_intercept:
1.46 logistic.py(288):         intercept = w[:, -1]
1.46 logistic.py(289):         w = w[:, :-1]
1.46 logistic.py(292):     p = safe_sparse_dot(X, w.T)
1.46 logistic.py(293):     p += intercept
1.46 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
1.46 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
1.46 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
1.46 logistic.py(297):     p = np.exp(p, p)
1.46 logistic.py(298):     return loss, p, w
1.46 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
1.46 logistic.py(346):     diff = sample_weight * (p - Y)
1.46 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
1.46 logistic.py(348):     grad[:, :n_features] += alpha * w
1.46 logistic.py(349):     if fit_intercept:
1.46 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
1.46 logistic.py(351):     return loss, grad.ravel(), p
1.46 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
1.46 logistic.py(339):     n_classes = Y.shape[1]
1.46 logistic.py(340):     n_features = X.shape[1]
1.46 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
1.46 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
1.46 logistic.py(343):                     dtype=X.dtype)
1.46 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
1.46 logistic.py(282):     n_classes = Y.shape[1]
1.46 logistic.py(283):     n_features = X.shape[1]
1.46 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
1.46 logistic.py(285):     w = w.reshape(n_classes, -1)
1.46 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
1.46 logistic.py(287):     if fit_intercept:
1.46 logistic.py(288):         intercept = w[:, -1]
1.46 logistic.py(289):         w = w[:, :-1]
1.46 logistic.py(292):     p = safe_sparse_dot(X, w.T)
1.46 logistic.py(293):     p += intercept
1.46 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
1.46 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
1.46 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
1.46 logistic.py(297):     p = np.exp(p, p)
1.46 logistic.py(298):     return loss, p, w
1.46 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
1.46 logistic.py(346):     diff = sample_weight * (p - Y)
1.46 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
1.46 logistic.py(348):     grad[:, :n_features] += alpha * w
1.46 logistic.py(349):     if fit_intercept:
1.46 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
1.46 logistic.py(351):     return loss, grad.ravel(), p
1.46 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
1.46 logistic.py(339):     n_classes = Y.shape[1]
1.46 logistic.py(340):     n_features = X.shape[1]
1.46 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
1.46 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
1.46 logistic.py(343):                     dtype=X.dtype)
1.46 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
1.46 logistic.py(282):     n_classes = Y.shape[1]
1.46 logistic.py(283):     n_features = X.shape[1]
1.46 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
1.46 logistic.py(285):     w = w.reshape(n_classes, -1)
1.46 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
1.46 logistic.py(287):     if fit_intercept:
1.46 logistic.py(288):         intercept = w[:, -1]
1.46 logistic.py(289):         w = w[:, :-1]
1.46 logistic.py(292):     p = safe_sparse_dot(X, w.T)
1.46 logistic.py(293):     p += intercept
1.46 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
1.46 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
1.46 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
1.46 logistic.py(297):     p = np.exp(p, p)
1.46 logistic.py(298):     return loss, p, w
1.46 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
1.46 logistic.py(346):     diff = sample_weight * (p - Y)
1.46 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
1.46 logistic.py(348):     grad[:, :n_features] += alpha * w
1.46 logistic.py(349):     if fit_intercept:
1.46 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
1.46 logistic.py(351):     return loss, grad.ravel(), p
1.46 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
1.46 logistic.py(339):     n_classes = Y.shape[1]
1.46 logistic.py(340):     n_features = X.shape[1]
1.46 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
1.46 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
1.46 logistic.py(343):                     dtype=X.dtype)
1.46 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
1.46 logistic.py(282):     n_classes = Y.shape[1]
1.46 logistic.py(283):     n_features = X.shape[1]
1.46 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
1.46 logistic.py(285):     w = w.reshape(n_classes, -1)
1.46 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
1.46 logistic.py(287):     if fit_intercept:
1.46 logistic.py(288):         intercept = w[:, -1]
1.46 logistic.py(289):         w = w[:, :-1]
1.46 logistic.py(292):     p = safe_sparse_dot(X, w.T)
1.46 logistic.py(293):     p += intercept
1.46 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
1.46 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
1.46 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
1.46 logistic.py(297):     p = np.exp(p, p)
1.46 logistic.py(298):     return loss, p, w
1.46 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
1.46 logistic.py(346):     diff = sample_weight * (p - Y)
1.46 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
1.46 logistic.py(348):     grad[:, :n_features] += alpha * w
1.46 logistic.py(349):     if fit_intercept:
1.46 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
1.46 logistic.py(351):     return loss, grad.ravel(), p
1.46 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
1.46 logistic.py(339):     n_classes = Y.shape[1]
1.46 logistic.py(340):     n_features = X.shape[1]
1.46 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
1.46 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
1.46 logistic.py(343):                     dtype=X.dtype)
1.46 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
1.46 logistic.py(282):     n_classes = Y.shape[1]
1.46 logistic.py(283):     n_features = X.shape[1]
1.46 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
1.46 logistic.py(285):     w = w.reshape(n_classes, -1)
1.46 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
1.46 logistic.py(287):     if fit_intercept:
1.46 logistic.py(288):         intercept = w[:, -1]
1.46 logistic.py(289):         w = w[:, :-1]
1.46 logistic.py(292):     p = safe_sparse_dot(X, w.T)
1.46 logistic.py(293):     p += intercept
1.46 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
1.46 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
1.46 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
1.46 logistic.py(297):     p = np.exp(p, p)
1.46 logistic.py(298):     return loss, p, w
1.46 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
1.46 logistic.py(346):     diff = sample_weight * (p - Y)
1.46 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
1.46 logistic.py(348):     grad[:, :n_features] += alpha * w
1.46 logistic.py(349):     if fit_intercept:
1.46 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
1.46 logistic.py(351):     return loss, grad.ravel(), p
1.46 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
1.46 logistic.py(339):     n_classes = Y.shape[1]
1.46 logistic.py(340):     n_features = X.shape[1]
1.46 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
1.46 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
1.46 logistic.py(343):                     dtype=X.dtype)
1.46 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
1.46 logistic.py(282):     n_classes = Y.shape[1]
1.46 logistic.py(283):     n_features = X.shape[1]
1.46 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
1.46 logistic.py(285):     w = w.reshape(n_classes, -1)
1.46 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
1.46 logistic.py(287):     if fit_intercept:
1.46 logistic.py(288):         intercept = w[:, -1]
1.46 logistic.py(289):         w = w[:, :-1]
1.46 logistic.py(292):     p = safe_sparse_dot(X, w.T)
1.46 logistic.py(293):     p += intercept
1.46 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
1.46 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
1.46 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
1.46 logistic.py(297):     p = np.exp(p, p)
1.46 logistic.py(298):     return loss, p, w
1.46 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
1.46 logistic.py(346):     diff = sample_weight * (p - Y)
1.46 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
1.46 logistic.py(348):     grad[:, :n_features] += alpha * w
1.46 logistic.py(349):     if fit_intercept:
1.46 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
1.46 logistic.py(351):     return loss, grad.ravel(), p
1.46 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
1.46 logistic.py(339):     n_classes = Y.shape[1]
1.46 logistic.py(340):     n_features = X.shape[1]
1.46 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
1.46 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
1.46 logistic.py(343):                     dtype=X.dtype)
1.46 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
1.46 logistic.py(282):     n_classes = Y.shape[1]
1.46 logistic.py(283):     n_features = X.shape[1]
1.46 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
1.46 logistic.py(285):     w = w.reshape(n_classes, -1)
1.46 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
1.46 logistic.py(287):     if fit_intercept:
1.46 logistic.py(288):         intercept = w[:, -1]
1.46 logistic.py(289):         w = w[:, :-1]
1.46 logistic.py(292):     p = safe_sparse_dot(X, w.T)
1.46 logistic.py(293):     p += intercept
1.46 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
1.46 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
1.46 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
1.46 logistic.py(297):     p = np.exp(p, p)
1.46 logistic.py(298):     return loss, p, w
1.46 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
1.46 logistic.py(346):     diff = sample_weight * (p - Y)
1.46 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
1.46 logistic.py(348):     grad[:, :n_features] += alpha * w
1.46 logistic.py(349):     if fit_intercept:
1.46 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
1.46 logistic.py(351):     return loss, grad.ravel(), p
1.46 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
1.46 logistic.py(339):     n_classes = Y.shape[1]
1.46 logistic.py(340):     n_features = X.shape[1]
1.46 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
1.46 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
1.46 logistic.py(343):                     dtype=X.dtype)
1.46 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
1.46 logistic.py(282):     n_classes = Y.shape[1]
1.46 logistic.py(283):     n_features = X.shape[1]
1.46 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
1.46 logistic.py(285):     w = w.reshape(n_classes, -1)
1.46 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
1.46 logistic.py(287):     if fit_intercept:
1.46 logistic.py(288):         intercept = w[:, -1]
1.46 logistic.py(289):         w = w[:, :-1]
1.46 logistic.py(292):     p = safe_sparse_dot(X, w.T)
1.46 logistic.py(293):     p += intercept
1.46 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
1.46 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
1.46 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
1.46 logistic.py(297):     p = np.exp(p, p)
1.46 logistic.py(298):     return loss, p, w
1.46 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
1.46 logistic.py(346):     diff = sample_weight * (p - Y)
1.46 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
1.46 logistic.py(348):     grad[:, :n_features] += alpha * w
1.46 logistic.py(349):     if fit_intercept:
1.46 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
1.46 logistic.py(351):     return loss, grad.ravel(), p
1.46 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
1.46 logistic.py(339):     n_classes = Y.shape[1]
1.46 logistic.py(340):     n_features = X.shape[1]
1.46 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
1.46 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
1.46 logistic.py(343):                     dtype=X.dtype)
1.46 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
1.46 logistic.py(282):     n_classes = Y.shape[1]
1.46 logistic.py(283):     n_features = X.shape[1]
1.46 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
1.46 logistic.py(285):     w = w.reshape(n_classes, -1)
1.46 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
1.46 logistic.py(287):     if fit_intercept:
1.46 logistic.py(288):         intercept = w[:, -1]
1.46 logistic.py(289):         w = w[:, :-1]
1.46 logistic.py(292):     p = safe_sparse_dot(X, w.T)
1.46 logistic.py(293):     p += intercept
1.46 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
1.46 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
1.46 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
1.46 logistic.py(297):     p = np.exp(p, p)
1.46 logistic.py(298):     return loss, p, w
1.46 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
1.46 logistic.py(346):     diff = sample_weight * (p - Y)
1.46 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
1.46 logistic.py(348):     grad[:, :n_features] += alpha * w
1.46 logistic.py(349):     if fit_intercept:
1.46 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
1.46 logistic.py(351):     return loss, grad.ravel(), p
1.46 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
1.46 logistic.py(339):     n_classes = Y.shape[1]
1.46 logistic.py(340):     n_features = X.shape[1]
1.46 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
1.46 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
1.46 logistic.py(343):                     dtype=X.dtype)
1.46 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
1.46 logistic.py(282):     n_classes = Y.shape[1]
1.46 logistic.py(283):     n_features = X.shape[1]
1.46 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
1.46 logistic.py(285):     w = w.reshape(n_classes, -1)
1.46 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
1.46 logistic.py(287):     if fit_intercept:
1.46 logistic.py(288):         intercept = w[:, -1]
1.46 logistic.py(289):         w = w[:, :-1]
1.46 logistic.py(292):     p = safe_sparse_dot(X, w.T)
1.46 logistic.py(293):     p += intercept
1.46 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
1.46 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
1.46 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
1.46 logistic.py(297):     p = np.exp(p, p)
1.46 logistic.py(298):     return loss, p, w
1.46 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
1.46 logistic.py(346):     diff = sample_weight * (p - Y)
1.46 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
1.46 logistic.py(348):     grad[:, :n_features] += alpha * w
1.46 logistic.py(349):     if fit_intercept:
1.46 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
1.46 logistic.py(351):     return loss, grad.ravel(), p
1.46 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
1.46 logistic.py(339):     n_classes = Y.shape[1]
1.46 logistic.py(340):     n_features = X.shape[1]
1.46 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
1.46 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
1.46 logistic.py(343):                     dtype=X.dtype)
1.46 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
1.46 logistic.py(282):     n_classes = Y.shape[1]
1.46 logistic.py(283):     n_features = X.shape[1]
1.46 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
1.46 logistic.py(285):     w = w.reshape(n_classes, -1)
1.46 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
1.46 logistic.py(287):     if fit_intercept:
1.46 logistic.py(288):         intercept = w[:, -1]
1.46 logistic.py(289):         w = w[:, :-1]
1.46 logistic.py(292):     p = safe_sparse_dot(X, w.T)
1.46 logistic.py(293):     p += intercept
1.46 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
1.46 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
1.46 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
1.46 logistic.py(297):     p = np.exp(p, p)
1.46 logistic.py(298):     return loss, p, w
1.46 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
1.46 logistic.py(346):     diff = sample_weight * (p - Y)
1.46 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
1.46 logistic.py(348):     grad[:, :n_features] += alpha * w
1.46 logistic.py(349):     if fit_intercept:
1.46 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
1.46 logistic.py(351):     return loss, grad.ravel(), p
1.46 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
1.46 logistic.py(339):     n_classes = Y.shape[1]
1.46 logistic.py(340):     n_features = X.shape[1]
1.46 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
1.46 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
1.46 logistic.py(343):                     dtype=X.dtype)
1.46 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
1.46 logistic.py(282):     n_classes = Y.shape[1]
1.46 logistic.py(283):     n_features = X.shape[1]
1.46 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
1.46 logistic.py(285):     w = w.reshape(n_classes, -1)
1.46 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
1.46 logistic.py(287):     if fit_intercept:
1.46 logistic.py(288):         intercept = w[:, -1]
1.46 logistic.py(289):         w = w[:, :-1]
1.46 logistic.py(292):     p = safe_sparse_dot(X, w.T)
1.46 logistic.py(293):     p += intercept
1.46 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
1.46 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
1.46 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
1.46 logistic.py(297):     p = np.exp(p, p)
1.46 logistic.py(298):     return loss, p, w
1.46 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
1.46 logistic.py(346):     diff = sample_weight * (p - Y)
1.46 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
1.46 logistic.py(348):     grad[:, :n_features] += alpha * w
1.46 logistic.py(349):     if fit_intercept:
1.46 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
1.46 logistic.py(351):     return loss, grad.ravel(), p
1.46 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
1.46 logistic.py(339):     n_classes = Y.shape[1]
1.46 logistic.py(340):     n_features = X.shape[1]
1.46 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
1.46 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
1.46 logistic.py(343):                     dtype=X.dtype)
1.46 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
1.46 logistic.py(282):     n_classes = Y.shape[1]
1.46 logistic.py(283):     n_features = X.shape[1]
1.46 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
1.46 logistic.py(285):     w = w.reshape(n_classes, -1)
1.46 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
1.46 logistic.py(287):     if fit_intercept:
1.46 logistic.py(288):         intercept = w[:, -1]
1.46 logistic.py(289):         w = w[:, :-1]
1.46 logistic.py(292):     p = safe_sparse_dot(X, w.T)
1.46 logistic.py(293):     p += intercept
1.46 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
1.46 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
1.46 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
1.46 logistic.py(297):     p = np.exp(p, p)
1.46 logistic.py(298):     return loss, p, w
1.46 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
1.46 logistic.py(346):     diff = sample_weight * (p - Y)
1.46 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
1.46 logistic.py(348):     grad[:, :n_features] += alpha * w
1.46 logistic.py(349):     if fit_intercept:
1.46 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
1.46 logistic.py(351):     return loss, grad.ravel(), p
1.46 logistic.py(718):             if info["warnflag"] == 1:
1.46 logistic.py(723):             n_iter_i = min(info['nit'], max_iter)
1.46 logistic.py(760):         if multi_class == 'multinomial':
1.46 logistic.py(761):             multi_w0 = np.reshape(w0, (classes.size, -1))
1.46 logistic.py(762):             if classes.size == 2:
1.46 logistic.py(764):             coefs.append(multi_w0)
1.46 logistic.py(768):         n_iter[i] = n_iter_i
1.46 logistic.py(712):     for i, C in enumerate(Cs):
1.46 logistic.py(713):         if solver == 'lbfgs':
1.46 logistic.py(714):             w0, loss, info = optimize.fmin_l_bfgs_b(
1.46 logistic.py(715):                 func, w0, fprime=None,
1.46 logistic.py(716):                 args=(X, target, 1. / C, sample_weight),
1.46 logistic.py(717):                 iprint=(verbose > 0) - 1, pgtol=tol, maxiter=max_iter)
1.46 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
1.46 logistic.py(339):     n_classes = Y.shape[1]
1.46 logistic.py(340):     n_features = X.shape[1]
1.46 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
1.46 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
1.46 logistic.py(343):                     dtype=X.dtype)
1.46 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
1.46 logistic.py(282):     n_classes = Y.shape[1]
1.46 logistic.py(283):     n_features = X.shape[1]
1.46 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
1.46 logistic.py(285):     w = w.reshape(n_classes, -1)
1.46 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
1.46 logistic.py(287):     if fit_intercept:
1.46 logistic.py(288):         intercept = w[:, -1]
1.46 logistic.py(289):         w = w[:, :-1]
1.46 logistic.py(292):     p = safe_sparse_dot(X, w.T)
1.46 logistic.py(293):     p += intercept
1.46 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
1.46 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
1.46 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
1.46 logistic.py(297):     p = np.exp(p, p)
1.46 logistic.py(298):     return loss, p, w
1.46 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
1.46 logistic.py(346):     diff = sample_weight * (p - Y)
1.46 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
1.46 logistic.py(348):     grad[:, :n_features] += alpha * w
1.46 logistic.py(349):     if fit_intercept:
1.46 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
1.46 logistic.py(351):     return loss, grad.ravel(), p
1.46 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
1.46 logistic.py(339):     n_classes = Y.shape[1]
1.47 logistic.py(340):     n_features = X.shape[1]
1.47 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
1.47 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
1.47 logistic.py(343):                     dtype=X.dtype)
1.47 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
1.47 logistic.py(282):     n_classes = Y.shape[1]
1.47 logistic.py(283):     n_features = X.shape[1]
1.47 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
1.47 logistic.py(285):     w = w.reshape(n_classes, -1)
1.47 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
1.47 logistic.py(287):     if fit_intercept:
1.47 logistic.py(288):         intercept = w[:, -1]
1.47 logistic.py(289):         w = w[:, :-1]
1.47 logistic.py(292):     p = safe_sparse_dot(X, w.T)
1.47 logistic.py(293):     p += intercept
1.47 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
1.47 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
1.47 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
1.47 logistic.py(297):     p = np.exp(p, p)
1.47 logistic.py(298):     return loss, p, w
1.47 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
1.47 logistic.py(346):     diff = sample_weight * (p - Y)
1.47 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
1.47 logistic.py(348):     grad[:, :n_features] += alpha * w
1.47 logistic.py(349):     if fit_intercept:
1.47 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
1.47 logistic.py(351):     return loss, grad.ravel(), p
1.47 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
1.47 logistic.py(339):     n_classes = Y.shape[1]
1.47 logistic.py(340):     n_features = X.shape[1]
1.47 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
1.47 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
1.47 logistic.py(343):                     dtype=X.dtype)
1.47 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
1.47 logistic.py(282):     n_classes = Y.shape[1]
1.47 logistic.py(283):     n_features = X.shape[1]
1.47 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
1.47 logistic.py(285):     w = w.reshape(n_classes, -1)
1.47 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
1.47 logistic.py(287):     if fit_intercept:
1.47 logistic.py(288):         intercept = w[:, -1]
1.47 logistic.py(289):         w = w[:, :-1]
1.47 logistic.py(292):     p = safe_sparse_dot(X, w.T)
1.47 logistic.py(293):     p += intercept
1.47 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
1.47 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
1.47 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
1.47 logistic.py(297):     p = np.exp(p, p)
1.47 logistic.py(298):     return loss, p, w
1.47 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
1.47 logistic.py(346):     diff = sample_weight * (p - Y)
1.47 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
1.47 logistic.py(348):     grad[:, :n_features] += alpha * w
1.47 logistic.py(349):     if fit_intercept:
1.47 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
1.47 logistic.py(351):     return loss, grad.ravel(), p
1.47 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
1.47 logistic.py(339):     n_classes = Y.shape[1]
1.47 logistic.py(340):     n_features = X.shape[1]
1.47 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
1.47 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
1.47 logistic.py(343):                     dtype=X.dtype)
1.47 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
1.47 logistic.py(282):     n_classes = Y.shape[1]
1.47 logistic.py(283):     n_features = X.shape[1]
1.47 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
1.47 logistic.py(285):     w = w.reshape(n_classes, -1)
1.47 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
1.47 logistic.py(287):     if fit_intercept:
1.47 logistic.py(288):         intercept = w[:, -1]
1.47 logistic.py(289):         w = w[:, :-1]
1.47 logistic.py(292):     p = safe_sparse_dot(X, w.T)
1.47 logistic.py(293):     p += intercept
1.47 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
1.47 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
1.47 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
1.47 logistic.py(297):     p = np.exp(p, p)
1.47 logistic.py(298):     return loss, p, w
1.47 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
1.47 logistic.py(346):     diff = sample_weight * (p - Y)
1.47 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
1.47 logistic.py(348):     grad[:, :n_features] += alpha * w
1.47 logistic.py(349):     if fit_intercept:
1.47 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
1.47 logistic.py(351):     return loss, grad.ravel(), p
1.47 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
1.47 logistic.py(339):     n_classes = Y.shape[1]
1.47 logistic.py(340):     n_features = X.shape[1]
1.47 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
1.47 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
1.47 logistic.py(343):                     dtype=X.dtype)
1.47 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
1.47 logistic.py(282):     n_classes = Y.shape[1]
1.47 logistic.py(283):     n_features = X.shape[1]
1.47 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
1.47 logistic.py(285):     w = w.reshape(n_classes, -1)
1.47 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
1.47 logistic.py(287):     if fit_intercept:
1.47 logistic.py(288):         intercept = w[:, -1]
1.47 logistic.py(289):         w = w[:, :-1]
1.47 logistic.py(292):     p = safe_sparse_dot(X, w.T)
1.47 logistic.py(293):     p += intercept
1.47 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
1.47 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
1.47 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
1.47 logistic.py(297):     p = np.exp(p, p)
1.47 logistic.py(298):     return loss, p, w
1.47 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
1.47 logistic.py(346):     diff = sample_weight * (p - Y)
1.47 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
1.47 logistic.py(348):     grad[:, :n_features] += alpha * w
1.47 logistic.py(349):     if fit_intercept:
1.47 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
1.47 logistic.py(351):     return loss, grad.ravel(), p
1.47 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
1.47 logistic.py(339):     n_classes = Y.shape[1]
1.47 logistic.py(340):     n_features = X.shape[1]
1.47 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
1.47 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
1.47 logistic.py(343):                     dtype=X.dtype)
1.47 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
1.47 logistic.py(282):     n_classes = Y.shape[1]
1.47 logistic.py(283):     n_features = X.shape[1]
1.47 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
1.47 logistic.py(285):     w = w.reshape(n_classes, -1)
1.47 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
1.47 logistic.py(287):     if fit_intercept:
1.47 logistic.py(288):         intercept = w[:, -1]
1.47 logistic.py(289):         w = w[:, :-1]
1.47 logistic.py(292):     p = safe_sparse_dot(X, w.T)
1.47 logistic.py(293):     p += intercept
1.47 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
1.47 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
1.47 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
1.47 logistic.py(297):     p = np.exp(p, p)
1.47 logistic.py(298):     return loss, p, w
1.47 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
1.47 logistic.py(346):     diff = sample_weight * (p - Y)
1.47 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
1.47 logistic.py(348):     grad[:, :n_features] += alpha * w
1.47 logistic.py(349):     if fit_intercept:
1.47 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
1.47 logistic.py(351):     return loss, grad.ravel(), p
1.47 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
1.47 logistic.py(339):     n_classes = Y.shape[1]
1.47 logistic.py(340):     n_features = X.shape[1]
1.47 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
1.47 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
1.47 logistic.py(343):                     dtype=X.dtype)
1.47 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
1.47 logistic.py(282):     n_classes = Y.shape[1]
1.47 logistic.py(283):     n_features = X.shape[1]
1.47 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
1.47 logistic.py(285):     w = w.reshape(n_classes, -1)
1.47 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
1.47 logistic.py(287):     if fit_intercept:
1.47 logistic.py(288):         intercept = w[:, -1]
1.47 logistic.py(289):         w = w[:, :-1]
1.47 logistic.py(292):     p = safe_sparse_dot(X, w.T)
1.47 logistic.py(293):     p += intercept
1.47 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
1.47 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
1.47 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
1.47 logistic.py(297):     p = np.exp(p, p)
1.47 logistic.py(298):     return loss, p, w
1.47 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
1.47 logistic.py(346):     diff = sample_weight * (p - Y)
1.47 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
1.47 logistic.py(348):     grad[:, :n_features] += alpha * w
1.47 logistic.py(349):     if fit_intercept:
1.47 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
1.47 logistic.py(351):     return loss, grad.ravel(), p
1.47 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
1.47 logistic.py(339):     n_classes = Y.shape[1]
1.47 logistic.py(340):     n_features = X.shape[1]
1.47 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
1.47 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
1.47 logistic.py(343):                     dtype=X.dtype)
1.47 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
1.47 logistic.py(282):     n_classes = Y.shape[1]
1.47 logistic.py(283):     n_features = X.shape[1]
1.47 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
1.47 logistic.py(285):     w = w.reshape(n_classes, -1)
1.47 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
1.47 logistic.py(287):     if fit_intercept:
1.47 logistic.py(288):         intercept = w[:, -1]
1.47 logistic.py(289):         w = w[:, :-1]
1.47 logistic.py(292):     p = safe_sparse_dot(X, w.T)
1.47 logistic.py(293):     p += intercept
1.47 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
1.47 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
1.47 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
1.47 logistic.py(297):     p = np.exp(p, p)
1.47 logistic.py(298):     return loss, p, w
1.47 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
1.47 logistic.py(346):     diff = sample_weight * (p - Y)
1.47 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
1.47 logistic.py(348):     grad[:, :n_features] += alpha * w
1.47 logistic.py(349):     if fit_intercept:
1.47 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
1.47 logistic.py(351):     return loss, grad.ravel(), p
1.47 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
1.47 logistic.py(339):     n_classes = Y.shape[1]
1.47 logistic.py(340):     n_features = X.shape[1]
1.47 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
1.47 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
1.47 logistic.py(343):                     dtype=X.dtype)
1.47 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
1.47 logistic.py(282):     n_classes = Y.shape[1]
1.47 logistic.py(283):     n_features = X.shape[1]
1.47 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
1.47 logistic.py(285):     w = w.reshape(n_classes, -1)
1.47 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
1.47 logistic.py(287):     if fit_intercept:
1.47 logistic.py(288):         intercept = w[:, -1]
1.47 logistic.py(289):         w = w[:, :-1]
1.47 logistic.py(292):     p = safe_sparse_dot(X, w.T)
1.47 logistic.py(293):     p += intercept
1.47 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
1.47 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
1.47 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
1.47 logistic.py(297):     p = np.exp(p, p)
1.47 logistic.py(298):     return loss, p, w
1.47 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
1.47 logistic.py(346):     diff = sample_weight * (p - Y)
1.47 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
1.47 logistic.py(348):     grad[:, :n_features] += alpha * w
1.47 logistic.py(349):     if fit_intercept:
1.47 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
1.47 logistic.py(351):     return loss, grad.ravel(), p
1.47 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
1.47 logistic.py(339):     n_classes = Y.shape[1]
1.47 logistic.py(340):     n_features = X.shape[1]
1.47 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
1.47 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
1.47 logistic.py(343):                     dtype=X.dtype)
1.47 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
1.47 logistic.py(282):     n_classes = Y.shape[1]
1.47 logistic.py(283):     n_features = X.shape[1]
1.47 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
1.47 logistic.py(285):     w = w.reshape(n_classes, -1)
1.47 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
1.47 logistic.py(287):     if fit_intercept:
1.47 logistic.py(288):         intercept = w[:, -1]
1.47 logistic.py(289):         w = w[:, :-1]
1.47 logistic.py(292):     p = safe_sparse_dot(X, w.T)
1.47 logistic.py(293):     p += intercept
1.47 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
1.47 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
1.47 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
1.47 logistic.py(297):     p = np.exp(p, p)
1.47 logistic.py(298):     return loss, p, w
1.47 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
1.47 logistic.py(346):     diff = sample_weight * (p - Y)
1.47 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
1.47 logistic.py(348):     grad[:, :n_features] += alpha * w
1.47 logistic.py(349):     if fit_intercept:
1.47 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
1.47 logistic.py(351):     return loss, grad.ravel(), p
1.47 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
1.47 logistic.py(339):     n_classes = Y.shape[1]
1.47 logistic.py(340):     n_features = X.shape[1]
1.47 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
1.47 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
1.47 logistic.py(343):                     dtype=X.dtype)
1.47 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
1.47 logistic.py(282):     n_classes = Y.shape[1]
1.47 logistic.py(283):     n_features = X.shape[1]
1.47 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
1.47 logistic.py(285):     w = w.reshape(n_classes, -1)
1.47 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
1.47 logistic.py(287):     if fit_intercept:
1.47 logistic.py(288):         intercept = w[:, -1]
1.47 logistic.py(289):         w = w[:, :-1]
1.47 logistic.py(292):     p = safe_sparse_dot(X, w.T)
1.47 logistic.py(293):     p += intercept
1.47 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
1.47 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
1.47 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
1.47 logistic.py(297):     p = np.exp(p, p)
1.47 logistic.py(298):     return loss, p, w
1.47 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
1.47 logistic.py(346):     diff = sample_weight * (p - Y)
1.47 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
1.47 logistic.py(348):     grad[:, :n_features] += alpha * w
1.47 logistic.py(349):     if fit_intercept:
1.47 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
1.47 logistic.py(351):     return loss, grad.ravel(), p
1.47 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
1.47 logistic.py(339):     n_classes = Y.shape[1]
1.47 logistic.py(340):     n_features = X.shape[1]
1.47 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
1.47 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
1.47 logistic.py(343):                     dtype=X.dtype)
1.47 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
1.47 logistic.py(282):     n_classes = Y.shape[1]
1.47 logistic.py(283):     n_features = X.shape[1]
1.47 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
1.47 logistic.py(285):     w = w.reshape(n_classes, -1)
1.47 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
1.47 logistic.py(287):     if fit_intercept:
1.47 logistic.py(288):         intercept = w[:, -1]
1.47 logistic.py(289):         w = w[:, :-1]
1.47 logistic.py(292):     p = safe_sparse_dot(X, w.T)
1.47 logistic.py(293):     p += intercept
1.47 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
1.47 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
1.47 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
1.47 logistic.py(297):     p = np.exp(p, p)
1.47 logistic.py(298):     return loss, p, w
1.47 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
1.47 logistic.py(346):     diff = sample_weight * (p - Y)
1.47 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
1.47 logistic.py(348):     grad[:, :n_features] += alpha * w
1.47 logistic.py(349):     if fit_intercept:
1.47 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
1.47 logistic.py(351):     return loss, grad.ravel(), p
1.47 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
1.47 logistic.py(339):     n_classes = Y.shape[1]
1.47 logistic.py(340):     n_features = X.shape[1]
1.47 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
1.47 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
1.47 logistic.py(343):                     dtype=X.dtype)
1.47 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
1.47 logistic.py(282):     n_classes = Y.shape[1]
1.47 logistic.py(283):     n_features = X.shape[1]
1.47 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
1.47 logistic.py(285):     w = w.reshape(n_classes, -1)
1.47 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
1.47 logistic.py(287):     if fit_intercept:
1.47 logistic.py(288):         intercept = w[:, -1]
1.47 logistic.py(289):         w = w[:, :-1]
1.47 logistic.py(292):     p = safe_sparse_dot(X, w.T)
1.47 logistic.py(293):     p += intercept
1.47 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
1.47 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
1.47 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
1.47 logistic.py(297):     p = np.exp(p, p)
1.47 logistic.py(298):     return loss, p, w
1.47 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
1.47 logistic.py(346):     diff = sample_weight * (p - Y)
1.47 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
1.47 logistic.py(348):     grad[:, :n_features] += alpha * w
1.47 logistic.py(349):     if fit_intercept:
1.47 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
1.47 logistic.py(351):     return loss, grad.ravel(), p
1.47 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
1.47 logistic.py(339):     n_classes = Y.shape[1]
1.47 logistic.py(340):     n_features = X.shape[1]
1.47 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
1.47 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
1.47 logistic.py(343):                     dtype=X.dtype)
1.47 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
1.47 logistic.py(282):     n_classes = Y.shape[1]
1.47 logistic.py(283):     n_features = X.shape[1]
1.47 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
1.47 logistic.py(285):     w = w.reshape(n_classes, -1)
1.47 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
1.47 logistic.py(287):     if fit_intercept:
1.47 logistic.py(288):         intercept = w[:, -1]
1.47 logistic.py(289):         w = w[:, :-1]
1.47 logistic.py(292):     p = safe_sparse_dot(X, w.T)
1.47 logistic.py(293):     p += intercept
1.47 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
1.47 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
1.47 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
1.47 logistic.py(297):     p = np.exp(p, p)
1.47 logistic.py(298):     return loss, p, w
1.47 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
1.47 logistic.py(346):     diff = sample_weight * (p - Y)
1.47 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
1.47 logistic.py(348):     grad[:, :n_features] += alpha * w
1.47 logistic.py(349):     if fit_intercept:
1.47 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
1.47 logistic.py(351):     return loss, grad.ravel(), p
1.47 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
1.47 logistic.py(339):     n_classes = Y.shape[1]
1.47 logistic.py(340):     n_features = X.shape[1]
1.47 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
1.47 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
1.47 logistic.py(343):                     dtype=X.dtype)
1.47 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
1.47 logistic.py(282):     n_classes = Y.shape[1]
1.47 logistic.py(283):     n_features = X.shape[1]
1.47 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
1.47 logistic.py(285):     w = w.reshape(n_classes, -1)
1.47 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
1.47 logistic.py(287):     if fit_intercept:
1.47 logistic.py(288):         intercept = w[:, -1]
1.47 logistic.py(289):         w = w[:, :-1]
1.47 logistic.py(292):     p = safe_sparse_dot(X, w.T)
1.47 logistic.py(293):     p += intercept
1.47 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
1.47 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
1.47 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
1.47 logistic.py(297):     p = np.exp(p, p)
1.47 logistic.py(298):     return loss, p, w
1.47 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
1.47 logistic.py(346):     diff = sample_weight * (p - Y)
1.47 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
1.47 logistic.py(348):     grad[:, :n_features] += alpha * w
1.47 logistic.py(349):     if fit_intercept:
1.47 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
1.47 logistic.py(351):     return loss, grad.ravel(), p
1.47 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
1.47 logistic.py(339):     n_classes = Y.shape[1]
1.47 logistic.py(340):     n_features = X.shape[1]
1.47 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
1.47 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
1.47 logistic.py(343):                     dtype=X.dtype)
1.47 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
1.47 logistic.py(282):     n_classes = Y.shape[1]
1.47 logistic.py(283):     n_features = X.shape[1]
1.47 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
1.47 logistic.py(285):     w = w.reshape(n_classes, -1)
1.47 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
1.47 logistic.py(287):     if fit_intercept:
1.47 logistic.py(288):         intercept = w[:, -1]
1.47 logistic.py(289):         w = w[:, :-1]
1.47 logistic.py(292):     p = safe_sparse_dot(X, w.T)
1.47 logistic.py(293):     p += intercept
1.47 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
1.47 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
1.47 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
1.47 logistic.py(297):     p = np.exp(p, p)
1.47 logistic.py(298):     return loss, p, w
1.47 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
1.47 logistic.py(346):     diff = sample_weight * (p - Y)
1.47 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
1.47 logistic.py(348):     grad[:, :n_features] += alpha * w
1.47 logistic.py(349):     if fit_intercept:
1.47 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
1.47 logistic.py(351):     return loss, grad.ravel(), p
1.47 logistic.py(718):             if info["warnflag"] == 1:
1.47 logistic.py(723):             n_iter_i = min(info['nit'], max_iter)
1.47 logistic.py(760):         if multi_class == 'multinomial':
1.47 logistic.py(761):             multi_w0 = np.reshape(w0, (classes.size, -1))
1.47 logistic.py(762):             if classes.size == 2:
1.47 logistic.py(764):             coefs.append(multi_w0)
1.47 logistic.py(768):         n_iter[i] = n_iter_i
1.47 logistic.py(712):     for i, C in enumerate(Cs):
1.47 logistic.py(713):         if solver == 'lbfgs':
1.47 logistic.py(714):             w0, loss, info = optimize.fmin_l_bfgs_b(
1.47 logistic.py(715):                 func, w0, fprime=None,
1.47 logistic.py(716):                 args=(X, target, 1. / C, sample_weight),
1.47 logistic.py(717):                 iprint=(verbose > 0) - 1, pgtol=tol, maxiter=max_iter)
1.47 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
1.47 logistic.py(339):     n_classes = Y.shape[1]
1.47 logistic.py(340):     n_features = X.shape[1]
1.47 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
1.47 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
1.47 logistic.py(343):                     dtype=X.dtype)
1.47 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
1.47 logistic.py(282):     n_classes = Y.shape[1]
1.47 logistic.py(283):     n_features = X.shape[1]
1.47 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
1.47 logistic.py(285):     w = w.reshape(n_classes, -1)
1.47 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
1.47 logistic.py(287):     if fit_intercept:
1.47 logistic.py(288):         intercept = w[:, -1]
1.47 logistic.py(289):         w = w[:, :-1]
1.47 logistic.py(292):     p = safe_sparse_dot(X, w.T)
1.47 logistic.py(293):     p += intercept
1.47 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
1.47 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
1.47 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
1.47 logistic.py(297):     p = np.exp(p, p)
1.47 logistic.py(298):     return loss, p, w
1.47 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
1.47 logistic.py(346):     diff = sample_weight * (p - Y)
1.47 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
1.47 logistic.py(348):     grad[:, :n_features] += alpha * w
1.47 logistic.py(349):     if fit_intercept:
1.47 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
1.47 logistic.py(351):     return loss, grad.ravel(), p
1.47 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
1.47 logistic.py(339):     n_classes = Y.shape[1]
1.47 logistic.py(340):     n_features = X.shape[1]
1.47 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
1.47 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
1.47 logistic.py(343):                     dtype=X.dtype)
1.48 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
1.48 logistic.py(282):     n_classes = Y.shape[1]
1.48 logistic.py(283):     n_features = X.shape[1]
1.48 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
1.48 logistic.py(285):     w = w.reshape(n_classes, -1)
1.48 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
1.48 logistic.py(287):     if fit_intercept:
1.48 logistic.py(288):         intercept = w[:, -1]
1.48 logistic.py(289):         w = w[:, :-1]
1.48 logistic.py(292):     p = safe_sparse_dot(X, w.T)
1.48 logistic.py(293):     p += intercept
1.48 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
1.48 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
1.48 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
1.48 logistic.py(297):     p = np.exp(p, p)
1.48 logistic.py(298):     return loss, p, w
1.48 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
1.48 logistic.py(346):     diff = sample_weight * (p - Y)
1.48 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
1.48 logistic.py(348):     grad[:, :n_features] += alpha * w
1.48 logistic.py(349):     if fit_intercept:
1.48 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
1.48 logistic.py(351):     return loss, grad.ravel(), p
1.48 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
1.48 logistic.py(339):     n_classes = Y.shape[1]
1.48 logistic.py(340):     n_features = X.shape[1]
1.48 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
1.48 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
1.48 logistic.py(343):                     dtype=X.dtype)
1.48 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
1.48 logistic.py(282):     n_classes = Y.shape[1]
1.48 logistic.py(283):     n_features = X.shape[1]
1.48 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
1.48 logistic.py(285):     w = w.reshape(n_classes, -1)
1.48 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
1.48 logistic.py(287):     if fit_intercept:
1.48 logistic.py(288):         intercept = w[:, -1]
1.48 logistic.py(289):         w = w[:, :-1]
1.48 logistic.py(292):     p = safe_sparse_dot(X, w.T)
1.48 logistic.py(293):     p += intercept
1.48 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
1.48 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
1.48 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
1.48 logistic.py(297):     p = np.exp(p, p)
1.48 logistic.py(298):     return loss, p, w
1.48 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
1.48 logistic.py(346):     diff = sample_weight * (p - Y)
1.48 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
1.48 logistic.py(348):     grad[:, :n_features] += alpha * w
1.48 logistic.py(349):     if fit_intercept:
1.48 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
1.48 logistic.py(351):     return loss, grad.ravel(), p
1.48 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
1.48 logistic.py(339):     n_classes = Y.shape[1]
1.48 logistic.py(340):     n_features = X.shape[1]
1.48 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
1.48 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
1.48 logistic.py(343):                     dtype=X.dtype)
1.48 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
1.48 logistic.py(282):     n_classes = Y.shape[1]
1.48 logistic.py(283):     n_features = X.shape[1]
1.48 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
1.48 logistic.py(285):     w = w.reshape(n_classes, -1)
1.48 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
1.48 logistic.py(287):     if fit_intercept:
1.48 logistic.py(288):         intercept = w[:, -1]
1.48 logistic.py(289):         w = w[:, :-1]
1.48 logistic.py(292):     p = safe_sparse_dot(X, w.T)
1.48 logistic.py(293):     p += intercept
1.48 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
1.48 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
1.48 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
1.48 logistic.py(297):     p = np.exp(p, p)
1.48 logistic.py(298):     return loss, p, w
1.48 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
1.48 logistic.py(346):     diff = sample_weight * (p - Y)
1.48 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
1.48 logistic.py(348):     grad[:, :n_features] += alpha * w
1.48 logistic.py(349):     if fit_intercept:
1.48 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
1.48 logistic.py(351):     return loss, grad.ravel(), p
1.48 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
1.48 logistic.py(339):     n_classes = Y.shape[1]
1.48 logistic.py(340):     n_features = X.shape[1]
1.48 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
1.48 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
1.48 logistic.py(343):                     dtype=X.dtype)
1.48 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
1.48 logistic.py(282):     n_classes = Y.shape[1]
1.48 logistic.py(283):     n_features = X.shape[1]
1.48 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
1.48 logistic.py(285):     w = w.reshape(n_classes, -1)
1.48 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
1.48 logistic.py(287):     if fit_intercept:
1.48 logistic.py(288):         intercept = w[:, -1]
1.48 logistic.py(289):         w = w[:, :-1]
1.48 logistic.py(292):     p = safe_sparse_dot(X, w.T)
1.48 logistic.py(293):     p += intercept
1.48 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
1.48 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
1.48 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
1.48 logistic.py(297):     p = np.exp(p, p)
1.48 logistic.py(298):     return loss, p, w
1.48 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
1.48 logistic.py(346):     diff = sample_weight * (p - Y)
1.48 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
1.48 logistic.py(348):     grad[:, :n_features] += alpha * w
1.48 logistic.py(349):     if fit_intercept:
1.48 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
1.48 logistic.py(351):     return loss, grad.ravel(), p
1.48 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
1.48 logistic.py(339):     n_classes = Y.shape[1]
1.48 logistic.py(340):     n_features = X.shape[1]
1.48 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
1.48 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
1.48 logistic.py(343):                     dtype=X.dtype)
1.48 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
1.48 logistic.py(282):     n_classes = Y.shape[1]
1.48 logistic.py(283):     n_features = X.shape[1]
1.48 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
1.48 logistic.py(285):     w = w.reshape(n_classes, -1)
1.48 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
1.48 logistic.py(287):     if fit_intercept:
1.48 logistic.py(288):         intercept = w[:, -1]
1.48 logistic.py(289):         w = w[:, :-1]
1.48 logistic.py(292):     p = safe_sparse_dot(X, w.T)
1.48 logistic.py(293):     p += intercept
1.48 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
1.48 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
1.48 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
1.48 logistic.py(297):     p = np.exp(p, p)
1.48 logistic.py(298):     return loss, p, w
1.48 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
1.48 logistic.py(346):     diff = sample_weight * (p - Y)
1.48 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
1.48 logistic.py(348):     grad[:, :n_features] += alpha * w
1.48 logistic.py(349):     if fit_intercept:
1.48 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
1.48 logistic.py(351):     return loss, grad.ravel(), p
1.48 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
1.48 logistic.py(339):     n_classes = Y.shape[1]
1.48 logistic.py(340):     n_features = X.shape[1]
1.48 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
1.48 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
1.48 logistic.py(343):                     dtype=X.dtype)
1.48 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
1.48 logistic.py(282):     n_classes = Y.shape[1]
1.48 logistic.py(283):     n_features = X.shape[1]
1.48 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
1.48 logistic.py(285):     w = w.reshape(n_classes, -1)
1.48 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
1.48 logistic.py(287):     if fit_intercept:
1.48 logistic.py(288):         intercept = w[:, -1]
1.48 logistic.py(289):         w = w[:, :-1]
1.48 logistic.py(292):     p = safe_sparse_dot(X, w.T)
1.48 logistic.py(293):     p += intercept
1.48 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
1.48 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
1.48 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
1.48 logistic.py(297):     p = np.exp(p, p)
1.48 logistic.py(298):     return loss, p, w
1.48 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
1.48 logistic.py(346):     diff = sample_weight * (p - Y)
1.48 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
1.48 logistic.py(348):     grad[:, :n_features] += alpha * w
1.48 logistic.py(349):     if fit_intercept:
1.48 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
1.48 logistic.py(351):     return loss, grad.ravel(), p
1.48 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
1.48 logistic.py(339):     n_classes = Y.shape[1]
1.48 logistic.py(340):     n_features = X.shape[1]
1.48 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
1.48 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
1.48 logistic.py(343):                     dtype=X.dtype)
1.48 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
1.48 logistic.py(282):     n_classes = Y.shape[1]
1.48 logistic.py(283):     n_features = X.shape[1]
1.48 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
1.48 logistic.py(285):     w = w.reshape(n_classes, -1)
1.48 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
1.48 logistic.py(287):     if fit_intercept:
1.48 logistic.py(288):         intercept = w[:, -1]
1.48 logistic.py(289):         w = w[:, :-1]
1.48 logistic.py(292):     p = safe_sparse_dot(X, w.T)
1.48 logistic.py(293):     p += intercept
1.48 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
1.48 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
1.48 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
1.48 logistic.py(297):     p = np.exp(p, p)
1.48 logistic.py(298):     return loss, p, w
1.48 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
1.48 logistic.py(346):     diff = sample_weight * (p - Y)
1.48 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
1.48 logistic.py(348):     grad[:, :n_features] += alpha * w
1.48 logistic.py(349):     if fit_intercept:
1.48 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
1.48 logistic.py(351):     return loss, grad.ravel(), p
1.48 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
1.48 logistic.py(339):     n_classes = Y.shape[1]
1.48 logistic.py(340):     n_features = X.shape[1]
1.48 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
1.48 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
1.48 logistic.py(343):                     dtype=X.dtype)
1.48 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
1.48 logistic.py(282):     n_classes = Y.shape[1]
1.48 logistic.py(283):     n_features = X.shape[1]
1.48 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
1.48 logistic.py(285):     w = w.reshape(n_classes, -1)
1.48 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
1.48 logistic.py(287):     if fit_intercept:
1.48 logistic.py(288):         intercept = w[:, -1]
1.48 logistic.py(289):         w = w[:, :-1]
1.48 logistic.py(292):     p = safe_sparse_dot(X, w.T)
1.48 logistic.py(293):     p += intercept
1.48 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
1.48 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
1.48 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
1.48 logistic.py(297):     p = np.exp(p, p)
1.48 logistic.py(298):     return loss, p, w
1.48 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
1.48 logistic.py(346):     diff = sample_weight * (p - Y)
1.48 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
1.48 logistic.py(348):     grad[:, :n_features] += alpha * w
1.48 logistic.py(349):     if fit_intercept:
1.48 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
1.48 logistic.py(351):     return loss, grad.ravel(), p
1.48 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
1.48 logistic.py(339):     n_classes = Y.shape[1]
1.48 logistic.py(340):     n_features = X.shape[1]
1.48 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
1.48 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
1.48 logistic.py(343):                     dtype=X.dtype)
1.48 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
1.48 logistic.py(282):     n_classes = Y.shape[1]
1.48 logistic.py(283):     n_features = X.shape[1]
1.48 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
1.48 logistic.py(285):     w = w.reshape(n_classes, -1)
1.48 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
1.48 logistic.py(287):     if fit_intercept:
1.48 logistic.py(288):         intercept = w[:, -1]
1.48 logistic.py(289):         w = w[:, :-1]
1.48 logistic.py(292):     p = safe_sparse_dot(X, w.T)
1.48 logistic.py(293):     p += intercept
1.48 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
1.48 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
1.48 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
1.48 logistic.py(297):     p = np.exp(p, p)
1.48 logistic.py(298):     return loss, p, w
1.48 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
1.48 logistic.py(346):     diff = sample_weight * (p - Y)
1.48 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
1.48 logistic.py(348):     grad[:, :n_features] += alpha * w
1.48 logistic.py(349):     if fit_intercept:
1.48 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
1.48 logistic.py(351):     return loss, grad.ravel(), p
1.48 logistic.py(718):             if info["warnflag"] == 1:
1.48 logistic.py(723):             n_iter_i = min(info['nit'], max_iter)
1.48 logistic.py(760):         if multi_class == 'multinomial':
1.48 logistic.py(761):             multi_w0 = np.reshape(w0, (classes.size, -1))
1.48 logistic.py(762):             if classes.size == 2:
1.48 logistic.py(764):             coefs.append(multi_w0)
1.48 logistic.py(768):         n_iter[i] = n_iter_i
1.48 logistic.py(712):     for i, C in enumerate(Cs):
1.48 logistic.py(713):         if solver == 'lbfgs':
1.48 logistic.py(714):             w0, loss, info = optimize.fmin_l_bfgs_b(
1.48 logistic.py(715):                 func, w0, fprime=None,
1.48 logistic.py(716):                 args=(X, target, 1. / C, sample_weight),
1.48 logistic.py(717):                 iprint=(verbose > 0) - 1, pgtol=tol, maxiter=max_iter)
1.48 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
1.48 logistic.py(339):     n_classes = Y.shape[1]
1.48 logistic.py(340):     n_features = X.shape[1]
1.48 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
1.48 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
1.48 logistic.py(343):                     dtype=X.dtype)
1.48 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
1.48 logistic.py(282):     n_classes = Y.shape[1]
1.48 logistic.py(283):     n_features = X.shape[1]
1.48 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
1.48 logistic.py(285):     w = w.reshape(n_classes, -1)
1.48 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
1.48 logistic.py(287):     if fit_intercept:
1.48 logistic.py(288):         intercept = w[:, -1]
1.48 logistic.py(289):         w = w[:, :-1]
1.48 logistic.py(292):     p = safe_sparse_dot(X, w.T)
1.48 logistic.py(293):     p += intercept
1.48 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
1.48 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
1.48 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
1.48 logistic.py(297):     p = np.exp(p, p)
1.48 logistic.py(298):     return loss, p, w
1.48 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
1.48 logistic.py(346):     diff = sample_weight * (p - Y)
1.48 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
1.48 logistic.py(348):     grad[:, :n_features] += alpha * w
1.48 logistic.py(349):     if fit_intercept:
1.48 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
1.48 logistic.py(351):     return loss, grad.ravel(), p
1.48 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
1.48 logistic.py(339):     n_classes = Y.shape[1]
1.48 logistic.py(340):     n_features = X.shape[1]
1.48 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
1.48 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
1.48 logistic.py(343):                     dtype=X.dtype)
1.48 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
1.48 logistic.py(282):     n_classes = Y.shape[1]
1.48 logistic.py(283):     n_features = X.shape[1]
1.48 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
1.48 logistic.py(285):     w = w.reshape(n_classes, -1)
1.48 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
1.48 logistic.py(287):     if fit_intercept:
1.48 logistic.py(288):         intercept = w[:, -1]
1.48 logistic.py(289):         w = w[:, :-1]
1.48 logistic.py(292):     p = safe_sparse_dot(X, w.T)
1.48 logistic.py(293):     p += intercept
1.48 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
1.48 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
1.48 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
1.48 logistic.py(297):     p = np.exp(p, p)
1.48 logistic.py(298):     return loss, p, w
1.48 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
1.48 logistic.py(346):     diff = sample_weight * (p - Y)
1.48 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
1.48 logistic.py(348):     grad[:, :n_features] += alpha * w
1.48 logistic.py(349):     if fit_intercept:
1.48 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
1.48 logistic.py(351):     return loss, grad.ravel(), p
1.48 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
1.48 logistic.py(339):     n_classes = Y.shape[1]
1.48 logistic.py(340):     n_features = X.shape[1]
1.48 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
1.48 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
1.48 logistic.py(343):                     dtype=X.dtype)
1.48 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
1.48 logistic.py(282):     n_classes = Y.shape[1]
1.48 logistic.py(283):     n_features = X.shape[1]
1.48 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
1.48 logistic.py(285):     w = w.reshape(n_classes, -1)
1.48 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
1.48 logistic.py(287):     if fit_intercept:
1.48 logistic.py(288):         intercept = w[:, -1]
1.48 logistic.py(289):         w = w[:, :-1]
1.48 logistic.py(292):     p = safe_sparse_dot(X, w.T)
1.48 logistic.py(293):     p += intercept
1.48 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
1.48 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
1.48 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
1.48 logistic.py(297):     p = np.exp(p, p)
1.48 logistic.py(298):     return loss, p, w
1.48 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
1.48 logistic.py(346):     diff = sample_weight * (p - Y)
1.48 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
1.48 logistic.py(348):     grad[:, :n_features] += alpha * w
1.48 logistic.py(349):     if fit_intercept:
1.48 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
1.48 logistic.py(351):     return loss, grad.ravel(), p
1.48 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
1.48 logistic.py(339):     n_classes = Y.shape[1]
1.48 logistic.py(340):     n_features = X.shape[1]
1.48 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
1.48 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
1.48 logistic.py(343):                     dtype=X.dtype)
1.48 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
1.48 logistic.py(282):     n_classes = Y.shape[1]
1.48 logistic.py(283):     n_features = X.shape[1]
1.48 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
1.48 logistic.py(285):     w = w.reshape(n_classes, -1)
1.48 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
1.48 logistic.py(287):     if fit_intercept:
1.48 logistic.py(288):         intercept = w[:, -1]
1.48 logistic.py(289):         w = w[:, :-1]
1.48 logistic.py(292):     p = safe_sparse_dot(X, w.T)
1.48 logistic.py(293):     p += intercept
1.48 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
1.48 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
1.48 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
1.48 logistic.py(297):     p = np.exp(p, p)
1.48 logistic.py(298):     return loss, p, w
1.48 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
1.48 logistic.py(346):     diff = sample_weight * (p - Y)
1.48 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
1.48 logistic.py(348):     grad[:, :n_features] += alpha * w
1.48 logistic.py(349):     if fit_intercept:
1.48 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
1.48 logistic.py(351):     return loss, grad.ravel(), p
1.48 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
1.48 logistic.py(339):     n_classes = Y.shape[1]
1.48 logistic.py(340):     n_features = X.shape[1]
1.48 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
1.48 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
1.48 logistic.py(343):                     dtype=X.dtype)
1.48 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
1.48 logistic.py(282):     n_classes = Y.shape[1]
1.48 logistic.py(283):     n_features = X.shape[1]
1.48 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
1.48 logistic.py(285):     w = w.reshape(n_classes, -1)
1.48 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
1.48 logistic.py(287):     if fit_intercept:
1.48 logistic.py(288):         intercept = w[:, -1]
1.48 logistic.py(289):         w = w[:, :-1]
1.48 logistic.py(292):     p = safe_sparse_dot(X, w.T)
1.48 logistic.py(293):     p += intercept
1.48 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
1.48 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
1.48 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
1.48 logistic.py(297):     p = np.exp(p, p)
1.48 logistic.py(298):     return loss, p, w
1.48 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
1.48 logistic.py(346):     diff = sample_weight * (p - Y)
1.48 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
1.48 logistic.py(348):     grad[:, :n_features] += alpha * w
1.48 logistic.py(349):     if fit_intercept:
1.48 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
1.48 logistic.py(351):     return loss, grad.ravel(), p
1.48 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
1.48 logistic.py(339):     n_classes = Y.shape[1]
1.48 logistic.py(340):     n_features = X.shape[1]
1.48 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
1.48 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
1.48 logistic.py(343):                     dtype=X.dtype)
1.48 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
1.48 logistic.py(282):     n_classes = Y.shape[1]
1.48 logistic.py(283):     n_features = X.shape[1]
1.48 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
1.48 logistic.py(285):     w = w.reshape(n_classes, -1)
1.48 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
1.48 logistic.py(287):     if fit_intercept:
1.48 logistic.py(288):         intercept = w[:, -1]
1.48 logistic.py(289):         w = w[:, :-1]
1.48 logistic.py(292):     p = safe_sparse_dot(X, w.T)
1.48 logistic.py(293):     p += intercept
1.48 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
1.48 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
1.48 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
1.48 logistic.py(297):     p = np.exp(p, p)
1.48 logistic.py(298):     return loss, p, w
1.48 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
1.48 logistic.py(346):     diff = sample_weight * (p - Y)
1.48 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
1.48 logistic.py(348):     grad[:, :n_features] += alpha * w
1.48 logistic.py(349):     if fit_intercept:
1.48 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
1.48 logistic.py(351):     return loss, grad.ravel(), p
1.48 logistic.py(718):             if info["warnflag"] == 1:
1.48 logistic.py(723):             n_iter_i = min(info['nit'], max_iter)
1.48 logistic.py(760):         if multi_class == 'multinomial':
1.48 logistic.py(761):             multi_w0 = np.reshape(w0, (classes.size, -1))
1.48 logistic.py(762):             if classes.size == 2:
1.48 logistic.py(764):             coefs.append(multi_w0)
1.48 logistic.py(768):         n_iter[i] = n_iter_i
1.48 logistic.py(712):     for i, C in enumerate(Cs):
1.48 logistic.py(713):         if solver == 'lbfgs':
1.48 logistic.py(714):             w0, loss, info = optimize.fmin_l_bfgs_b(
1.48 logistic.py(715):                 func, w0, fprime=None,
1.48 logistic.py(716):                 args=(X, target, 1. / C, sample_weight),
1.48 logistic.py(717):                 iprint=(verbose > 0) - 1, pgtol=tol, maxiter=max_iter)
1.48 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
1.48 logistic.py(339):     n_classes = Y.shape[1]
1.48 logistic.py(340):     n_features = X.shape[1]
1.48 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
1.48 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
1.48 logistic.py(343):                     dtype=X.dtype)
1.48 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
1.48 logistic.py(282):     n_classes = Y.shape[1]
1.48 logistic.py(283):     n_features = X.shape[1]
1.48 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
1.48 logistic.py(285):     w = w.reshape(n_classes, -1)
1.48 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
1.48 logistic.py(287):     if fit_intercept:
1.48 logistic.py(288):         intercept = w[:, -1]
1.48 logistic.py(289):         w = w[:, :-1]
1.48 logistic.py(292):     p = safe_sparse_dot(X, w.T)
1.48 logistic.py(293):     p += intercept
1.48 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
1.49 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
1.49 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
1.49 logistic.py(297):     p = np.exp(p, p)
1.49 logistic.py(298):     return loss, p, w
1.49 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
1.49 logistic.py(346):     diff = sample_weight * (p - Y)
1.49 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
1.49 logistic.py(348):     grad[:, :n_features] += alpha * w
1.49 logistic.py(349):     if fit_intercept:
1.49 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
1.49 logistic.py(351):     return loss, grad.ravel(), p
1.49 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
1.49 logistic.py(339):     n_classes = Y.shape[1]
1.49 logistic.py(340):     n_features = X.shape[1]
1.49 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
1.49 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
1.49 logistic.py(343):                     dtype=X.dtype)
1.49 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
1.49 logistic.py(282):     n_classes = Y.shape[1]
1.49 logistic.py(283):     n_features = X.shape[1]
1.49 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
1.49 logistic.py(285):     w = w.reshape(n_classes, -1)
1.49 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
1.49 logistic.py(287):     if fit_intercept:
1.49 logistic.py(288):         intercept = w[:, -1]
1.49 logistic.py(289):         w = w[:, :-1]
1.49 logistic.py(292):     p = safe_sparse_dot(X, w.T)
1.49 logistic.py(293):     p += intercept
1.49 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
1.49 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
1.49 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
1.49 logistic.py(297):     p = np.exp(p, p)
1.49 logistic.py(298):     return loss, p, w
1.49 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
1.49 logistic.py(346):     diff = sample_weight * (p - Y)
1.49 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
1.49 logistic.py(348):     grad[:, :n_features] += alpha * w
1.49 logistic.py(349):     if fit_intercept:
1.49 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
1.49 logistic.py(351):     return loss, grad.ravel(), p
1.49 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
1.49 logistic.py(339):     n_classes = Y.shape[1]
1.49 logistic.py(340):     n_features = X.shape[1]
1.49 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
1.49 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
1.49 logistic.py(343):                     dtype=X.dtype)
1.49 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
1.49 logistic.py(282):     n_classes = Y.shape[1]
1.49 logistic.py(283):     n_features = X.shape[1]
1.49 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
1.49 logistic.py(285):     w = w.reshape(n_classes, -1)
1.49 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
1.49 logistic.py(287):     if fit_intercept:
1.49 logistic.py(288):         intercept = w[:, -1]
1.49 logistic.py(289):         w = w[:, :-1]
1.49 logistic.py(292):     p = safe_sparse_dot(X, w.T)
1.49 logistic.py(293):     p += intercept
1.49 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
1.49 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
1.49 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
1.49 logistic.py(297):     p = np.exp(p, p)
1.49 logistic.py(298):     return loss, p, w
1.49 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
1.49 logistic.py(346):     diff = sample_weight * (p - Y)
1.49 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
1.49 logistic.py(348):     grad[:, :n_features] += alpha * w
1.49 logistic.py(349):     if fit_intercept:
1.49 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
1.49 logistic.py(351):     return loss, grad.ravel(), p
1.49 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
1.49 logistic.py(339):     n_classes = Y.shape[1]
1.49 logistic.py(340):     n_features = X.shape[1]
1.49 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
1.49 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
1.49 logistic.py(343):                     dtype=X.dtype)
1.49 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
1.49 logistic.py(282):     n_classes = Y.shape[1]
1.49 logistic.py(283):     n_features = X.shape[1]
1.49 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
1.49 logistic.py(285):     w = w.reshape(n_classes, -1)
1.49 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
1.49 logistic.py(287):     if fit_intercept:
1.49 logistic.py(288):         intercept = w[:, -1]
1.49 logistic.py(289):         w = w[:, :-1]
1.49 logistic.py(292):     p = safe_sparse_dot(X, w.T)
1.49 logistic.py(293):     p += intercept
1.49 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
1.49 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
1.49 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
1.49 logistic.py(297):     p = np.exp(p, p)
1.49 logistic.py(298):     return loss, p, w
1.49 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
1.49 logistic.py(346):     diff = sample_weight * (p - Y)
1.49 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
1.49 logistic.py(348):     grad[:, :n_features] += alpha * w
1.49 logistic.py(349):     if fit_intercept:
1.49 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
1.49 logistic.py(351):     return loss, grad.ravel(), p
1.49 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
1.49 logistic.py(339):     n_classes = Y.shape[1]
1.49 logistic.py(340):     n_features = X.shape[1]
1.49 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
1.49 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
1.49 logistic.py(343):                     dtype=X.dtype)
1.49 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
1.49 logistic.py(282):     n_classes = Y.shape[1]
1.49 logistic.py(283):     n_features = X.shape[1]
1.49 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
1.49 logistic.py(285):     w = w.reshape(n_classes, -1)
1.49 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
1.49 logistic.py(287):     if fit_intercept:
1.49 logistic.py(288):         intercept = w[:, -1]
1.49 logistic.py(289):         w = w[:, :-1]
1.49 logistic.py(292):     p = safe_sparse_dot(X, w.T)
1.49 logistic.py(293):     p += intercept
1.49 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
1.49 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
1.49 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
1.49 logistic.py(297):     p = np.exp(p, p)
1.49 logistic.py(298):     return loss, p, w
1.49 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
1.49 logistic.py(346):     diff = sample_weight * (p - Y)
1.49 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
1.49 logistic.py(348):     grad[:, :n_features] += alpha * w
1.49 logistic.py(349):     if fit_intercept:
1.49 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
1.49 logistic.py(351):     return loss, grad.ravel(), p
1.49 logistic.py(718):             if info["warnflag"] == 1:
1.49 logistic.py(723):             n_iter_i = min(info['nit'], max_iter)
1.49 logistic.py(760):         if multi_class == 'multinomial':
1.49 logistic.py(761):             multi_w0 = np.reshape(w0, (classes.size, -1))
1.49 logistic.py(762):             if classes.size == 2:
1.49 logistic.py(764):             coefs.append(multi_w0)
1.49 logistic.py(768):         n_iter[i] = n_iter_i
1.49 logistic.py(712):     for i, C in enumerate(Cs):
1.49 logistic.py(770):     return coefs, np.array(Cs), n_iter
1.49 logistic.py(925):     log_reg = LogisticRegression(multi_class=multi_class)
1.49 logistic.py(1171):         self.penalty = penalty
1.49 logistic.py(1172):         self.dual = dual
1.49 logistic.py(1173):         self.tol = tol
1.49 logistic.py(1174):         self.C = C
1.49 logistic.py(1175):         self.fit_intercept = fit_intercept
1.49 logistic.py(1176):         self.intercept_scaling = intercept_scaling
1.49 logistic.py(1177):         self.class_weight = class_weight
1.49 logistic.py(1178):         self.random_state = random_state
1.49 logistic.py(1179):         self.solver = solver
1.49 logistic.py(1180):         self.max_iter = max_iter
1.49 logistic.py(1181):         self.multi_class = multi_class
1.49 logistic.py(1182):         self.verbose = verbose
1.49 logistic.py(1183):         self.warm_start = warm_start
1.49 logistic.py(1184):         self.n_jobs = n_jobs
1.49 logistic.py(928):     if multi_class == 'ovr':
1.49 logistic.py(930):     elif multi_class == 'multinomial':
1.49 logistic.py(931):         log_reg.classes_ = np.unique(y_train)
1.49 logistic.py(936):     if pos_class is not None:
1.49 logistic.py(941):     scores = list()
1.49 logistic.py(943):     if isinstance(scoring, six.string_types):
1.49 logistic.py(945):     for w in coefs:
1.49 logistic.py(946):         if multi_class == 'ovr':
1.49 logistic.py(948):         if fit_intercept:
1.49 logistic.py(949):             log_reg.coef_ = w[:, :-1]
1.49 logistic.py(950):             log_reg.intercept_ = w[:, -1]
1.49 logistic.py(955):         if scoring is None:
1.49 logistic.py(956):             scores.append(log_reg.score(X_test, y_test))
1.49 logistic.py(945):     for w in coefs:
1.49 logistic.py(946):         if multi_class == 'ovr':
1.49 logistic.py(948):         if fit_intercept:
1.49 logistic.py(949):             log_reg.coef_ = w[:, :-1]
1.49 logistic.py(950):             log_reg.intercept_ = w[:, -1]
1.49 logistic.py(955):         if scoring is None:
1.49 logistic.py(956):             scores.append(log_reg.score(X_test, y_test))
1.49 logistic.py(945):     for w in coefs:
1.49 logistic.py(946):         if multi_class == 'ovr':
1.49 logistic.py(948):         if fit_intercept:
1.49 logistic.py(949):             log_reg.coef_ = w[:, :-1]
1.49 logistic.py(950):             log_reg.intercept_ = w[:, -1]
1.49 logistic.py(955):         if scoring is None:
1.49 logistic.py(956):             scores.append(log_reg.score(X_test, y_test))
1.49 logistic.py(945):     for w in coefs:
1.49 logistic.py(946):         if multi_class == 'ovr':
1.49 logistic.py(948):         if fit_intercept:
1.49 logistic.py(949):             log_reg.coef_ = w[:, :-1]
1.49 logistic.py(950):             log_reg.intercept_ = w[:, -1]
1.49 logistic.py(955):         if scoring is None:
1.49 logistic.py(956):             scores.append(log_reg.score(X_test, y_test))
1.49 logistic.py(945):     for w in coefs:
1.49 logistic.py(946):         if multi_class == 'ovr':
1.49 logistic.py(948):         if fit_intercept:
1.49 logistic.py(949):             log_reg.coef_ = w[:, :-1]
1.49 logistic.py(950):             log_reg.intercept_ = w[:, -1]
1.49 logistic.py(955):         if scoring is None:
1.49 logistic.py(956):             scores.append(log_reg.score(X_test, y_test))
1.49 logistic.py(945):     for w in coefs:
1.49 logistic.py(946):         if multi_class == 'ovr':
1.49 logistic.py(948):         if fit_intercept:
1.49 logistic.py(949):             log_reg.coef_ = w[:, :-1]
1.49 logistic.py(950):             log_reg.intercept_ = w[:, -1]
1.49 logistic.py(955):         if scoring is None:
1.49 logistic.py(956):             scores.append(log_reg.score(X_test, y_test))
1.49 logistic.py(945):     for w in coefs:
1.49 logistic.py(946):         if multi_class == 'ovr':
1.49 logistic.py(948):         if fit_intercept:
1.49 logistic.py(949):             log_reg.coef_ = w[:, :-1]
1.49 logistic.py(950):             log_reg.intercept_ = w[:, -1]
1.49 logistic.py(955):         if scoring is None:
1.49 logistic.py(956):             scores.append(log_reg.score(X_test, y_test))
1.49 logistic.py(945):     for w in coefs:
1.49 logistic.py(946):         if multi_class == 'ovr':
1.49 logistic.py(948):         if fit_intercept:
1.49 logistic.py(949):             log_reg.coef_ = w[:, :-1]
1.49 logistic.py(950):             log_reg.intercept_ = w[:, -1]
1.49 logistic.py(955):         if scoring is None:
1.49 logistic.py(956):             scores.append(log_reg.score(X_test, y_test))
1.49 logistic.py(945):     for w in coefs:
1.49 logistic.py(946):         if multi_class == 'ovr':
1.49 logistic.py(948):         if fit_intercept:
1.49 logistic.py(949):             log_reg.coef_ = w[:, :-1]
1.49 logistic.py(950):             log_reg.intercept_ = w[:, -1]
1.49 logistic.py(955):         if scoring is None:
1.49 logistic.py(956):             scores.append(log_reg.score(X_test, y_test))
1.49 logistic.py(945):     for w in coefs:
1.49 logistic.py(946):         if multi_class == 'ovr':
1.49 logistic.py(948):         if fit_intercept:
1.49 logistic.py(949):             log_reg.coef_ = w[:, :-1]
1.49 logistic.py(950):             log_reg.intercept_ = w[:, -1]
1.49 logistic.py(955):         if scoring is None:
1.49 logistic.py(956):             scores.append(log_reg.score(X_test, y_test))
1.49 logistic.py(945):     for w in coefs:
1.49 logistic.py(959):     return coefs, Cs, np.array(scores), n_iter
1.49 logistic.py(1703):             for train, test in folds)
1.49 logistic.py(903):     _check_solver_option(solver, multi_class, penalty, dual)
1.49 logistic.py(428):     if solver not in ['liblinear', 'newton-cg', 'lbfgs', 'sag', 'saga']:
1.49 logistic.py(433):     if multi_class not in ['multinomial', 'ovr']:
1.49 logistic.py(437):     if multi_class == 'multinomial' and solver == 'liblinear':
1.49 logistic.py(441):     if solver not in ['liblinear', 'saga']:
1.49 logistic.py(442):         if penalty != 'l2':
1.49 logistic.py(445):     if solver != 'liblinear':
1.49 logistic.py(446):         if dual:
1.49 logistic.py(905):     X_train = X[train]
1.49 logistic.py(906):     X_test = X[test]
1.49 logistic.py(907):     y_train = y[train]
1.49 logistic.py(908):     y_test = y[test]
1.49 logistic.py(910):     if sample_weight is not None:
1.49 logistic.py(916):     coefs, Cs, n_iter = logistic_regression_path(
1.49 logistic.py(917):         X_train, y_train, Cs=Cs, fit_intercept=fit_intercept,
1.49 logistic.py(918):         solver=solver, max_iter=max_iter, class_weight=class_weight,
1.49 logistic.py(919):         pos_class=pos_class, multi_class=multi_class,
1.49 logistic.py(920):         tol=tol, verbose=verbose, dual=dual, penalty=penalty,
1.49 logistic.py(921):         intercept_scaling=intercept_scaling, random_state=random_state,
1.49 logistic.py(922):         check_input=False, max_squared_sum=max_squared_sum,
1.49 logistic.py(923):         sample_weight=sample_weight)
1.49 logistic.py(591):     if isinstance(Cs, numbers.Integral):
1.49 logistic.py(592):         Cs = np.logspace(-4, 4, Cs)
1.49 logistic.py(594):     _check_solver_option(solver, multi_class, penalty, dual)
1.49 logistic.py(428):     if solver not in ['liblinear', 'newton-cg', 'lbfgs', 'sag', 'saga']:
1.49 logistic.py(433):     if multi_class not in ['multinomial', 'ovr']:
1.49 logistic.py(437):     if multi_class == 'multinomial' and solver == 'liblinear':
1.49 logistic.py(441):     if solver not in ['liblinear', 'saga']:
1.49 logistic.py(442):         if penalty != 'l2':
1.49 logistic.py(445):     if solver != 'liblinear':
1.49 logistic.py(446):         if dual:
1.49 logistic.py(597):     if check_input:
1.49 logistic.py(602):     _, n_features = X.shape
1.49 logistic.py(603):     classes = np.unique(y)
1.49 logistic.py(604):     random_state = check_random_state(random_state)
1.49 logistic.py(606):     if pos_class is None and multi_class != 'multinomial':
1.49 logistic.py(615):     if sample_weight is not None:
1.49 logistic.py(619):         sample_weight = np.ones(X.shape[0], dtype=X.dtype)
1.49 logistic.py(624):     le = LabelEncoder()
1.49 logistic.py(625):     if isinstance(class_weight, dict) or multi_class == 'multinomial':
1.49 logistic.py(626):         class_weight_ = compute_class_weight(class_weight, classes, y)
1.49 logistic.py(627):         sample_weight *= class_weight_[le.fit_transform(y)]
1.49 logistic.py(631):     if multi_class == 'ovr':
1.49 logistic.py(645):         if solver not in ['sag', 'saga']:
1.49 logistic.py(646):             lbin = LabelBinarizer()
1.49 logistic.py(647):             Y_multi = lbin.fit_transform(y)
1.49 logistic.py(648):             if Y_multi.shape[1] == 1:
1.49 logistic.py(655):         w0 = np.zeros((classes.size, n_features + int(fit_intercept)),
1.49 logistic.py(656):                       order='F', dtype=X.dtype)
1.49 logistic.py(658):     if coef is not None:
1.49 logistic.py(688):     if multi_class == 'multinomial':
1.49 logistic.py(690):         if solver in ['lbfgs', 'newton-cg']:
1.49 logistic.py(691):             w0 = w0.ravel()
1.49 logistic.py(692):         target = Y_multi
1.49 logistic.py(693):         if solver == 'lbfgs':
1.49 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
1.49 logistic.py(699):         warm_start_sag = {'coef': w0.T}
1.49 logistic.py(710):     coefs = list()
1.49 logistic.py(711):     n_iter = np.zeros(len(Cs), dtype=np.int32)
1.49 logistic.py(712):     for i, C in enumerate(Cs):
1.49 logistic.py(713):         if solver == 'lbfgs':
1.49 logistic.py(714):             w0, loss, info = optimize.fmin_l_bfgs_b(
1.49 logistic.py(715):                 func, w0, fprime=None,
1.49 logistic.py(716):                 args=(X, target, 1. / C, sample_weight),
1.49 logistic.py(717):                 iprint=(verbose > 0) - 1, pgtol=tol, maxiter=max_iter)
1.49 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
1.49 logistic.py(339):     n_classes = Y.shape[1]
1.49 logistic.py(340):     n_features = X.shape[1]
1.49 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
1.49 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
1.49 logistic.py(343):                     dtype=X.dtype)
1.49 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
1.49 logistic.py(282):     n_classes = Y.shape[1]
1.49 logistic.py(283):     n_features = X.shape[1]
1.49 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
1.49 logistic.py(285):     w = w.reshape(n_classes, -1)
1.49 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
1.49 logistic.py(287):     if fit_intercept:
1.49 logistic.py(288):         intercept = w[:, -1]
1.49 logistic.py(289):         w = w[:, :-1]
1.49 logistic.py(292):     p = safe_sparse_dot(X, w.T)
1.49 logistic.py(293):     p += intercept
1.49 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
1.49 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
1.49 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
1.49 logistic.py(297):     p = np.exp(p, p)
1.49 logistic.py(298):     return loss, p, w
1.49 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
1.49 logistic.py(346):     diff = sample_weight * (p - Y)
1.49 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
1.49 logistic.py(348):     grad[:, :n_features] += alpha * w
1.49 logistic.py(349):     if fit_intercept:
1.49 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
1.49 logistic.py(351):     return loss, grad.ravel(), p
1.49 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
1.49 logistic.py(339):     n_classes = Y.shape[1]
1.49 logistic.py(340):     n_features = X.shape[1]
1.49 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
1.49 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
1.49 logistic.py(343):                     dtype=X.dtype)
1.49 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
1.49 logistic.py(282):     n_classes = Y.shape[1]
1.49 logistic.py(283):     n_features = X.shape[1]
1.49 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
1.49 logistic.py(285):     w = w.reshape(n_classes, -1)
1.49 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
1.49 logistic.py(287):     if fit_intercept:
1.49 logistic.py(288):         intercept = w[:, -1]
1.49 logistic.py(289):         w = w[:, :-1]
1.49 logistic.py(292):     p = safe_sparse_dot(X, w.T)
1.49 logistic.py(293):     p += intercept
1.49 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
1.49 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
1.49 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
1.49 logistic.py(297):     p = np.exp(p, p)
1.49 logistic.py(298):     return loss, p, w
1.49 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
1.49 logistic.py(346):     diff = sample_weight * (p - Y)
1.49 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
1.49 logistic.py(348):     grad[:, :n_features] += alpha * w
1.49 logistic.py(349):     if fit_intercept:
1.49 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
1.49 logistic.py(351):     return loss, grad.ravel(), p
1.49 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
1.49 logistic.py(339):     n_classes = Y.shape[1]
1.49 logistic.py(340):     n_features = X.shape[1]
1.49 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
1.49 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
1.49 logistic.py(343):                     dtype=X.dtype)
1.49 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
1.49 logistic.py(282):     n_classes = Y.shape[1]
1.49 logistic.py(283):     n_features = X.shape[1]
1.49 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
1.49 logistic.py(285):     w = w.reshape(n_classes, -1)
1.49 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
1.49 logistic.py(287):     if fit_intercept:
1.49 logistic.py(288):         intercept = w[:, -1]
1.49 logistic.py(289):         w = w[:, :-1]
1.49 logistic.py(292):     p = safe_sparse_dot(X, w.T)
1.49 logistic.py(293):     p += intercept
1.49 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
1.49 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
1.49 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
1.49 logistic.py(297):     p = np.exp(p, p)
1.49 logistic.py(298):     return loss, p, w
1.49 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
1.49 logistic.py(346):     diff = sample_weight * (p - Y)
1.49 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
1.49 logistic.py(348):     grad[:, :n_features] += alpha * w
1.49 logistic.py(349):     if fit_intercept:
1.49 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
1.49 logistic.py(351):     return loss, grad.ravel(), p
1.49 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
1.49 logistic.py(339):     n_classes = Y.shape[1]
1.49 logistic.py(340):     n_features = X.shape[1]
1.49 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
1.49 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
1.49 logistic.py(343):                     dtype=X.dtype)
1.49 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
1.49 logistic.py(282):     n_classes = Y.shape[1]
1.49 logistic.py(283):     n_features = X.shape[1]
1.49 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
1.49 logistic.py(285):     w = w.reshape(n_classes, -1)
1.49 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
1.49 logistic.py(287):     if fit_intercept:
1.49 logistic.py(288):         intercept = w[:, -1]
1.49 logistic.py(289):         w = w[:, :-1]
1.49 logistic.py(292):     p = safe_sparse_dot(X, w.T)
1.49 logistic.py(293):     p += intercept
1.49 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
1.50 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
1.50 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
1.50 logistic.py(297):     p = np.exp(p, p)
1.50 logistic.py(298):     return loss, p, w
1.50 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
1.50 logistic.py(346):     diff = sample_weight * (p - Y)
1.50 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
1.50 logistic.py(348):     grad[:, :n_features] += alpha * w
1.50 logistic.py(349):     if fit_intercept:
1.50 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
1.50 logistic.py(351):     return loss, grad.ravel(), p
1.50 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
1.50 logistic.py(339):     n_classes = Y.shape[1]
1.50 logistic.py(340):     n_features = X.shape[1]
1.50 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
1.50 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
1.50 logistic.py(343):                     dtype=X.dtype)
1.50 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
1.50 logistic.py(282):     n_classes = Y.shape[1]
1.50 logistic.py(283):     n_features = X.shape[1]
1.50 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
1.50 logistic.py(285):     w = w.reshape(n_classes, -1)
1.50 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
1.50 logistic.py(287):     if fit_intercept:
1.50 logistic.py(288):         intercept = w[:, -1]
1.50 logistic.py(289):         w = w[:, :-1]
1.50 logistic.py(292):     p = safe_sparse_dot(X, w.T)
1.50 logistic.py(293):     p += intercept
1.50 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
1.50 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
1.50 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
1.50 logistic.py(297):     p = np.exp(p, p)
1.50 logistic.py(298):     return loss, p, w
1.50 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
1.50 logistic.py(346):     diff = sample_weight * (p - Y)
1.50 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
1.50 logistic.py(348):     grad[:, :n_features] += alpha * w
1.50 logistic.py(349):     if fit_intercept:
1.50 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
1.50 logistic.py(351):     return loss, grad.ravel(), p
1.50 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
1.50 logistic.py(339):     n_classes = Y.shape[1]
1.50 logistic.py(340):     n_features = X.shape[1]
1.50 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
1.50 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
1.50 logistic.py(343):                     dtype=X.dtype)
1.50 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
1.50 logistic.py(282):     n_classes = Y.shape[1]
1.50 logistic.py(283):     n_features = X.shape[1]
1.50 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
1.50 logistic.py(285):     w = w.reshape(n_classes, -1)
1.50 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
1.50 logistic.py(287):     if fit_intercept:
1.50 logistic.py(288):         intercept = w[:, -1]
1.50 logistic.py(289):         w = w[:, :-1]
1.50 logistic.py(292):     p = safe_sparse_dot(X, w.T)
1.50 logistic.py(293):     p += intercept
1.50 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
1.50 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
1.50 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
1.50 logistic.py(297):     p = np.exp(p, p)
1.50 logistic.py(298):     return loss, p, w
1.50 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
1.50 logistic.py(346):     diff = sample_weight * (p - Y)
1.50 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
1.50 logistic.py(348):     grad[:, :n_features] += alpha * w
1.50 logistic.py(349):     if fit_intercept:
1.50 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
1.50 logistic.py(351):     return loss, grad.ravel(), p
1.50 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
1.50 logistic.py(339):     n_classes = Y.shape[1]
1.50 logistic.py(340):     n_features = X.shape[1]
1.50 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
1.50 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
1.50 logistic.py(343):                     dtype=X.dtype)
1.50 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
1.50 logistic.py(282):     n_classes = Y.shape[1]
1.50 logistic.py(283):     n_features = X.shape[1]
1.50 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
1.50 logistic.py(285):     w = w.reshape(n_classes, -1)
1.50 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
1.50 logistic.py(287):     if fit_intercept:
1.50 logistic.py(288):         intercept = w[:, -1]
1.50 logistic.py(289):         w = w[:, :-1]
1.50 logistic.py(292):     p = safe_sparse_dot(X, w.T)
1.50 logistic.py(293):     p += intercept
1.50 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
1.50 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
1.50 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
1.50 logistic.py(297):     p = np.exp(p, p)
1.50 logistic.py(298):     return loss, p, w
1.50 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
1.50 logistic.py(346):     diff = sample_weight * (p - Y)
1.50 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
1.50 logistic.py(348):     grad[:, :n_features] += alpha * w
1.50 logistic.py(349):     if fit_intercept:
1.50 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
1.50 logistic.py(351):     return loss, grad.ravel(), p
1.50 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
1.50 logistic.py(339):     n_classes = Y.shape[1]
1.50 logistic.py(340):     n_features = X.shape[1]
1.50 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
1.50 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
1.50 logistic.py(343):                     dtype=X.dtype)
1.50 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
1.50 logistic.py(282):     n_classes = Y.shape[1]
1.50 logistic.py(283):     n_features = X.shape[1]
1.50 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
1.50 logistic.py(285):     w = w.reshape(n_classes, -1)
1.50 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
1.50 logistic.py(287):     if fit_intercept:
1.50 logistic.py(288):         intercept = w[:, -1]
1.50 logistic.py(289):         w = w[:, :-1]
1.50 logistic.py(292):     p = safe_sparse_dot(X, w.T)
1.50 logistic.py(293):     p += intercept
1.50 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
1.50 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
1.50 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
1.50 logistic.py(297):     p = np.exp(p, p)
1.50 logistic.py(298):     return loss, p, w
1.50 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
1.50 logistic.py(346):     diff = sample_weight * (p - Y)
1.50 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
1.50 logistic.py(348):     grad[:, :n_features] += alpha * w
1.50 logistic.py(349):     if fit_intercept:
1.50 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
1.50 logistic.py(351):     return loss, grad.ravel(), p
1.50 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
1.50 logistic.py(339):     n_classes = Y.shape[1]
1.50 logistic.py(340):     n_features = X.shape[1]
1.50 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
1.50 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
1.50 logistic.py(343):                     dtype=X.dtype)
1.50 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
1.50 logistic.py(282):     n_classes = Y.shape[1]
1.50 logistic.py(283):     n_features = X.shape[1]
1.50 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
1.50 logistic.py(285):     w = w.reshape(n_classes, -1)
1.50 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
1.50 logistic.py(287):     if fit_intercept:
1.50 logistic.py(288):         intercept = w[:, -1]
1.50 logistic.py(289):         w = w[:, :-1]
1.50 logistic.py(292):     p = safe_sparse_dot(X, w.T)
1.50 logistic.py(293):     p += intercept
1.50 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
1.50 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
1.50 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
1.50 logistic.py(297):     p = np.exp(p, p)
1.50 logistic.py(298):     return loss, p, w
1.50 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
1.50 logistic.py(346):     diff = sample_weight * (p - Y)
1.50 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
1.50 logistic.py(348):     grad[:, :n_features] += alpha * w
1.50 logistic.py(349):     if fit_intercept:
1.50 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
1.50 logistic.py(351):     return loss, grad.ravel(), p
1.50 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
1.50 logistic.py(339):     n_classes = Y.shape[1]
1.50 logistic.py(340):     n_features = X.shape[1]
1.50 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
1.50 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
1.50 logistic.py(343):                     dtype=X.dtype)
1.50 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
1.50 logistic.py(282):     n_classes = Y.shape[1]
1.50 logistic.py(283):     n_features = X.shape[1]
1.50 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
1.50 logistic.py(285):     w = w.reshape(n_classes, -1)
1.50 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
1.50 logistic.py(287):     if fit_intercept:
1.50 logistic.py(288):         intercept = w[:, -1]
1.50 logistic.py(289):         w = w[:, :-1]
1.50 logistic.py(292):     p = safe_sparse_dot(X, w.T)
1.50 logistic.py(293):     p += intercept
1.50 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
1.50 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
1.50 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
1.50 logistic.py(297):     p = np.exp(p, p)
1.50 logistic.py(298):     return loss, p, w
1.50 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
1.50 logistic.py(346):     diff = sample_weight * (p - Y)
1.50 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
1.50 logistic.py(348):     grad[:, :n_features] += alpha * w
1.50 logistic.py(349):     if fit_intercept:
1.50 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
1.50 logistic.py(351):     return loss, grad.ravel(), p
1.50 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
1.50 logistic.py(339):     n_classes = Y.shape[1]
1.50 logistic.py(340):     n_features = X.shape[1]
1.50 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
1.50 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
1.50 logistic.py(343):                     dtype=X.dtype)
1.50 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
1.50 logistic.py(282):     n_classes = Y.shape[1]
1.50 logistic.py(283):     n_features = X.shape[1]
1.50 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
1.50 logistic.py(285):     w = w.reshape(n_classes, -1)
1.50 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
1.50 logistic.py(287):     if fit_intercept:
1.50 logistic.py(288):         intercept = w[:, -1]
1.50 logistic.py(289):         w = w[:, :-1]
1.50 logistic.py(292):     p = safe_sparse_dot(X, w.T)
1.50 logistic.py(293):     p += intercept
1.50 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
1.50 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
1.50 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
1.50 logistic.py(297):     p = np.exp(p, p)
1.50 logistic.py(298):     return loss, p, w
1.50 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
1.50 logistic.py(346):     diff = sample_weight * (p - Y)
1.50 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
1.50 logistic.py(348):     grad[:, :n_features] += alpha * w
1.50 logistic.py(349):     if fit_intercept:
1.50 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
1.50 logistic.py(351):     return loss, grad.ravel(), p
1.50 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
1.50 logistic.py(339):     n_classes = Y.shape[1]
1.50 logistic.py(340):     n_features = X.shape[1]
1.50 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
1.50 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
1.50 logistic.py(343):                     dtype=X.dtype)
1.50 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
1.50 logistic.py(282):     n_classes = Y.shape[1]
1.50 logistic.py(283):     n_features = X.shape[1]
1.50 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
1.50 logistic.py(285):     w = w.reshape(n_classes, -1)
1.50 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
1.50 logistic.py(287):     if fit_intercept:
1.50 logistic.py(288):         intercept = w[:, -1]
1.50 logistic.py(289):         w = w[:, :-1]
1.50 logistic.py(292):     p = safe_sparse_dot(X, w.T)
1.50 logistic.py(293):     p += intercept
1.50 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
1.50 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
1.50 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
1.50 logistic.py(297):     p = np.exp(p, p)
1.50 logistic.py(298):     return loss, p, w
1.50 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
1.50 logistic.py(346):     diff = sample_weight * (p - Y)
1.50 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
1.50 logistic.py(348):     grad[:, :n_features] += alpha * w
1.50 logistic.py(349):     if fit_intercept:
1.50 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
1.50 logistic.py(351):     return loss, grad.ravel(), p
1.50 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
1.50 logistic.py(339):     n_classes = Y.shape[1]
1.50 logistic.py(340):     n_features = X.shape[1]
1.50 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
1.50 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
1.50 logistic.py(343):                     dtype=X.dtype)
1.50 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
1.50 logistic.py(282):     n_classes = Y.shape[1]
1.50 logistic.py(283):     n_features = X.shape[1]
1.50 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
1.50 logistic.py(285):     w = w.reshape(n_classes, -1)
1.50 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
1.50 logistic.py(287):     if fit_intercept:
1.50 logistic.py(288):         intercept = w[:, -1]
1.50 logistic.py(289):         w = w[:, :-1]
1.50 logistic.py(292):     p = safe_sparse_dot(X, w.T)
1.50 logistic.py(293):     p += intercept
1.50 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
1.50 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
1.50 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
1.50 logistic.py(297):     p = np.exp(p, p)
1.50 logistic.py(298):     return loss, p, w
1.50 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
1.50 logistic.py(346):     diff = sample_weight * (p - Y)
1.50 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
1.50 logistic.py(348):     grad[:, :n_features] += alpha * w
1.50 logistic.py(349):     if fit_intercept:
1.50 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
1.50 logistic.py(351):     return loss, grad.ravel(), p
1.50 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
1.50 logistic.py(339):     n_classes = Y.shape[1]
1.50 logistic.py(340):     n_features = X.shape[1]
1.50 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
1.50 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
1.50 logistic.py(343):                     dtype=X.dtype)
1.50 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
1.50 logistic.py(282):     n_classes = Y.shape[1]
1.50 logistic.py(283):     n_features = X.shape[1]
1.50 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
1.50 logistic.py(285):     w = w.reshape(n_classes, -1)
1.50 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
1.50 logistic.py(287):     if fit_intercept:
1.50 logistic.py(288):         intercept = w[:, -1]
1.50 logistic.py(289):         w = w[:, :-1]
1.50 logistic.py(292):     p = safe_sparse_dot(X, w.T)
1.50 logistic.py(293):     p += intercept
1.50 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
1.50 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
1.50 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
1.50 logistic.py(297):     p = np.exp(p, p)
1.50 logistic.py(298):     return loss, p, w
1.50 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
1.50 logistic.py(346):     diff = sample_weight * (p - Y)
1.50 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
1.50 logistic.py(348):     grad[:, :n_features] += alpha * w
1.50 logistic.py(349):     if fit_intercept:
1.50 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
1.50 logistic.py(351):     return loss, grad.ravel(), p
1.50 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
1.50 logistic.py(339):     n_classes = Y.shape[1]
1.50 logistic.py(340):     n_features = X.shape[1]
1.50 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
1.50 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
1.50 logistic.py(343):                     dtype=X.dtype)
1.50 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
1.50 logistic.py(282):     n_classes = Y.shape[1]
1.50 logistic.py(283):     n_features = X.shape[1]
1.50 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
1.50 logistic.py(285):     w = w.reshape(n_classes, -1)
1.50 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
1.50 logistic.py(287):     if fit_intercept:
1.50 logistic.py(288):         intercept = w[:, -1]
1.50 logistic.py(289):         w = w[:, :-1]
1.50 logistic.py(292):     p = safe_sparse_dot(X, w.T)
1.50 logistic.py(293):     p += intercept
1.50 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
1.50 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
1.50 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
1.50 logistic.py(297):     p = np.exp(p, p)
1.50 logistic.py(298):     return loss, p, w
1.50 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
1.50 logistic.py(346):     diff = sample_weight * (p - Y)
1.50 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
1.50 logistic.py(348):     grad[:, :n_features] += alpha * w
1.50 logistic.py(349):     if fit_intercept:
1.50 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
1.50 logistic.py(351):     return loss, grad.ravel(), p
1.50 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
1.50 logistic.py(339):     n_classes = Y.shape[1]
1.50 logistic.py(340):     n_features = X.shape[1]
1.50 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
1.50 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
1.50 logistic.py(343):                     dtype=X.dtype)
1.50 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
1.50 logistic.py(282):     n_classes = Y.shape[1]
1.50 logistic.py(283):     n_features = X.shape[1]
1.50 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
1.50 logistic.py(285):     w = w.reshape(n_classes, -1)
1.50 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
1.50 logistic.py(287):     if fit_intercept:
1.50 logistic.py(288):         intercept = w[:, -1]
1.50 logistic.py(289):         w = w[:, :-1]
1.50 logistic.py(292):     p = safe_sparse_dot(X, w.T)
1.50 logistic.py(293):     p += intercept
1.50 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
1.50 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
1.50 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
1.50 logistic.py(297):     p = np.exp(p, p)
1.50 logistic.py(298):     return loss, p, w
1.50 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
1.50 logistic.py(346):     diff = sample_weight * (p - Y)
1.50 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
1.50 logistic.py(348):     grad[:, :n_features] += alpha * w
1.50 logistic.py(349):     if fit_intercept:
1.50 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
1.50 logistic.py(351):     return loss, grad.ravel(), p
1.50 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
1.50 logistic.py(339):     n_classes = Y.shape[1]
1.50 logistic.py(340):     n_features = X.shape[1]
1.50 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
1.50 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
1.50 logistic.py(343):                     dtype=X.dtype)
1.50 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
1.50 logistic.py(282):     n_classes = Y.shape[1]
1.50 logistic.py(283):     n_features = X.shape[1]
1.50 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
1.50 logistic.py(285):     w = w.reshape(n_classes, -1)
1.50 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
1.50 logistic.py(287):     if fit_intercept:
1.50 logistic.py(288):         intercept = w[:, -1]
1.50 logistic.py(289):         w = w[:, :-1]
1.50 logistic.py(292):     p = safe_sparse_dot(X, w.T)
1.50 logistic.py(293):     p += intercept
1.50 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
1.50 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
1.50 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
1.50 logistic.py(297):     p = np.exp(p, p)
1.50 logistic.py(298):     return loss, p, w
1.50 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
1.50 logistic.py(346):     diff = sample_weight * (p - Y)
1.50 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
1.50 logistic.py(348):     grad[:, :n_features] += alpha * w
1.50 logistic.py(349):     if fit_intercept:
1.50 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
1.50 logistic.py(351):     return loss, grad.ravel(), p
1.50 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
1.50 logistic.py(339):     n_classes = Y.shape[1]
1.50 logistic.py(340):     n_features = X.shape[1]
1.50 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
1.50 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
1.50 logistic.py(343):                     dtype=X.dtype)
1.50 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
1.50 logistic.py(282):     n_classes = Y.shape[1]
1.50 logistic.py(283):     n_features = X.shape[1]
1.50 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
1.50 logistic.py(285):     w = w.reshape(n_classes, -1)
1.50 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
1.50 logistic.py(287):     if fit_intercept:
1.50 logistic.py(288):         intercept = w[:, -1]
1.50 logistic.py(289):         w = w[:, :-1]
1.50 logistic.py(292):     p = safe_sparse_dot(X, w.T)
1.50 logistic.py(293):     p += intercept
1.50 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
1.50 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
1.50 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
1.50 logistic.py(297):     p = np.exp(p, p)
1.50 logistic.py(298):     return loss, p, w
1.50 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
1.50 logistic.py(346):     diff = sample_weight * (p - Y)
1.50 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
1.50 logistic.py(348):     grad[:, :n_features] += alpha * w
1.50 logistic.py(349):     if fit_intercept:
1.50 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
1.50 logistic.py(351):     return loss, grad.ravel(), p
1.50 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
1.50 logistic.py(339):     n_classes = Y.shape[1]
1.50 logistic.py(340):     n_features = X.shape[1]
1.50 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
1.50 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
1.50 logistic.py(343):                     dtype=X.dtype)
1.50 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
1.50 logistic.py(282):     n_classes = Y.shape[1]
1.50 logistic.py(283):     n_features = X.shape[1]
1.50 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
1.50 logistic.py(285):     w = w.reshape(n_classes, -1)
1.50 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
1.50 logistic.py(287):     if fit_intercept:
1.50 logistic.py(288):         intercept = w[:, -1]
1.50 logistic.py(289):         w = w[:, :-1]
1.50 logistic.py(292):     p = safe_sparse_dot(X, w.T)
1.50 logistic.py(293):     p += intercept
1.50 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
1.50 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
1.50 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
1.50 logistic.py(297):     p = np.exp(p, p)
1.50 logistic.py(298):     return loss, p, w
1.50 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
1.50 logistic.py(346):     diff = sample_weight * (p - Y)
1.50 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
1.50 logistic.py(348):     grad[:, :n_features] += alpha * w
1.50 logistic.py(349):     if fit_intercept:
1.50 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
1.50 logistic.py(351):     return loss, grad.ravel(), p
1.50 logistic.py(718):             if info["warnflag"] == 1:
1.50 logistic.py(723):             n_iter_i = min(info['nit'], max_iter)
1.50 logistic.py(760):         if multi_class == 'multinomial':
1.50 logistic.py(761):             multi_w0 = np.reshape(w0, (classes.size, -1))
1.50 logistic.py(762):             if classes.size == 2:
1.50 logistic.py(764):             coefs.append(multi_w0)
1.50 logistic.py(768):         n_iter[i] = n_iter_i
1.50 logistic.py(712):     for i, C in enumerate(Cs):
1.50 logistic.py(713):         if solver == 'lbfgs':
1.50 logistic.py(714):             w0, loss, info = optimize.fmin_l_bfgs_b(
1.50 logistic.py(715):                 func, w0, fprime=None,
1.50 logistic.py(716):                 args=(X, target, 1. / C, sample_weight),
1.50 logistic.py(717):                 iprint=(verbose > 0) - 1, pgtol=tol, maxiter=max_iter)
1.50 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
1.50 logistic.py(339):     n_classes = Y.shape[1]
1.50 logistic.py(340):     n_features = X.shape[1]
1.50 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
1.50 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
1.50 logistic.py(343):                     dtype=X.dtype)
1.50 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
1.50 logistic.py(282):     n_classes = Y.shape[1]
1.50 logistic.py(283):     n_features = X.shape[1]
1.50 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
1.50 logistic.py(285):     w = w.reshape(n_classes, -1)
1.50 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
1.50 logistic.py(287):     if fit_intercept:
1.50 logistic.py(288):         intercept = w[:, -1]
1.50 logistic.py(289):         w = w[:, :-1]
1.50 logistic.py(292):     p = safe_sparse_dot(X, w.T)
1.50 logistic.py(293):     p += intercept
1.50 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
1.51 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
1.51 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
1.51 logistic.py(297):     p = np.exp(p, p)
1.51 logistic.py(298):     return loss, p, w
1.51 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
1.51 logistic.py(346):     diff = sample_weight * (p - Y)
1.51 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
1.51 logistic.py(348):     grad[:, :n_features] += alpha * w
1.51 logistic.py(349):     if fit_intercept:
1.51 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
1.51 logistic.py(351):     return loss, grad.ravel(), p
1.51 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
1.51 logistic.py(339):     n_classes = Y.shape[1]
1.51 logistic.py(340):     n_features = X.shape[1]
1.51 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
1.51 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
1.51 logistic.py(343):                     dtype=X.dtype)
1.51 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
1.51 logistic.py(282):     n_classes = Y.shape[1]
1.51 logistic.py(283):     n_features = X.shape[1]
1.51 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
1.51 logistic.py(285):     w = w.reshape(n_classes, -1)
1.51 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
1.51 logistic.py(287):     if fit_intercept:
1.51 logistic.py(288):         intercept = w[:, -1]
1.51 logistic.py(289):         w = w[:, :-1]
1.51 logistic.py(292):     p = safe_sparse_dot(X, w.T)
1.51 logistic.py(293):     p += intercept
1.51 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
1.51 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
1.51 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
1.51 logistic.py(297):     p = np.exp(p, p)
1.51 logistic.py(298):     return loss, p, w
1.51 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
1.51 logistic.py(346):     diff = sample_weight * (p - Y)
1.51 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
1.51 logistic.py(348):     grad[:, :n_features] += alpha * w
1.51 logistic.py(349):     if fit_intercept:
1.51 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
1.51 logistic.py(351):     return loss, grad.ravel(), p
1.51 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
1.51 logistic.py(339):     n_classes = Y.shape[1]
1.51 logistic.py(340):     n_features = X.shape[1]
1.51 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
1.51 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
1.51 logistic.py(343):                     dtype=X.dtype)
1.51 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
1.51 logistic.py(282):     n_classes = Y.shape[1]
1.51 logistic.py(283):     n_features = X.shape[1]
1.51 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
1.51 logistic.py(285):     w = w.reshape(n_classes, -1)
1.51 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
1.51 logistic.py(287):     if fit_intercept:
1.51 logistic.py(288):         intercept = w[:, -1]
1.51 logistic.py(289):         w = w[:, :-1]
1.51 logistic.py(292):     p = safe_sparse_dot(X, w.T)
1.51 logistic.py(293):     p += intercept
1.51 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
1.51 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
1.51 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
1.51 logistic.py(297):     p = np.exp(p, p)
1.51 logistic.py(298):     return loss, p, w
1.51 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
1.51 logistic.py(346):     diff = sample_weight * (p - Y)
1.51 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
1.51 logistic.py(348):     grad[:, :n_features] += alpha * w
1.51 logistic.py(349):     if fit_intercept:
1.51 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
1.51 logistic.py(351):     return loss, grad.ravel(), p
1.51 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
1.51 logistic.py(339):     n_classes = Y.shape[1]
1.51 logistic.py(340):     n_features = X.shape[1]
1.51 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
1.51 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
1.51 logistic.py(343):                     dtype=X.dtype)
1.51 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
1.51 logistic.py(282):     n_classes = Y.shape[1]
1.51 logistic.py(283):     n_features = X.shape[1]
1.51 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
1.51 logistic.py(285):     w = w.reshape(n_classes, -1)
1.51 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
1.51 logistic.py(287):     if fit_intercept:
1.51 logistic.py(288):         intercept = w[:, -1]
1.51 logistic.py(289):         w = w[:, :-1]
1.51 logistic.py(292):     p = safe_sparse_dot(X, w.T)
1.51 logistic.py(293):     p += intercept
1.51 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
1.51 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
1.51 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
1.51 logistic.py(297):     p = np.exp(p, p)
1.51 logistic.py(298):     return loss, p, w
1.51 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
1.51 logistic.py(346):     diff = sample_weight * (p - Y)
1.51 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
1.51 logistic.py(348):     grad[:, :n_features] += alpha * w
1.51 logistic.py(349):     if fit_intercept:
1.51 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
1.51 logistic.py(351):     return loss, grad.ravel(), p
1.51 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
1.51 logistic.py(339):     n_classes = Y.shape[1]
1.51 logistic.py(340):     n_features = X.shape[1]
1.51 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
1.51 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
1.51 logistic.py(343):                     dtype=X.dtype)
1.51 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
1.51 logistic.py(282):     n_classes = Y.shape[1]
1.51 logistic.py(283):     n_features = X.shape[1]
1.51 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
1.51 logistic.py(285):     w = w.reshape(n_classes, -1)
1.51 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
1.51 logistic.py(287):     if fit_intercept:
1.51 logistic.py(288):         intercept = w[:, -1]
1.51 logistic.py(289):         w = w[:, :-1]
1.51 logistic.py(292):     p = safe_sparse_dot(X, w.T)
1.51 logistic.py(293):     p += intercept
1.51 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
1.51 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
1.51 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
1.51 logistic.py(297):     p = np.exp(p, p)
1.51 logistic.py(298):     return loss, p, w
1.51 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
1.51 logistic.py(346):     diff = sample_weight * (p - Y)
1.51 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
1.51 logistic.py(348):     grad[:, :n_features] += alpha * w
1.51 logistic.py(349):     if fit_intercept:
1.51 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
1.51 logistic.py(351):     return loss, grad.ravel(), p
1.51 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
1.51 logistic.py(339):     n_classes = Y.shape[1]
1.51 logistic.py(340):     n_features = X.shape[1]
1.51 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
1.51 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
1.51 logistic.py(343):                     dtype=X.dtype)
1.51 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
1.51 logistic.py(282):     n_classes = Y.shape[1]
1.51 logistic.py(283):     n_features = X.shape[1]
1.51 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
1.51 logistic.py(285):     w = w.reshape(n_classes, -1)
1.51 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
1.51 logistic.py(287):     if fit_intercept:
1.51 logistic.py(288):         intercept = w[:, -1]
1.51 logistic.py(289):         w = w[:, :-1]
1.51 logistic.py(292):     p = safe_sparse_dot(X, w.T)
1.51 logistic.py(293):     p += intercept
1.51 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
1.51 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
1.51 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
1.51 logistic.py(297):     p = np.exp(p, p)
1.51 logistic.py(298):     return loss, p, w
1.51 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
1.51 logistic.py(346):     diff = sample_weight * (p - Y)
1.51 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
1.51 logistic.py(348):     grad[:, :n_features] += alpha * w
1.51 logistic.py(349):     if fit_intercept:
1.51 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
1.51 logistic.py(351):     return loss, grad.ravel(), p
1.51 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
1.51 logistic.py(339):     n_classes = Y.shape[1]
1.51 logistic.py(340):     n_features = X.shape[1]
1.51 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
1.51 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
1.51 logistic.py(343):                     dtype=X.dtype)
1.51 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
1.51 logistic.py(282):     n_classes = Y.shape[1]
1.51 logistic.py(283):     n_features = X.shape[1]
1.51 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
1.51 logistic.py(285):     w = w.reshape(n_classes, -1)
1.51 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
1.51 logistic.py(287):     if fit_intercept:
1.51 logistic.py(288):         intercept = w[:, -1]
1.51 logistic.py(289):         w = w[:, :-1]
1.51 logistic.py(292):     p = safe_sparse_dot(X, w.T)
1.51 logistic.py(293):     p += intercept
1.51 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
1.51 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
1.51 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
1.51 logistic.py(297):     p = np.exp(p, p)
1.51 logistic.py(298):     return loss, p, w
1.51 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
1.51 logistic.py(346):     diff = sample_weight * (p - Y)
1.51 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
1.51 logistic.py(348):     grad[:, :n_features] += alpha * w
1.51 logistic.py(349):     if fit_intercept:
1.51 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
1.51 logistic.py(351):     return loss, grad.ravel(), p
1.51 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
1.51 logistic.py(339):     n_classes = Y.shape[1]
1.51 logistic.py(340):     n_features = X.shape[1]
1.51 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
1.51 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
1.51 logistic.py(343):                     dtype=X.dtype)
1.51 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
1.51 logistic.py(282):     n_classes = Y.shape[1]
1.51 logistic.py(283):     n_features = X.shape[1]
1.51 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
1.51 logistic.py(285):     w = w.reshape(n_classes, -1)
1.51 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
1.51 logistic.py(287):     if fit_intercept:
1.51 logistic.py(288):         intercept = w[:, -1]
1.51 logistic.py(289):         w = w[:, :-1]
1.51 logistic.py(292):     p = safe_sparse_dot(X, w.T)
1.51 logistic.py(293):     p += intercept
1.51 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
1.51 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
1.51 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
1.51 logistic.py(297):     p = np.exp(p, p)
1.51 logistic.py(298):     return loss, p, w
1.51 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
1.51 logistic.py(346):     diff = sample_weight * (p - Y)
1.51 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
1.51 logistic.py(348):     grad[:, :n_features] += alpha * w
1.51 logistic.py(349):     if fit_intercept:
1.51 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
1.51 logistic.py(351):     return loss, grad.ravel(), p
1.51 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
1.51 logistic.py(339):     n_classes = Y.shape[1]
1.51 logistic.py(340):     n_features = X.shape[1]
1.51 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
1.51 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
1.51 logistic.py(343):                     dtype=X.dtype)
1.51 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
1.51 logistic.py(282):     n_classes = Y.shape[1]
1.51 logistic.py(283):     n_features = X.shape[1]
1.51 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
1.51 logistic.py(285):     w = w.reshape(n_classes, -1)
1.51 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
1.51 logistic.py(287):     if fit_intercept:
1.51 logistic.py(288):         intercept = w[:, -1]
1.51 logistic.py(289):         w = w[:, :-1]
1.51 logistic.py(292):     p = safe_sparse_dot(X, w.T)
1.51 logistic.py(293):     p += intercept
1.51 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
1.51 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
1.51 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
1.51 logistic.py(297):     p = np.exp(p, p)
1.51 logistic.py(298):     return loss, p, w
1.51 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
1.51 logistic.py(346):     diff = sample_weight * (p - Y)
1.51 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
1.51 logistic.py(348):     grad[:, :n_features] += alpha * w
1.51 logistic.py(349):     if fit_intercept:
1.51 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
1.51 logistic.py(351):     return loss, grad.ravel(), p
1.51 logistic.py(718):             if info["warnflag"] == 1:
1.51 logistic.py(723):             n_iter_i = min(info['nit'], max_iter)
1.51 logistic.py(760):         if multi_class == 'multinomial':
1.51 logistic.py(761):             multi_w0 = np.reshape(w0, (classes.size, -1))
1.51 logistic.py(762):             if classes.size == 2:
1.51 logistic.py(764):             coefs.append(multi_w0)
1.51 logistic.py(768):         n_iter[i] = n_iter_i
1.51 logistic.py(712):     for i, C in enumerate(Cs):
1.51 logistic.py(713):         if solver == 'lbfgs':
1.51 logistic.py(714):             w0, loss, info = optimize.fmin_l_bfgs_b(
1.51 logistic.py(715):                 func, w0, fprime=None,
1.51 logistic.py(716):                 args=(X, target, 1. / C, sample_weight),
1.51 logistic.py(717):                 iprint=(verbose > 0) - 1, pgtol=tol, maxiter=max_iter)
1.51 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
1.51 logistic.py(339):     n_classes = Y.shape[1]
1.51 logistic.py(340):     n_features = X.shape[1]
1.51 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
1.51 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
1.51 logistic.py(343):                     dtype=X.dtype)
1.51 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
1.51 logistic.py(282):     n_classes = Y.shape[1]
1.51 logistic.py(283):     n_features = X.shape[1]
1.51 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
1.51 logistic.py(285):     w = w.reshape(n_classes, -1)
1.51 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
1.51 logistic.py(287):     if fit_intercept:
1.51 logistic.py(288):         intercept = w[:, -1]
1.51 logistic.py(289):         w = w[:, :-1]
1.51 logistic.py(292):     p = safe_sparse_dot(X, w.T)
1.51 logistic.py(293):     p += intercept
1.51 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
1.51 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
1.51 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
1.51 logistic.py(297):     p = np.exp(p, p)
1.51 logistic.py(298):     return loss, p, w
1.51 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
1.51 logistic.py(346):     diff = sample_weight * (p - Y)
1.51 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
1.51 logistic.py(348):     grad[:, :n_features] += alpha * w
1.51 logistic.py(349):     if fit_intercept:
1.51 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
1.51 logistic.py(351):     return loss, grad.ravel(), p
1.51 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
1.51 logistic.py(339):     n_classes = Y.shape[1]
1.51 logistic.py(340):     n_features = X.shape[1]
1.51 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
1.51 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
1.51 logistic.py(343):                     dtype=X.dtype)
1.51 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
1.51 logistic.py(282):     n_classes = Y.shape[1]
1.51 logistic.py(283):     n_features = X.shape[1]
1.51 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
1.51 logistic.py(285):     w = w.reshape(n_classes, -1)
1.51 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
1.51 logistic.py(287):     if fit_intercept:
1.51 logistic.py(288):         intercept = w[:, -1]
1.51 logistic.py(289):         w = w[:, :-1]
1.51 logistic.py(292):     p = safe_sparse_dot(X, w.T)
1.51 logistic.py(293):     p += intercept
1.51 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
1.51 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
1.51 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
1.51 logistic.py(297):     p = np.exp(p, p)
1.51 logistic.py(298):     return loss, p, w
1.51 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
1.51 logistic.py(346):     diff = sample_weight * (p - Y)
1.51 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
1.51 logistic.py(348):     grad[:, :n_features] += alpha * w
1.51 logistic.py(349):     if fit_intercept:
1.51 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
1.51 logistic.py(351):     return loss, grad.ravel(), p
1.51 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
1.51 logistic.py(339):     n_classes = Y.shape[1]
1.51 logistic.py(340):     n_features = X.shape[1]
1.51 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
1.51 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
1.51 logistic.py(343):                     dtype=X.dtype)
1.51 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
1.51 logistic.py(282):     n_classes = Y.shape[1]
1.51 logistic.py(283):     n_features = X.shape[1]
1.51 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
1.51 logistic.py(285):     w = w.reshape(n_classes, -1)
1.51 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
1.51 logistic.py(287):     if fit_intercept:
1.51 logistic.py(288):         intercept = w[:, -1]
1.51 logistic.py(289):         w = w[:, :-1]
1.51 logistic.py(292):     p = safe_sparse_dot(X, w.T)
1.51 logistic.py(293):     p += intercept
1.51 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
1.51 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
1.51 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
1.51 logistic.py(297):     p = np.exp(p, p)
1.51 logistic.py(298):     return loss, p, w
1.51 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
1.51 logistic.py(346):     diff = sample_weight * (p - Y)
1.51 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
1.51 logistic.py(348):     grad[:, :n_features] += alpha * w
1.51 logistic.py(349):     if fit_intercept:
1.51 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
1.51 logistic.py(351):     return loss, grad.ravel(), p
1.51 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
1.51 logistic.py(339):     n_classes = Y.shape[1]
1.51 logistic.py(340):     n_features = X.shape[1]
1.51 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
1.51 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
1.51 logistic.py(343):                     dtype=X.dtype)
1.51 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
1.51 logistic.py(282):     n_classes = Y.shape[1]
1.51 logistic.py(283):     n_features = X.shape[1]
1.51 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
1.51 logistic.py(285):     w = w.reshape(n_classes, -1)
1.51 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
1.51 logistic.py(287):     if fit_intercept:
1.51 logistic.py(288):         intercept = w[:, -1]
1.51 logistic.py(289):         w = w[:, :-1]
1.51 logistic.py(292):     p = safe_sparse_dot(X, w.T)
1.51 logistic.py(293):     p += intercept
1.51 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
1.51 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
1.51 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
1.51 logistic.py(297):     p = np.exp(p, p)
1.51 logistic.py(298):     return loss, p, w
1.51 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
1.51 logistic.py(346):     diff = sample_weight * (p - Y)
1.51 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
1.51 logistic.py(348):     grad[:, :n_features] += alpha * w
1.51 logistic.py(349):     if fit_intercept:
1.51 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
1.51 logistic.py(351):     return loss, grad.ravel(), p
1.51 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
1.51 logistic.py(339):     n_classes = Y.shape[1]
1.51 logistic.py(340):     n_features = X.shape[1]
1.51 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
1.51 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
1.51 logistic.py(343):                     dtype=X.dtype)
1.51 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
1.51 logistic.py(282):     n_classes = Y.shape[1]
1.51 logistic.py(283):     n_features = X.shape[1]
1.51 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
1.51 logistic.py(285):     w = w.reshape(n_classes, -1)
1.51 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
1.51 logistic.py(287):     if fit_intercept:
1.51 logistic.py(288):         intercept = w[:, -1]
1.51 logistic.py(289):         w = w[:, :-1]
1.51 logistic.py(292):     p = safe_sparse_dot(X, w.T)
1.51 logistic.py(293):     p += intercept
1.51 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
1.51 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
1.51 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
1.51 logistic.py(297):     p = np.exp(p, p)
1.51 logistic.py(298):     return loss, p, w
1.51 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
1.51 logistic.py(346):     diff = sample_weight * (p - Y)
1.51 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
1.51 logistic.py(348):     grad[:, :n_features] += alpha * w
1.51 logistic.py(349):     if fit_intercept:
1.51 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
1.51 logistic.py(351):     return loss, grad.ravel(), p
1.51 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
1.51 logistic.py(339):     n_classes = Y.shape[1]
1.51 logistic.py(340):     n_features = X.shape[1]
1.51 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
1.51 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
1.51 logistic.py(343):                     dtype=X.dtype)
1.51 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
1.51 logistic.py(282):     n_classes = Y.shape[1]
1.51 logistic.py(283):     n_features = X.shape[1]
1.51 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
1.51 logistic.py(285):     w = w.reshape(n_classes, -1)
1.51 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
1.51 logistic.py(287):     if fit_intercept:
1.51 logistic.py(288):         intercept = w[:, -1]
1.51 logistic.py(289):         w = w[:, :-1]
1.51 logistic.py(292):     p = safe_sparse_dot(X, w.T)
1.51 logistic.py(293):     p += intercept
1.51 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
1.51 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
1.51 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
1.51 logistic.py(297):     p = np.exp(p, p)
1.51 logistic.py(298):     return loss, p, w
1.51 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
1.51 logistic.py(346):     diff = sample_weight * (p - Y)
1.51 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
1.51 logistic.py(348):     grad[:, :n_features] += alpha * w
1.51 logistic.py(349):     if fit_intercept:
1.51 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
1.51 logistic.py(351):     return loss, grad.ravel(), p
1.51 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
1.51 logistic.py(339):     n_classes = Y.shape[1]
1.51 logistic.py(340):     n_features = X.shape[1]
1.51 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
1.51 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
1.51 logistic.py(343):                     dtype=X.dtype)
1.51 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
1.51 logistic.py(282):     n_classes = Y.shape[1]
1.51 logistic.py(283):     n_features = X.shape[1]
1.51 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
1.51 logistic.py(285):     w = w.reshape(n_classes, -1)
1.51 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
1.51 logistic.py(287):     if fit_intercept:
1.51 logistic.py(288):         intercept = w[:, -1]
1.51 logistic.py(289):         w = w[:, :-1]
1.51 logistic.py(292):     p = safe_sparse_dot(X, w.T)
1.51 logistic.py(293):     p += intercept
1.51 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
1.51 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
1.51 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
1.51 logistic.py(297):     p = np.exp(p, p)
1.51 logistic.py(298):     return loss, p, w
1.51 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
1.51 logistic.py(346):     diff = sample_weight * (p - Y)
1.51 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
1.51 logistic.py(348):     grad[:, :n_features] += alpha * w
1.51 logistic.py(349):     if fit_intercept:
1.51 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
1.51 logistic.py(351):     return loss, grad.ravel(), p
1.51 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
1.51 logistic.py(339):     n_classes = Y.shape[1]
1.51 logistic.py(340):     n_features = X.shape[1]
1.51 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
1.51 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
1.51 logistic.py(343):                     dtype=X.dtype)
1.51 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
1.51 logistic.py(282):     n_classes = Y.shape[1]
1.51 logistic.py(283):     n_features = X.shape[1]
1.51 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
1.51 logistic.py(285):     w = w.reshape(n_classes, -1)
1.51 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
1.51 logistic.py(287):     if fit_intercept:
1.51 logistic.py(288):         intercept = w[:, -1]
1.51 logistic.py(289):         w = w[:, :-1]
1.52 logistic.py(292):     p = safe_sparse_dot(X, w.T)
1.52 logistic.py(293):     p += intercept
1.52 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
1.52 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
1.52 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
1.52 logistic.py(297):     p = np.exp(p, p)
1.52 logistic.py(298):     return loss, p, w
1.52 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
1.52 logistic.py(346):     diff = sample_weight * (p - Y)
1.52 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
1.52 logistic.py(348):     grad[:, :n_features] += alpha * w
1.52 logistic.py(349):     if fit_intercept:
1.52 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
1.52 logistic.py(351):     return loss, grad.ravel(), p
1.52 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
1.52 logistic.py(339):     n_classes = Y.shape[1]
1.52 logistic.py(340):     n_features = X.shape[1]
1.52 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
1.52 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
1.52 logistic.py(343):                     dtype=X.dtype)
1.52 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
1.52 logistic.py(282):     n_classes = Y.shape[1]
1.52 logistic.py(283):     n_features = X.shape[1]
1.52 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
1.52 logistic.py(285):     w = w.reshape(n_classes, -1)
1.52 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
1.52 logistic.py(287):     if fit_intercept:
1.52 logistic.py(288):         intercept = w[:, -1]
1.52 logistic.py(289):         w = w[:, :-1]
1.52 logistic.py(292):     p = safe_sparse_dot(X, w.T)
1.52 logistic.py(293):     p += intercept
1.52 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
1.52 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
1.52 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
1.52 logistic.py(297):     p = np.exp(p, p)
1.52 logistic.py(298):     return loss, p, w
1.52 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
1.52 logistic.py(346):     diff = sample_weight * (p - Y)
1.52 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
1.52 logistic.py(348):     grad[:, :n_features] += alpha * w
1.52 logistic.py(349):     if fit_intercept:
1.52 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
1.52 logistic.py(351):     return loss, grad.ravel(), p
1.52 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
1.52 logistic.py(339):     n_classes = Y.shape[1]
1.52 logistic.py(340):     n_features = X.shape[1]
1.52 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
1.52 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
1.52 logistic.py(343):                     dtype=X.dtype)
1.52 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
1.52 logistic.py(282):     n_classes = Y.shape[1]
1.52 logistic.py(283):     n_features = X.shape[1]
1.52 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
1.52 logistic.py(285):     w = w.reshape(n_classes, -1)
1.52 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
1.52 logistic.py(287):     if fit_intercept:
1.52 logistic.py(288):         intercept = w[:, -1]
1.52 logistic.py(289):         w = w[:, :-1]
1.52 logistic.py(292):     p = safe_sparse_dot(X, w.T)
1.52 logistic.py(293):     p += intercept
1.52 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
1.52 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
1.52 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
1.52 logistic.py(297):     p = np.exp(p, p)
1.52 logistic.py(298):     return loss, p, w
1.52 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
1.52 logistic.py(346):     diff = sample_weight * (p - Y)
1.52 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
1.52 logistic.py(348):     grad[:, :n_features] += alpha * w
1.52 logistic.py(349):     if fit_intercept:
1.52 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
1.52 logistic.py(351):     return loss, grad.ravel(), p
1.52 logistic.py(718):             if info["warnflag"] == 1:
1.52 logistic.py(723):             n_iter_i = min(info['nit'], max_iter)
1.52 logistic.py(760):         if multi_class == 'multinomial':
1.52 logistic.py(761):             multi_w0 = np.reshape(w0, (classes.size, -1))
1.52 logistic.py(762):             if classes.size == 2:
1.52 logistic.py(764):             coefs.append(multi_w0)
1.52 logistic.py(768):         n_iter[i] = n_iter_i
1.52 logistic.py(712):     for i, C in enumerate(Cs):
1.52 logistic.py(713):         if solver == 'lbfgs':
1.52 logistic.py(714):             w0, loss, info = optimize.fmin_l_bfgs_b(
1.52 logistic.py(715):                 func, w0, fprime=None,
1.52 logistic.py(716):                 args=(X, target, 1. / C, sample_weight),
1.52 logistic.py(717):                 iprint=(verbose > 0) - 1, pgtol=tol, maxiter=max_iter)
1.52 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
1.52 logistic.py(339):     n_classes = Y.shape[1]
1.52 logistic.py(340):     n_features = X.shape[1]
1.52 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
1.52 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
1.52 logistic.py(343):                     dtype=X.dtype)
1.52 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
1.52 logistic.py(282):     n_classes = Y.shape[1]
1.52 logistic.py(283):     n_features = X.shape[1]
1.52 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
1.52 logistic.py(285):     w = w.reshape(n_classes, -1)
1.52 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
1.52 logistic.py(287):     if fit_intercept:
1.52 logistic.py(288):         intercept = w[:, -1]
1.52 logistic.py(289):         w = w[:, :-1]
1.52 logistic.py(292):     p = safe_sparse_dot(X, w.T)
1.52 logistic.py(293):     p += intercept
1.52 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
1.52 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
1.52 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
1.52 logistic.py(297):     p = np.exp(p, p)
1.52 logistic.py(298):     return loss, p, w
1.52 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
1.52 logistic.py(346):     diff = sample_weight * (p - Y)
1.52 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
1.52 logistic.py(348):     grad[:, :n_features] += alpha * w
1.52 logistic.py(349):     if fit_intercept:
1.52 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
1.52 logistic.py(351):     return loss, grad.ravel(), p
1.52 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
1.52 logistic.py(339):     n_classes = Y.shape[1]
1.52 logistic.py(340):     n_features = X.shape[1]
1.52 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
1.52 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
1.52 logistic.py(343):                     dtype=X.dtype)
1.52 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
1.52 logistic.py(282):     n_classes = Y.shape[1]
1.52 logistic.py(283):     n_features = X.shape[1]
1.52 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
1.52 logistic.py(285):     w = w.reshape(n_classes, -1)
1.52 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
1.52 logistic.py(287):     if fit_intercept:
1.52 logistic.py(288):         intercept = w[:, -1]
1.52 logistic.py(289):         w = w[:, :-1]
1.52 logistic.py(292):     p = safe_sparse_dot(X, w.T)
1.52 logistic.py(293):     p += intercept
1.52 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
1.52 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
1.52 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
1.52 logistic.py(297):     p = np.exp(p, p)
1.52 logistic.py(298):     return loss, p, w
1.52 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
1.52 logistic.py(346):     diff = sample_weight * (p - Y)
1.52 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
1.52 logistic.py(348):     grad[:, :n_features] += alpha * w
1.52 logistic.py(349):     if fit_intercept:
1.52 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
1.52 logistic.py(351):     return loss, grad.ravel(), p
1.52 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
1.52 logistic.py(339):     n_classes = Y.shape[1]
1.52 logistic.py(340):     n_features = X.shape[1]
1.52 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
1.52 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
1.52 logistic.py(343):                     dtype=X.dtype)
1.52 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
1.52 logistic.py(282):     n_classes = Y.shape[1]
1.52 logistic.py(283):     n_features = X.shape[1]
1.52 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
1.52 logistic.py(285):     w = w.reshape(n_classes, -1)
1.52 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
1.52 logistic.py(287):     if fit_intercept:
1.52 logistic.py(288):         intercept = w[:, -1]
1.52 logistic.py(289):         w = w[:, :-1]
1.52 logistic.py(292):     p = safe_sparse_dot(X, w.T)
1.52 logistic.py(293):     p += intercept
1.52 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
1.52 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
1.52 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
1.52 logistic.py(297):     p = np.exp(p, p)
1.52 logistic.py(298):     return loss, p, w
1.52 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
1.52 logistic.py(346):     diff = sample_weight * (p - Y)
1.52 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
1.52 logistic.py(348):     grad[:, :n_features] += alpha * w
1.52 logistic.py(349):     if fit_intercept:
1.52 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
1.52 logistic.py(351):     return loss, grad.ravel(), p
1.52 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
1.52 logistic.py(339):     n_classes = Y.shape[1]
1.52 logistic.py(340):     n_features = X.shape[1]
1.52 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
1.52 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
1.52 logistic.py(343):                     dtype=X.dtype)
1.52 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
1.52 logistic.py(282):     n_classes = Y.shape[1]
1.52 logistic.py(283):     n_features = X.shape[1]
1.52 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
1.52 logistic.py(285):     w = w.reshape(n_classes, -1)
1.52 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
1.52 logistic.py(287):     if fit_intercept:
1.52 logistic.py(288):         intercept = w[:, -1]
1.52 logistic.py(289):         w = w[:, :-1]
1.52 logistic.py(292):     p = safe_sparse_dot(X, w.T)
1.52 logistic.py(293):     p += intercept
1.52 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
1.52 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
1.52 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
1.52 logistic.py(297):     p = np.exp(p, p)
1.52 logistic.py(298):     return loss, p, w
1.52 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
1.52 logistic.py(346):     diff = sample_weight * (p - Y)
1.52 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
1.52 logistic.py(348):     grad[:, :n_features] += alpha * w
1.52 logistic.py(349):     if fit_intercept:
1.52 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
1.52 logistic.py(351):     return loss, grad.ravel(), p
1.52 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
1.52 logistic.py(339):     n_classes = Y.shape[1]
1.52 logistic.py(340):     n_features = X.shape[1]
1.52 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
1.52 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
1.52 logistic.py(343):                     dtype=X.dtype)
1.52 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
1.52 logistic.py(282):     n_classes = Y.shape[1]
1.52 logistic.py(283):     n_features = X.shape[1]
1.52 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
1.52 logistic.py(285):     w = w.reshape(n_classes, -1)
1.52 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
1.52 logistic.py(287):     if fit_intercept:
1.52 logistic.py(288):         intercept = w[:, -1]
1.52 logistic.py(289):         w = w[:, :-1]
1.52 logistic.py(292):     p = safe_sparse_dot(X, w.T)
1.52 logistic.py(293):     p += intercept
1.52 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
1.52 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
1.52 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
1.52 logistic.py(297):     p = np.exp(p, p)
1.52 logistic.py(298):     return loss, p, w
1.52 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
1.52 logistic.py(346):     diff = sample_weight * (p - Y)
1.52 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
1.52 logistic.py(348):     grad[:, :n_features] += alpha * w
1.52 logistic.py(349):     if fit_intercept:
1.52 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
1.52 logistic.py(351):     return loss, grad.ravel(), p
1.52 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
1.52 logistic.py(339):     n_classes = Y.shape[1]
1.52 logistic.py(340):     n_features = X.shape[1]
1.52 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
1.52 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
1.52 logistic.py(343):                     dtype=X.dtype)
1.52 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
1.52 logistic.py(282):     n_classes = Y.shape[1]
1.52 logistic.py(283):     n_features = X.shape[1]
1.52 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
1.52 logistic.py(285):     w = w.reshape(n_classes, -1)
1.52 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
1.52 logistic.py(287):     if fit_intercept:
1.52 logistic.py(288):         intercept = w[:, -1]
1.52 logistic.py(289):         w = w[:, :-1]
1.52 logistic.py(292):     p = safe_sparse_dot(X, w.T)
1.52 logistic.py(293):     p += intercept
1.52 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
1.52 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
1.52 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
1.52 logistic.py(297):     p = np.exp(p, p)
1.52 logistic.py(298):     return loss, p, w
1.52 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
1.52 logistic.py(346):     diff = sample_weight * (p - Y)
1.52 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
1.52 logistic.py(348):     grad[:, :n_features] += alpha * w
1.52 logistic.py(349):     if fit_intercept:
1.52 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
1.52 logistic.py(351):     return loss, grad.ravel(), p
1.52 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
1.52 logistic.py(339):     n_classes = Y.shape[1]
1.52 logistic.py(340):     n_features = X.shape[1]
1.52 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
1.52 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
1.52 logistic.py(343):                     dtype=X.dtype)
1.52 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
1.52 logistic.py(282):     n_classes = Y.shape[1]
1.52 logistic.py(283):     n_features = X.shape[1]
1.52 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
1.52 logistic.py(285):     w = w.reshape(n_classes, -1)
1.52 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
1.52 logistic.py(287):     if fit_intercept:
1.52 logistic.py(288):         intercept = w[:, -1]
1.52 logistic.py(289):         w = w[:, :-1]
1.52 logistic.py(292):     p = safe_sparse_dot(X, w.T)
1.52 logistic.py(293):     p += intercept
1.52 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
1.52 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
1.52 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
1.52 logistic.py(297):     p = np.exp(p, p)
1.52 logistic.py(298):     return loss, p, w
1.52 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
1.52 logistic.py(346):     diff = sample_weight * (p - Y)
1.52 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
1.52 logistic.py(348):     grad[:, :n_features] += alpha * w
1.52 logistic.py(349):     if fit_intercept:
1.52 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
1.52 logistic.py(351):     return loss, grad.ravel(), p
1.52 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
1.52 logistic.py(339):     n_classes = Y.shape[1]
1.52 logistic.py(340):     n_features = X.shape[1]
1.52 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
1.52 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
1.52 logistic.py(343):                     dtype=X.dtype)
1.52 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
1.52 logistic.py(282):     n_classes = Y.shape[1]
1.52 logistic.py(283):     n_features = X.shape[1]
1.52 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
1.52 logistic.py(285):     w = w.reshape(n_classes, -1)
1.52 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
1.52 logistic.py(287):     if fit_intercept:
1.52 logistic.py(288):         intercept = w[:, -1]
1.52 logistic.py(289):         w = w[:, :-1]
1.52 logistic.py(292):     p = safe_sparse_dot(X, w.T)
1.52 logistic.py(293):     p += intercept
1.52 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
1.52 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
1.52 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
1.52 logistic.py(297):     p = np.exp(p, p)
1.52 logistic.py(298):     return loss, p, w
1.52 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
1.52 logistic.py(346):     diff = sample_weight * (p - Y)
1.52 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
1.52 logistic.py(348):     grad[:, :n_features] += alpha * w
1.52 logistic.py(349):     if fit_intercept:
1.52 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
1.52 logistic.py(351):     return loss, grad.ravel(), p
1.52 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
1.52 logistic.py(339):     n_classes = Y.shape[1]
1.52 logistic.py(340):     n_features = X.shape[1]
1.52 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
1.52 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
1.52 logistic.py(343):                     dtype=X.dtype)
1.52 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
1.52 logistic.py(282):     n_classes = Y.shape[1]
1.52 logistic.py(283):     n_features = X.shape[1]
1.52 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
1.52 logistic.py(285):     w = w.reshape(n_classes, -1)
1.52 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
1.52 logistic.py(287):     if fit_intercept:
1.52 logistic.py(288):         intercept = w[:, -1]
1.52 logistic.py(289):         w = w[:, :-1]
1.52 logistic.py(292):     p = safe_sparse_dot(X, w.T)
1.52 logistic.py(293):     p += intercept
1.52 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
1.52 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
1.52 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
1.52 logistic.py(297):     p = np.exp(p, p)
1.52 logistic.py(298):     return loss, p, w
1.52 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
1.52 logistic.py(346):     diff = sample_weight * (p - Y)
1.52 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
1.52 logistic.py(348):     grad[:, :n_features] += alpha * w
1.52 logistic.py(349):     if fit_intercept:
1.52 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
1.52 logistic.py(351):     return loss, grad.ravel(), p
1.52 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
1.52 logistic.py(339):     n_classes = Y.shape[1]
1.52 logistic.py(340):     n_features = X.shape[1]
1.52 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
1.52 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
1.52 logistic.py(343):                     dtype=X.dtype)
1.52 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
1.52 logistic.py(282):     n_classes = Y.shape[1]
1.52 logistic.py(283):     n_features = X.shape[1]
1.52 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
1.52 logistic.py(285):     w = w.reshape(n_classes, -1)
1.52 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
1.52 logistic.py(287):     if fit_intercept:
1.52 logistic.py(288):         intercept = w[:, -1]
1.52 logistic.py(289):         w = w[:, :-1]
1.52 logistic.py(292):     p = safe_sparse_dot(X, w.T)
1.52 logistic.py(293):     p += intercept
1.52 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
1.52 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
1.52 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
1.52 logistic.py(297):     p = np.exp(p, p)
1.52 logistic.py(298):     return loss, p, w
1.52 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
1.52 logistic.py(346):     diff = sample_weight * (p - Y)
1.52 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
1.52 logistic.py(348):     grad[:, :n_features] += alpha * w
1.52 logistic.py(349):     if fit_intercept:
1.52 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
1.52 logistic.py(351):     return loss, grad.ravel(), p
1.52 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
1.52 logistic.py(339):     n_classes = Y.shape[1]
1.52 logistic.py(340):     n_features = X.shape[1]
1.52 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
1.52 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
1.52 logistic.py(343):                     dtype=X.dtype)
1.52 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
1.52 logistic.py(282):     n_classes = Y.shape[1]
1.52 logistic.py(283):     n_features = X.shape[1]
1.52 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
1.52 logistic.py(285):     w = w.reshape(n_classes, -1)
1.52 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
1.52 logistic.py(287):     if fit_intercept:
1.52 logistic.py(288):         intercept = w[:, -1]
1.52 logistic.py(289):         w = w[:, :-1]
1.52 logistic.py(292):     p = safe_sparse_dot(X, w.T)
1.52 logistic.py(293):     p += intercept
1.52 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
1.52 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
1.52 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
1.52 logistic.py(297):     p = np.exp(p, p)
1.52 logistic.py(298):     return loss, p, w
1.52 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
1.52 logistic.py(346):     diff = sample_weight * (p - Y)
1.52 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
1.52 logistic.py(348):     grad[:, :n_features] += alpha * w
1.52 logistic.py(349):     if fit_intercept:
1.52 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
1.52 logistic.py(351):     return loss, grad.ravel(), p
1.52 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
1.52 logistic.py(339):     n_classes = Y.shape[1]
1.52 logistic.py(340):     n_features = X.shape[1]
1.52 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
1.52 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
1.52 logistic.py(343):                     dtype=X.dtype)
1.52 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
1.52 logistic.py(282):     n_classes = Y.shape[1]
1.52 logistic.py(283):     n_features = X.shape[1]
1.52 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
1.52 logistic.py(285):     w = w.reshape(n_classes, -1)
1.52 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
1.52 logistic.py(287):     if fit_intercept:
1.52 logistic.py(288):         intercept = w[:, -1]
1.52 logistic.py(289):         w = w[:, :-1]
1.52 logistic.py(292):     p = safe_sparse_dot(X, w.T)
1.52 logistic.py(293):     p += intercept
1.52 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
1.52 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
1.52 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
1.52 logistic.py(297):     p = np.exp(p, p)
1.52 logistic.py(298):     return loss, p, w
1.52 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
1.52 logistic.py(346):     diff = sample_weight * (p - Y)
1.52 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
1.52 logistic.py(348):     grad[:, :n_features] += alpha * w
1.52 logistic.py(349):     if fit_intercept:
1.52 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
1.52 logistic.py(351):     return loss, grad.ravel(), p
1.52 logistic.py(718):             if info["warnflag"] == 1:
1.52 logistic.py(723):             n_iter_i = min(info['nit'], max_iter)
1.52 logistic.py(760):         if multi_class == 'multinomial':
1.52 logistic.py(761):             multi_w0 = np.reshape(w0, (classes.size, -1))
1.52 logistic.py(762):             if classes.size == 2:
1.52 logistic.py(764):             coefs.append(multi_w0)
1.52 logistic.py(768):         n_iter[i] = n_iter_i
1.52 logistic.py(712):     for i, C in enumerate(Cs):
1.52 logistic.py(713):         if solver == 'lbfgs':
1.52 logistic.py(714):             w0, loss, info = optimize.fmin_l_bfgs_b(
1.52 logistic.py(715):                 func, w0, fprime=None,
1.52 logistic.py(716):                 args=(X, target, 1. / C, sample_weight),
1.52 logistic.py(717):                 iprint=(verbose > 0) - 1, pgtol=tol, maxiter=max_iter)
1.52 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
1.52 logistic.py(339):     n_classes = Y.shape[1]
1.52 logistic.py(340):     n_features = X.shape[1]
1.52 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
1.52 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
1.52 logistic.py(343):                     dtype=X.dtype)
1.52 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
1.52 logistic.py(282):     n_classes = Y.shape[1]
1.52 logistic.py(283):     n_features = X.shape[1]
1.52 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
1.52 logistic.py(285):     w = w.reshape(n_classes, -1)
1.52 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
1.52 logistic.py(287):     if fit_intercept:
1.52 logistic.py(288):         intercept = w[:, -1]
1.52 logistic.py(289):         w = w[:, :-1]
1.52 logistic.py(292):     p = safe_sparse_dot(X, w.T)
1.52 logistic.py(293):     p += intercept
1.52 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
1.52 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
1.52 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
1.52 logistic.py(297):     p = np.exp(p, p)
1.52 logistic.py(298):     return loss, p, w
1.52 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
1.52 logistic.py(346):     diff = sample_weight * (p - Y)
1.53 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
1.53 logistic.py(348):     grad[:, :n_features] += alpha * w
1.53 logistic.py(349):     if fit_intercept:
1.53 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
1.53 logistic.py(351):     return loss, grad.ravel(), p
1.53 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
1.53 logistic.py(339):     n_classes = Y.shape[1]
1.53 logistic.py(340):     n_features = X.shape[1]
1.53 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
1.53 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
1.53 logistic.py(343):                     dtype=X.dtype)
1.53 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
1.53 logistic.py(282):     n_classes = Y.shape[1]
1.53 logistic.py(283):     n_features = X.shape[1]
1.53 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
1.53 logistic.py(285):     w = w.reshape(n_classes, -1)
1.53 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
1.53 logistic.py(287):     if fit_intercept:
1.53 logistic.py(288):         intercept = w[:, -1]
1.53 logistic.py(289):         w = w[:, :-1]
1.53 logistic.py(292):     p = safe_sparse_dot(X, w.T)
1.53 logistic.py(293):     p += intercept
1.53 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
1.53 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
1.53 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
1.53 logistic.py(297):     p = np.exp(p, p)
1.53 logistic.py(298):     return loss, p, w
1.53 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
1.53 logistic.py(346):     diff = sample_weight * (p - Y)
1.53 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
1.53 logistic.py(348):     grad[:, :n_features] += alpha * w
1.53 logistic.py(349):     if fit_intercept:
1.53 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
1.53 logistic.py(351):     return loss, grad.ravel(), p
1.53 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
1.53 logistic.py(339):     n_classes = Y.shape[1]
1.53 logistic.py(340):     n_features = X.shape[1]
1.53 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
1.53 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
1.53 logistic.py(343):                     dtype=X.dtype)
1.53 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
1.53 logistic.py(282):     n_classes = Y.shape[1]
1.53 logistic.py(283):     n_features = X.shape[1]
1.53 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
1.53 logistic.py(285):     w = w.reshape(n_classes, -1)
1.53 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
1.53 logistic.py(287):     if fit_intercept:
1.53 logistic.py(288):         intercept = w[:, -1]
1.53 logistic.py(289):         w = w[:, :-1]
1.53 logistic.py(292):     p = safe_sparse_dot(X, w.T)
1.53 logistic.py(293):     p += intercept
1.53 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
1.53 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
1.53 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
1.53 logistic.py(297):     p = np.exp(p, p)
1.53 logistic.py(298):     return loss, p, w
1.53 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
1.53 logistic.py(346):     diff = sample_weight * (p - Y)
1.53 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
1.53 logistic.py(348):     grad[:, :n_features] += alpha * w
1.53 logistic.py(349):     if fit_intercept:
1.53 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
1.53 logistic.py(351):     return loss, grad.ravel(), p
1.53 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
1.53 logistic.py(339):     n_classes = Y.shape[1]
1.53 logistic.py(340):     n_features = X.shape[1]
1.53 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
1.53 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
1.53 logistic.py(343):                     dtype=X.dtype)
1.53 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
1.53 logistic.py(282):     n_classes = Y.shape[1]
1.53 logistic.py(283):     n_features = X.shape[1]
1.53 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
1.53 logistic.py(285):     w = w.reshape(n_classes, -1)
1.53 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
1.53 logistic.py(287):     if fit_intercept:
1.53 logistic.py(288):         intercept = w[:, -1]
1.53 logistic.py(289):         w = w[:, :-1]
1.53 logistic.py(292):     p = safe_sparse_dot(X, w.T)
1.53 logistic.py(293):     p += intercept
1.53 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
1.53 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
1.53 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
1.53 logistic.py(297):     p = np.exp(p, p)
1.53 logistic.py(298):     return loss, p, w
1.53 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
1.53 logistic.py(346):     diff = sample_weight * (p - Y)
1.53 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
1.53 logistic.py(348):     grad[:, :n_features] += alpha * w
1.53 logistic.py(349):     if fit_intercept:
1.53 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
1.53 logistic.py(351):     return loss, grad.ravel(), p
1.53 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
1.53 logistic.py(339):     n_classes = Y.shape[1]
1.53 logistic.py(340):     n_features = X.shape[1]
1.53 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
1.53 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
1.53 logistic.py(343):                     dtype=X.dtype)
1.53 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
1.53 logistic.py(282):     n_classes = Y.shape[1]
1.53 logistic.py(283):     n_features = X.shape[1]
1.53 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
1.53 logistic.py(285):     w = w.reshape(n_classes, -1)
1.53 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
1.53 logistic.py(287):     if fit_intercept:
1.53 logistic.py(288):         intercept = w[:, -1]
1.53 logistic.py(289):         w = w[:, :-1]
1.53 logistic.py(292):     p = safe_sparse_dot(X, w.T)
1.53 logistic.py(293):     p += intercept
1.53 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
1.53 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
1.53 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
1.53 logistic.py(297):     p = np.exp(p, p)
1.53 logistic.py(298):     return loss, p, w
1.53 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
1.53 logistic.py(346):     diff = sample_weight * (p - Y)
1.53 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
1.53 logistic.py(348):     grad[:, :n_features] += alpha * w
1.53 logistic.py(349):     if fit_intercept:
1.53 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
1.53 logistic.py(351):     return loss, grad.ravel(), p
1.53 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
1.53 logistic.py(339):     n_classes = Y.shape[1]
1.53 logistic.py(340):     n_features = X.shape[1]
1.53 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
1.53 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
1.53 logistic.py(343):                     dtype=X.dtype)
1.53 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
1.53 logistic.py(282):     n_classes = Y.shape[1]
1.53 logistic.py(283):     n_features = X.shape[1]
1.53 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
1.53 logistic.py(285):     w = w.reshape(n_classes, -1)
1.53 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
1.53 logistic.py(287):     if fit_intercept:
1.53 logistic.py(288):         intercept = w[:, -1]
1.53 logistic.py(289):         w = w[:, :-1]
1.53 logistic.py(292):     p = safe_sparse_dot(X, w.T)
1.53 logistic.py(293):     p += intercept
1.53 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
1.53 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
1.53 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
1.53 logistic.py(297):     p = np.exp(p, p)
1.53 logistic.py(298):     return loss, p, w
1.53 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
1.53 logistic.py(346):     diff = sample_weight * (p - Y)
1.53 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
1.53 logistic.py(348):     grad[:, :n_features] += alpha * w
1.53 logistic.py(349):     if fit_intercept:
1.53 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
1.53 logistic.py(351):     return loss, grad.ravel(), p
1.53 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
1.53 logistic.py(339):     n_classes = Y.shape[1]
1.53 logistic.py(340):     n_features = X.shape[1]
1.53 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
1.53 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
1.53 logistic.py(343):                     dtype=X.dtype)
1.53 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
1.53 logistic.py(282):     n_classes = Y.shape[1]
1.53 logistic.py(283):     n_features = X.shape[1]
1.53 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
1.53 logistic.py(285):     w = w.reshape(n_classes, -1)
1.53 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
1.53 logistic.py(287):     if fit_intercept:
1.53 logistic.py(288):         intercept = w[:, -1]
1.53 logistic.py(289):         w = w[:, :-1]
1.53 logistic.py(292):     p = safe_sparse_dot(X, w.T)
1.53 logistic.py(293):     p += intercept
1.53 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
1.53 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
1.53 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
1.53 logistic.py(297):     p = np.exp(p, p)
1.53 logistic.py(298):     return loss, p, w
1.53 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
1.53 logistic.py(346):     diff = sample_weight * (p - Y)
1.53 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
1.53 logistic.py(348):     grad[:, :n_features] += alpha * w
1.53 logistic.py(349):     if fit_intercept:
1.53 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
1.53 logistic.py(351):     return loss, grad.ravel(), p
1.53 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
1.53 logistic.py(339):     n_classes = Y.shape[1]
1.53 logistic.py(340):     n_features = X.shape[1]
1.53 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
1.53 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
1.53 logistic.py(343):                     dtype=X.dtype)
1.53 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
1.53 logistic.py(282):     n_classes = Y.shape[1]
1.53 logistic.py(283):     n_features = X.shape[1]
1.53 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
1.53 logistic.py(285):     w = w.reshape(n_classes, -1)
1.53 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
1.53 logistic.py(287):     if fit_intercept:
1.53 logistic.py(288):         intercept = w[:, -1]
1.53 logistic.py(289):         w = w[:, :-1]
1.53 logistic.py(292):     p = safe_sparse_dot(X, w.T)
1.53 logistic.py(293):     p += intercept
1.53 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
1.53 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
1.53 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
1.53 logistic.py(297):     p = np.exp(p, p)
1.53 logistic.py(298):     return loss, p, w
1.53 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
1.53 logistic.py(346):     diff = sample_weight * (p - Y)
1.53 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
1.53 logistic.py(348):     grad[:, :n_features] += alpha * w
1.53 logistic.py(349):     if fit_intercept:
1.53 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
1.53 logistic.py(351):     return loss, grad.ravel(), p
1.53 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
1.53 logistic.py(339):     n_classes = Y.shape[1]
1.53 logistic.py(340):     n_features = X.shape[1]
1.53 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
1.53 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
1.53 logistic.py(343):                     dtype=X.dtype)
1.53 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
1.53 logistic.py(282):     n_classes = Y.shape[1]
1.53 logistic.py(283):     n_features = X.shape[1]
1.53 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
1.53 logistic.py(285):     w = w.reshape(n_classes, -1)
1.53 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
1.53 logistic.py(287):     if fit_intercept:
1.53 logistic.py(288):         intercept = w[:, -1]
1.53 logistic.py(289):         w = w[:, :-1]
1.53 logistic.py(292):     p = safe_sparse_dot(X, w.T)
1.53 logistic.py(293):     p += intercept
1.53 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
1.53 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
1.53 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
1.53 logistic.py(297):     p = np.exp(p, p)
1.53 logistic.py(298):     return loss, p, w
1.53 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
1.53 logistic.py(346):     diff = sample_weight * (p - Y)
1.53 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
1.53 logistic.py(348):     grad[:, :n_features] += alpha * w
1.53 logistic.py(349):     if fit_intercept:
1.53 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
1.53 logistic.py(351):     return loss, grad.ravel(), p
1.53 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
1.53 logistic.py(339):     n_classes = Y.shape[1]
1.53 logistic.py(340):     n_features = X.shape[1]
1.53 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
1.53 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
1.53 logistic.py(343):                     dtype=X.dtype)
1.53 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
1.53 logistic.py(282):     n_classes = Y.shape[1]
1.53 logistic.py(283):     n_features = X.shape[1]
1.53 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
1.53 logistic.py(285):     w = w.reshape(n_classes, -1)
1.53 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
1.53 logistic.py(287):     if fit_intercept:
1.53 logistic.py(288):         intercept = w[:, -1]
1.53 logistic.py(289):         w = w[:, :-1]
1.53 logistic.py(292):     p = safe_sparse_dot(X, w.T)
1.53 logistic.py(293):     p += intercept
1.53 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
1.53 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
1.53 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
1.53 logistic.py(297):     p = np.exp(p, p)
1.53 logistic.py(298):     return loss, p, w
1.53 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
1.53 logistic.py(346):     diff = sample_weight * (p - Y)
1.53 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
1.53 logistic.py(348):     grad[:, :n_features] += alpha * w
1.53 logistic.py(349):     if fit_intercept:
1.53 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
1.53 logistic.py(351):     return loss, grad.ravel(), p
1.53 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
1.53 logistic.py(339):     n_classes = Y.shape[1]
1.53 logistic.py(340):     n_features = X.shape[1]
1.53 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
1.53 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
1.53 logistic.py(343):                     dtype=X.dtype)
1.53 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
1.53 logistic.py(282):     n_classes = Y.shape[1]
1.53 logistic.py(283):     n_features = X.shape[1]
1.53 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
1.53 logistic.py(285):     w = w.reshape(n_classes, -1)
1.53 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
1.53 logistic.py(287):     if fit_intercept:
1.53 logistic.py(288):         intercept = w[:, -1]
1.53 logistic.py(289):         w = w[:, :-1]
1.53 logistic.py(292):     p = safe_sparse_dot(X, w.T)
1.53 logistic.py(293):     p += intercept
1.53 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
1.53 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
1.53 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
1.53 logistic.py(297):     p = np.exp(p, p)
1.53 logistic.py(298):     return loss, p, w
1.53 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
1.53 logistic.py(346):     diff = sample_weight * (p - Y)
1.53 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
1.53 logistic.py(348):     grad[:, :n_features] += alpha * w
1.53 logistic.py(349):     if fit_intercept:
1.53 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
1.53 logistic.py(351):     return loss, grad.ravel(), p
1.53 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
1.53 logistic.py(339):     n_classes = Y.shape[1]
1.53 logistic.py(340):     n_features = X.shape[1]
1.53 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
1.53 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
1.53 logistic.py(343):                     dtype=X.dtype)
1.53 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
1.53 logistic.py(282):     n_classes = Y.shape[1]
1.53 logistic.py(283):     n_features = X.shape[1]
1.53 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
1.53 logistic.py(285):     w = w.reshape(n_classes, -1)
1.53 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
1.53 logistic.py(287):     if fit_intercept:
1.53 logistic.py(288):         intercept = w[:, -1]
1.53 logistic.py(289):         w = w[:, :-1]
1.53 logistic.py(292):     p = safe_sparse_dot(X, w.T)
1.53 logistic.py(293):     p += intercept
1.53 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
1.53 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
1.53 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
1.53 logistic.py(297):     p = np.exp(p, p)
1.53 logistic.py(298):     return loss, p, w
1.53 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
1.53 logistic.py(346):     diff = sample_weight * (p - Y)
1.53 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
1.53 logistic.py(348):     grad[:, :n_features] += alpha * w
1.53 logistic.py(349):     if fit_intercept:
1.53 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
1.53 logistic.py(351):     return loss, grad.ravel(), p
1.53 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
1.53 logistic.py(339):     n_classes = Y.shape[1]
1.53 logistic.py(340):     n_features = X.shape[1]
1.53 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
1.53 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
1.53 logistic.py(343):                     dtype=X.dtype)
1.53 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
1.53 logistic.py(282):     n_classes = Y.shape[1]
1.53 logistic.py(283):     n_features = X.shape[1]
1.53 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
1.53 logistic.py(285):     w = w.reshape(n_classes, -1)
1.53 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
1.53 logistic.py(287):     if fit_intercept:
1.53 logistic.py(288):         intercept = w[:, -1]
1.53 logistic.py(289):         w = w[:, :-1]
1.53 logistic.py(292):     p = safe_sparse_dot(X, w.T)
1.53 logistic.py(293):     p += intercept
1.53 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
1.53 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
1.53 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
1.53 logistic.py(297):     p = np.exp(p, p)
1.53 logistic.py(298):     return loss, p, w
1.53 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
1.53 logistic.py(346):     diff = sample_weight * (p - Y)
1.53 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
1.53 logistic.py(348):     grad[:, :n_features] += alpha * w
1.53 logistic.py(349):     if fit_intercept:
1.53 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
1.53 logistic.py(351):     return loss, grad.ravel(), p
1.53 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
1.53 logistic.py(339):     n_classes = Y.shape[1]
1.53 logistic.py(340):     n_features = X.shape[1]
1.53 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
1.53 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
1.53 logistic.py(343):                     dtype=X.dtype)
1.53 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
1.53 logistic.py(282):     n_classes = Y.shape[1]
1.53 logistic.py(283):     n_features = X.shape[1]
1.53 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
1.53 logistic.py(285):     w = w.reshape(n_classes, -1)
1.53 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
1.53 logistic.py(287):     if fit_intercept:
1.53 logistic.py(288):         intercept = w[:, -1]
1.53 logistic.py(289):         w = w[:, :-1]
1.53 logistic.py(292):     p = safe_sparse_dot(X, w.T)
1.53 logistic.py(293):     p += intercept
1.53 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
1.53 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
1.53 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
1.53 logistic.py(297):     p = np.exp(p, p)
1.53 logistic.py(298):     return loss, p, w
1.53 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
1.53 logistic.py(346):     diff = sample_weight * (p - Y)
1.53 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
1.53 logistic.py(348):     grad[:, :n_features] += alpha * w
1.53 logistic.py(349):     if fit_intercept:
1.53 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
1.53 logistic.py(351):     return loss, grad.ravel(), p
1.53 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
1.53 logistic.py(339):     n_classes = Y.shape[1]
1.53 logistic.py(340):     n_features = X.shape[1]
1.53 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
1.53 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
1.53 logistic.py(343):                     dtype=X.dtype)
1.53 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
1.53 logistic.py(282):     n_classes = Y.shape[1]
1.53 logistic.py(283):     n_features = X.shape[1]
1.53 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
1.53 logistic.py(285):     w = w.reshape(n_classes, -1)
1.53 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
1.53 logistic.py(287):     if fit_intercept:
1.53 logistic.py(288):         intercept = w[:, -1]
1.53 logistic.py(289):         w = w[:, :-1]
1.53 logistic.py(292):     p = safe_sparse_dot(X, w.T)
1.53 logistic.py(293):     p += intercept
1.53 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
1.53 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
1.53 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
1.53 logistic.py(297):     p = np.exp(p, p)
1.53 logistic.py(298):     return loss, p, w
1.53 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
1.53 logistic.py(346):     diff = sample_weight * (p - Y)
1.53 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
1.53 logistic.py(348):     grad[:, :n_features] += alpha * w
1.53 logistic.py(349):     if fit_intercept:
1.53 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
1.53 logistic.py(351):     return loss, grad.ravel(), p
1.53 logistic.py(718):             if info["warnflag"] == 1:
1.53 logistic.py(723):             n_iter_i = min(info['nit'], max_iter)
1.53 logistic.py(760):         if multi_class == 'multinomial':
1.53 logistic.py(761):             multi_w0 = np.reshape(w0, (classes.size, -1))
1.53 logistic.py(762):             if classes.size == 2:
1.53 logistic.py(764):             coefs.append(multi_w0)
1.53 logistic.py(768):         n_iter[i] = n_iter_i
1.53 logistic.py(712):     for i, C in enumerate(Cs):
1.53 logistic.py(713):         if solver == 'lbfgs':
1.53 logistic.py(714):             w0, loss, info = optimize.fmin_l_bfgs_b(
1.53 logistic.py(715):                 func, w0, fprime=None,
1.53 logistic.py(716):                 args=(X, target, 1. / C, sample_weight),
1.53 logistic.py(717):                 iprint=(verbose > 0) - 1, pgtol=tol, maxiter=max_iter)
1.53 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
1.53 logistic.py(339):     n_classes = Y.shape[1]
1.53 logistic.py(340):     n_features = X.shape[1]
1.53 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
1.53 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
1.53 logistic.py(343):                     dtype=X.dtype)
1.53 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
1.53 logistic.py(282):     n_classes = Y.shape[1]
1.53 logistic.py(283):     n_features = X.shape[1]
1.53 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
1.53 logistic.py(285):     w = w.reshape(n_classes, -1)
1.53 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
1.53 logistic.py(287):     if fit_intercept:
1.53 logistic.py(288):         intercept = w[:, -1]
1.53 logistic.py(289):         w = w[:, :-1]
1.53 logistic.py(292):     p = safe_sparse_dot(X, w.T)
1.53 logistic.py(293):     p += intercept
1.53 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
1.53 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
1.53 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
1.53 logistic.py(297):     p = np.exp(p, p)
1.53 logistic.py(298):     return loss, p, w
1.53 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
1.53 logistic.py(346):     diff = sample_weight * (p - Y)
1.53 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
1.53 logistic.py(348):     grad[:, :n_features] += alpha * w
1.53 logistic.py(349):     if fit_intercept:
1.53 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
1.53 logistic.py(351):     return loss, grad.ravel(), p
1.53 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
1.53 logistic.py(339):     n_classes = Y.shape[1]
1.53 logistic.py(340):     n_features = X.shape[1]
1.53 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
1.53 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
1.53 logistic.py(343):                     dtype=X.dtype)
1.53 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
1.53 logistic.py(282):     n_classes = Y.shape[1]
1.53 logistic.py(283):     n_features = X.shape[1]
1.53 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
1.53 logistic.py(285):     w = w.reshape(n_classes, -1)
1.53 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
1.53 logistic.py(287):     if fit_intercept:
1.53 logistic.py(288):         intercept = w[:, -1]
1.53 logistic.py(289):         w = w[:, :-1]
1.53 logistic.py(292):     p = safe_sparse_dot(X, w.T)
1.53 logistic.py(293):     p += intercept
1.53 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
1.53 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
1.53 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
1.53 logistic.py(297):     p = np.exp(p, p)
1.53 logistic.py(298):     return loss, p, w
1.53 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
1.54 logistic.py(346):     diff = sample_weight * (p - Y)
1.54 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
1.54 logistic.py(348):     grad[:, :n_features] += alpha * w
1.54 logistic.py(349):     if fit_intercept:
1.54 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
1.54 logistic.py(351):     return loss, grad.ravel(), p
1.54 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
1.54 logistic.py(339):     n_classes = Y.shape[1]
1.54 logistic.py(340):     n_features = X.shape[1]
1.54 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
1.54 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
1.54 logistic.py(343):                     dtype=X.dtype)
1.54 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
1.54 logistic.py(282):     n_classes = Y.shape[1]
1.54 logistic.py(283):     n_features = X.shape[1]
1.54 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
1.54 logistic.py(285):     w = w.reshape(n_classes, -1)
1.54 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
1.54 logistic.py(287):     if fit_intercept:
1.54 logistic.py(288):         intercept = w[:, -1]
1.54 logistic.py(289):         w = w[:, :-1]
1.54 logistic.py(292):     p = safe_sparse_dot(X, w.T)
1.54 logistic.py(293):     p += intercept
1.54 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
1.54 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
1.54 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
1.54 logistic.py(297):     p = np.exp(p, p)
1.54 logistic.py(298):     return loss, p, w
1.54 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
1.54 logistic.py(346):     diff = sample_weight * (p - Y)
1.54 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
1.54 logistic.py(348):     grad[:, :n_features] += alpha * w
1.54 logistic.py(349):     if fit_intercept:
1.54 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
1.54 logistic.py(351):     return loss, grad.ravel(), p
1.54 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
1.54 logistic.py(339):     n_classes = Y.shape[1]
1.54 logistic.py(340):     n_features = X.shape[1]
1.54 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
1.54 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
1.54 logistic.py(343):                     dtype=X.dtype)
1.54 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
1.54 logistic.py(282):     n_classes = Y.shape[1]
1.54 logistic.py(283):     n_features = X.shape[1]
1.54 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
1.54 logistic.py(285):     w = w.reshape(n_classes, -1)
1.54 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
1.54 logistic.py(287):     if fit_intercept:
1.54 logistic.py(288):         intercept = w[:, -1]
1.54 logistic.py(289):         w = w[:, :-1]
1.54 logistic.py(292):     p = safe_sparse_dot(X, w.T)
1.54 logistic.py(293):     p += intercept
1.54 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
1.54 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
1.54 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
1.54 logistic.py(297):     p = np.exp(p, p)
1.54 logistic.py(298):     return loss, p, w
1.54 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
1.54 logistic.py(346):     diff = sample_weight * (p - Y)
1.54 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
1.54 logistic.py(348):     grad[:, :n_features] += alpha * w
1.54 logistic.py(349):     if fit_intercept:
1.54 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
1.54 logistic.py(351):     return loss, grad.ravel(), p
1.54 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
1.54 logistic.py(339):     n_classes = Y.shape[1]
1.54 logistic.py(340):     n_features = X.shape[1]
1.54 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
1.54 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
1.54 logistic.py(343):                     dtype=X.dtype)
1.54 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
1.54 logistic.py(282):     n_classes = Y.shape[1]
1.54 logistic.py(283):     n_features = X.shape[1]
1.54 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
1.54 logistic.py(285):     w = w.reshape(n_classes, -1)
1.54 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
1.54 logistic.py(287):     if fit_intercept:
1.54 logistic.py(288):         intercept = w[:, -1]
1.54 logistic.py(289):         w = w[:, :-1]
1.54 logistic.py(292):     p = safe_sparse_dot(X, w.T)
1.54 logistic.py(293):     p += intercept
1.54 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
1.54 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
1.54 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
1.54 logistic.py(297):     p = np.exp(p, p)
1.54 logistic.py(298):     return loss, p, w
1.54 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
1.54 logistic.py(346):     diff = sample_weight * (p - Y)
1.54 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
1.54 logistic.py(348):     grad[:, :n_features] += alpha * w
1.54 logistic.py(349):     if fit_intercept:
1.54 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
1.54 logistic.py(351):     return loss, grad.ravel(), p
1.54 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
1.54 logistic.py(339):     n_classes = Y.shape[1]
1.54 logistic.py(340):     n_features = X.shape[1]
1.54 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
1.54 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
1.54 logistic.py(343):                     dtype=X.dtype)
1.54 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
1.54 logistic.py(282):     n_classes = Y.shape[1]
1.54 logistic.py(283):     n_features = X.shape[1]
1.54 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
1.54 logistic.py(285):     w = w.reshape(n_classes, -1)
1.54 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
1.54 logistic.py(287):     if fit_intercept:
1.54 logistic.py(288):         intercept = w[:, -1]
1.54 logistic.py(289):         w = w[:, :-1]
1.54 logistic.py(292):     p = safe_sparse_dot(X, w.T)
1.54 logistic.py(293):     p += intercept
1.54 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
1.54 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
1.54 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
1.54 logistic.py(297):     p = np.exp(p, p)
1.54 logistic.py(298):     return loss, p, w
1.54 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
1.54 logistic.py(346):     diff = sample_weight * (p - Y)
1.54 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
1.54 logistic.py(348):     grad[:, :n_features] += alpha * w
1.54 logistic.py(349):     if fit_intercept:
1.54 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
1.54 logistic.py(351):     return loss, grad.ravel(), p
1.54 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
1.54 logistic.py(339):     n_classes = Y.shape[1]
1.54 logistic.py(340):     n_features = X.shape[1]
1.54 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
1.54 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
1.54 logistic.py(343):                     dtype=X.dtype)
1.54 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
1.54 logistic.py(282):     n_classes = Y.shape[1]
1.54 logistic.py(283):     n_features = X.shape[1]
1.54 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
1.54 logistic.py(285):     w = w.reshape(n_classes, -1)
1.54 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
1.54 logistic.py(287):     if fit_intercept:
1.54 logistic.py(288):         intercept = w[:, -1]
1.54 logistic.py(289):         w = w[:, :-1]
1.54 logistic.py(292):     p = safe_sparse_dot(X, w.T)
1.54 logistic.py(293):     p += intercept
1.54 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
1.54 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
1.54 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
1.54 logistic.py(297):     p = np.exp(p, p)
1.54 logistic.py(298):     return loss, p, w
1.54 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
1.54 logistic.py(346):     diff = sample_weight * (p - Y)
1.54 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
1.54 logistic.py(348):     grad[:, :n_features] += alpha * w
1.54 logistic.py(349):     if fit_intercept:
1.54 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
1.54 logistic.py(351):     return loss, grad.ravel(), p
1.54 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
1.54 logistic.py(339):     n_classes = Y.shape[1]
1.54 logistic.py(340):     n_features = X.shape[1]
1.54 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
1.54 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
1.54 logistic.py(343):                     dtype=X.dtype)
1.54 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
1.54 logistic.py(282):     n_classes = Y.shape[1]
1.54 logistic.py(283):     n_features = X.shape[1]
1.54 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
1.54 logistic.py(285):     w = w.reshape(n_classes, -1)
1.54 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
1.54 logistic.py(287):     if fit_intercept:
1.54 logistic.py(288):         intercept = w[:, -1]
1.54 logistic.py(289):         w = w[:, :-1]
1.54 logistic.py(292):     p = safe_sparse_dot(X, w.T)
1.54 logistic.py(293):     p += intercept
1.54 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
1.54 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
1.54 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
1.54 logistic.py(297):     p = np.exp(p, p)
1.54 logistic.py(298):     return loss, p, w
1.54 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
1.54 logistic.py(346):     diff = sample_weight * (p - Y)
1.54 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
1.54 logistic.py(348):     grad[:, :n_features] += alpha * w
1.54 logistic.py(349):     if fit_intercept:
1.54 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
1.54 logistic.py(351):     return loss, grad.ravel(), p
1.54 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
1.54 logistic.py(339):     n_classes = Y.shape[1]
1.54 logistic.py(340):     n_features = X.shape[1]
1.54 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
1.54 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
1.54 logistic.py(343):                     dtype=X.dtype)
1.54 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
1.54 logistic.py(282):     n_classes = Y.shape[1]
1.54 logistic.py(283):     n_features = X.shape[1]
1.54 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
1.54 logistic.py(285):     w = w.reshape(n_classes, -1)
1.54 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
1.54 logistic.py(287):     if fit_intercept:
1.54 logistic.py(288):         intercept = w[:, -1]
1.54 logistic.py(289):         w = w[:, :-1]
1.54 logistic.py(292):     p = safe_sparse_dot(X, w.T)
1.54 logistic.py(293):     p += intercept
1.54 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
1.54 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
1.54 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
1.54 logistic.py(297):     p = np.exp(p, p)
1.54 logistic.py(298):     return loss, p, w
1.54 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
1.54 logistic.py(346):     diff = sample_weight * (p - Y)
1.54 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
1.54 logistic.py(348):     grad[:, :n_features] += alpha * w
1.54 logistic.py(349):     if fit_intercept:
1.54 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
1.54 logistic.py(351):     return loss, grad.ravel(), p
1.54 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
1.54 logistic.py(339):     n_classes = Y.shape[1]
1.54 logistic.py(340):     n_features = X.shape[1]
1.54 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
1.54 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
1.54 logistic.py(343):                     dtype=X.dtype)
1.54 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
1.54 logistic.py(282):     n_classes = Y.shape[1]
1.54 logistic.py(283):     n_features = X.shape[1]
1.54 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
1.54 logistic.py(285):     w = w.reshape(n_classes, -1)
1.54 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
1.54 logistic.py(287):     if fit_intercept:
1.54 logistic.py(288):         intercept = w[:, -1]
1.54 logistic.py(289):         w = w[:, :-1]
1.54 logistic.py(292):     p = safe_sparse_dot(X, w.T)
1.54 logistic.py(293):     p += intercept
1.54 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
1.54 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
1.54 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
1.54 logistic.py(297):     p = np.exp(p, p)
1.54 logistic.py(298):     return loss, p, w
1.54 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
1.54 logistic.py(346):     diff = sample_weight * (p - Y)
1.54 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
1.54 logistic.py(348):     grad[:, :n_features] += alpha * w
1.54 logistic.py(349):     if fit_intercept:
1.54 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
1.54 logistic.py(351):     return loss, grad.ravel(), p
1.54 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
1.54 logistic.py(339):     n_classes = Y.shape[1]
1.54 logistic.py(340):     n_features = X.shape[1]
1.54 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
1.54 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
1.54 logistic.py(343):                     dtype=X.dtype)
1.54 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
1.54 logistic.py(282):     n_classes = Y.shape[1]
1.54 logistic.py(283):     n_features = X.shape[1]
1.54 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
1.54 logistic.py(285):     w = w.reshape(n_classes, -1)
1.54 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
1.54 logistic.py(287):     if fit_intercept:
1.54 logistic.py(288):         intercept = w[:, -1]
1.54 logistic.py(289):         w = w[:, :-1]
1.54 logistic.py(292):     p = safe_sparse_dot(X, w.T)
1.54 logistic.py(293):     p += intercept
1.54 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
1.54 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
1.54 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
1.54 logistic.py(297):     p = np.exp(p, p)
1.54 logistic.py(298):     return loss, p, w
1.54 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
1.54 logistic.py(346):     diff = sample_weight * (p - Y)
1.54 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
1.54 logistic.py(348):     grad[:, :n_features] += alpha * w
1.54 logistic.py(349):     if fit_intercept:
1.54 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
1.54 logistic.py(351):     return loss, grad.ravel(), p
1.54 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
1.54 logistic.py(339):     n_classes = Y.shape[1]
1.54 logistic.py(340):     n_features = X.shape[1]
1.54 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
1.54 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
1.54 logistic.py(343):                     dtype=X.dtype)
1.54 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
1.54 logistic.py(282):     n_classes = Y.shape[1]
1.54 logistic.py(283):     n_features = X.shape[1]
1.54 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
1.54 logistic.py(285):     w = w.reshape(n_classes, -1)
1.54 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
1.54 logistic.py(287):     if fit_intercept:
1.54 logistic.py(288):         intercept = w[:, -1]
1.54 logistic.py(289):         w = w[:, :-1]
1.54 logistic.py(292):     p = safe_sparse_dot(X, w.T)
1.54 logistic.py(293):     p += intercept
1.54 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
1.54 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
1.54 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
1.54 logistic.py(297):     p = np.exp(p, p)
1.54 logistic.py(298):     return loss, p, w
1.54 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
1.54 logistic.py(346):     diff = sample_weight * (p - Y)
1.54 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
1.54 logistic.py(348):     grad[:, :n_features] += alpha * w
1.54 logistic.py(349):     if fit_intercept:
1.54 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
1.54 logistic.py(351):     return loss, grad.ravel(), p
1.54 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
1.54 logistic.py(339):     n_classes = Y.shape[1]
1.54 logistic.py(340):     n_features = X.shape[1]
1.54 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
1.54 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
1.54 logistic.py(343):                     dtype=X.dtype)
1.54 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
1.54 logistic.py(282):     n_classes = Y.shape[1]
1.54 logistic.py(283):     n_features = X.shape[1]
1.54 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
1.54 logistic.py(285):     w = w.reshape(n_classes, -1)
1.54 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
1.54 logistic.py(287):     if fit_intercept:
1.54 logistic.py(288):         intercept = w[:, -1]
1.54 logistic.py(289):         w = w[:, :-1]
1.54 logistic.py(292):     p = safe_sparse_dot(X, w.T)
1.54 logistic.py(293):     p += intercept
1.54 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
1.54 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
1.54 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
1.54 logistic.py(297):     p = np.exp(p, p)
1.54 logistic.py(298):     return loss, p, w
1.54 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
1.54 logistic.py(346):     diff = sample_weight * (p - Y)
1.54 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
1.54 logistic.py(348):     grad[:, :n_features] += alpha * w
1.54 logistic.py(349):     if fit_intercept:
1.54 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
1.54 logistic.py(351):     return loss, grad.ravel(), p
1.54 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
1.54 logistic.py(339):     n_classes = Y.shape[1]
1.54 logistic.py(340):     n_features = X.shape[1]
1.54 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
1.54 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
1.54 logistic.py(343):                     dtype=X.dtype)
1.54 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
1.54 logistic.py(282):     n_classes = Y.shape[1]
1.54 logistic.py(283):     n_features = X.shape[1]
1.54 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
1.54 logistic.py(285):     w = w.reshape(n_classes, -1)
1.54 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
1.54 logistic.py(287):     if fit_intercept:
1.54 logistic.py(288):         intercept = w[:, -1]
1.54 logistic.py(289):         w = w[:, :-1]
1.54 logistic.py(292):     p = safe_sparse_dot(X, w.T)
1.54 logistic.py(293):     p += intercept
1.54 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
1.54 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
1.54 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
1.54 logistic.py(297):     p = np.exp(p, p)
1.54 logistic.py(298):     return loss, p, w
1.54 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
1.54 logistic.py(346):     diff = sample_weight * (p - Y)
1.54 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
1.54 logistic.py(348):     grad[:, :n_features] += alpha * w
1.54 logistic.py(349):     if fit_intercept:
1.54 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
1.54 logistic.py(351):     return loss, grad.ravel(), p
1.54 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
1.54 logistic.py(339):     n_classes = Y.shape[1]
1.54 logistic.py(340):     n_features = X.shape[1]
1.54 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
1.54 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
1.54 logistic.py(343):                     dtype=X.dtype)
1.54 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
1.54 logistic.py(282):     n_classes = Y.shape[1]
1.54 logistic.py(283):     n_features = X.shape[1]
1.54 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
1.54 logistic.py(285):     w = w.reshape(n_classes, -1)
1.54 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
1.54 logistic.py(287):     if fit_intercept:
1.54 logistic.py(288):         intercept = w[:, -1]
1.54 logistic.py(289):         w = w[:, :-1]
1.54 logistic.py(292):     p = safe_sparse_dot(X, w.T)
1.54 logistic.py(293):     p += intercept
1.54 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
1.54 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
1.54 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
1.54 logistic.py(297):     p = np.exp(p, p)
1.54 logistic.py(298):     return loss, p, w
1.54 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
1.54 logistic.py(346):     diff = sample_weight * (p - Y)
1.54 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
1.54 logistic.py(348):     grad[:, :n_features] += alpha * w
1.54 logistic.py(349):     if fit_intercept:
1.54 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
1.54 logistic.py(351):     return loss, grad.ravel(), p
1.54 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
1.54 logistic.py(339):     n_classes = Y.shape[1]
1.54 logistic.py(340):     n_features = X.shape[1]
1.54 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
1.54 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
1.54 logistic.py(343):                     dtype=X.dtype)
1.54 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
1.54 logistic.py(282):     n_classes = Y.shape[1]
1.54 logistic.py(283):     n_features = X.shape[1]
1.54 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
1.54 logistic.py(285):     w = w.reshape(n_classes, -1)
1.54 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
1.54 logistic.py(287):     if fit_intercept:
1.54 logistic.py(288):         intercept = w[:, -1]
1.54 logistic.py(289):         w = w[:, :-1]
1.54 logistic.py(292):     p = safe_sparse_dot(X, w.T)
1.54 logistic.py(293):     p += intercept
1.54 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
1.54 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
1.54 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
1.54 logistic.py(297):     p = np.exp(p, p)
1.54 logistic.py(298):     return loss, p, w
1.54 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
1.54 logistic.py(346):     diff = sample_weight * (p - Y)
1.54 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
1.54 logistic.py(348):     grad[:, :n_features] += alpha * w
1.54 logistic.py(349):     if fit_intercept:
1.54 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
1.54 logistic.py(351):     return loss, grad.ravel(), p
1.54 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
1.54 logistic.py(339):     n_classes = Y.shape[1]
1.54 logistic.py(340):     n_features = X.shape[1]
1.54 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
1.54 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
1.54 logistic.py(343):                     dtype=X.dtype)
1.54 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
1.54 logistic.py(282):     n_classes = Y.shape[1]
1.54 logistic.py(283):     n_features = X.shape[1]
1.54 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
1.54 logistic.py(285):     w = w.reshape(n_classes, -1)
1.54 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
1.54 logistic.py(287):     if fit_intercept:
1.54 logistic.py(288):         intercept = w[:, -1]
1.54 logistic.py(289):         w = w[:, :-1]
1.54 logistic.py(292):     p = safe_sparse_dot(X, w.T)
1.54 logistic.py(293):     p += intercept
1.54 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
1.54 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
1.54 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
1.54 logistic.py(297):     p = np.exp(p, p)
1.54 logistic.py(298):     return loss, p, w
1.54 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
1.54 logistic.py(346):     diff = sample_weight * (p - Y)
1.54 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
1.54 logistic.py(348):     grad[:, :n_features] += alpha * w
1.54 logistic.py(349):     if fit_intercept:
1.54 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
1.54 logistic.py(351):     return loss, grad.ravel(), p
1.54 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
1.54 logistic.py(339):     n_classes = Y.shape[1]
1.54 logistic.py(340):     n_features = X.shape[1]
1.54 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
1.54 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
1.54 logistic.py(343):                     dtype=X.dtype)
1.54 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
1.54 logistic.py(282):     n_classes = Y.shape[1]
1.54 logistic.py(283):     n_features = X.shape[1]
1.54 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
1.54 logistic.py(285):     w = w.reshape(n_classes, -1)
1.54 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
1.54 logistic.py(287):     if fit_intercept:
1.54 logistic.py(288):         intercept = w[:, -1]
1.54 logistic.py(289):         w = w[:, :-1]
1.54 logistic.py(292):     p = safe_sparse_dot(X, w.T)
1.54 logistic.py(293):     p += intercept
1.54 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
1.54 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
1.54 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
1.54 logistic.py(297):     p = np.exp(p, p)
1.54 logistic.py(298):     return loss, p, w
1.54 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
1.54 logistic.py(346):     diff = sample_weight * (p - Y)
1.54 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
1.54 logistic.py(348):     grad[:, :n_features] += alpha * w
1.54 logistic.py(349):     if fit_intercept:
1.54 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
1.54 logistic.py(351):     return loss, grad.ravel(), p
1.54 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
1.54 logistic.py(339):     n_classes = Y.shape[1]
1.54 logistic.py(340):     n_features = X.shape[1]
1.54 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
1.54 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
1.54 logistic.py(343):                     dtype=X.dtype)
1.54 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
1.54 logistic.py(282):     n_classes = Y.shape[1]
1.54 logistic.py(283):     n_features = X.shape[1]
1.54 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
1.54 logistic.py(285):     w = w.reshape(n_classes, -1)
1.54 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
1.54 logistic.py(287):     if fit_intercept:
1.54 logistic.py(288):         intercept = w[:, -1]
1.54 logistic.py(289):         w = w[:, :-1]
1.54 logistic.py(292):     p = safe_sparse_dot(X, w.T)
1.54 logistic.py(293):     p += intercept
1.54 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
1.55 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
1.55 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
1.55 logistic.py(297):     p = np.exp(p, p)
1.55 logistic.py(298):     return loss, p, w
1.55 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
1.55 logistic.py(346):     diff = sample_weight * (p - Y)
1.55 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
1.55 logistic.py(348):     grad[:, :n_features] += alpha * w
1.55 logistic.py(349):     if fit_intercept:
1.55 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
1.55 logistic.py(351):     return loss, grad.ravel(), p
1.55 logistic.py(718):             if info["warnflag"] == 1:
1.55 logistic.py(723):             n_iter_i = min(info['nit'], max_iter)
1.55 logistic.py(760):         if multi_class == 'multinomial':
1.55 logistic.py(761):             multi_w0 = np.reshape(w0, (classes.size, -1))
1.55 logistic.py(762):             if classes.size == 2:
1.55 logistic.py(764):             coefs.append(multi_w0)
1.55 logistic.py(768):         n_iter[i] = n_iter_i
1.55 logistic.py(712):     for i, C in enumerate(Cs):
1.55 logistic.py(713):         if solver == 'lbfgs':
1.55 logistic.py(714):             w0, loss, info = optimize.fmin_l_bfgs_b(
1.55 logistic.py(715):                 func, w0, fprime=None,
1.55 logistic.py(716):                 args=(X, target, 1. / C, sample_weight),
1.55 logistic.py(717):                 iprint=(verbose > 0) - 1, pgtol=tol, maxiter=max_iter)
1.55 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
1.55 logistic.py(339):     n_classes = Y.shape[1]
1.55 logistic.py(340):     n_features = X.shape[1]
1.55 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
1.55 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
1.55 logistic.py(343):                     dtype=X.dtype)
1.55 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
1.55 logistic.py(282):     n_classes = Y.shape[1]
1.55 logistic.py(283):     n_features = X.shape[1]
1.55 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
1.55 logistic.py(285):     w = w.reshape(n_classes, -1)
1.55 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
1.55 logistic.py(287):     if fit_intercept:
1.55 logistic.py(288):         intercept = w[:, -1]
1.55 logistic.py(289):         w = w[:, :-1]
1.55 logistic.py(292):     p = safe_sparse_dot(X, w.T)
1.55 logistic.py(293):     p += intercept
1.55 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
1.55 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
1.55 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
1.55 logistic.py(297):     p = np.exp(p, p)
1.55 logistic.py(298):     return loss, p, w
1.55 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
1.55 logistic.py(346):     diff = sample_weight * (p - Y)
1.55 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
1.55 logistic.py(348):     grad[:, :n_features] += alpha * w
1.55 logistic.py(349):     if fit_intercept:
1.55 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
1.55 logistic.py(351):     return loss, grad.ravel(), p
1.55 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
1.55 logistic.py(339):     n_classes = Y.shape[1]
1.55 logistic.py(340):     n_features = X.shape[1]
1.55 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
1.55 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
1.55 logistic.py(343):                     dtype=X.dtype)
1.55 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
1.55 logistic.py(282):     n_classes = Y.shape[1]
1.55 logistic.py(283):     n_features = X.shape[1]
1.55 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
1.55 logistic.py(285):     w = w.reshape(n_classes, -1)
1.55 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
1.55 logistic.py(287):     if fit_intercept:
1.55 logistic.py(288):         intercept = w[:, -1]
1.55 logistic.py(289):         w = w[:, :-1]
1.55 logistic.py(292):     p = safe_sparse_dot(X, w.T)
1.55 logistic.py(293):     p += intercept
1.55 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
1.55 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
1.55 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
1.55 logistic.py(297):     p = np.exp(p, p)
1.55 logistic.py(298):     return loss, p, w
1.55 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
1.55 logistic.py(346):     diff = sample_weight * (p - Y)
1.55 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
1.55 logistic.py(348):     grad[:, :n_features] += alpha * w
1.55 logistic.py(349):     if fit_intercept:
1.55 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
1.55 logistic.py(351):     return loss, grad.ravel(), p
1.55 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
1.55 logistic.py(339):     n_classes = Y.shape[1]
1.55 logistic.py(340):     n_features = X.shape[1]
1.55 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
1.55 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
1.55 logistic.py(343):                     dtype=X.dtype)
1.55 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
1.55 logistic.py(282):     n_classes = Y.shape[1]
1.55 logistic.py(283):     n_features = X.shape[1]
1.55 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
1.55 logistic.py(285):     w = w.reshape(n_classes, -1)
1.55 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
1.55 logistic.py(287):     if fit_intercept:
1.55 logistic.py(288):         intercept = w[:, -1]
1.55 logistic.py(289):         w = w[:, :-1]
1.55 logistic.py(292):     p = safe_sparse_dot(X, w.T)
1.55 logistic.py(293):     p += intercept
1.55 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
1.55 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
1.55 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
1.55 logistic.py(297):     p = np.exp(p, p)
1.55 logistic.py(298):     return loss, p, w
1.55 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
1.55 logistic.py(346):     diff = sample_weight * (p - Y)
1.55 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
1.55 logistic.py(348):     grad[:, :n_features] += alpha * w
1.55 logistic.py(349):     if fit_intercept:
1.55 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
1.55 logistic.py(351):     return loss, grad.ravel(), p
1.55 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
1.55 logistic.py(339):     n_classes = Y.shape[1]
1.55 logistic.py(340):     n_features = X.shape[1]
1.55 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
1.55 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
1.55 logistic.py(343):                     dtype=X.dtype)
1.55 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
1.55 logistic.py(282):     n_classes = Y.shape[1]
1.55 logistic.py(283):     n_features = X.shape[1]
1.55 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
1.55 logistic.py(285):     w = w.reshape(n_classes, -1)
1.55 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
1.55 logistic.py(287):     if fit_intercept:
1.55 logistic.py(288):         intercept = w[:, -1]
1.55 logistic.py(289):         w = w[:, :-1]
1.55 logistic.py(292):     p = safe_sparse_dot(X, w.T)
1.55 logistic.py(293):     p += intercept
1.55 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
1.55 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
1.55 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
1.55 logistic.py(297):     p = np.exp(p, p)
1.55 logistic.py(298):     return loss, p, w
1.55 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
1.55 logistic.py(346):     diff = sample_weight * (p - Y)
1.55 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
1.55 logistic.py(348):     grad[:, :n_features] += alpha * w
1.55 logistic.py(349):     if fit_intercept:
1.55 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
1.55 logistic.py(351):     return loss, grad.ravel(), p
1.55 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
1.55 logistic.py(339):     n_classes = Y.shape[1]
1.55 logistic.py(340):     n_features = X.shape[1]
1.55 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
1.55 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
1.55 logistic.py(343):                     dtype=X.dtype)
1.55 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
1.55 logistic.py(282):     n_classes = Y.shape[1]
1.55 logistic.py(283):     n_features = X.shape[1]
1.55 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
1.55 logistic.py(285):     w = w.reshape(n_classes, -1)
1.55 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
1.55 logistic.py(287):     if fit_intercept:
1.55 logistic.py(288):         intercept = w[:, -1]
1.55 logistic.py(289):         w = w[:, :-1]
1.55 logistic.py(292):     p = safe_sparse_dot(X, w.T)
1.55 logistic.py(293):     p += intercept
1.55 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
1.55 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
1.55 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
1.55 logistic.py(297):     p = np.exp(p, p)
1.55 logistic.py(298):     return loss, p, w
1.55 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
1.55 logistic.py(346):     diff = sample_weight * (p - Y)
1.55 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
1.55 logistic.py(348):     grad[:, :n_features] += alpha * w
1.55 logistic.py(349):     if fit_intercept:
1.55 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
1.55 logistic.py(351):     return loss, grad.ravel(), p
1.55 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
1.55 logistic.py(339):     n_classes = Y.shape[1]
1.55 logistic.py(340):     n_features = X.shape[1]
1.55 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
1.55 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
1.55 logistic.py(343):                     dtype=X.dtype)
1.55 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
1.55 logistic.py(282):     n_classes = Y.shape[1]
1.55 logistic.py(283):     n_features = X.shape[1]
1.55 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
1.55 logistic.py(285):     w = w.reshape(n_classes, -1)
1.55 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
1.55 logistic.py(287):     if fit_intercept:
1.55 logistic.py(288):         intercept = w[:, -1]
1.55 logistic.py(289):         w = w[:, :-1]
1.55 logistic.py(292):     p = safe_sparse_dot(X, w.T)
1.55 logistic.py(293):     p += intercept
1.55 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
1.55 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
1.55 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
1.55 logistic.py(297):     p = np.exp(p, p)
1.55 logistic.py(298):     return loss, p, w
1.55 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
1.55 logistic.py(346):     diff = sample_weight * (p - Y)
1.55 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
1.55 logistic.py(348):     grad[:, :n_features] += alpha * w
1.55 logistic.py(349):     if fit_intercept:
1.55 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
1.55 logistic.py(351):     return loss, grad.ravel(), p
1.55 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
1.55 logistic.py(339):     n_classes = Y.shape[1]
1.55 logistic.py(340):     n_features = X.shape[1]
1.55 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
1.55 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
1.55 logistic.py(343):                     dtype=X.dtype)
1.55 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
1.55 logistic.py(282):     n_classes = Y.shape[1]
1.55 logistic.py(283):     n_features = X.shape[1]
1.55 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
1.55 logistic.py(285):     w = w.reshape(n_classes, -1)
1.55 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
1.55 logistic.py(287):     if fit_intercept:
1.55 logistic.py(288):         intercept = w[:, -1]
1.55 logistic.py(289):         w = w[:, :-1]
1.55 logistic.py(292):     p = safe_sparse_dot(X, w.T)
1.55 logistic.py(293):     p += intercept
1.55 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
1.55 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
1.55 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
1.55 logistic.py(297):     p = np.exp(p, p)
1.55 logistic.py(298):     return loss, p, w
1.55 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
1.55 logistic.py(346):     diff = sample_weight * (p - Y)
1.55 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
1.55 logistic.py(348):     grad[:, :n_features] += alpha * w
1.55 logistic.py(349):     if fit_intercept:
1.55 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
1.55 logistic.py(351):     return loss, grad.ravel(), p
1.55 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
1.55 logistic.py(339):     n_classes = Y.shape[1]
1.55 logistic.py(340):     n_features = X.shape[1]
1.55 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
1.55 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
1.55 logistic.py(343):                     dtype=X.dtype)
1.55 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
1.55 logistic.py(282):     n_classes = Y.shape[1]
1.55 logistic.py(283):     n_features = X.shape[1]
1.55 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
1.55 logistic.py(285):     w = w.reshape(n_classes, -1)
1.55 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
1.55 logistic.py(287):     if fit_intercept:
1.55 logistic.py(288):         intercept = w[:, -1]
1.55 logistic.py(289):         w = w[:, :-1]
1.55 logistic.py(292):     p = safe_sparse_dot(X, w.T)
1.55 logistic.py(293):     p += intercept
1.55 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
1.55 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
1.55 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
1.55 logistic.py(297):     p = np.exp(p, p)
1.55 logistic.py(298):     return loss, p, w
1.55 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
1.55 logistic.py(346):     diff = sample_weight * (p - Y)
1.55 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
1.55 logistic.py(348):     grad[:, :n_features] += alpha * w
1.55 logistic.py(349):     if fit_intercept:
1.55 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
1.55 logistic.py(351):     return loss, grad.ravel(), p
1.55 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
1.55 logistic.py(339):     n_classes = Y.shape[1]
1.55 logistic.py(340):     n_features = X.shape[1]
1.55 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
1.55 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
1.55 logistic.py(343):                     dtype=X.dtype)
1.55 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
1.55 logistic.py(282):     n_classes = Y.shape[1]
1.55 logistic.py(283):     n_features = X.shape[1]
1.55 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
1.55 logistic.py(285):     w = w.reshape(n_classes, -1)
1.55 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
1.55 logistic.py(287):     if fit_intercept:
1.55 logistic.py(288):         intercept = w[:, -1]
1.55 logistic.py(289):         w = w[:, :-1]
1.55 logistic.py(292):     p = safe_sparse_dot(X, w.T)
1.55 logistic.py(293):     p += intercept
1.55 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
1.55 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
1.55 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
1.55 logistic.py(297):     p = np.exp(p, p)
1.55 logistic.py(298):     return loss, p, w
1.55 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
1.55 logistic.py(346):     diff = sample_weight * (p - Y)
1.55 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
1.55 logistic.py(348):     grad[:, :n_features] += alpha * w
1.55 logistic.py(349):     if fit_intercept:
1.55 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
1.55 logistic.py(351):     return loss, grad.ravel(), p
1.55 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
1.55 logistic.py(339):     n_classes = Y.shape[1]
1.55 logistic.py(340):     n_features = X.shape[1]
1.55 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
1.55 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
1.55 logistic.py(343):                     dtype=X.dtype)
1.55 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
1.55 logistic.py(282):     n_classes = Y.shape[1]
1.55 logistic.py(283):     n_features = X.shape[1]
1.55 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
1.55 logistic.py(285):     w = w.reshape(n_classes, -1)
1.55 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
1.55 logistic.py(287):     if fit_intercept:
1.55 logistic.py(288):         intercept = w[:, -1]
1.55 logistic.py(289):         w = w[:, :-1]
1.55 logistic.py(292):     p = safe_sparse_dot(X, w.T)
1.55 logistic.py(293):     p += intercept
1.55 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
1.55 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
1.55 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
1.55 logistic.py(297):     p = np.exp(p, p)
1.55 logistic.py(298):     return loss, p, w
1.55 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
1.55 logistic.py(346):     diff = sample_weight * (p - Y)
1.55 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
1.55 logistic.py(348):     grad[:, :n_features] += alpha * w
1.55 logistic.py(349):     if fit_intercept:
1.55 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
1.55 logistic.py(351):     return loss, grad.ravel(), p
1.55 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
1.55 logistic.py(339):     n_classes = Y.shape[1]
1.55 logistic.py(340):     n_features = X.shape[1]
1.55 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
1.55 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
1.55 logistic.py(343):                     dtype=X.dtype)
1.55 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
1.55 logistic.py(282):     n_classes = Y.shape[1]
1.55 logistic.py(283):     n_features = X.shape[1]
1.55 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
1.55 logistic.py(285):     w = w.reshape(n_classes, -1)
1.55 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
1.55 logistic.py(287):     if fit_intercept:
1.55 logistic.py(288):         intercept = w[:, -1]
1.55 logistic.py(289):         w = w[:, :-1]
1.55 logistic.py(292):     p = safe_sparse_dot(X, w.T)
1.55 logistic.py(293):     p += intercept
1.55 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
1.55 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
1.55 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
1.55 logistic.py(297):     p = np.exp(p, p)
1.55 logistic.py(298):     return loss, p, w
1.55 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
1.55 logistic.py(346):     diff = sample_weight * (p - Y)
1.55 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
1.55 logistic.py(348):     grad[:, :n_features] += alpha * w
1.55 logistic.py(349):     if fit_intercept:
1.55 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
1.55 logistic.py(351):     return loss, grad.ravel(), p
1.55 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
1.55 logistic.py(339):     n_classes = Y.shape[1]
1.55 logistic.py(340):     n_features = X.shape[1]
1.55 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
1.55 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
1.55 logistic.py(343):                     dtype=X.dtype)
1.55 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
1.55 logistic.py(282):     n_classes = Y.shape[1]
1.55 logistic.py(283):     n_features = X.shape[1]
1.55 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
1.55 logistic.py(285):     w = w.reshape(n_classes, -1)
1.55 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
1.55 logistic.py(287):     if fit_intercept:
1.55 logistic.py(288):         intercept = w[:, -1]
1.55 logistic.py(289):         w = w[:, :-1]
1.55 logistic.py(292):     p = safe_sparse_dot(X, w.T)
1.55 logistic.py(293):     p += intercept
1.55 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
1.55 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
1.55 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
1.55 logistic.py(297):     p = np.exp(p, p)
1.55 logistic.py(298):     return loss, p, w
1.55 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
1.55 logistic.py(346):     diff = sample_weight * (p - Y)
1.55 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
1.55 logistic.py(348):     grad[:, :n_features] += alpha * w
1.55 logistic.py(349):     if fit_intercept:
1.55 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
1.55 logistic.py(351):     return loss, grad.ravel(), p
1.55 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
1.55 logistic.py(339):     n_classes = Y.shape[1]
1.55 logistic.py(340):     n_features = X.shape[1]
1.55 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
1.55 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
1.55 logistic.py(343):                     dtype=X.dtype)
1.55 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
1.55 logistic.py(282):     n_classes = Y.shape[1]
1.55 logistic.py(283):     n_features = X.shape[1]
1.55 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
1.55 logistic.py(285):     w = w.reshape(n_classes, -1)
1.55 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
1.55 logistic.py(287):     if fit_intercept:
1.55 logistic.py(288):         intercept = w[:, -1]
1.55 logistic.py(289):         w = w[:, :-1]
1.55 logistic.py(292):     p = safe_sparse_dot(X, w.T)
1.55 logistic.py(293):     p += intercept
1.55 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
1.55 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
1.55 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
1.55 logistic.py(297):     p = np.exp(p, p)
1.55 logistic.py(298):     return loss, p, w
1.55 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
1.55 logistic.py(346):     diff = sample_weight * (p - Y)
1.55 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
1.55 logistic.py(348):     grad[:, :n_features] += alpha * w
1.55 logistic.py(349):     if fit_intercept:
1.55 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
1.55 logistic.py(351):     return loss, grad.ravel(), p
1.55 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
1.55 logistic.py(339):     n_classes = Y.shape[1]
1.55 logistic.py(340):     n_features = X.shape[1]
1.55 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
1.55 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
1.55 logistic.py(343):                     dtype=X.dtype)
1.55 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
1.55 logistic.py(282):     n_classes = Y.shape[1]
1.55 logistic.py(283):     n_features = X.shape[1]
1.55 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
1.55 logistic.py(285):     w = w.reshape(n_classes, -1)
1.55 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
1.55 logistic.py(287):     if fit_intercept:
1.55 logistic.py(288):         intercept = w[:, -1]
1.55 logistic.py(289):         w = w[:, :-1]
1.55 logistic.py(292):     p = safe_sparse_dot(X, w.T)
1.55 logistic.py(293):     p += intercept
1.55 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
1.55 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
1.55 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
1.55 logistic.py(297):     p = np.exp(p, p)
1.55 logistic.py(298):     return loss, p, w
1.55 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
1.55 logistic.py(346):     diff = sample_weight * (p - Y)
1.55 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
1.55 logistic.py(348):     grad[:, :n_features] += alpha * w
1.55 logistic.py(349):     if fit_intercept:
1.55 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
1.55 logistic.py(351):     return loss, grad.ravel(), p
1.55 logistic.py(718):             if info["warnflag"] == 1:
1.55 logistic.py(723):             n_iter_i = min(info['nit'], max_iter)
1.55 logistic.py(760):         if multi_class == 'multinomial':
1.55 logistic.py(761):             multi_w0 = np.reshape(w0, (classes.size, -1))
1.55 logistic.py(762):             if classes.size == 2:
1.55 logistic.py(764):             coefs.append(multi_w0)
1.55 logistic.py(768):         n_iter[i] = n_iter_i
1.55 logistic.py(712):     for i, C in enumerate(Cs):
1.55 logistic.py(713):         if solver == 'lbfgs':
1.55 logistic.py(714):             w0, loss, info = optimize.fmin_l_bfgs_b(
1.55 logistic.py(715):                 func, w0, fprime=None,
1.55 logistic.py(716):                 args=(X, target, 1. / C, sample_weight),
1.55 logistic.py(717):                 iprint=(verbose > 0) - 1, pgtol=tol, maxiter=max_iter)
1.55 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
1.55 logistic.py(339):     n_classes = Y.shape[1]
1.55 logistic.py(340):     n_features = X.shape[1]
1.55 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
1.55 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
1.55 logistic.py(343):                     dtype=X.dtype)
1.55 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
1.55 logistic.py(282):     n_classes = Y.shape[1]
1.55 logistic.py(283):     n_features = X.shape[1]
1.55 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
1.55 logistic.py(285):     w = w.reshape(n_classes, -1)
1.55 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
1.55 logistic.py(287):     if fit_intercept:
1.55 logistic.py(288):         intercept = w[:, -1]
1.55 logistic.py(289):         w = w[:, :-1]
1.55 logistic.py(292):     p = safe_sparse_dot(X, w.T)
1.55 logistic.py(293):     p += intercept
1.55 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
1.55 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
1.55 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
1.55 logistic.py(297):     p = np.exp(p, p)
1.55 logistic.py(298):     return loss, p, w
1.55 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
1.55 logistic.py(346):     diff = sample_weight * (p - Y)
1.55 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
1.55 logistic.py(348):     grad[:, :n_features] += alpha * w
1.55 logistic.py(349):     if fit_intercept:
1.55 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
1.55 logistic.py(351):     return loss, grad.ravel(), p
1.55 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
1.56 logistic.py(339):     n_classes = Y.shape[1]
1.56 logistic.py(340):     n_features = X.shape[1]
1.56 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
1.56 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
1.56 logistic.py(343):                     dtype=X.dtype)
1.56 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
1.56 logistic.py(282):     n_classes = Y.shape[1]
1.56 logistic.py(283):     n_features = X.shape[1]
1.56 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
1.56 logistic.py(285):     w = w.reshape(n_classes, -1)
1.56 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
1.56 logistic.py(287):     if fit_intercept:
1.56 logistic.py(288):         intercept = w[:, -1]
1.56 logistic.py(289):         w = w[:, :-1]
1.56 logistic.py(292):     p = safe_sparse_dot(X, w.T)
1.56 logistic.py(293):     p += intercept
1.56 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
1.56 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
1.56 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
1.56 logistic.py(297):     p = np.exp(p, p)
1.56 logistic.py(298):     return loss, p, w
1.56 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
1.56 logistic.py(346):     diff = sample_weight * (p - Y)
1.56 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
1.56 logistic.py(348):     grad[:, :n_features] += alpha * w
1.56 logistic.py(349):     if fit_intercept:
1.56 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
1.56 logistic.py(351):     return loss, grad.ravel(), p
1.56 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
1.56 logistic.py(339):     n_classes = Y.shape[1]
1.56 logistic.py(340):     n_features = X.shape[1]
1.56 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
1.56 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
1.56 logistic.py(343):                     dtype=X.dtype)
1.56 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
1.56 logistic.py(282):     n_classes = Y.shape[1]
1.56 logistic.py(283):     n_features = X.shape[1]
1.56 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
1.56 logistic.py(285):     w = w.reshape(n_classes, -1)
1.56 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
1.56 logistic.py(287):     if fit_intercept:
1.56 logistic.py(288):         intercept = w[:, -1]
1.56 logistic.py(289):         w = w[:, :-1]
1.56 logistic.py(292):     p = safe_sparse_dot(X, w.T)
1.56 logistic.py(293):     p += intercept
1.56 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
1.56 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
1.56 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
1.56 logistic.py(297):     p = np.exp(p, p)
1.56 logistic.py(298):     return loss, p, w
1.56 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
1.56 logistic.py(346):     diff = sample_weight * (p - Y)
1.56 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
1.56 logistic.py(348):     grad[:, :n_features] += alpha * w
1.56 logistic.py(349):     if fit_intercept:
1.56 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
1.56 logistic.py(351):     return loss, grad.ravel(), p
1.56 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
1.56 logistic.py(339):     n_classes = Y.shape[1]
1.56 logistic.py(340):     n_features = X.shape[1]
1.56 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
1.56 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
1.56 logistic.py(343):                     dtype=X.dtype)
1.56 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
1.56 logistic.py(282):     n_classes = Y.shape[1]
1.56 logistic.py(283):     n_features = X.shape[1]
1.56 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
1.56 logistic.py(285):     w = w.reshape(n_classes, -1)
1.56 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
1.56 logistic.py(287):     if fit_intercept:
1.56 logistic.py(288):         intercept = w[:, -1]
1.56 logistic.py(289):         w = w[:, :-1]
1.56 logistic.py(292):     p = safe_sparse_dot(X, w.T)
1.56 logistic.py(293):     p += intercept
1.56 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
1.56 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
1.56 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
1.56 logistic.py(297):     p = np.exp(p, p)
1.56 logistic.py(298):     return loss, p, w
1.56 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
1.56 logistic.py(346):     diff = sample_weight * (p - Y)
1.56 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
1.56 logistic.py(348):     grad[:, :n_features] += alpha * w
1.56 logistic.py(349):     if fit_intercept:
1.56 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
1.56 logistic.py(351):     return loss, grad.ravel(), p
1.56 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
1.56 logistic.py(339):     n_classes = Y.shape[1]
1.56 logistic.py(340):     n_features = X.shape[1]
1.56 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
1.56 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
1.56 logistic.py(343):                     dtype=X.dtype)
1.56 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
1.56 logistic.py(282):     n_classes = Y.shape[1]
1.56 logistic.py(283):     n_features = X.shape[1]
1.56 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
1.56 logistic.py(285):     w = w.reshape(n_classes, -1)
1.56 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
1.56 logistic.py(287):     if fit_intercept:
1.56 logistic.py(288):         intercept = w[:, -1]
1.56 logistic.py(289):         w = w[:, :-1]
1.56 logistic.py(292):     p = safe_sparse_dot(X, w.T)
1.56 logistic.py(293):     p += intercept
1.56 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
1.56 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
1.56 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
1.56 logistic.py(297):     p = np.exp(p, p)
1.56 logistic.py(298):     return loss, p, w
1.56 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
1.56 logistic.py(346):     diff = sample_weight * (p - Y)
1.56 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
1.56 logistic.py(348):     grad[:, :n_features] += alpha * w
1.56 logistic.py(349):     if fit_intercept:
1.56 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
1.56 logistic.py(351):     return loss, grad.ravel(), p
1.56 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
1.56 logistic.py(339):     n_classes = Y.shape[1]
1.56 logistic.py(340):     n_features = X.shape[1]
1.56 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
1.56 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
1.56 logistic.py(343):                     dtype=X.dtype)
1.56 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
1.56 logistic.py(282):     n_classes = Y.shape[1]
1.56 logistic.py(283):     n_features = X.shape[1]
1.56 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
1.56 logistic.py(285):     w = w.reshape(n_classes, -1)
1.56 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
1.56 logistic.py(287):     if fit_intercept:
1.56 logistic.py(288):         intercept = w[:, -1]
1.56 logistic.py(289):         w = w[:, :-1]
1.56 logistic.py(292):     p = safe_sparse_dot(X, w.T)
1.56 logistic.py(293):     p += intercept
1.56 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
1.56 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
1.56 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
1.56 logistic.py(297):     p = np.exp(p, p)
1.56 logistic.py(298):     return loss, p, w
1.56 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
1.56 logistic.py(346):     diff = sample_weight * (p - Y)
1.56 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
1.56 logistic.py(348):     grad[:, :n_features] += alpha * w
1.56 logistic.py(349):     if fit_intercept:
1.56 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
1.56 logistic.py(351):     return loss, grad.ravel(), p
1.56 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
1.56 logistic.py(339):     n_classes = Y.shape[1]
1.56 logistic.py(340):     n_features = X.shape[1]
1.56 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
1.56 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
1.56 logistic.py(343):                     dtype=X.dtype)
1.56 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
1.56 logistic.py(282):     n_classes = Y.shape[1]
1.56 logistic.py(283):     n_features = X.shape[1]
1.56 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
1.56 logistic.py(285):     w = w.reshape(n_classes, -1)
1.56 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
1.56 logistic.py(287):     if fit_intercept:
1.56 logistic.py(288):         intercept = w[:, -1]
1.56 logistic.py(289):         w = w[:, :-1]
1.56 logistic.py(292):     p = safe_sparse_dot(X, w.T)
1.56 logistic.py(293):     p += intercept
1.56 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
1.56 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
1.56 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
1.56 logistic.py(297):     p = np.exp(p, p)
1.56 logistic.py(298):     return loss, p, w
1.56 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
1.56 logistic.py(346):     diff = sample_weight * (p - Y)
1.56 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
1.56 logistic.py(348):     grad[:, :n_features] += alpha * w
1.56 logistic.py(349):     if fit_intercept:
1.56 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
1.56 logistic.py(351):     return loss, grad.ravel(), p
1.56 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
1.56 logistic.py(339):     n_classes = Y.shape[1]
1.56 logistic.py(340):     n_features = X.shape[1]
1.56 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
1.56 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
1.56 logistic.py(343):                     dtype=X.dtype)
1.56 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
1.56 logistic.py(282):     n_classes = Y.shape[1]
1.56 logistic.py(283):     n_features = X.shape[1]
1.56 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
1.56 logistic.py(285):     w = w.reshape(n_classes, -1)
1.56 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
1.56 logistic.py(287):     if fit_intercept:
1.56 logistic.py(288):         intercept = w[:, -1]
1.56 logistic.py(289):         w = w[:, :-1]
1.56 logistic.py(292):     p = safe_sparse_dot(X, w.T)
1.56 logistic.py(293):     p += intercept
1.56 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
1.56 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
1.56 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
1.56 logistic.py(297):     p = np.exp(p, p)
1.56 logistic.py(298):     return loss, p, w
1.56 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
1.56 logistic.py(346):     diff = sample_weight * (p - Y)
1.56 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
1.56 logistic.py(348):     grad[:, :n_features] += alpha * w
1.56 logistic.py(349):     if fit_intercept:
1.56 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
1.56 logistic.py(351):     return loss, grad.ravel(), p
1.56 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
1.56 logistic.py(339):     n_classes = Y.shape[1]
1.56 logistic.py(340):     n_features = X.shape[1]
1.56 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
1.56 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
1.56 logistic.py(343):                     dtype=X.dtype)
1.56 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
1.56 logistic.py(282):     n_classes = Y.shape[1]
1.56 logistic.py(283):     n_features = X.shape[1]
1.56 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
1.56 logistic.py(285):     w = w.reshape(n_classes, -1)
1.56 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
1.56 logistic.py(287):     if fit_intercept:
1.56 logistic.py(288):         intercept = w[:, -1]
1.56 logistic.py(289):         w = w[:, :-1]
1.56 logistic.py(292):     p = safe_sparse_dot(X, w.T)
1.56 logistic.py(293):     p += intercept
1.56 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
1.56 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
1.56 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
1.56 logistic.py(297):     p = np.exp(p, p)
1.56 logistic.py(298):     return loss, p, w
1.56 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
1.56 logistic.py(346):     diff = sample_weight * (p - Y)
1.56 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
1.56 logistic.py(348):     grad[:, :n_features] += alpha * w
1.56 logistic.py(349):     if fit_intercept:
1.56 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
1.56 logistic.py(351):     return loss, grad.ravel(), p
1.56 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
1.56 logistic.py(339):     n_classes = Y.shape[1]
1.56 logistic.py(340):     n_features = X.shape[1]
1.56 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
1.56 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
1.56 logistic.py(343):                     dtype=X.dtype)
1.56 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
1.56 logistic.py(282):     n_classes = Y.shape[1]
1.56 logistic.py(283):     n_features = X.shape[1]
1.56 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
1.56 logistic.py(285):     w = w.reshape(n_classes, -1)
1.56 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
1.56 logistic.py(287):     if fit_intercept:
1.56 logistic.py(288):         intercept = w[:, -1]
1.56 logistic.py(289):         w = w[:, :-1]
1.56 logistic.py(292):     p = safe_sparse_dot(X, w.T)
1.56 logistic.py(293):     p += intercept
1.56 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
1.56 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
1.56 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
1.56 logistic.py(297):     p = np.exp(p, p)
1.56 logistic.py(298):     return loss, p, w
1.56 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
1.56 logistic.py(346):     diff = sample_weight * (p - Y)
1.56 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
1.56 logistic.py(348):     grad[:, :n_features] += alpha * w
1.56 logistic.py(349):     if fit_intercept:
1.56 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
1.56 logistic.py(351):     return loss, grad.ravel(), p
1.56 logistic.py(718):             if info["warnflag"] == 1:
1.56 logistic.py(723):             n_iter_i = min(info['nit'], max_iter)
1.56 logistic.py(760):         if multi_class == 'multinomial':
1.56 logistic.py(761):             multi_w0 = np.reshape(w0, (classes.size, -1))
1.56 logistic.py(762):             if classes.size == 2:
1.56 logistic.py(764):             coefs.append(multi_w0)
1.56 logistic.py(768):         n_iter[i] = n_iter_i
1.56 logistic.py(712):     for i, C in enumerate(Cs):
1.56 logistic.py(713):         if solver == 'lbfgs':
1.56 logistic.py(714):             w0, loss, info = optimize.fmin_l_bfgs_b(
1.56 logistic.py(715):                 func, w0, fprime=None,
1.56 logistic.py(716):                 args=(X, target, 1. / C, sample_weight),
1.56 logistic.py(717):                 iprint=(verbose > 0) - 1, pgtol=tol, maxiter=max_iter)
1.56 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
1.56 logistic.py(339):     n_classes = Y.shape[1]
1.56 logistic.py(340):     n_features = X.shape[1]
1.56 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
1.56 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
1.56 logistic.py(343):                     dtype=X.dtype)
1.56 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
1.56 logistic.py(282):     n_classes = Y.shape[1]
1.56 logistic.py(283):     n_features = X.shape[1]
1.56 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
1.56 logistic.py(285):     w = w.reshape(n_classes, -1)
1.56 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
1.56 logistic.py(287):     if fit_intercept:
1.56 logistic.py(288):         intercept = w[:, -1]
1.56 logistic.py(289):         w = w[:, :-1]
1.56 logistic.py(292):     p = safe_sparse_dot(X, w.T)
1.56 logistic.py(293):     p += intercept
1.56 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
1.56 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
1.56 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
1.56 logistic.py(297):     p = np.exp(p, p)
1.56 logistic.py(298):     return loss, p, w
1.56 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
1.56 logistic.py(346):     diff = sample_weight * (p - Y)
1.56 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
1.56 logistic.py(348):     grad[:, :n_features] += alpha * w
1.56 logistic.py(349):     if fit_intercept:
1.56 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
1.56 logistic.py(351):     return loss, grad.ravel(), p
1.56 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
1.56 logistic.py(339):     n_classes = Y.shape[1]
1.56 logistic.py(340):     n_features = X.shape[1]
1.56 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
1.56 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
1.56 logistic.py(343):                     dtype=X.dtype)
1.56 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
1.56 logistic.py(282):     n_classes = Y.shape[1]
1.56 logistic.py(283):     n_features = X.shape[1]
1.56 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
1.56 logistic.py(285):     w = w.reshape(n_classes, -1)
1.56 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
1.56 logistic.py(287):     if fit_intercept:
1.56 logistic.py(288):         intercept = w[:, -1]
1.56 logistic.py(289):         w = w[:, :-1]
1.56 logistic.py(292):     p = safe_sparse_dot(X, w.T)
1.56 logistic.py(293):     p += intercept
1.56 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
1.56 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
1.56 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
1.56 logistic.py(297):     p = np.exp(p, p)
1.56 logistic.py(298):     return loss, p, w
1.56 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
1.56 logistic.py(346):     diff = sample_weight * (p - Y)
1.56 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
1.56 logistic.py(348):     grad[:, :n_features] += alpha * w
1.56 logistic.py(349):     if fit_intercept:
1.56 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
1.56 logistic.py(351):     return loss, grad.ravel(), p
1.56 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
1.56 logistic.py(339):     n_classes = Y.shape[1]
1.56 logistic.py(340):     n_features = X.shape[1]
1.56 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
1.56 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
1.56 logistic.py(343):                     dtype=X.dtype)
1.56 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
1.56 logistic.py(282):     n_classes = Y.shape[1]
1.56 logistic.py(283):     n_features = X.shape[1]
1.56 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
1.56 logistic.py(285):     w = w.reshape(n_classes, -1)
1.56 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
1.56 logistic.py(287):     if fit_intercept:
1.56 logistic.py(288):         intercept = w[:, -1]
1.56 logistic.py(289):         w = w[:, :-1]
1.56 logistic.py(292):     p = safe_sparse_dot(X, w.T)
1.56 logistic.py(293):     p += intercept
1.56 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
1.56 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
1.56 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
1.56 logistic.py(297):     p = np.exp(p, p)
1.56 logistic.py(298):     return loss, p, w
1.56 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
1.56 logistic.py(346):     diff = sample_weight * (p - Y)
1.56 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
1.56 logistic.py(348):     grad[:, :n_features] += alpha * w
1.56 logistic.py(349):     if fit_intercept:
1.56 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
1.56 logistic.py(351):     return loss, grad.ravel(), p
1.56 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
1.56 logistic.py(339):     n_classes = Y.shape[1]
1.56 logistic.py(340):     n_features = X.shape[1]
1.56 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
1.56 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
1.56 logistic.py(343):                     dtype=X.dtype)
1.56 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
1.56 logistic.py(282):     n_classes = Y.shape[1]
1.56 logistic.py(283):     n_features = X.shape[1]
1.56 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
1.56 logistic.py(285):     w = w.reshape(n_classes, -1)
1.56 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
1.56 logistic.py(287):     if fit_intercept:
1.56 logistic.py(288):         intercept = w[:, -1]
1.56 logistic.py(289):         w = w[:, :-1]
1.56 logistic.py(292):     p = safe_sparse_dot(X, w.T)
1.56 logistic.py(293):     p += intercept
1.56 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
1.56 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
1.56 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
1.56 logistic.py(297):     p = np.exp(p, p)
1.56 logistic.py(298):     return loss, p, w
1.56 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
1.56 logistic.py(346):     diff = sample_weight * (p - Y)
1.56 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
1.56 logistic.py(348):     grad[:, :n_features] += alpha * w
1.56 logistic.py(349):     if fit_intercept:
1.56 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
1.56 logistic.py(351):     return loss, grad.ravel(), p
1.56 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
1.56 logistic.py(339):     n_classes = Y.shape[1]
1.56 logistic.py(340):     n_features = X.shape[1]
1.56 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
1.56 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
1.56 logistic.py(343):                     dtype=X.dtype)
1.56 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
1.56 logistic.py(282):     n_classes = Y.shape[1]
1.56 logistic.py(283):     n_features = X.shape[1]
1.56 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
1.56 logistic.py(285):     w = w.reshape(n_classes, -1)
1.56 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
1.56 logistic.py(287):     if fit_intercept:
1.56 logistic.py(288):         intercept = w[:, -1]
1.56 logistic.py(289):         w = w[:, :-1]
1.56 logistic.py(292):     p = safe_sparse_dot(X, w.T)
1.56 logistic.py(293):     p += intercept
1.56 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
1.56 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
1.56 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
1.56 logistic.py(297):     p = np.exp(p, p)
1.56 logistic.py(298):     return loss, p, w
1.56 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
1.56 logistic.py(346):     diff = sample_weight * (p - Y)
1.56 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
1.56 logistic.py(348):     grad[:, :n_features] += alpha * w
1.56 logistic.py(349):     if fit_intercept:
1.56 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
1.56 logistic.py(351):     return loss, grad.ravel(), p
1.56 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
1.56 logistic.py(339):     n_classes = Y.shape[1]
1.56 logistic.py(340):     n_features = X.shape[1]
1.56 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
1.56 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
1.56 logistic.py(343):                     dtype=X.dtype)
1.56 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
1.56 logistic.py(282):     n_classes = Y.shape[1]
1.56 logistic.py(283):     n_features = X.shape[1]
1.56 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
1.56 logistic.py(285):     w = w.reshape(n_classes, -1)
1.56 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
1.56 logistic.py(287):     if fit_intercept:
1.56 logistic.py(288):         intercept = w[:, -1]
1.56 logistic.py(289):         w = w[:, :-1]
1.56 logistic.py(292):     p = safe_sparse_dot(X, w.T)
1.56 logistic.py(293):     p += intercept
1.56 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
1.56 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
1.56 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
1.56 logistic.py(297):     p = np.exp(p, p)
1.56 logistic.py(298):     return loss, p, w
1.56 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
1.56 logistic.py(346):     diff = sample_weight * (p - Y)
1.56 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
1.56 logistic.py(348):     grad[:, :n_features] += alpha * w
1.56 logistic.py(349):     if fit_intercept:
1.56 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
1.56 logistic.py(351):     return loss, grad.ravel(), p
1.56 logistic.py(718):             if info["warnflag"] == 1:
1.56 logistic.py(723):             n_iter_i = min(info['nit'], max_iter)
1.56 logistic.py(760):         if multi_class == 'multinomial':
1.56 logistic.py(761):             multi_w0 = np.reshape(w0, (classes.size, -1))
1.56 logistic.py(762):             if classes.size == 2:
1.56 logistic.py(764):             coefs.append(multi_w0)
1.56 logistic.py(768):         n_iter[i] = n_iter_i
1.56 logistic.py(712):     for i, C in enumerate(Cs):
1.56 logistic.py(713):         if solver == 'lbfgs':
1.56 logistic.py(714):             w0, loss, info = optimize.fmin_l_bfgs_b(
1.56 logistic.py(715):                 func, w0, fprime=None,
1.56 logistic.py(716):                 args=(X, target, 1. / C, sample_weight),
1.56 logistic.py(717):                 iprint=(verbose > 0) - 1, pgtol=tol, maxiter=max_iter)
1.56 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
1.56 logistic.py(339):     n_classes = Y.shape[1]
1.56 logistic.py(340):     n_features = X.shape[1]
1.56 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
1.56 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
1.56 logistic.py(343):                     dtype=X.dtype)
1.56 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
1.56 logistic.py(282):     n_classes = Y.shape[1]
1.56 logistic.py(283):     n_features = X.shape[1]
1.56 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
1.56 logistic.py(285):     w = w.reshape(n_classes, -1)
1.56 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
1.56 logistic.py(287):     if fit_intercept:
1.56 logistic.py(288):         intercept = w[:, -1]
1.56 logistic.py(289):         w = w[:, :-1]
1.56 logistic.py(292):     p = safe_sparse_dot(X, w.T)
1.56 logistic.py(293):     p += intercept
1.57 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
1.57 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
1.57 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
1.57 logistic.py(297):     p = np.exp(p, p)
1.57 logistic.py(298):     return loss, p, w
1.57 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
1.57 logistic.py(346):     diff = sample_weight * (p - Y)
1.57 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
1.57 logistic.py(348):     grad[:, :n_features] += alpha * w
1.57 logistic.py(349):     if fit_intercept:
1.57 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
1.57 logistic.py(351):     return loss, grad.ravel(), p
1.57 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
1.57 logistic.py(339):     n_classes = Y.shape[1]
1.57 logistic.py(340):     n_features = X.shape[1]
1.57 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
1.57 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
1.57 logistic.py(343):                     dtype=X.dtype)
1.57 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
1.57 logistic.py(282):     n_classes = Y.shape[1]
1.57 logistic.py(283):     n_features = X.shape[1]
1.57 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
1.57 logistic.py(285):     w = w.reshape(n_classes, -1)
1.57 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
1.57 logistic.py(287):     if fit_intercept:
1.57 logistic.py(288):         intercept = w[:, -1]
1.57 logistic.py(289):         w = w[:, :-1]
1.57 logistic.py(292):     p = safe_sparse_dot(X, w.T)
1.57 logistic.py(293):     p += intercept
1.57 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
1.57 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
1.57 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
1.57 logistic.py(297):     p = np.exp(p, p)
1.57 logistic.py(298):     return loss, p, w
1.57 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
1.57 logistic.py(346):     diff = sample_weight * (p - Y)
1.57 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
1.57 logistic.py(348):     grad[:, :n_features] += alpha * w
1.57 logistic.py(349):     if fit_intercept:
1.57 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
1.57 logistic.py(351):     return loss, grad.ravel(), p
1.57 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
1.57 logistic.py(339):     n_classes = Y.shape[1]
1.57 logistic.py(340):     n_features = X.shape[1]
1.57 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
1.57 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
1.57 logistic.py(343):                     dtype=X.dtype)
1.57 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
1.57 logistic.py(282):     n_classes = Y.shape[1]
1.57 logistic.py(283):     n_features = X.shape[1]
1.57 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
1.57 logistic.py(285):     w = w.reshape(n_classes, -1)
1.57 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
1.57 logistic.py(287):     if fit_intercept:
1.57 logistic.py(288):         intercept = w[:, -1]
1.57 logistic.py(289):         w = w[:, :-1]
1.57 logistic.py(292):     p = safe_sparse_dot(X, w.T)
1.57 logistic.py(293):     p += intercept
1.57 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
1.57 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
1.57 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
1.57 logistic.py(297):     p = np.exp(p, p)
1.57 logistic.py(298):     return loss, p, w
1.57 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
1.57 logistic.py(346):     diff = sample_weight * (p - Y)
1.57 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
1.57 logistic.py(348):     grad[:, :n_features] += alpha * w
1.57 logistic.py(349):     if fit_intercept:
1.57 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
1.57 logistic.py(351):     return loss, grad.ravel(), p
1.57 logistic.py(718):             if info["warnflag"] == 1:
1.57 logistic.py(723):             n_iter_i = min(info['nit'], max_iter)
1.57 logistic.py(760):         if multi_class == 'multinomial':
1.57 logistic.py(761):             multi_w0 = np.reshape(w0, (classes.size, -1))
1.57 logistic.py(762):             if classes.size == 2:
1.57 logistic.py(764):             coefs.append(multi_w0)
1.57 logistic.py(768):         n_iter[i] = n_iter_i
1.57 logistic.py(712):     for i, C in enumerate(Cs):
1.57 logistic.py(770):     return coefs, np.array(Cs), n_iter
1.57 logistic.py(925):     log_reg = LogisticRegression(multi_class=multi_class)
1.57 logistic.py(1171):         self.penalty = penalty
1.57 logistic.py(1172):         self.dual = dual
1.57 logistic.py(1173):         self.tol = tol
1.57 logistic.py(1174):         self.C = C
1.57 logistic.py(1175):         self.fit_intercept = fit_intercept
1.57 logistic.py(1176):         self.intercept_scaling = intercept_scaling
1.57 logistic.py(1177):         self.class_weight = class_weight
1.57 logistic.py(1178):         self.random_state = random_state
1.57 logistic.py(1179):         self.solver = solver
1.57 logistic.py(1180):         self.max_iter = max_iter
1.57 logistic.py(1181):         self.multi_class = multi_class
1.57 logistic.py(1182):         self.verbose = verbose
1.57 logistic.py(1183):         self.warm_start = warm_start
1.57 logistic.py(1184):         self.n_jobs = n_jobs
1.57 logistic.py(928):     if multi_class == 'ovr':
1.57 logistic.py(930):     elif multi_class == 'multinomial':
1.57 logistic.py(931):         log_reg.classes_ = np.unique(y_train)
1.57 logistic.py(936):     if pos_class is not None:
1.57 logistic.py(941):     scores = list()
1.57 logistic.py(943):     if isinstance(scoring, six.string_types):
1.57 logistic.py(945):     for w in coefs:
1.57 logistic.py(946):         if multi_class == 'ovr':
1.57 logistic.py(948):         if fit_intercept:
1.57 logistic.py(949):             log_reg.coef_ = w[:, :-1]
1.57 logistic.py(950):             log_reg.intercept_ = w[:, -1]
1.57 logistic.py(955):         if scoring is None:
1.57 logistic.py(956):             scores.append(log_reg.score(X_test, y_test))
1.57 logistic.py(945):     for w in coefs:
1.57 logistic.py(946):         if multi_class == 'ovr':
1.57 logistic.py(948):         if fit_intercept:
1.57 logistic.py(949):             log_reg.coef_ = w[:, :-1]
1.57 logistic.py(950):             log_reg.intercept_ = w[:, -1]
1.57 logistic.py(955):         if scoring is None:
1.57 logistic.py(956):             scores.append(log_reg.score(X_test, y_test))
1.57 logistic.py(945):     for w in coefs:
1.57 logistic.py(946):         if multi_class == 'ovr':
1.57 logistic.py(948):         if fit_intercept:
1.57 logistic.py(949):             log_reg.coef_ = w[:, :-1]
1.57 logistic.py(950):             log_reg.intercept_ = w[:, -1]
1.57 logistic.py(955):         if scoring is None:
1.57 logistic.py(956):             scores.append(log_reg.score(X_test, y_test))
1.57 logistic.py(945):     for w in coefs:
1.57 logistic.py(946):         if multi_class == 'ovr':
1.57 logistic.py(948):         if fit_intercept:
1.57 logistic.py(949):             log_reg.coef_ = w[:, :-1]
1.57 logistic.py(950):             log_reg.intercept_ = w[:, -1]
1.57 logistic.py(955):         if scoring is None:
1.57 logistic.py(956):             scores.append(log_reg.score(X_test, y_test))
1.57 logistic.py(945):     for w in coefs:
1.57 logistic.py(946):         if multi_class == 'ovr':
1.57 logistic.py(948):         if fit_intercept:
1.57 logistic.py(949):             log_reg.coef_ = w[:, :-1]
1.57 logistic.py(950):             log_reg.intercept_ = w[:, -1]
1.57 logistic.py(955):         if scoring is None:
1.57 logistic.py(956):             scores.append(log_reg.score(X_test, y_test))
1.57 logistic.py(945):     for w in coefs:
1.57 logistic.py(946):         if multi_class == 'ovr':
1.57 logistic.py(948):         if fit_intercept:
1.57 logistic.py(949):             log_reg.coef_ = w[:, :-1]
1.57 logistic.py(950):             log_reg.intercept_ = w[:, -1]
1.57 logistic.py(955):         if scoring is None:
1.57 logistic.py(956):             scores.append(log_reg.score(X_test, y_test))
1.57 logistic.py(945):     for w in coefs:
1.57 logistic.py(946):         if multi_class == 'ovr':
1.57 logistic.py(948):         if fit_intercept:
1.57 logistic.py(949):             log_reg.coef_ = w[:, :-1]
1.57 logistic.py(950):             log_reg.intercept_ = w[:, -1]
1.57 logistic.py(955):         if scoring is None:
1.57 logistic.py(956):             scores.append(log_reg.score(X_test, y_test))
1.57 logistic.py(945):     for w in coefs:
1.57 logistic.py(946):         if multi_class == 'ovr':
1.57 logistic.py(948):         if fit_intercept:
1.57 logistic.py(949):             log_reg.coef_ = w[:, :-1]
1.57 logistic.py(950):             log_reg.intercept_ = w[:, -1]
1.57 logistic.py(955):         if scoring is None:
1.57 logistic.py(956):             scores.append(log_reg.score(X_test, y_test))
1.57 logistic.py(945):     for w in coefs:
1.57 logistic.py(946):         if multi_class == 'ovr':
1.57 logistic.py(948):         if fit_intercept:
1.57 logistic.py(949):             log_reg.coef_ = w[:, :-1]
1.57 logistic.py(950):             log_reg.intercept_ = w[:, -1]
1.57 logistic.py(955):         if scoring is None:
1.57 logistic.py(956):             scores.append(log_reg.score(X_test, y_test))
1.57 logistic.py(945):     for w in coefs:
1.57 logistic.py(946):         if multi_class == 'ovr':
1.57 logistic.py(948):         if fit_intercept:
1.57 logistic.py(949):             log_reg.coef_ = w[:, :-1]
1.57 logistic.py(950):             log_reg.intercept_ = w[:, -1]
1.57 logistic.py(955):         if scoring is None:
1.57 logistic.py(956):             scores.append(log_reg.score(X_test, y_test))
1.57 logistic.py(945):     for w in coefs:
1.57 logistic.py(959):     return coefs, Cs, np.array(scores), n_iter
1.57 logistic.py(1703):             for train, test in folds)
1.57 logistic.py(1691):             path_func(X, y, train, test, pos_class=label, Cs=self.Cs,
1.57 logistic.py(1705):         if self.multi_class == 'multinomial':
1.57 logistic.py(1706):             multi_coefs_paths, Cs, multi_scores, n_iter_ = zip(*fold_coefs_)
1.57 logistic.py(1707):             multi_coefs_paths = np.asarray(multi_coefs_paths)
1.57 logistic.py(1708):             multi_scores = np.asarray(multi_scores)
1.57 logistic.py(1715):             coefs_paths = np.rollaxis(multi_coefs_paths, 2, 0)
1.57 logistic.py(1720):             scores = np.tile(multi_scores, (n_classes, 1, 1))
1.57 logistic.py(1721):             self.Cs_ = Cs[0]
1.57 logistic.py(1722):             self.n_iter_ = np.reshape(n_iter_, (1, len(folds),
1.57 logistic.py(1723):                                                 len(self.Cs_)))
1.57 logistic.py(1733):         self.coefs_paths_ = dict(zip(classes, coefs_paths))
1.57 logistic.py(1734):         scores = np.reshape(scores, (n_classes, len(folds), -1))
1.57 logistic.py(1735):         self.scores_ = dict(zip(classes, scores))
1.57 logistic.py(1737):         self.C_ = list()
1.57 logistic.py(1738):         self.coef_ = np.empty((n_classes, X.shape[1]))
1.57 logistic.py(1739):         self.intercept_ = np.zeros(n_classes)
1.57 logistic.py(1742):         if self.multi_class == 'multinomial':
1.57 logistic.py(1743):             scores = multi_scores
1.57 logistic.py(1744):             coefs_paths = multi_coefs_paths
1.57 logistic.py(1746):         for index, (cls, encoded_label) in enumerate(
1.57 logistic.py(1747):                 zip(iter_classes, iter_encoded_labels)):
1.57 logistic.py(1749):             if self.multi_class == 'ovr':
1.57 logistic.py(1755):             if self.refit:
1.57 logistic.py(1756):                 best_index = scores.sum(axis=0).argmax()
1.57 logistic.py(1758):                 C_ = self.Cs_[best_index]
1.57 logistic.py(1759):                 self.C_.append(C_)
1.57 logistic.py(1760):                 if self.multi_class == 'multinomial':
1.57 logistic.py(1761):                     coef_init = np.mean(coefs_paths[:, best_index, :, :],
1.57 logistic.py(1762):                                         axis=0)
1.57 logistic.py(1768):                 w, _, _ = logistic_regression_path(
1.57 logistic.py(1769):                     X, y, pos_class=encoded_label, Cs=[C_], solver=self.solver,
1.57 logistic.py(1770):                     fit_intercept=self.fit_intercept, coef=coef_init,
1.57 logistic.py(1771):                     max_iter=self.max_iter, tol=self.tol,
1.57 logistic.py(1772):                     penalty=self.penalty,
1.57 logistic.py(1773):                     class_weight=class_weight,
1.57 logistic.py(1774):                     multi_class=self.multi_class,
1.57 logistic.py(1775):                     verbose=max(0, self.verbose - 1),
1.57 logistic.py(1776):                     random_state=self.random_state,
1.57 logistic.py(1777):                     check_input=False, max_squared_sum=max_squared_sum,
1.57 logistic.py(1778):                     sample_weight=sample_weight)
1.57 logistic.py(591):     if isinstance(Cs, numbers.Integral):
1.57 logistic.py(594):     _check_solver_option(solver, multi_class, penalty, dual)
1.57 logistic.py(428):     if solver not in ['liblinear', 'newton-cg', 'lbfgs', 'sag', 'saga']:
1.57 logistic.py(433):     if multi_class not in ['multinomial', 'ovr']:
1.57 logistic.py(437):     if multi_class == 'multinomial' and solver == 'liblinear':
1.57 logistic.py(441):     if solver not in ['liblinear', 'saga']:
1.57 logistic.py(442):         if penalty != 'l2':
1.57 logistic.py(445):     if solver != 'liblinear':
1.57 logistic.py(446):         if dual:
1.57 logistic.py(597):     if check_input:
1.57 logistic.py(602):     _, n_features = X.shape
1.57 logistic.py(603):     classes = np.unique(y)
1.57 logistic.py(604):     random_state = check_random_state(random_state)
1.57 logistic.py(606):     if pos_class is None and multi_class != 'multinomial':
1.57 logistic.py(615):     if sample_weight is not None:
1.57 logistic.py(619):         sample_weight = np.ones(X.shape[0], dtype=X.dtype)
1.57 logistic.py(624):     le = LabelEncoder()
1.57 logistic.py(625):     if isinstance(class_weight, dict) or multi_class == 'multinomial':
1.57 logistic.py(626):         class_weight_ = compute_class_weight(class_weight, classes, y)
1.57 logistic.py(627):         sample_weight *= class_weight_[le.fit_transform(y)]
1.57 logistic.py(631):     if multi_class == 'ovr':
1.57 logistic.py(645):         if solver not in ['sag', 'saga']:
1.57 logistic.py(646):             lbin = LabelBinarizer()
1.57 logistic.py(647):             Y_multi = lbin.fit_transform(y)
1.57 logistic.py(648):             if Y_multi.shape[1] == 1:
1.57 logistic.py(655):         w0 = np.zeros((classes.size, n_features + int(fit_intercept)),
1.57 logistic.py(656):                       order='F', dtype=X.dtype)
1.57 logistic.py(658):     if coef is not None:
1.57 logistic.py(660):         if multi_class == 'ovr':
1.57 logistic.py(669):             n_classes = classes.size
1.57 logistic.py(670):             if n_classes == 2:
1.57 logistic.py(673):             if (coef.shape[0] != n_classes or
1.57 logistic.py(674):                     coef.shape[1] not in (n_features, n_features + 1)):
1.57 logistic.py(681):             if n_classes == 1:
1.57 logistic.py(685):                 w0[:, :coef.shape[1]] = coef
1.57 logistic.py(688):     if multi_class == 'multinomial':
1.57 logistic.py(690):         if solver in ['lbfgs', 'newton-cg']:
1.57 logistic.py(691):             w0 = w0.ravel()
1.57 logistic.py(692):         target = Y_multi
1.57 logistic.py(693):         if solver == 'lbfgs':
1.57 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
1.57 logistic.py(699):         warm_start_sag = {'coef': w0.T}
1.57 logistic.py(710):     coefs = list()
1.57 logistic.py(711):     n_iter = np.zeros(len(Cs), dtype=np.int32)
1.57 logistic.py(712):     for i, C in enumerate(Cs):
1.57 logistic.py(713):         if solver == 'lbfgs':
1.57 logistic.py(714):             w0, loss, info = optimize.fmin_l_bfgs_b(
1.57 logistic.py(715):                 func, w0, fprime=None,
1.57 logistic.py(716):                 args=(X, target, 1. / C, sample_weight),
1.57 logistic.py(717):                 iprint=(verbose > 0) - 1, pgtol=tol, maxiter=max_iter)
1.57 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
1.57 logistic.py(339):     n_classes = Y.shape[1]
1.57 logistic.py(340):     n_features = X.shape[1]
1.57 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
1.57 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
1.57 logistic.py(343):                     dtype=X.dtype)
1.57 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
1.57 logistic.py(282):     n_classes = Y.shape[1]
1.57 logistic.py(283):     n_features = X.shape[1]
1.57 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
1.57 logistic.py(285):     w = w.reshape(n_classes, -1)
1.57 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
1.57 logistic.py(287):     if fit_intercept:
1.57 logistic.py(288):         intercept = w[:, -1]
1.57 logistic.py(289):         w = w[:, :-1]
1.57 logistic.py(292):     p = safe_sparse_dot(X, w.T)
1.57 logistic.py(293):     p += intercept
1.57 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
1.57 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
1.57 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
1.57 logistic.py(297):     p = np.exp(p, p)
1.57 logistic.py(298):     return loss, p, w
1.57 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
1.57 logistic.py(346):     diff = sample_weight * (p - Y)
1.57 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
1.57 logistic.py(348):     grad[:, :n_features] += alpha * w
1.57 logistic.py(349):     if fit_intercept:
1.57 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
1.57 logistic.py(351):     return loss, grad.ravel(), p
1.57 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
1.57 logistic.py(339):     n_classes = Y.shape[1]
1.57 logistic.py(340):     n_features = X.shape[1]
1.57 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
1.57 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
1.57 logistic.py(343):                     dtype=X.dtype)
1.57 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
1.57 logistic.py(282):     n_classes = Y.shape[1]
1.57 logistic.py(283):     n_features = X.shape[1]
1.57 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
1.57 logistic.py(285):     w = w.reshape(n_classes, -1)
1.57 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
1.57 logistic.py(287):     if fit_intercept:
1.57 logistic.py(288):         intercept = w[:, -1]
1.57 logistic.py(289):         w = w[:, :-1]
1.57 logistic.py(292):     p = safe_sparse_dot(X, w.T)
1.57 logistic.py(293):     p += intercept
1.57 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
1.57 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
1.57 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
1.57 logistic.py(297):     p = np.exp(p, p)
1.57 logistic.py(298):     return loss, p, w
1.57 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
1.57 logistic.py(346):     diff = sample_weight * (p - Y)
1.57 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
1.57 logistic.py(348):     grad[:, :n_features] += alpha * w
1.57 logistic.py(349):     if fit_intercept:
1.57 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
1.57 logistic.py(351):     return loss, grad.ravel(), p
1.57 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
1.57 logistic.py(339):     n_classes = Y.shape[1]
1.57 logistic.py(340):     n_features = X.shape[1]
1.57 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
1.57 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
1.57 logistic.py(343):                     dtype=X.dtype)
1.57 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
1.57 logistic.py(282):     n_classes = Y.shape[1]
1.57 logistic.py(283):     n_features = X.shape[1]
1.57 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
1.57 logistic.py(285):     w = w.reshape(n_classes, -1)
1.57 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
1.57 logistic.py(287):     if fit_intercept:
1.57 logistic.py(288):         intercept = w[:, -1]
1.57 logistic.py(289):         w = w[:, :-1]
1.57 logistic.py(292):     p = safe_sparse_dot(X, w.T)
1.57 logistic.py(293):     p += intercept
1.57 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
1.57 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
1.57 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
1.57 logistic.py(297):     p = np.exp(p, p)
1.57 logistic.py(298):     return loss, p, w
1.57 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
1.57 logistic.py(346):     diff = sample_weight * (p - Y)
1.57 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
1.57 logistic.py(348):     grad[:, :n_features] += alpha * w
1.57 logistic.py(349):     if fit_intercept:
1.57 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
1.57 logistic.py(351):     return loss, grad.ravel(), p
1.57 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
1.57 logistic.py(339):     n_classes = Y.shape[1]
1.57 logistic.py(340):     n_features = X.shape[1]
1.57 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
1.57 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
1.57 logistic.py(343):                     dtype=X.dtype)
1.57 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
1.57 logistic.py(282):     n_classes = Y.shape[1]
1.57 logistic.py(283):     n_features = X.shape[1]
1.57 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
1.57 logistic.py(285):     w = w.reshape(n_classes, -1)
1.57 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
1.57 logistic.py(287):     if fit_intercept:
1.57 logistic.py(288):         intercept = w[:, -1]
1.57 logistic.py(289):         w = w[:, :-1]
1.57 logistic.py(292):     p = safe_sparse_dot(X, w.T)
1.57 logistic.py(293):     p += intercept
1.57 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
1.57 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
1.57 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
1.57 logistic.py(297):     p = np.exp(p, p)
1.57 logistic.py(298):     return loss, p, w
1.57 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
1.57 logistic.py(346):     diff = sample_weight * (p - Y)
1.57 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
1.57 logistic.py(348):     grad[:, :n_features] += alpha * w
1.57 logistic.py(349):     if fit_intercept:
1.57 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
1.57 logistic.py(351):     return loss, grad.ravel(), p
1.57 logistic.py(718):             if info["warnflag"] == 1:
1.57 logistic.py(723):             n_iter_i = min(info['nit'], max_iter)
1.57 logistic.py(760):         if multi_class == 'multinomial':
1.57 logistic.py(761):             multi_w0 = np.reshape(w0, (classes.size, -1))
1.57 logistic.py(762):             if classes.size == 2:
1.58 logistic.py(764):             coefs.append(multi_w0)
1.58 logistic.py(768):         n_iter[i] = n_iter_i
1.58 logistic.py(712):     for i, C in enumerate(Cs):
1.58 logistic.py(770):     return coefs, np.array(Cs), n_iter
1.58 logistic.py(1779):                 w = w[0]
1.58 logistic.py(1789):             if self.multi_class == 'multinomial':
1.58 logistic.py(1790):                 self.C_ = np.tile(self.C_, n_classes)
1.58 logistic.py(1791):                 self.coef_ = w[:, :X.shape[1]]
1.58 logistic.py(1792):                 if self.fit_intercept:
1.58 logistic.py(1793):                     self.intercept_ = w[:, -1]
1.58 logistic.py(1747):                 zip(iter_classes, iter_encoded_labels)):
1.58 logistic.py(1799):         self.C_ = np.asarray(self.C_)
1.58 logistic.py(1800):         return self
1.58 logistic.py(1341):         if not hasattr(self, "coef_"):
1.58 logistic.py(1343):         if self.multi_class == "ovr":
1.58 logistic.py(1346):             decision = self.decision_function(X)
1.58 logistic.py(1347):             if decision.ndim == 1:
1.58 logistic.py(1352):                 decision_2d = decision
1.58 logistic.py(1353):             return softmax(decision_2d, copy=False)
1.58 logistic.py(903):     _check_solver_option(solver, multi_class, penalty, dual)
1.58 logistic.py(428):     if solver not in ['liblinear', 'newton-cg', 'lbfgs', 'sag', 'saga']:
1.58 logistic.py(433):     if multi_class not in ['multinomial', 'ovr']:
1.58 logistic.py(437):     if multi_class == 'multinomial' and solver == 'liblinear':
1.58 logistic.py(441):     if solver not in ['liblinear', 'saga']:
1.58 logistic.py(442):         if penalty != 'l2':
1.58 logistic.py(445):     if solver != 'liblinear':
1.58 logistic.py(446):         if dual:
1.58 logistic.py(905):     X_train = X[train]
1.58 logistic.py(906):     X_test = X[test]
1.58 logistic.py(907):     y_train = y[train]
1.58 logistic.py(908):     y_test = y[test]
1.58 logistic.py(910):     if sample_weight is not None:
1.58 logistic.py(916):     coefs, Cs, n_iter = logistic_regression_path(
1.58 logistic.py(917):         X_train, y_train, Cs=Cs, fit_intercept=fit_intercept,
1.58 logistic.py(918):         solver=solver, max_iter=max_iter, class_weight=class_weight,
1.58 logistic.py(919):         pos_class=pos_class, multi_class=multi_class,
1.58 logistic.py(920):         tol=tol, verbose=verbose, dual=dual, penalty=penalty,
1.58 logistic.py(921):         intercept_scaling=intercept_scaling, random_state=random_state,
1.58 logistic.py(922):         check_input=False, max_squared_sum=max_squared_sum,
1.58 logistic.py(923):         sample_weight=sample_weight)
1.58 logistic.py(591):     if isinstance(Cs, numbers.Integral):
1.58 logistic.py(592):         Cs = np.logspace(-4, 4, Cs)
1.58 logistic.py(594):     _check_solver_option(solver, multi_class, penalty, dual)
1.58 logistic.py(428):     if solver not in ['liblinear', 'newton-cg', 'lbfgs', 'sag', 'saga']:
1.58 logistic.py(433):     if multi_class not in ['multinomial', 'ovr']:
1.58 logistic.py(437):     if multi_class == 'multinomial' and solver == 'liblinear':
1.58 logistic.py(441):     if solver not in ['liblinear', 'saga']:
1.58 logistic.py(442):         if penalty != 'l2':
1.58 logistic.py(445):     if solver != 'liblinear':
1.58 logistic.py(446):         if dual:
1.58 logistic.py(597):     if check_input:
1.58 logistic.py(602):     _, n_features = X.shape
1.58 logistic.py(603):     classes = np.unique(y)
1.58 logistic.py(604):     random_state = check_random_state(random_state)
1.58 logistic.py(606):     if pos_class is None and multi_class != 'multinomial':
1.58 logistic.py(615):     if sample_weight is not None:
1.58 logistic.py(619):         sample_weight = np.ones(X.shape[0], dtype=X.dtype)
1.58 logistic.py(624):     le = LabelEncoder()
1.58 logistic.py(625):     if isinstance(class_weight, dict) or multi_class == 'multinomial':
1.58 logistic.py(626):         class_weight_ = compute_class_weight(class_weight, classes, y)
1.58 logistic.py(627):         sample_weight *= class_weight_[le.fit_transform(y)]
1.58 logistic.py(631):     if multi_class == 'ovr':
1.58 logistic.py(645):         if solver not in ['sag', 'saga']:
1.58 logistic.py(646):             lbin = LabelBinarizer()
1.58 logistic.py(647):             Y_multi = lbin.fit_transform(y)
1.58 logistic.py(648):             if Y_multi.shape[1] == 1:
1.58 logistic.py(655):         w0 = np.zeros((classes.size, n_features + int(fit_intercept)),
1.58 logistic.py(656):                       order='F', dtype=X.dtype)
1.58 logistic.py(658):     if coef is not None:
1.58 logistic.py(688):     if multi_class == 'multinomial':
1.58 logistic.py(690):         if solver in ['lbfgs', 'newton-cg']:
1.58 logistic.py(691):             w0 = w0.ravel()
1.58 logistic.py(692):         target = Y_multi
1.58 logistic.py(693):         if solver == 'lbfgs':
1.58 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
1.58 logistic.py(699):         warm_start_sag = {'coef': w0.T}
1.58 logistic.py(710):     coefs = list()
1.58 logistic.py(711):     n_iter = np.zeros(len(Cs), dtype=np.int32)
1.58 logistic.py(712):     for i, C in enumerate(Cs):
1.58 logistic.py(713):         if solver == 'lbfgs':
1.58 logistic.py(714):             w0, loss, info = optimize.fmin_l_bfgs_b(
1.58 logistic.py(715):                 func, w0, fprime=None,
1.58 logistic.py(716):                 args=(X, target, 1. / C, sample_weight),
1.58 logistic.py(717):                 iprint=(verbose > 0) - 1, pgtol=tol, maxiter=max_iter)
1.58 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
1.58 logistic.py(339):     n_classes = Y.shape[1]
1.58 logistic.py(340):     n_features = X.shape[1]
1.58 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
1.58 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
1.58 logistic.py(343):                     dtype=X.dtype)
1.58 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
1.58 logistic.py(282):     n_classes = Y.shape[1]
1.58 logistic.py(283):     n_features = X.shape[1]
1.58 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
1.58 logistic.py(285):     w = w.reshape(n_classes, -1)
1.58 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
1.58 logistic.py(287):     if fit_intercept:
1.58 logistic.py(288):         intercept = w[:, -1]
1.58 logistic.py(289):         w = w[:, :-1]
1.58 logistic.py(292):     p = safe_sparse_dot(X, w.T)
1.58 logistic.py(293):     p += intercept
1.58 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
1.58 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
1.58 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
1.58 logistic.py(297):     p = np.exp(p, p)
1.58 logistic.py(298):     return loss, p, w
1.58 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
1.58 logistic.py(346):     diff = sample_weight * (p - Y)
1.58 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
1.58 logistic.py(348):     grad[:, :n_features] += alpha * w
1.58 logistic.py(349):     if fit_intercept:
1.58 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
1.58 logistic.py(351):     return loss, grad.ravel(), p
1.58 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
1.58 logistic.py(339):     n_classes = Y.shape[1]
1.58 logistic.py(340):     n_features = X.shape[1]
1.58 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
1.58 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
1.58 logistic.py(343):                     dtype=X.dtype)
1.58 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
1.58 logistic.py(282):     n_classes = Y.shape[1]
1.58 logistic.py(283):     n_features = X.shape[1]
1.58 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
1.58 logistic.py(285):     w = w.reshape(n_classes, -1)
1.58 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
1.58 logistic.py(287):     if fit_intercept:
1.58 logistic.py(288):         intercept = w[:, -1]
1.58 logistic.py(289):         w = w[:, :-1]
1.58 logistic.py(292):     p = safe_sparse_dot(X, w.T)
1.58 logistic.py(293):     p += intercept
1.58 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
1.58 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
1.58 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
1.58 logistic.py(297):     p = np.exp(p, p)
1.58 logistic.py(298):     return loss, p, w
1.58 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
1.58 logistic.py(346):     diff = sample_weight * (p - Y)
1.58 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
1.58 logistic.py(348):     grad[:, :n_features] += alpha * w
1.58 logistic.py(349):     if fit_intercept:
1.58 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
1.58 logistic.py(351):     return loss, grad.ravel(), p
1.58 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
1.58 logistic.py(339):     n_classes = Y.shape[1]
1.58 logistic.py(340):     n_features = X.shape[1]
1.58 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
1.58 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
1.58 logistic.py(343):                     dtype=X.dtype)
1.58 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
1.58 logistic.py(282):     n_classes = Y.shape[1]
1.58 logistic.py(283):     n_features = X.shape[1]
1.58 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
1.58 logistic.py(285):     w = w.reshape(n_classes, -1)
1.58 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
1.58 logistic.py(287):     if fit_intercept:
1.58 logistic.py(288):         intercept = w[:, -1]
1.58 logistic.py(289):         w = w[:, :-1]
1.58 logistic.py(292):     p = safe_sparse_dot(X, w.T)
1.58 logistic.py(293):     p += intercept
1.58 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
1.58 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
1.58 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
1.58 logistic.py(297):     p = np.exp(p, p)
1.58 logistic.py(298):     return loss, p, w
1.58 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
1.58 logistic.py(346):     diff = sample_weight * (p - Y)
1.58 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
1.58 logistic.py(348):     grad[:, :n_features] += alpha * w
1.58 logistic.py(349):     if fit_intercept:
1.58 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
1.58 logistic.py(351):     return loss, grad.ravel(), p
1.58 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
1.58 logistic.py(339):     n_classes = Y.shape[1]
1.58 logistic.py(340):     n_features = X.shape[1]
1.58 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
1.58 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
1.58 logistic.py(343):                     dtype=X.dtype)
1.58 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
1.58 logistic.py(282):     n_classes = Y.shape[1]
1.58 logistic.py(283):     n_features = X.shape[1]
1.58 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
1.58 logistic.py(285):     w = w.reshape(n_classes, -1)
1.58 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
1.58 logistic.py(287):     if fit_intercept:
1.58 logistic.py(288):         intercept = w[:, -1]
1.58 logistic.py(289):         w = w[:, :-1]
1.58 logistic.py(292):     p = safe_sparse_dot(X, w.T)
1.58 logistic.py(293):     p += intercept
1.58 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
1.58 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
1.58 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
1.58 logistic.py(297):     p = np.exp(p, p)
1.58 logistic.py(298):     return loss, p, w
1.58 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
1.58 logistic.py(346):     diff = sample_weight * (p - Y)
1.58 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
1.58 logistic.py(348):     grad[:, :n_features] += alpha * w
1.58 logistic.py(349):     if fit_intercept:
1.58 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
1.58 logistic.py(351):     return loss, grad.ravel(), p
1.58 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
1.58 logistic.py(339):     n_classes = Y.shape[1]
1.58 logistic.py(340):     n_features = X.shape[1]
1.58 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
1.58 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
1.58 logistic.py(343):                     dtype=X.dtype)
1.58 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
1.58 logistic.py(282):     n_classes = Y.shape[1]
1.58 logistic.py(283):     n_features = X.shape[1]
1.58 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
1.58 logistic.py(285):     w = w.reshape(n_classes, -1)
1.58 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
1.58 logistic.py(287):     if fit_intercept:
1.58 logistic.py(288):         intercept = w[:, -1]
1.58 logistic.py(289):         w = w[:, :-1]
1.58 logistic.py(292):     p = safe_sparse_dot(X, w.T)
1.58 logistic.py(293):     p += intercept
1.58 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
1.58 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
1.58 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
1.58 logistic.py(297):     p = np.exp(p, p)
1.58 logistic.py(298):     return loss, p, w
1.58 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
1.58 logistic.py(346):     diff = sample_weight * (p - Y)
1.58 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
1.58 logistic.py(348):     grad[:, :n_features] += alpha * w
1.58 logistic.py(349):     if fit_intercept:
1.58 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
1.58 logistic.py(351):     return loss, grad.ravel(), p
1.58 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
1.58 logistic.py(339):     n_classes = Y.shape[1]
1.58 logistic.py(340):     n_features = X.shape[1]
1.58 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
1.58 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
1.58 logistic.py(343):                     dtype=X.dtype)
1.58 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
1.58 logistic.py(282):     n_classes = Y.shape[1]
1.58 logistic.py(283):     n_features = X.shape[1]
1.58 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
1.58 logistic.py(285):     w = w.reshape(n_classes, -1)
1.58 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
1.58 logistic.py(287):     if fit_intercept:
1.58 logistic.py(288):         intercept = w[:, -1]
1.58 logistic.py(289):         w = w[:, :-1]
1.58 logistic.py(292):     p = safe_sparse_dot(X, w.T)
1.58 logistic.py(293):     p += intercept
1.58 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
1.58 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
1.58 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
1.58 logistic.py(297):     p = np.exp(p, p)
1.58 logistic.py(298):     return loss, p, w
1.58 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
1.58 logistic.py(346):     diff = sample_weight * (p - Y)
1.58 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
1.58 logistic.py(348):     grad[:, :n_features] += alpha * w
1.58 logistic.py(349):     if fit_intercept:
1.58 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
1.58 logistic.py(351):     return loss, grad.ravel(), p
1.58 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
1.58 logistic.py(339):     n_classes = Y.shape[1]
1.58 logistic.py(340):     n_features = X.shape[1]
1.58 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
1.58 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
1.58 logistic.py(343):                     dtype=X.dtype)
1.58 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
1.58 logistic.py(282):     n_classes = Y.shape[1]
1.58 logistic.py(283):     n_features = X.shape[1]
1.58 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
1.58 logistic.py(285):     w = w.reshape(n_classes, -1)
1.58 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
1.58 logistic.py(287):     if fit_intercept:
1.58 logistic.py(288):         intercept = w[:, -1]
1.58 logistic.py(289):         w = w[:, :-1]
1.58 logistic.py(292):     p = safe_sparse_dot(X, w.T)
1.58 logistic.py(293):     p += intercept
1.58 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
1.58 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
1.58 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
1.58 logistic.py(297):     p = np.exp(p, p)
1.58 logistic.py(298):     return loss, p, w
1.58 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
1.58 logistic.py(346):     diff = sample_weight * (p - Y)
1.58 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
1.58 logistic.py(348):     grad[:, :n_features] += alpha * w
1.58 logistic.py(349):     if fit_intercept:
1.58 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
1.58 logistic.py(351):     return loss, grad.ravel(), p
1.58 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
1.58 logistic.py(339):     n_classes = Y.shape[1]
1.58 logistic.py(340):     n_features = X.shape[1]
1.58 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
1.58 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
1.58 logistic.py(343):                     dtype=X.dtype)
1.58 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
1.58 logistic.py(282):     n_classes = Y.shape[1]
1.58 logistic.py(283):     n_features = X.shape[1]
1.58 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
1.58 logistic.py(285):     w = w.reshape(n_classes, -1)
1.58 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
1.58 logistic.py(287):     if fit_intercept:
1.58 logistic.py(288):         intercept = w[:, -1]
1.58 logistic.py(289):         w = w[:, :-1]
1.58 logistic.py(292):     p = safe_sparse_dot(X, w.T)
1.58 logistic.py(293):     p += intercept
1.58 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
1.58 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
1.58 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
1.58 logistic.py(297):     p = np.exp(p, p)
1.58 logistic.py(298):     return loss, p, w
1.58 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
1.58 logistic.py(346):     diff = sample_weight * (p - Y)
1.58 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
1.58 logistic.py(348):     grad[:, :n_features] += alpha * w
1.58 logistic.py(349):     if fit_intercept:
1.58 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
1.58 logistic.py(351):     return loss, grad.ravel(), p
1.58 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
1.58 logistic.py(339):     n_classes = Y.shape[1]
1.58 logistic.py(340):     n_features = X.shape[1]
1.58 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
1.58 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
1.58 logistic.py(343):                     dtype=X.dtype)
1.58 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
1.58 logistic.py(282):     n_classes = Y.shape[1]
1.58 logistic.py(283):     n_features = X.shape[1]
1.58 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
1.58 logistic.py(285):     w = w.reshape(n_classes, -1)
1.58 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
1.58 logistic.py(287):     if fit_intercept:
1.58 logistic.py(288):         intercept = w[:, -1]
1.58 logistic.py(289):         w = w[:, :-1]
1.58 logistic.py(292):     p = safe_sparse_dot(X, w.T)
1.58 logistic.py(293):     p += intercept
1.58 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
1.58 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
1.58 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
1.58 logistic.py(297):     p = np.exp(p, p)
1.58 logistic.py(298):     return loss, p, w
1.58 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
1.58 logistic.py(346):     diff = sample_weight * (p - Y)
1.58 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
1.58 logistic.py(348):     grad[:, :n_features] += alpha * w
1.58 logistic.py(349):     if fit_intercept:
1.58 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
1.58 logistic.py(351):     return loss, grad.ravel(), p
1.58 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
1.58 logistic.py(339):     n_classes = Y.shape[1]
1.58 logistic.py(340):     n_features = X.shape[1]
1.58 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
1.58 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
1.58 logistic.py(343):                     dtype=X.dtype)
1.58 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
1.58 logistic.py(282):     n_classes = Y.shape[1]
1.58 logistic.py(283):     n_features = X.shape[1]
1.58 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
1.58 logistic.py(285):     w = w.reshape(n_classes, -1)
1.58 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
1.58 logistic.py(287):     if fit_intercept:
1.58 logistic.py(288):         intercept = w[:, -1]
1.58 logistic.py(289):         w = w[:, :-1]
1.58 logistic.py(292):     p = safe_sparse_dot(X, w.T)
1.58 logistic.py(293):     p += intercept
1.58 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
1.58 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
1.58 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
1.58 logistic.py(297):     p = np.exp(p, p)
1.58 logistic.py(298):     return loss, p, w
1.58 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
1.58 logistic.py(346):     diff = sample_weight * (p - Y)
1.58 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
1.58 logistic.py(348):     grad[:, :n_features] += alpha * w
1.58 logistic.py(349):     if fit_intercept:
1.58 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
1.58 logistic.py(351):     return loss, grad.ravel(), p
1.58 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
1.58 logistic.py(339):     n_classes = Y.shape[1]
1.58 logistic.py(340):     n_features = X.shape[1]
1.58 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
1.58 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
1.58 logistic.py(343):                     dtype=X.dtype)
1.58 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
1.58 logistic.py(282):     n_classes = Y.shape[1]
1.58 logistic.py(283):     n_features = X.shape[1]
1.58 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
1.58 logistic.py(285):     w = w.reshape(n_classes, -1)
1.58 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
1.58 logistic.py(287):     if fit_intercept:
1.58 logistic.py(288):         intercept = w[:, -1]
1.58 logistic.py(289):         w = w[:, :-1]
1.58 logistic.py(292):     p = safe_sparse_dot(X, w.T)
1.58 logistic.py(293):     p += intercept
1.58 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
1.58 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
1.58 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
1.58 logistic.py(297):     p = np.exp(p, p)
1.58 logistic.py(298):     return loss, p, w
1.58 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
1.58 logistic.py(346):     diff = sample_weight * (p - Y)
1.58 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
1.58 logistic.py(348):     grad[:, :n_features] += alpha * w
1.58 logistic.py(349):     if fit_intercept:
1.58 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
1.58 logistic.py(351):     return loss, grad.ravel(), p
1.58 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
1.58 logistic.py(339):     n_classes = Y.shape[1]
1.58 logistic.py(340):     n_features = X.shape[1]
1.58 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
1.58 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
1.58 logistic.py(343):                     dtype=X.dtype)
1.58 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
1.58 logistic.py(282):     n_classes = Y.shape[1]
1.58 logistic.py(283):     n_features = X.shape[1]
1.58 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
1.58 logistic.py(285):     w = w.reshape(n_classes, -1)
1.58 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
1.58 logistic.py(287):     if fit_intercept:
1.58 logistic.py(288):         intercept = w[:, -1]
1.58 logistic.py(289):         w = w[:, :-1]
1.58 logistic.py(292):     p = safe_sparse_dot(X, w.T)
1.58 logistic.py(293):     p += intercept
1.58 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
1.58 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
1.58 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
1.58 logistic.py(297):     p = np.exp(p, p)
1.58 logistic.py(298):     return loss, p, w
1.58 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
1.58 logistic.py(346):     diff = sample_weight * (p - Y)
1.58 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
1.58 logistic.py(348):     grad[:, :n_features] += alpha * w
1.58 logistic.py(349):     if fit_intercept:
1.58 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
1.58 logistic.py(351):     return loss, grad.ravel(), p
1.58 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
1.58 logistic.py(339):     n_classes = Y.shape[1]
1.58 logistic.py(340):     n_features = X.shape[1]
1.58 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
1.58 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
1.58 logistic.py(343):                     dtype=X.dtype)
1.58 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
1.58 logistic.py(282):     n_classes = Y.shape[1]
1.58 logistic.py(283):     n_features = X.shape[1]
1.58 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
1.58 logistic.py(285):     w = w.reshape(n_classes, -1)
1.58 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
1.58 logistic.py(287):     if fit_intercept:
1.58 logistic.py(288):         intercept = w[:, -1]
1.58 logistic.py(289):         w = w[:, :-1]
1.58 logistic.py(292):     p = safe_sparse_dot(X, w.T)
1.58 logistic.py(293):     p += intercept
1.58 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
1.58 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
1.58 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
1.59 logistic.py(297):     p = np.exp(p, p)
1.59 logistic.py(298):     return loss, p, w
1.59 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
1.59 logistic.py(346):     diff = sample_weight * (p - Y)
1.59 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
1.59 logistic.py(348):     grad[:, :n_features] += alpha * w
1.59 logistic.py(349):     if fit_intercept:
1.59 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
1.59 logistic.py(351):     return loss, grad.ravel(), p
1.59 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
1.59 logistic.py(339):     n_classes = Y.shape[1]
1.59 logistic.py(340):     n_features = X.shape[1]
1.59 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
1.59 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
1.59 logistic.py(343):                     dtype=X.dtype)
1.59 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
1.59 logistic.py(282):     n_classes = Y.shape[1]
1.59 logistic.py(283):     n_features = X.shape[1]
1.59 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
1.59 logistic.py(285):     w = w.reshape(n_classes, -1)
1.59 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
1.59 logistic.py(287):     if fit_intercept:
1.59 logistic.py(288):         intercept = w[:, -1]
1.59 logistic.py(289):         w = w[:, :-1]
1.59 logistic.py(292):     p = safe_sparse_dot(X, w.T)
1.59 logistic.py(293):     p += intercept
1.59 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
1.59 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
1.59 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
1.59 logistic.py(297):     p = np.exp(p, p)
1.59 logistic.py(298):     return loss, p, w
1.59 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
1.59 logistic.py(346):     diff = sample_weight * (p - Y)
1.59 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
1.59 logistic.py(348):     grad[:, :n_features] += alpha * w
1.59 logistic.py(349):     if fit_intercept:
1.59 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
1.59 logistic.py(351):     return loss, grad.ravel(), p
1.59 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
1.59 logistic.py(339):     n_classes = Y.shape[1]
1.59 logistic.py(340):     n_features = X.shape[1]
1.59 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
1.59 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
1.59 logistic.py(343):                     dtype=X.dtype)
1.59 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
1.59 logistic.py(282):     n_classes = Y.shape[1]
1.59 logistic.py(283):     n_features = X.shape[1]
1.59 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
1.59 logistic.py(285):     w = w.reshape(n_classes, -1)
1.59 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
1.59 logistic.py(287):     if fit_intercept:
1.59 logistic.py(288):         intercept = w[:, -1]
1.59 logistic.py(289):         w = w[:, :-1]
1.59 logistic.py(292):     p = safe_sparse_dot(X, w.T)
1.59 logistic.py(293):     p += intercept
1.59 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
1.59 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
1.59 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
1.59 logistic.py(297):     p = np.exp(p, p)
1.59 logistic.py(298):     return loss, p, w
1.59 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
1.59 logistic.py(346):     diff = sample_weight * (p - Y)
1.59 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
1.59 logistic.py(348):     grad[:, :n_features] += alpha * w
1.59 logistic.py(349):     if fit_intercept:
1.59 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
1.59 logistic.py(351):     return loss, grad.ravel(), p
1.59 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
1.59 logistic.py(339):     n_classes = Y.shape[1]
1.59 logistic.py(340):     n_features = X.shape[1]
1.59 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
1.59 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
1.59 logistic.py(343):                     dtype=X.dtype)
1.59 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
1.59 logistic.py(282):     n_classes = Y.shape[1]
1.59 logistic.py(283):     n_features = X.shape[1]
1.59 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
1.59 logistic.py(285):     w = w.reshape(n_classes, -1)
1.59 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
1.59 logistic.py(287):     if fit_intercept:
1.59 logistic.py(288):         intercept = w[:, -1]
1.59 logistic.py(289):         w = w[:, :-1]
1.59 logistic.py(292):     p = safe_sparse_dot(X, w.T)
1.59 logistic.py(293):     p += intercept
1.59 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
1.59 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
1.59 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
1.59 logistic.py(297):     p = np.exp(p, p)
1.59 logistic.py(298):     return loss, p, w
1.59 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
1.59 logistic.py(346):     diff = sample_weight * (p - Y)
1.59 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
1.59 logistic.py(348):     grad[:, :n_features] += alpha * w
1.59 logistic.py(349):     if fit_intercept:
1.59 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
1.59 logistic.py(351):     return loss, grad.ravel(), p
1.59 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
1.59 logistic.py(339):     n_classes = Y.shape[1]
1.59 logistic.py(340):     n_features = X.shape[1]
1.59 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
1.59 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
1.59 logistic.py(343):                     dtype=X.dtype)
1.59 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
1.59 logistic.py(282):     n_classes = Y.shape[1]
1.59 logistic.py(283):     n_features = X.shape[1]
1.59 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
1.59 logistic.py(285):     w = w.reshape(n_classes, -1)
1.59 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
1.59 logistic.py(287):     if fit_intercept:
1.59 logistic.py(288):         intercept = w[:, -1]
1.59 logistic.py(289):         w = w[:, :-1]
1.59 logistic.py(292):     p = safe_sparse_dot(X, w.T)
1.59 logistic.py(293):     p += intercept
1.59 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
1.59 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
1.59 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
1.59 logistic.py(297):     p = np.exp(p, p)
1.59 logistic.py(298):     return loss, p, w
1.59 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
1.59 logistic.py(346):     diff = sample_weight * (p - Y)
1.59 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
1.59 logistic.py(348):     grad[:, :n_features] += alpha * w
1.59 logistic.py(349):     if fit_intercept:
1.59 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
1.59 logistic.py(351):     return loss, grad.ravel(), p
1.59 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
1.59 logistic.py(339):     n_classes = Y.shape[1]
1.59 logistic.py(340):     n_features = X.shape[1]
1.59 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
1.59 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
1.59 logistic.py(343):                     dtype=X.dtype)
1.59 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
1.59 logistic.py(282):     n_classes = Y.shape[1]
1.59 logistic.py(283):     n_features = X.shape[1]
1.59 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
1.59 logistic.py(285):     w = w.reshape(n_classes, -1)
1.59 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
1.59 logistic.py(287):     if fit_intercept:
1.59 logistic.py(288):         intercept = w[:, -1]
1.59 logistic.py(289):         w = w[:, :-1]
1.59 logistic.py(292):     p = safe_sparse_dot(X, w.T)
1.59 logistic.py(293):     p += intercept
1.59 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
1.59 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
1.59 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
1.59 logistic.py(297):     p = np.exp(p, p)
1.59 logistic.py(298):     return loss, p, w
1.59 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
1.59 logistic.py(346):     diff = sample_weight * (p - Y)
1.59 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
1.59 logistic.py(348):     grad[:, :n_features] += alpha * w
1.59 logistic.py(349):     if fit_intercept:
1.59 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
1.59 logistic.py(351):     return loss, grad.ravel(), p
1.59 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
1.59 logistic.py(339):     n_classes = Y.shape[1]
1.59 logistic.py(340):     n_features = X.shape[1]
1.59 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
1.59 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
1.59 logistic.py(343):                     dtype=X.dtype)
1.59 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
1.59 logistic.py(282):     n_classes = Y.shape[1]
1.59 logistic.py(283):     n_features = X.shape[1]
1.59 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
1.59 logistic.py(285):     w = w.reshape(n_classes, -1)
1.59 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
1.59 logistic.py(287):     if fit_intercept:
1.59 logistic.py(288):         intercept = w[:, -1]
1.59 logistic.py(289):         w = w[:, :-1]
1.59 logistic.py(292):     p = safe_sparse_dot(X, w.T)
1.59 logistic.py(293):     p += intercept
1.59 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
1.59 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
1.59 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
1.59 logistic.py(297):     p = np.exp(p, p)
1.59 logistic.py(298):     return loss, p, w
1.59 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
1.59 logistic.py(346):     diff = sample_weight * (p - Y)
1.59 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
1.59 logistic.py(348):     grad[:, :n_features] += alpha * w
1.59 logistic.py(349):     if fit_intercept:
1.59 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
1.59 logistic.py(351):     return loss, grad.ravel(), p
1.59 logistic.py(718):             if info["warnflag"] == 1:
1.59 logistic.py(723):             n_iter_i = min(info['nit'], max_iter)
1.59 logistic.py(760):         if multi_class == 'multinomial':
1.59 logistic.py(761):             multi_w0 = np.reshape(w0, (classes.size, -1))
1.59 logistic.py(762):             if classes.size == 2:
1.59 logistic.py(764):             coefs.append(multi_w0)
1.59 logistic.py(768):         n_iter[i] = n_iter_i
1.59 logistic.py(712):     for i, C in enumerate(Cs):
1.59 logistic.py(713):         if solver == 'lbfgs':
1.59 logistic.py(714):             w0, loss, info = optimize.fmin_l_bfgs_b(
1.59 logistic.py(715):                 func, w0, fprime=None,
1.59 logistic.py(716):                 args=(X, target, 1. / C, sample_weight),
1.59 logistic.py(717):                 iprint=(verbose > 0) - 1, pgtol=tol, maxiter=max_iter)
1.59 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
1.59 logistic.py(339):     n_classes = Y.shape[1]
1.59 logistic.py(340):     n_features = X.shape[1]
1.59 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
1.59 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
1.59 logistic.py(343):                     dtype=X.dtype)
1.59 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
1.59 logistic.py(282):     n_classes = Y.shape[1]
1.59 logistic.py(283):     n_features = X.shape[1]
1.59 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
1.59 logistic.py(285):     w = w.reshape(n_classes, -1)
1.59 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
1.59 logistic.py(287):     if fit_intercept:
1.59 logistic.py(288):         intercept = w[:, -1]
1.59 logistic.py(289):         w = w[:, :-1]
1.59 logistic.py(292):     p = safe_sparse_dot(X, w.T)
1.59 logistic.py(293):     p += intercept
1.59 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
1.59 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
1.59 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
1.59 logistic.py(297):     p = np.exp(p, p)
1.59 logistic.py(298):     return loss, p, w
1.59 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
1.59 logistic.py(346):     diff = sample_weight * (p - Y)
1.59 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
1.59 logistic.py(348):     grad[:, :n_features] += alpha * w
1.59 logistic.py(349):     if fit_intercept:
1.59 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
1.59 logistic.py(351):     return loss, grad.ravel(), p
1.59 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
1.59 logistic.py(339):     n_classes = Y.shape[1]
1.59 logistic.py(340):     n_features = X.shape[1]
1.59 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
1.59 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
1.59 logistic.py(343):                     dtype=X.dtype)
1.59 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
1.59 logistic.py(282):     n_classes = Y.shape[1]
1.59 logistic.py(283):     n_features = X.shape[1]
1.59 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
1.59 logistic.py(285):     w = w.reshape(n_classes, -1)
1.59 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
1.59 logistic.py(287):     if fit_intercept:
1.59 logistic.py(288):         intercept = w[:, -1]
1.59 logistic.py(289):         w = w[:, :-1]
1.59 logistic.py(292):     p = safe_sparse_dot(X, w.T)
1.59 logistic.py(293):     p += intercept
1.59 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
1.59 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
1.59 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
1.59 logistic.py(297):     p = np.exp(p, p)
1.59 logistic.py(298):     return loss, p, w
1.59 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
1.59 logistic.py(346):     diff = sample_weight * (p - Y)
1.59 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
1.59 logistic.py(348):     grad[:, :n_features] += alpha * w
1.59 logistic.py(349):     if fit_intercept:
1.59 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
1.59 logistic.py(351):     return loss, grad.ravel(), p
1.59 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
1.59 logistic.py(339):     n_classes = Y.shape[1]
1.59 logistic.py(340):     n_features = X.shape[1]
1.59 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
1.59 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
1.59 logistic.py(343):                     dtype=X.dtype)
1.59 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
1.59 logistic.py(282):     n_classes = Y.shape[1]
1.59 logistic.py(283):     n_features = X.shape[1]
1.59 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
1.59 logistic.py(285):     w = w.reshape(n_classes, -1)
1.59 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
1.59 logistic.py(287):     if fit_intercept:
1.59 logistic.py(288):         intercept = w[:, -1]
1.59 logistic.py(289):         w = w[:, :-1]
1.59 logistic.py(292):     p = safe_sparse_dot(X, w.T)
1.59 logistic.py(293):     p += intercept
1.59 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
1.59 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
1.59 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
1.59 logistic.py(297):     p = np.exp(p, p)
1.59 logistic.py(298):     return loss, p, w
1.59 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
1.59 logistic.py(346):     diff = sample_weight * (p - Y)
1.59 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
1.59 logistic.py(348):     grad[:, :n_features] += alpha * w
1.59 logistic.py(349):     if fit_intercept:
1.59 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
1.59 logistic.py(351):     return loss, grad.ravel(), p
1.59 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
1.59 logistic.py(339):     n_classes = Y.shape[1]
1.59 logistic.py(340):     n_features = X.shape[1]
1.59 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
1.59 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
1.59 logistic.py(343):                     dtype=X.dtype)
1.59 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
1.59 logistic.py(282):     n_classes = Y.shape[1]
1.59 logistic.py(283):     n_features = X.shape[1]
1.59 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
1.59 logistic.py(285):     w = w.reshape(n_classes, -1)
1.59 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
1.59 logistic.py(287):     if fit_intercept:
1.59 logistic.py(288):         intercept = w[:, -1]
1.59 logistic.py(289):         w = w[:, :-1]
1.59 logistic.py(292):     p = safe_sparse_dot(X, w.T)
1.59 logistic.py(293):     p += intercept
1.59 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
1.59 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
1.59 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
1.59 logistic.py(297):     p = np.exp(p, p)
1.59 logistic.py(298):     return loss, p, w
1.59 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
1.59 logistic.py(346):     diff = sample_weight * (p - Y)
1.59 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
1.59 logistic.py(348):     grad[:, :n_features] += alpha * w
1.59 logistic.py(349):     if fit_intercept:
1.59 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
1.59 logistic.py(351):     return loss, grad.ravel(), p
1.59 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
1.59 logistic.py(339):     n_classes = Y.shape[1]
1.59 logistic.py(340):     n_features = X.shape[1]
1.59 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
1.59 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
1.59 logistic.py(343):                     dtype=X.dtype)
1.59 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
1.59 logistic.py(282):     n_classes = Y.shape[1]
1.59 logistic.py(283):     n_features = X.shape[1]
1.59 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
1.59 logistic.py(285):     w = w.reshape(n_classes, -1)
1.59 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
1.59 logistic.py(287):     if fit_intercept:
1.59 logistic.py(288):         intercept = w[:, -1]
1.59 logistic.py(289):         w = w[:, :-1]
1.59 logistic.py(292):     p = safe_sparse_dot(X, w.T)
1.59 logistic.py(293):     p += intercept
1.59 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
1.59 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
1.59 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
1.59 logistic.py(297):     p = np.exp(p, p)
1.59 logistic.py(298):     return loss, p, w
1.59 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
1.59 logistic.py(346):     diff = sample_weight * (p - Y)
1.59 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
1.59 logistic.py(348):     grad[:, :n_features] += alpha * w
1.59 logistic.py(349):     if fit_intercept:
1.59 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
1.59 logistic.py(351):     return loss, grad.ravel(), p
1.59 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
1.59 logistic.py(339):     n_classes = Y.shape[1]
1.59 logistic.py(340):     n_features = X.shape[1]
1.59 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
1.59 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
1.59 logistic.py(343):                     dtype=X.dtype)
1.59 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
1.59 logistic.py(282):     n_classes = Y.shape[1]
1.59 logistic.py(283):     n_features = X.shape[1]
1.59 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
1.59 logistic.py(285):     w = w.reshape(n_classes, -1)
1.59 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
1.59 logistic.py(287):     if fit_intercept:
1.59 logistic.py(288):         intercept = w[:, -1]
1.59 logistic.py(289):         w = w[:, :-1]
1.59 logistic.py(292):     p = safe_sparse_dot(X, w.T)
1.59 logistic.py(293):     p += intercept
1.59 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
1.59 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
1.59 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
1.59 logistic.py(297):     p = np.exp(p, p)
1.59 logistic.py(298):     return loss, p, w
1.59 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
1.59 logistic.py(346):     diff = sample_weight * (p - Y)
1.59 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
1.59 logistic.py(348):     grad[:, :n_features] += alpha * w
1.59 logistic.py(349):     if fit_intercept:
1.59 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
1.59 logistic.py(351):     return loss, grad.ravel(), p
1.59 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
1.59 logistic.py(339):     n_classes = Y.shape[1]
1.59 logistic.py(340):     n_features = X.shape[1]
1.59 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
1.59 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
1.59 logistic.py(343):                     dtype=X.dtype)
1.59 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
1.59 logistic.py(282):     n_classes = Y.shape[1]
1.59 logistic.py(283):     n_features = X.shape[1]
1.59 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
1.59 logistic.py(285):     w = w.reshape(n_classes, -1)
1.59 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
1.59 logistic.py(287):     if fit_intercept:
1.59 logistic.py(288):         intercept = w[:, -1]
1.59 logistic.py(289):         w = w[:, :-1]
1.59 logistic.py(292):     p = safe_sparse_dot(X, w.T)
1.59 logistic.py(293):     p += intercept
1.59 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
1.59 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
1.59 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
1.59 logistic.py(297):     p = np.exp(p, p)
1.59 logistic.py(298):     return loss, p, w
1.59 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
1.59 logistic.py(346):     diff = sample_weight * (p - Y)
1.59 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
1.59 logistic.py(348):     grad[:, :n_features] += alpha * w
1.59 logistic.py(349):     if fit_intercept:
1.59 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
1.59 logistic.py(351):     return loss, grad.ravel(), p
1.59 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
1.59 logistic.py(339):     n_classes = Y.shape[1]
1.59 logistic.py(340):     n_features = X.shape[1]
1.59 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
1.59 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
1.59 logistic.py(343):                     dtype=X.dtype)
1.59 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
1.59 logistic.py(282):     n_classes = Y.shape[1]
1.59 logistic.py(283):     n_features = X.shape[1]
1.59 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
1.59 logistic.py(285):     w = w.reshape(n_classes, -1)
1.59 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
1.59 logistic.py(287):     if fit_intercept:
1.59 logistic.py(288):         intercept = w[:, -1]
1.59 logistic.py(289):         w = w[:, :-1]
1.59 logistic.py(292):     p = safe_sparse_dot(X, w.T)
1.59 logistic.py(293):     p += intercept
1.59 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
1.59 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
1.59 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
1.59 logistic.py(297):     p = np.exp(p, p)
1.59 logistic.py(298):     return loss, p, w
1.59 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
1.59 logistic.py(346):     diff = sample_weight * (p - Y)
1.59 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
1.59 logistic.py(348):     grad[:, :n_features] += alpha * w
1.59 logistic.py(349):     if fit_intercept:
1.59 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
1.59 logistic.py(351):     return loss, grad.ravel(), p
1.59 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
1.59 logistic.py(339):     n_classes = Y.shape[1]
1.59 logistic.py(340):     n_features = X.shape[1]
1.59 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
1.59 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
1.59 logistic.py(343):                     dtype=X.dtype)
1.59 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
1.59 logistic.py(282):     n_classes = Y.shape[1]
1.59 logistic.py(283):     n_features = X.shape[1]
1.59 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
1.59 logistic.py(285):     w = w.reshape(n_classes, -1)
1.59 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
1.59 logistic.py(287):     if fit_intercept:
1.59 logistic.py(288):         intercept = w[:, -1]
1.59 logistic.py(289):         w = w[:, :-1]
1.59 logistic.py(292):     p = safe_sparse_dot(X, w.T)
1.59 logistic.py(293):     p += intercept
1.59 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
1.59 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
1.59 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
1.59 logistic.py(297):     p = np.exp(p, p)
1.59 logistic.py(298):     return loss, p, w
1.59 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
1.59 logistic.py(346):     diff = sample_weight * (p - Y)
1.59 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
1.59 logistic.py(348):     grad[:, :n_features] += alpha * w
1.59 logistic.py(349):     if fit_intercept:
1.59 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
1.59 logistic.py(351):     return loss, grad.ravel(), p
1.59 logistic.py(718):             if info["warnflag"] == 1:
1.59 logistic.py(723):             n_iter_i = min(info['nit'], max_iter)
1.59 logistic.py(760):         if multi_class == 'multinomial':
1.59 logistic.py(761):             multi_w0 = np.reshape(w0, (classes.size, -1))
1.59 logistic.py(762):             if classes.size == 2:
1.59 logistic.py(764):             coefs.append(multi_w0)
1.59 logistic.py(768):         n_iter[i] = n_iter_i
1.59 logistic.py(712):     for i, C in enumerate(Cs):
1.59 logistic.py(713):         if solver == 'lbfgs':
1.59 logistic.py(714):             w0, loss, info = optimize.fmin_l_bfgs_b(
1.59 logistic.py(715):                 func, w0, fprime=None,
1.59 logistic.py(716):                 args=(X, target, 1. / C, sample_weight),
1.59 logistic.py(717):                 iprint=(verbose > 0) - 1, pgtol=tol, maxiter=max_iter)
1.60 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
1.60 logistic.py(339):     n_classes = Y.shape[1]
1.60 logistic.py(340):     n_features = X.shape[1]
1.60 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
1.60 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
1.60 logistic.py(343):                     dtype=X.dtype)
1.60 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
1.60 logistic.py(282):     n_classes = Y.shape[1]
1.60 logistic.py(283):     n_features = X.shape[1]
1.60 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
1.60 logistic.py(285):     w = w.reshape(n_classes, -1)
1.60 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
1.60 logistic.py(287):     if fit_intercept:
1.60 logistic.py(288):         intercept = w[:, -1]
1.60 logistic.py(289):         w = w[:, :-1]
1.60 logistic.py(292):     p = safe_sparse_dot(X, w.T)
1.60 logistic.py(293):     p += intercept
1.60 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
1.60 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
1.60 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
1.60 logistic.py(297):     p = np.exp(p, p)
1.60 logistic.py(298):     return loss, p, w
1.60 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
1.60 logistic.py(346):     diff = sample_weight * (p - Y)
1.60 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
1.60 logistic.py(348):     grad[:, :n_features] += alpha * w
1.60 logistic.py(349):     if fit_intercept:
1.60 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
1.60 logistic.py(351):     return loss, grad.ravel(), p
1.60 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
1.60 logistic.py(339):     n_classes = Y.shape[1]
1.60 logistic.py(340):     n_features = X.shape[1]
1.60 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
1.60 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
1.60 logistic.py(343):                     dtype=X.dtype)
1.60 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
1.60 logistic.py(282):     n_classes = Y.shape[1]
1.60 logistic.py(283):     n_features = X.shape[1]
1.60 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
1.60 logistic.py(285):     w = w.reshape(n_classes, -1)
1.60 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
1.60 logistic.py(287):     if fit_intercept:
1.60 logistic.py(288):         intercept = w[:, -1]
1.60 logistic.py(289):         w = w[:, :-1]
1.60 logistic.py(292):     p = safe_sparse_dot(X, w.T)
1.60 logistic.py(293):     p += intercept
1.60 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
1.60 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
1.60 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
1.60 logistic.py(297):     p = np.exp(p, p)
1.60 logistic.py(298):     return loss, p, w
1.60 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
1.60 logistic.py(346):     diff = sample_weight * (p - Y)
1.60 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
1.60 logistic.py(348):     grad[:, :n_features] += alpha * w
1.60 logistic.py(349):     if fit_intercept:
1.60 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
1.60 logistic.py(351):     return loss, grad.ravel(), p
1.60 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
1.60 logistic.py(339):     n_classes = Y.shape[1]
1.60 logistic.py(340):     n_features = X.shape[1]
1.60 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
1.60 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
1.60 logistic.py(343):                     dtype=X.dtype)
1.60 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
1.60 logistic.py(282):     n_classes = Y.shape[1]
1.60 logistic.py(283):     n_features = X.shape[1]
1.60 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
1.60 logistic.py(285):     w = w.reshape(n_classes, -1)
1.60 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
1.60 logistic.py(287):     if fit_intercept:
1.60 logistic.py(288):         intercept = w[:, -1]
1.60 logistic.py(289):         w = w[:, :-1]
1.60 logistic.py(292):     p = safe_sparse_dot(X, w.T)
1.60 logistic.py(293):     p += intercept
1.60 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
1.60 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
1.60 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
1.60 logistic.py(297):     p = np.exp(p, p)
1.60 logistic.py(298):     return loss, p, w
1.60 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
1.60 logistic.py(346):     diff = sample_weight * (p - Y)
1.60 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
1.60 logistic.py(348):     grad[:, :n_features] += alpha * w
1.60 logistic.py(349):     if fit_intercept:
1.60 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
1.60 logistic.py(351):     return loss, grad.ravel(), p
1.60 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
1.60 logistic.py(339):     n_classes = Y.shape[1]
1.60 logistic.py(340):     n_features = X.shape[1]
1.60 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
1.60 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
1.60 logistic.py(343):                     dtype=X.dtype)
1.60 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
1.60 logistic.py(282):     n_classes = Y.shape[1]
1.60 logistic.py(283):     n_features = X.shape[1]
1.60 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
1.60 logistic.py(285):     w = w.reshape(n_classes, -1)
1.60 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
1.60 logistic.py(287):     if fit_intercept:
1.60 logistic.py(288):         intercept = w[:, -1]
1.60 logistic.py(289):         w = w[:, :-1]
1.60 logistic.py(292):     p = safe_sparse_dot(X, w.T)
1.60 logistic.py(293):     p += intercept
1.60 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
1.60 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
1.60 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
1.60 logistic.py(297):     p = np.exp(p, p)
1.60 logistic.py(298):     return loss, p, w
1.60 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
1.60 logistic.py(346):     diff = sample_weight * (p - Y)
1.60 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
1.60 logistic.py(348):     grad[:, :n_features] += alpha * w
1.60 logistic.py(349):     if fit_intercept:
1.60 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
1.60 logistic.py(351):     return loss, grad.ravel(), p
1.60 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
1.60 logistic.py(339):     n_classes = Y.shape[1]
1.60 logistic.py(340):     n_features = X.shape[1]
1.60 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
1.60 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
1.60 logistic.py(343):                     dtype=X.dtype)
1.60 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
1.60 logistic.py(282):     n_classes = Y.shape[1]
1.60 logistic.py(283):     n_features = X.shape[1]
1.60 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
1.60 logistic.py(285):     w = w.reshape(n_classes, -1)
1.60 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
1.60 logistic.py(287):     if fit_intercept:
1.60 logistic.py(288):         intercept = w[:, -1]
1.60 logistic.py(289):         w = w[:, :-1]
1.60 logistic.py(292):     p = safe_sparse_dot(X, w.T)
1.60 logistic.py(293):     p += intercept
1.60 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
1.60 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
1.60 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
1.60 logistic.py(297):     p = np.exp(p, p)
1.60 logistic.py(298):     return loss, p, w
1.60 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
1.60 logistic.py(346):     diff = sample_weight * (p - Y)
1.60 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
1.60 logistic.py(348):     grad[:, :n_features] += alpha * w
1.60 logistic.py(349):     if fit_intercept:
1.60 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
1.60 logistic.py(351):     return loss, grad.ravel(), p
1.60 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
1.60 logistic.py(339):     n_classes = Y.shape[1]
1.60 logistic.py(340):     n_features = X.shape[1]
1.60 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
1.60 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
1.60 logistic.py(343):                     dtype=X.dtype)
1.60 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
1.60 logistic.py(282):     n_classes = Y.shape[1]
1.60 logistic.py(283):     n_features = X.shape[1]
1.60 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
1.60 logistic.py(285):     w = w.reshape(n_classes, -1)
1.60 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
1.60 logistic.py(287):     if fit_intercept:
1.60 logistic.py(288):         intercept = w[:, -1]
1.60 logistic.py(289):         w = w[:, :-1]
1.60 logistic.py(292):     p = safe_sparse_dot(X, w.T)
1.60 logistic.py(293):     p += intercept
1.60 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
1.60 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
1.60 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
1.60 logistic.py(297):     p = np.exp(p, p)
1.60 logistic.py(298):     return loss, p, w
1.60 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
1.60 logistic.py(346):     diff = sample_weight * (p - Y)
1.60 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
1.60 logistic.py(348):     grad[:, :n_features] += alpha * w
1.60 logistic.py(349):     if fit_intercept:
1.60 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
1.60 logistic.py(351):     return loss, grad.ravel(), p
1.60 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
1.60 logistic.py(339):     n_classes = Y.shape[1]
1.60 logistic.py(340):     n_features = X.shape[1]
1.60 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
1.60 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
1.60 logistic.py(343):                     dtype=X.dtype)
1.60 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
1.60 logistic.py(282):     n_classes = Y.shape[1]
1.60 logistic.py(283):     n_features = X.shape[1]
1.60 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
1.60 logistic.py(285):     w = w.reshape(n_classes, -1)
1.60 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
1.60 logistic.py(287):     if fit_intercept:
1.60 logistic.py(288):         intercept = w[:, -1]
1.60 logistic.py(289):         w = w[:, :-1]
1.60 logistic.py(292):     p = safe_sparse_dot(X, w.T)
1.60 logistic.py(293):     p += intercept
1.60 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
1.60 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
1.60 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
1.60 logistic.py(297):     p = np.exp(p, p)
1.60 logistic.py(298):     return loss, p, w
1.60 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
1.60 logistic.py(346):     diff = sample_weight * (p - Y)
1.60 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
1.60 logistic.py(348):     grad[:, :n_features] += alpha * w
1.60 logistic.py(349):     if fit_intercept:
1.60 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
1.60 logistic.py(351):     return loss, grad.ravel(), p
1.60 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
1.60 logistic.py(339):     n_classes = Y.shape[1]
1.60 logistic.py(340):     n_features = X.shape[1]
1.60 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
1.60 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
1.60 logistic.py(343):                     dtype=X.dtype)
1.60 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
1.60 logistic.py(282):     n_classes = Y.shape[1]
1.60 logistic.py(283):     n_features = X.shape[1]
1.60 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
1.60 logistic.py(285):     w = w.reshape(n_classes, -1)
1.60 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
1.60 logistic.py(287):     if fit_intercept:
1.60 logistic.py(288):         intercept = w[:, -1]
1.60 logistic.py(289):         w = w[:, :-1]
1.60 logistic.py(292):     p = safe_sparse_dot(X, w.T)
1.60 logistic.py(293):     p += intercept
1.60 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
1.60 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
1.60 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
1.60 logistic.py(297):     p = np.exp(p, p)
1.60 logistic.py(298):     return loss, p, w
1.60 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
1.60 logistic.py(346):     diff = sample_weight * (p - Y)
1.60 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
1.60 logistic.py(348):     grad[:, :n_features] += alpha * w
1.60 logistic.py(349):     if fit_intercept:
1.60 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
1.60 logistic.py(351):     return loss, grad.ravel(), p
1.60 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
1.60 logistic.py(339):     n_classes = Y.shape[1]
1.60 logistic.py(340):     n_features = X.shape[1]
1.60 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
1.60 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
1.60 logistic.py(343):                     dtype=X.dtype)
1.60 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
1.60 logistic.py(282):     n_classes = Y.shape[1]
1.60 logistic.py(283):     n_features = X.shape[1]
1.60 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
1.60 logistic.py(285):     w = w.reshape(n_classes, -1)
1.60 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
1.60 logistic.py(287):     if fit_intercept:
1.60 logistic.py(288):         intercept = w[:, -1]
1.60 logistic.py(289):         w = w[:, :-1]
1.60 logistic.py(292):     p = safe_sparse_dot(X, w.T)
1.60 logistic.py(293):     p += intercept
1.60 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
1.60 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
1.60 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
1.60 logistic.py(297):     p = np.exp(p, p)
1.60 logistic.py(298):     return loss, p, w
1.60 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
1.60 logistic.py(346):     diff = sample_weight * (p - Y)
1.60 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
1.60 logistic.py(348):     grad[:, :n_features] += alpha * w
1.60 logistic.py(349):     if fit_intercept:
1.60 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
1.60 logistic.py(351):     return loss, grad.ravel(), p
1.60 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
1.60 logistic.py(339):     n_classes = Y.shape[1]
1.60 logistic.py(340):     n_features = X.shape[1]
1.60 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
1.60 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
1.60 logistic.py(343):                     dtype=X.dtype)
1.60 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
1.60 logistic.py(282):     n_classes = Y.shape[1]
1.60 logistic.py(283):     n_features = X.shape[1]
1.60 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
1.60 logistic.py(285):     w = w.reshape(n_classes, -1)
1.60 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
1.60 logistic.py(287):     if fit_intercept:
1.60 logistic.py(288):         intercept = w[:, -1]
1.60 logistic.py(289):         w = w[:, :-1]
1.60 logistic.py(292):     p = safe_sparse_dot(X, w.T)
1.60 logistic.py(293):     p += intercept
1.60 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
1.60 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
1.60 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
1.60 logistic.py(297):     p = np.exp(p, p)
1.60 logistic.py(298):     return loss, p, w
1.60 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
1.60 logistic.py(346):     diff = sample_weight * (p - Y)
1.60 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
1.60 logistic.py(348):     grad[:, :n_features] += alpha * w
1.60 logistic.py(349):     if fit_intercept:
1.60 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
1.60 logistic.py(351):     return loss, grad.ravel(), p
1.60 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
1.60 logistic.py(339):     n_classes = Y.shape[1]
1.60 logistic.py(340):     n_features = X.shape[1]
1.60 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
1.60 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
1.60 logistic.py(343):                     dtype=X.dtype)
1.60 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
1.60 logistic.py(282):     n_classes = Y.shape[1]
1.60 logistic.py(283):     n_features = X.shape[1]
1.60 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
1.60 logistic.py(285):     w = w.reshape(n_classes, -1)
1.60 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
1.60 logistic.py(287):     if fit_intercept:
1.60 logistic.py(288):         intercept = w[:, -1]
1.60 logistic.py(289):         w = w[:, :-1]
1.60 logistic.py(292):     p = safe_sparse_dot(X, w.T)
1.60 logistic.py(293):     p += intercept
1.60 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
1.60 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
1.60 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
1.60 logistic.py(297):     p = np.exp(p, p)
1.60 logistic.py(298):     return loss, p, w
1.60 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
1.60 logistic.py(346):     diff = sample_weight * (p - Y)
1.60 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
1.60 logistic.py(348):     grad[:, :n_features] += alpha * w
1.60 logistic.py(349):     if fit_intercept:
1.60 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
1.60 logistic.py(351):     return loss, grad.ravel(), p
1.60 logistic.py(718):             if info["warnflag"] == 1:
1.60 logistic.py(723):             n_iter_i = min(info['nit'], max_iter)
1.60 logistic.py(760):         if multi_class == 'multinomial':
1.60 logistic.py(761):             multi_w0 = np.reshape(w0, (classes.size, -1))
1.60 logistic.py(762):             if classes.size == 2:
1.60 logistic.py(764):             coefs.append(multi_w0)
1.60 logistic.py(768):         n_iter[i] = n_iter_i
1.60 logistic.py(712):     for i, C in enumerate(Cs):
1.60 logistic.py(713):         if solver == 'lbfgs':
1.60 logistic.py(714):             w0, loss, info = optimize.fmin_l_bfgs_b(
1.60 logistic.py(715):                 func, w0, fprime=None,
1.60 logistic.py(716):                 args=(X, target, 1. / C, sample_weight),
1.60 logistic.py(717):                 iprint=(verbose > 0) - 1, pgtol=tol, maxiter=max_iter)
1.60 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
1.60 logistic.py(339):     n_classes = Y.shape[1]
1.60 logistic.py(340):     n_features = X.shape[1]
1.60 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
1.60 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
1.60 logistic.py(343):                     dtype=X.dtype)
1.60 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
1.60 logistic.py(282):     n_classes = Y.shape[1]
1.60 logistic.py(283):     n_features = X.shape[1]
1.60 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
1.60 logistic.py(285):     w = w.reshape(n_classes, -1)
1.60 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
1.60 logistic.py(287):     if fit_intercept:
1.60 logistic.py(288):         intercept = w[:, -1]
1.60 logistic.py(289):         w = w[:, :-1]
1.60 logistic.py(292):     p = safe_sparse_dot(X, w.T)
1.60 logistic.py(293):     p += intercept
1.60 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
1.60 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
1.60 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
1.60 logistic.py(297):     p = np.exp(p, p)
1.60 logistic.py(298):     return loss, p, w
1.60 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
1.60 logistic.py(346):     diff = sample_weight * (p - Y)
1.60 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
1.60 logistic.py(348):     grad[:, :n_features] += alpha * w
1.60 logistic.py(349):     if fit_intercept:
1.60 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
1.60 logistic.py(351):     return loss, grad.ravel(), p
1.60 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
1.60 logistic.py(339):     n_classes = Y.shape[1]
1.60 logistic.py(340):     n_features = X.shape[1]
1.60 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
1.60 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
1.60 logistic.py(343):                     dtype=X.dtype)
1.60 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
1.60 logistic.py(282):     n_classes = Y.shape[1]
1.60 logistic.py(283):     n_features = X.shape[1]
1.60 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
1.60 logistic.py(285):     w = w.reshape(n_classes, -1)
1.60 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
1.60 logistic.py(287):     if fit_intercept:
1.60 logistic.py(288):         intercept = w[:, -1]
1.60 logistic.py(289):         w = w[:, :-1]
1.60 logistic.py(292):     p = safe_sparse_dot(X, w.T)
1.60 logistic.py(293):     p += intercept
1.60 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
1.60 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
1.60 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
1.60 logistic.py(297):     p = np.exp(p, p)
1.60 logistic.py(298):     return loss, p, w
1.60 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
1.60 logistic.py(346):     diff = sample_weight * (p - Y)
1.60 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
1.60 logistic.py(348):     grad[:, :n_features] += alpha * w
1.60 logistic.py(349):     if fit_intercept:
1.60 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
1.60 logistic.py(351):     return loss, grad.ravel(), p
1.60 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
1.60 logistic.py(339):     n_classes = Y.shape[1]
1.60 logistic.py(340):     n_features = X.shape[1]
1.60 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
1.60 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
1.60 logistic.py(343):                     dtype=X.dtype)
1.60 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
1.60 logistic.py(282):     n_classes = Y.shape[1]
1.60 logistic.py(283):     n_features = X.shape[1]
1.60 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
1.60 logistic.py(285):     w = w.reshape(n_classes, -1)
1.60 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
1.60 logistic.py(287):     if fit_intercept:
1.60 logistic.py(288):         intercept = w[:, -1]
1.60 logistic.py(289):         w = w[:, :-1]
1.60 logistic.py(292):     p = safe_sparse_dot(X, w.T)
1.60 logistic.py(293):     p += intercept
1.60 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
1.60 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
1.60 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
1.60 logistic.py(297):     p = np.exp(p, p)
1.60 logistic.py(298):     return loss, p, w
1.60 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
1.60 logistic.py(346):     diff = sample_weight * (p - Y)
1.60 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
1.60 logistic.py(348):     grad[:, :n_features] += alpha * w
1.60 logistic.py(349):     if fit_intercept:
1.60 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
1.60 logistic.py(351):     return loss, grad.ravel(), p
1.60 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
1.60 logistic.py(339):     n_classes = Y.shape[1]
1.60 logistic.py(340):     n_features = X.shape[1]
1.60 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
1.60 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
1.60 logistic.py(343):                     dtype=X.dtype)
1.60 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
1.60 logistic.py(282):     n_classes = Y.shape[1]
1.60 logistic.py(283):     n_features = X.shape[1]
1.60 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
1.60 logistic.py(285):     w = w.reshape(n_classes, -1)
1.60 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
1.60 logistic.py(287):     if fit_intercept:
1.60 logistic.py(288):         intercept = w[:, -1]
1.60 logistic.py(289):         w = w[:, :-1]
1.60 logistic.py(292):     p = safe_sparse_dot(X, w.T)
1.60 logistic.py(293):     p += intercept
1.60 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
1.60 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
1.60 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
1.60 logistic.py(297):     p = np.exp(p, p)
1.60 logistic.py(298):     return loss, p, w
1.60 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
1.60 logistic.py(346):     diff = sample_weight * (p - Y)
1.60 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
1.60 logistic.py(348):     grad[:, :n_features] += alpha * w
1.60 logistic.py(349):     if fit_intercept:
1.60 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
1.60 logistic.py(351):     return loss, grad.ravel(), p
1.60 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
1.60 logistic.py(339):     n_classes = Y.shape[1]
1.60 logistic.py(340):     n_features = X.shape[1]
1.60 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
1.60 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
1.60 logistic.py(343):                     dtype=X.dtype)
1.60 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
1.60 logistic.py(282):     n_classes = Y.shape[1]
1.60 logistic.py(283):     n_features = X.shape[1]
1.60 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
1.60 logistic.py(285):     w = w.reshape(n_classes, -1)
1.60 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
1.60 logistic.py(287):     if fit_intercept:
1.60 logistic.py(288):         intercept = w[:, -1]
1.60 logistic.py(289):         w = w[:, :-1]
1.60 logistic.py(292):     p = safe_sparse_dot(X, w.T)
1.60 logistic.py(293):     p += intercept
1.60 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
1.60 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
1.60 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
1.60 logistic.py(297):     p = np.exp(p, p)
1.60 logistic.py(298):     return loss, p, w
1.60 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
1.60 logistic.py(346):     diff = sample_weight * (p - Y)
1.60 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
1.60 logistic.py(348):     grad[:, :n_features] += alpha * w
1.61 logistic.py(349):     if fit_intercept:
1.61 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
1.61 logistic.py(351):     return loss, grad.ravel(), p
1.61 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
1.61 logistic.py(339):     n_classes = Y.shape[1]
1.61 logistic.py(340):     n_features = X.shape[1]
1.61 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
1.61 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
1.61 logistic.py(343):                     dtype=X.dtype)
1.61 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
1.61 logistic.py(282):     n_classes = Y.shape[1]
1.61 logistic.py(283):     n_features = X.shape[1]
1.61 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
1.61 logistic.py(285):     w = w.reshape(n_classes, -1)
1.61 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
1.61 logistic.py(287):     if fit_intercept:
1.61 logistic.py(288):         intercept = w[:, -1]
1.61 logistic.py(289):         w = w[:, :-1]
1.61 logistic.py(292):     p = safe_sparse_dot(X, w.T)
1.61 logistic.py(293):     p += intercept
1.61 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
1.61 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
1.61 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
1.61 logistic.py(297):     p = np.exp(p, p)
1.61 logistic.py(298):     return loss, p, w
1.61 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
1.61 logistic.py(346):     diff = sample_weight * (p - Y)
1.61 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
1.61 logistic.py(348):     grad[:, :n_features] += alpha * w
1.61 logistic.py(349):     if fit_intercept:
1.61 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
1.61 logistic.py(351):     return loss, grad.ravel(), p
1.61 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
1.61 logistic.py(339):     n_classes = Y.shape[1]
1.61 logistic.py(340):     n_features = X.shape[1]
1.61 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
1.61 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
1.61 logistic.py(343):                     dtype=X.dtype)
1.61 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
1.61 logistic.py(282):     n_classes = Y.shape[1]
1.61 logistic.py(283):     n_features = X.shape[1]
1.61 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
1.61 logistic.py(285):     w = w.reshape(n_classes, -1)
1.61 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
1.61 logistic.py(287):     if fit_intercept:
1.61 logistic.py(288):         intercept = w[:, -1]
1.61 logistic.py(289):         w = w[:, :-1]
1.61 logistic.py(292):     p = safe_sparse_dot(X, w.T)
1.61 logistic.py(293):     p += intercept
1.61 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
1.61 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
1.61 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
1.61 logistic.py(297):     p = np.exp(p, p)
1.61 logistic.py(298):     return loss, p, w
1.61 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
1.61 logistic.py(346):     diff = sample_weight * (p - Y)
1.61 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
1.61 logistic.py(348):     grad[:, :n_features] += alpha * w
1.61 logistic.py(349):     if fit_intercept:
1.61 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
1.61 logistic.py(351):     return loss, grad.ravel(), p
1.61 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
1.61 logistic.py(339):     n_classes = Y.shape[1]
1.61 logistic.py(340):     n_features = X.shape[1]
1.61 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
1.61 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
1.61 logistic.py(343):                     dtype=X.dtype)
1.61 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
1.61 logistic.py(282):     n_classes = Y.shape[1]
1.61 logistic.py(283):     n_features = X.shape[1]
1.61 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
1.61 logistic.py(285):     w = w.reshape(n_classes, -1)
1.61 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
1.61 logistic.py(287):     if fit_intercept:
1.61 logistic.py(288):         intercept = w[:, -1]
1.61 logistic.py(289):         w = w[:, :-1]
1.61 logistic.py(292):     p = safe_sparse_dot(X, w.T)
1.61 logistic.py(293):     p += intercept
1.61 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
1.61 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
1.61 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
1.61 logistic.py(297):     p = np.exp(p, p)
1.61 logistic.py(298):     return loss, p, w
1.61 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
1.61 logistic.py(346):     diff = sample_weight * (p - Y)
1.61 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
1.61 logistic.py(348):     grad[:, :n_features] += alpha * w
1.61 logistic.py(349):     if fit_intercept:
1.61 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
1.61 logistic.py(351):     return loss, grad.ravel(), p
1.61 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
1.61 logistic.py(339):     n_classes = Y.shape[1]
1.61 logistic.py(340):     n_features = X.shape[1]
1.61 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
1.61 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
1.61 logistic.py(343):                     dtype=X.dtype)
1.61 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
1.61 logistic.py(282):     n_classes = Y.shape[1]
1.61 logistic.py(283):     n_features = X.shape[1]
1.61 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
1.61 logistic.py(285):     w = w.reshape(n_classes, -1)
1.61 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
1.61 logistic.py(287):     if fit_intercept:
1.61 logistic.py(288):         intercept = w[:, -1]
1.61 logistic.py(289):         w = w[:, :-1]
1.61 logistic.py(292):     p = safe_sparse_dot(X, w.T)
1.61 logistic.py(293):     p += intercept
1.61 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
1.61 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
1.61 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
1.61 logistic.py(297):     p = np.exp(p, p)
1.61 logistic.py(298):     return loss, p, w
1.61 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
1.61 logistic.py(346):     diff = sample_weight * (p - Y)
1.61 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
1.61 logistic.py(348):     grad[:, :n_features] += alpha * w
1.61 logistic.py(349):     if fit_intercept:
1.61 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
1.61 logistic.py(351):     return loss, grad.ravel(), p
1.61 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
1.61 logistic.py(339):     n_classes = Y.shape[1]
1.61 logistic.py(340):     n_features = X.shape[1]
1.61 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
1.61 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
1.61 logistic.py(343):                     dtype=X.dtype)
1.61 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
1.61 logistic.py(282):     n_classes = Y.shape[1]
1.61 logistic.py(283):     n_features = X.shape[1]
1.61 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
1.61 logistic.py(285):     w = w.reshape(n_classes, -1)
1.61 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
1.61 logistic.py(287):     if fit_intercept:
1.61 logistic.py(288):         intercept = w[:, -1]
1.61 logistic.py(289):         w = w[:, :-1]
1.61 logistic.py(292):     p = safe_sparse_dot(X, w.T)
1.61 logistic.py(293):     p += intercept
1.61 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
1.61 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
1.61 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
1.61 logistic.py(297):     p = np.exp(p, p)
1.61 logistic.py(298):     return loss, p, w
1.61 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
1.61 logistic.py(346):     diff = sample_weight * (p - Y)
1.61 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
1.61 logistic.py(348):     grad[:, :n_features] += alpha * w
1.61 logistic.py(349):     if fit_intercept:
1.61 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
1.61 logistic.py(351):     return loss, grad.ravel(), p
1.61 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
1.61 logistic.py(339):     n_classes = Y.shape[1]
1.61 logistic.py(340):     n_features = X.shape[1]
1.61 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
1.61 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
1.61 logistic.py(343):                     dtype=X.dtype)
1.61 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
1.61 logistic.py(282):     n_classes = Y.shape[1]
1.61 logistic.py(283):     n_features = X.shape[1]
1.61 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
1.61 logistic.py(285):     w = w.reshape(n_classes, -1)
1.61 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
1.61 logistic.py(287):     if fit_intercept:
1.61 logistic.py(288):         intercept = w[:, -1]
1.61 logistic.py(289):         w = w[:, :-1]
1.61 logistic.py(292):     p = safe_sparse_dot(X, w.T)
1.61 logistic.py(293):     p += intercept
1.61 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
1.61 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
1.61 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
1.61 logistic.py(297):     p = np.exp(p, p)
1.61 logistic.py(298):     return loss, p, w
1.61 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
1.61 logistic.py(346):     diff = sample_weight * (p - Y)
1.61 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
1.61 logistic.py(348):     grad[:, :n_features] += alpha * w
1.61 logistic.py(349):     if fit_intercept:
1.61 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
1.61 logistic.py(351):     return loss, grad.ravel(), p
1.61 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
1.61 logistic.py(339):     n_classes = Y.shape[1]
1.61 logistic.py(340):     n_features = X.shape[1]
1.61 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
1.61 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
1.61 logistic.py(343):                     dtype=X.dtype)
1.61 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
1.61 logistic.py(282):     n_classes = Y.shape[1]
1.61 logistic.py(283):     n_features = X.shape[1]
1.61 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
1.61 logistic.py(285):     w = w.reshape(n_classes, -1)
1.61 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
1.61 logistic.py(287):     if fit_intercept:
1.61 logistic.py(288):         intercept = w[:, -1]
1.61 logistic.py(289):         w = w[:, :-1]
1.61 logistic.py(292):     p = safe_sparse_dot(X, w.T)
1.61 logistic.py(293):     p += intercept
1.61 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
1.61 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
1.61 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
1.61 logistic.py(297):     p = np.exp(p, p)
1.61 logistic.py(298):     return loss, p, w
1.61 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
1.61 logistic.py(346):     diff = sample_weight * (p - Y)
1.61 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
1.61 logistic.py(348):     grad[:, :n_features] += alpha * w
1.61 logistic.py(349):     if fit_intercept:
1.61 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
1.61 logistic.py(351):     return loss, grad.ravel(), p
1.61 logistic.py(718):             if info["warnflag"] == 1:
1.61 logistic.py(723):             n_iter_i = min(info['nit'], max_iter)
1.61 logistic.py(760):         if multi_class == 'multinomial':
1.61 logistic.py(761):             multi_w0 = np.reshape(w0, (classes.size, -1))
1.61 logistic.py(762):             if classes.size == 2:
1.61 logistic.py(764):             coefs.append(multi_w0)
1.61 logistic.py(768):         n_iter[i] = n_iter_i
1.61 logistic.py(712):     for i, C in enumerate(Cs):
1.61 logistic.py(713):         if solver == 'lbfgs':
1.61 logistic.py(714):             w0, loss, info = optimize.fmin_l_bfgs_b(
1.61 logistic.py(715):                 func, w0, fprime=None,
1.61 logistic.py(716):                 args=(X, target, 1. / C, sample_weight),
1.61 logistic.py(717):                 iprint=(verbose > 0) - 1, pgtol=tol, maxiter=max_iter)
1.61 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
1.61 logistic.py(339):     n_classes = Y.shape[1]
1.61 logistic.py(340):     n_features = X.shape[1]
1.61 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
1.61 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
1.61 logistic.py(343):                     dtype=X.dtype)
1.61 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
1.61 logistic.py(282):     n_classes = Y.shape[1]
1.61 logistic.py(283):     n_features = X.shape[1]
1.61 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
1.61 logistic.py(285):     w = w.reshape(n_classes, -1)
1.61 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
1.61 logistic.py(287):     if fit_intercept:
1.61 logistic.py(288):         intercept = w[:, -1]
1.61 logistic.py(289):         w = w[:, :-1]
1.61 logistic.py(292):     p = safe_sparse_dot(X, w.T)
1.61 logistic.py(293):     p += intercept
1.61 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
1.61 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
1.61 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
1.61 logistic.py(297):     p = np.exp(p, p)
1.61 logistic.py(298):     return loss, p, w
1.61 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
1.61 logistic.py(346):     diff = sample_weight * (p - Y)
1.61 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
1.61 logistic.py(348):     grad[:, :n_features] += alpha * w
1.61 logistic.py(349):     if fit_intercept:
1.61 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
1.61 logistic.py(351):     return loss, grad.ravel(), p
1.61 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
1.61 logistic.py(339):     n_classes = Y.shape[1]
1.61 logistic.py(340):     n_features = X.shape[1]
1.61 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
1.61 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
1.61 logistic.py(343):                     dtype=X.dtype)
1.61 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
1.61 logistic.py(282):     n_classes = Y.shape[1]
1.61 logistic.py(283):     n_features = X.shape[1]
1.61 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
1.61 logistic.py(285):     w = w.reshape(n_classes, -1)
1.61 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
1.61 logistic.py(287):     if fit_intercept:
1.61 logistic.py(288):         intercept = w[:, -1]
1.61 logistic.py(289):         w = w[:, :-1]
1.61 logistic.py(292):     p = safe_sparse_dot(X, w.T)
1.61 logistic.py(293):     p += intercept
1.61 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
1.61 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
1.61 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
1.61 logistic.py(297):     p = np.exp(p, p)
1.61 logistic.py(298):     return loss, p, w
1.61 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
1.61 logistic.py(346):     diff = sample_weight * (p - Y)
1.61 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
1.61 logistic.py(348):     grad[:, :n_features] += alpha * w
1.61 logistic.py(349):     if fit_intercept:
1.61 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
1.61 logistic.py(351):     return loss, grad.ravel(), p
1.61 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
1.61 logistic.py(339):     n_classes = Y.shape[1]
1.61 logistic.py(340):     n_features = X.shape[1]
1.61 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
1.61 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
1.61 logistic.py(343):                     dtype=X.dtype)
1.61 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
1.61 logistic.py(282):     n_classes = Y.shape[1]
1.61 logistic.py(283):     n_features = X.shape[1]
1.61 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
1.61 logistic.py(285):     w = w.reshape(n_classes, -1)
1.61 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
1.61 logistic.py(287):     if fit_intercept:
1.61 logistic.py(288):         intercept = w[:, -1]
1.61 logistic.py(289):         w = w[:, :-1]
1.61 logistic.py(292):     p = safe_sparse_dot(X, w.T)
1.61 logistic.py(293):     p += intercept
1.61 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
1.61 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
1.61 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
1.61 logistic.py(297):     p = np.exp(p, p)
1.61 logistic.py(298):     return loss, p, w
1.61 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
1.61 logistic.py(346):     diff = sample_weight * (p - Y)
1.61 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
1.61 logistic.py(348):     grad[:, :n_features] += alpha * w
1.61 logistic.py(349):     if fit_intercept:
1.61 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
1.61 logistic.py(351):     return loss, grad.ravel(), p
1.61 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
1.61 logistic.py(339):     n_classes = Y.shape[1]
1.61 logistic.py(340):     n_features = X.shape[1]
1.61 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
1.61 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
1.61 logistic.py(343):                     dtype=X.dtype)
1.61 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
1.61 logistic.py(282):     n_classes = Y.shape[1]
1.61 logistic.py(283):     n_features = X.shape[1]
1.61 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
1.61 logistic.py(285):     w = w.reshape(n_classes, -1)
1.61 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
1.61 logistic.py(287):     if fit_intercept:
1.61 logistic.py(288):         intercept = w[:, -1]
1.61 logistic.py(289):         w = w[:, :-1]
1.61 logistic.py(292):     p = safe_sparse_dot(X, w.T)
1.61 logistic.py(293):     p += intercept
1.61 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
1.61 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
1.61 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
1.61 logistic.py(297):     p = np.exp(p, p)
1.61 logistic.py(298):     return loss, p, w
1.61 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
1.61 logistic.py(346):     diff = sample_weight * (p - Y)
1.61 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
1.61 logistic.py(348):     grad[:, :n_features] += alpha * w
1.61 logistic.py(349):     if fit_intercept:
1.61 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
1.61 logistic.py(351):     return loss, grad.ravel(), p
1.61 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
1.61 logistic.py(339):     n_classes = Y.shape[1]
1.61 logistic.py(340):     n_features = X.shape[1]
1.61 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
1.61 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
1.61 logistic.py(343):                     dtype=X.dtype)
1.61 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
1.61 logistic.py(282):     n_classes = Y.shape[1]
1.61 logistic.py(283):     n_features = X.shape[1]
1.61 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
1.61 logistic.py(285):     w = w.reshape(n_classes, -1)
1.61 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
1.61 logistic.py(287):     if fit_intercept:
1.61 logistic.py(288):         intercept = w[:, -1]
1.61 logistic.py(289):         w = w[:, :-1]
1.61 logistic.py(292):     p = safe_sparse_dot(X, w.T)
1.61 logistic.py(293):     p += intercept
1.61 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
1.61 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
1.61 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
1.61 logistic.py(297):     p = np.exp(p, p)
1.61 logistic.py(298):     return loss, p, w
1.61 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
1.61 logistic.py(346):     diff = sample_weight * (p - Y)
1.61 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
1.61 logistic.py(348):     grad[:, :n_features] += alpha * w
1.61 logistic.py(349):     if fit_intercept:
1.61 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
1.61 logistic.py(351):     return loss, grad.ravel(), p
1.61 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
1.61 logistic.py(339):     n_classes = Y.shape[1]
1.61 logistic.py(340):     n_features = X.shape[1]
1.61 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
1.61 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
1.61 logistic.py(343):                     dtype=X.dtype)
1.61 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
1.61 logistic.py(282):     n_classes = Y.shape[1]
1.61 logistic.py(283):     n_features = X.shape[1]
1.61 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
1.61 logistic.py(285):     w = w.reshape(n_classes, -1)
1.61 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
1.61 logistic.py(287):     if fit_intercept:
1.61 logistic.py(288):         intercept = w[:, -1]
1.61 logistic.py(289):         w = w[:, :-1]
1.61 logistic.py(292):     p = safe_sparse_dot(X, w.T)
1.61 logistic.py(293):     p += intercept
1.61 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
1.61 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
1.61 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
1.61 logistic.py(297):     p = np.exp(p, p)
1.61 logistic.py(298):     return loss, p, w
1.61 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
1.61 logistic.py(346):     diff = sample_weight * (p - Y)
1.61 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
1.61 logistic.py(348):     grad[:, :n_features] += alpha * w
1.61 logistic.py(349):     if fit_intercept:
1.61 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
1.61 logistic.py(351):     return loss, grad.ravel(), p
1.61 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
1.61 logistic.py(339):     n_classes = Y.shape[1]
1.61 logistic.py(340):     n_features = X.shape[1]
1.61 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
1.61 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
1.61 logistic.py(343):                     dtype=X.dtype)
1.61 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
1.61 logistic.py(282):     n_classes = Y.shape[1]
1.61 logistic.py(283):     n_features = X.shape[1]
1.61 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
1.61 logistic.py(285):     w = w.reshape(n_classes, -1)
1.61 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
1.61 logistic.py(287):     if fit_intercept:
1.61 logistic.py(288):         intercept = w[:, -1]
1.61 logistic.py(289):         w = w[:, :-1]
1.61 logistic.py(292):     p = safe_sparse_dot(X, w.T)
1.61 logistic.py(293):     p += intercept
1.61 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
1.61 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
1.61 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
1.61 logistic.py(297):     p = np.exp(p, p)
1.61 logistic.py(298):     return loss, p, w
1.61 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
1.61 logistic.py(346):     diff = sample_weight * (p - Y)
1.61 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
1.61 logistic.py(348):     grad[:, :n_features] += alpha * w
1.61 logistic.py(349):     if fit_intercept:
1.61 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
1.61 logistic.py(351):     return loss, grad.ravel(), p
1.61 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
1.61 logistic.py(339):     n_classes = Y.shape[1]
1.61 logistic.py(340):     n_features = X.shape[1]
1.61 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
1.61 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
1.61 logistic.py(343):                     dtype=X.dtype)
1.61 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
1.61 logistic.py(282):     n_classes = Y.shape[1]
1.61 logistic.py(283):     n_features = X.shape[1]
1.61 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
1.61 logistic.py(285):     w = w.reshape(n_classes, -1)
1.61 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
1.61 logistic.py(287):     if fit_intercept:
1.61 logistic.py(288):         intercept = w[:, -1]
1.61 logistic.py(289):         w = w[:, :-1]
1.61 logistic.py(292):     p = safe_sparse_dot(X, w.T)
1.61 logistic.py(293):     p += intercept
1.61 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
1.61 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
1.61 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
1.61 logistic.py(297):     p = np.exp(p, p)
1.61 logistic.py(298):     return loss, p, w
1.61 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
1.61 logistic.py(346):     diff = sample_weight * (p - Y)
1.61 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
1.61 logistic.py(348):     grad[:, :n_features] += alpha * w
1.61 logistic.py(349):     if fit_intercept:
1.61 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
1.61 logistic.py(351):     return loss, grad.ravel(), p
1.61 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
1.61 logistic.py(339):     n_classes = Y.shape[1]
1.61 logistic.py(340):     n_features = X.shape[1]
1.61 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
1.61 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
1.61 logistic.py(343):                     dtype=X.dtype)
1.61 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
1.61 logistic.py(282):     n_classes = Y.shape[1]
1.61 logistic.py(283):     n_features = X.shape[1]
1.61 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
1.61 logistic.py(285):     w = w.reshape(n_classes, -1)
1.61 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
1.61 logistic.py(287):     if fit_intercept:
1.61 logistic.py(288):         intercept = w[:, -1]
1.61 logistic.py(289):         w = w[:, :-1]
1.61 logistic.py(292):     p = safe_sparse_dot(X, w.T)
1.61 logistic.py(293):     p += intercept
1.61 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
1.61 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
1.61 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
1.61 logistic.py(297):     p = np.exp(p, p)
1.61 logistic.py(298):     return loss, p, w
1.61 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
1.61 logistic.py(346):     diff = sample_weight * (p - Y)
1.61 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
1.61 logistic.py(348):     grad[:, :n_features] += alpha * w
1.61 logistic.py(349):     if fit_intercept:
1.61 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
1.62 logistic.py(351):     return loss, grad.ravel(), p
1.62 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
1.62 logistic.py(339):     n_classes = Y.shape[1]
1.62 logistic.py(340):     n_features = X.shape[1]
1.62 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
1.62 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
1.62 logistic.py(343):                     dtype=X.dtype)
1.62 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
1.62 logistic.py(282):     n_classes = Y.shape[1]
1.62 logistic.py(283):     n_features = X.shape[1]
1.62 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
1.62 logistic.py(285):     w = w.reshape(n_classes, -1)
1.62 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
1.62 logistic.py(287):     if fit_intercept:
1.62 logistic.py(288):         intercept = w[:, -1]
1.62 logistic.py(289):         w = w[:, :-1]
1.62 logistic.py(292):     p = safe_sparse_dot(X, w.T)
1.62 logistic.py(293):     p += intercept
1.62 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
1.62 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
1.62 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
1.62 logistic.py(297):     p = np.exp(p, p)
1.62 logistic.py(298):     return loss, p, w
1.62 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
1.62 logistic.py(346):     diff = sample_weight * (p - Y)
1.62 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
1.62 logistic.py(348):     grad[:, :n_features] += alpha * w
1.62 logistic.py(349):     if fit_intercept:
1.62 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
1.62 logistic.py(351):     return loss, grad.ravel(), p
1.62 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
1.62 logistic.py(339):     n_classes = Y.shape[1]
1.62 logistic.py(340):     n_features = X.shape[1]
1.62 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
1.62 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
1.62 logistic.py(343):                     dtype=X.dtype)
1.62 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
1.62 logistic.py(282):     n_classes = Y.shape[1]
1.62 logistic.py(283):     n_features = X.shape[1]
1.62 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
1.62 logistic.py(285):     w = w.reshape(n_classes, -1)
1.62 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
1.62 logistic.py(287):     if fit_intercept:
1.62 logistic.py(288):         intercept = w[:, -1]
1.62 logistic.py(289):         w = w[:, :-1]
1.62 logistic.py(292):     p = safe_sparse_dot(X, w.T)
1.62 logistic.py(293):     p += intercept
1.62 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
1.62 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
1.62 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
1.62 logistic.py(297):     p = np.exp(p, p)
1.62 logistic.py(298):     return loss, p, w
1.62 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
1.62 logistic.py(346):     diff = sample_weight * (p - Y)
1.62 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
1.62 logistic.py(348):     grad[:, :n_features] += alpha * w
1.62 logistic.py(349):     if fit_intercept:
1.62 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
1.62 logistic.py(351):     return loss, grad.ravel(), p
1.62 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
1.62 logistic.py(339):     n_classes = Y.shape[1]
1.62 logistic.py(340):     n_features = X.shape[1]
1.62 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
1.62 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
1.62 logistic.py(343):                     dtype=X.dtype)
1.62 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
1.62 logistic.py(282):     n_classes = Y.shape[1]
1.62 logistic.py(283):     n_features = X.shape[1]
1.62 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
1.62 logistic.py(285):     w = w.reshape(n_classes, -1)
1.62 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
1.62 logistic.py(287):     if fit_intercept:
1.62 logistic.py(288):         intercept = w[:, -1]
1.62 logistic.py(289):         w = w[:, :-1]
1.62 logistic.py(292):     p = safe_sparse_dot(X, w.T)
1.62 logistic.py(293):     p += intercept
1.62 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
1.62 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
1.62 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
1.62 logistic.py(297):     p = np.exp(p, p)
1.62 logistic.py(298):     return loss, p, w
1.62 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
1.62 logistic.py(346):     diff = sample_weight * (p - Y)
1.62 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
1.62 logistic.py(348):     grad[:, :n_features] += alpha * w
1.62 logistic.py(349):     if fit_intercept:
1.62 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
1.62 logistic.py(351):     return loss, grad.ravel(), p
1.62 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
1.62 logistic.py(339):     n_classes = Y.shape[1]
1.62 logistic.py(340):     n_features = X.shape[1]
1.62 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
1.62 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
1.62 logistic.py(343):                     dtype=X.dtype)
1.62 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
1.62 logistic.py(282):     n_classes = Y.shape[1]
1.62 logistic.py(283):     n_features = X.shape[1]
1.62 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
1.62 logistic.py(285):     w = w.reshape(n_classes, -1)
1.62 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
1.62 logistic.py(287):     if fit_intercept:
1.62 logistic.py(288):         intercept = w[:, -1]
1.62 logistic.py(289):         w = w[:, :-1]
1.62 logistic.py(292):     p = safe_sparse_dot(X, w.T)
1.62 logistic.py(293):     p += intercept
1.62 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
1.62 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
1.62 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
1.62 logistic.py(297):     p = np.exp(p, p)
1.62 logistic.py(298):     return loss, p, w
1.62 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
1.62 logistic.py(346):     diff = sample_weight * (p - Y)
1.62 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
1.62 logistic.py(348):     grad[:, :n_features] += alpha * w
1.62 logistic.py(349):     if fit_intercept:
1.62 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
1.62 logistic.py(351):     return loss, grad.ravel(), p
1.62 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
1.62 logistic.py(339):     n_classes = Y.shape[1]
1.62 logistic.py(340):     n_features = X.shape[1]
1.62 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
1.62 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
1.62 logistic.py(343):                     dtype=X.dtype)
1.62 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
1.62 logistic.py(282):     n_classes = Y.shape[1]
1.62 logistic.py(283):     n_features = X.shape[1]
1.62 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
1.62 logistic.py(285):     w = w.reshape(n_classes, -1)
1.62 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
1.62 logistic.py(287):     if fit_intercept:
1.62 logistic.py(288):         intercept = w[:, -1]
1.62 logistic.py(289):         w = w[:, :-1]
1.62 logistic.py(292):     p = safe_sparse_dot(X, w.T)
1.62 logistic.py(293):     p += intercept
1.62 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
1.62 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
1.62 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
1.62 logistic.py(297):     p = np.exp(p, p)
1.62 logistic.py(298):     return loss, p, w
1.62 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
1.62 logistic.py(346):     diff = sample_weight * (p - Y)
1.62 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
1.62 logistic.py(348):     grad[:, :n_features] += alpha * w
1.62 logistic.py(349):     if fit_intercept:
1.62 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
1.62 logistic.py(351):     return loss, grad.ravel(), p
1.62 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
1.62 logistic.py(339):     n_classes = Y.shape[1]
1.62 logistic.py(340):     n_features = X.shape[1]
1.62 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
1.62 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
1.62 logistic.py(343):                     dtype=X.dtype)
1.62 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
1.62 logistic.py(282):     n_classes = Y.shape[1]
1.62 logistic.py(283):     n_features = X.shape[1]
1.62 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
1.62 logistic.py(285):     w = w.reshape(n_classes, -1)
1.62 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
1.62 logistic.py(287):     if fit_intercept:
1.62 logistic.py(288):         intercept = w[:, -1]
1.62 logistic.py(289):         w = w[:, :-1]
1.62 logistic.py(292):     p = safe_sparse_dot(X, w.T)
1.62 logistic.py(293):     p += intercept
1.62 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
1.62 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
1.62 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
1.62 logistic.py(297):     p = np.exp(p, p)
1.62 logistic.py(298):     return loss, p, w
1.62 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
1.62 logistic.py(346):     diff = sample_weight * (p - Y)
1.62 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
1.62 logistic.py(348):     grad[:, :n_features] += alpha * w
1.62 logistic.py(349):     if fit_intercept:
1.62 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
1.62 logistic.py(351):     return loss, grad.ravel(), p
1.62 logistic.py(718):             if info["warnflag"] == 1:
1.62 logistic.py(723):             n_iter_i = min(info['nit'], max_iter)
1.62 logistic.py(760):         if multi_class == 'multinomial':
1.62 logistic.py(761):             multi_w0 = np.reshape(w0, (classes.size, -1))
1.62 logistic.py(762):             if classes.size == 2:
1.62 logistic.py(764):             coefs.append(multi_w0)
1.62 logistic.py(768):         n_iter[i] = n_iter_i
1.62 logistic.py(712):     for i, C in enumerate(Cs):
1.62 logistic.py(713):         if solver == 'lbfgs':
1.62 logistic.py(714):             w0, loss, info = optimize.fmin_l_bfgs_b(
1.62 logistic.py(715):                 func, w0, fprime=None,
1.62 logistic.py(716):                 args=(X, target, 1. / C, sample_weight),
1.62 logistic.py(717):                 iprint=(verbose > 0) - 1, pgtol=tol, maxiter=max_iter)
1.62 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
1.62 logistic.py(339):     n_classes = Y.shape[1]
1.62 logistic.py(340):     n_features = X.shape[1]
1.62 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
1.62 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
1.62 logistic.py(343):                     dtype=X.dtype)
1.62 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
1.62 logistic.py(282):     n_classes = Y.shape[1]
1.62 logistic.py(283):     n_features = X.shape[1]
1.62 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
1.62 logistic.py(285):     w = w.reshape(n_classes, -1)
1.62 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
1.62 logistic.py(287):     if fit_intercept:
1.62 logistic.py(288):         intercept = w[:, -1]
1.62 logistic.py(289):         w = w[:, :-1]
1.62 logistic.py(292):     p = safe_sparse_dot(X, w.T)
1.62 logistic.py(293):     p += intercept
1.62 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
1.62 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
1.62 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
1.62 logistic.py(297):     p = np.exp(p, p)
1.62 logistic.py(298):     return loss, p, w
1.62 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
1.62 logistic.py(346):     diff = sample_weight * (p - Y)
1.62 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
1.62 logistic.py(348):     grad[:, :n_features] += alpha * w
1.62 logistic.py(349):     if fit_intercept:
1.62 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
1.62 logistic.py(351):     return loss, grad.ravel(), p
1.62 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
1.62 logistic.py(339):     n_classes = Y.shape[1]
1.62 logistic.py(340):     n_features = X.shape[1]
1.62 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
1.62 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
1.62 logistic.py(343):                     dtype=X.dtype)
1.62 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
1.62 logistic.py(282):     n_classes = Y.shape[1]
1.62 logistic.py(283):     n_features = X.shape[1]
1.62 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
1.62 logistic.py(285):     w = w.reshape(n_classes, -1)
1.62 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
1.62 logistic.py(287):     if fit_intercept:
1.62 logistic.py(288):         intercept = w[:, -1]
1.62 logistic.py(289):         w = w[:, :-1]
1.62 logistic.py(292):     p = safe_sparse_dot(X, w.T)
1.62 logistic.py(293):     p += intercept
1.62 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
1.62 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
1.62 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
1.62 logistic.py(297):     p = np.exp(p, p)
1.62 logistic.py(298):     return loss, p, w
1.62 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
1.62 logistic.py(346):     diff = sample_weight * (p - Y)
1.62 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
1.62 logistic.py(348):     grad[:, :n_features] += alpha * w
1.62 logistic.py(349):     if fit_intercept:
1.62 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
1.62 logistic.py(351):     return loss, grad.ravel(), p
1.62 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
1.62 logistic.py(339):     n_classes = Y.shape[1]
1.62 logistic.py(340):     n_features = X.shape[1]
1.62 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
1.62 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
1.62 logistic.py(343):                     dtype=X.dtype)
1.62 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
1.62 logistic.py(282):     n_classes = Y.shape[1]
1.62 logistic.py(283):     n_features = X.shape[1]
1.62 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
1.62 logistic.py(285):     w = w.reshape(n_classes, -1)
1.62 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
1.62 logistic.py(287):     if fit_intercept:
1.62 logistic.py(288):         intercept = w[:, -1]
1.62 logistic.py(289):         w = w[:, :-1]
1.62 logistic.py(292):     p = safe_sparse_dot(X, w.T)
1.62 logistic.py(293):     p += intercept
1.62 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
1.62 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
1.62 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
1.62 logistic.py(297):     p = np.exp(p, p)
1.62 logistic.py(298):     return loss, p, w
1.62 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
1.62 logistic.py(346):     diff = sample_weight * (p - Y)
1.62 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
1.62 logistic.py(348):     grad[:, :n_features] += alpha * w
1.62 logistic.py(349):     if fit_intercept:
1.62 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
1.62 logistic.py(351):     return loss, grad.ravel(), p
1.62 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
1.62 logistic.py(339):     n_classes = Y.shape[1]
1.62 logistic.py(340):     n_features = X.shape[1]
1.62 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
1.62 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
1.62 logistic.py(343):                     dtype=X.dtype)
1.62 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
1.62 logistic.py(282):     n_classes = Y.shape[1]
1.62 logistic.py(283):     n_features = X.shape[1]
1.62 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
1.62 logistic.py(285):     w = w.reshape(n_classes, -1)
1.62 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
1.62 logistic.py(287):     if fit_intercept:
1.62 logistic.py(288):         intercept = w[:, -1]
1.62 logistic.py(289):         w = w[:, :-1]
1.62 logistic.py(292):     p = safe_sparse_dot(X, w.T)
1.62 logistic.py(293):     p += intercept
1.62 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
1.62 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
1.62 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
1.62 logistic.py(297):     p = np.exp(p, p)
1.62 logistic.py(298):     return loss, p, w
1.62 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
1.62 logistic.py(346):     diff = sample_weight * (p - Y)
1.62 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
1.62 logistic.py(348):     grad[:, :n_features] += alpha * w
1.62 logistic.py(349):     if fit_intercept:
1.62 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
1.62 logistic.py(351):     return loss, grad.ravel(), p
1.62 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
1.62 logistic.py(339):     n_classes = Y.shape[1]
1.62 logistic.py(340):     n_features = X.shape[1]
1.62 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
1.62 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
1.62 logistic.py(343):                     dtype=X.dtype)
1.62 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
1.62 logistic.py(282):     n_classes = Y.shape[1]
1.62 logistic.py(283):     n_features = X.shape[1]
1.62 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
1.62 logistic.py(285):     w = w.reshape(n_classes, -1)
1.62 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
1.62 logistic.py(287):     if fit_intercept:
1.62 logistic.py(288):         intercept = w[:, -1]
1.62 logistic.py(289):         w = w[:, :-1]
1.62 logistic.py(292):     p = safe_sparse_dot(X, w.T)
1.62 logistic.py(293):     p += intercept
1.62 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
1.62 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
1.62 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
1.62 logistic.py(297):     p = np.exp(p, p)
1.62 logistic.py(298):     return loss, p, w
1.62 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
1.62 logistic.py(346):     diff = sample_weight * (p - Y)
1.62 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
1.62 logistic.py(348):     grad[:, :n_features] += alpha * w
1.62 logistic.py(349):     if fit_intercept:
1.62 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
1.62 logistic.py(351):     return loss, grad.ravel(), p
1.62 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
1.62 logistic.py(339):     n_classes = Y.shape[1]
1.62 logistic.py(340):     n_features = X.shape[1]
1.62 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
1.62 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
1.62 logistic.py(343):                     dtype=X.dtype)
1.62 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
1.62 logistic.py(282):     n_classes = Y.shape[1]
1.62 logistic.py(283):     n_features = X.shape[1]
1.62 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
1.62 logistic.py(285):     w = w.reshape(n_classes, -1)
1.62 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
1.62 logistic.py(287):     if fit_intercept:
1.62 logistic.py(288):         intercept = w[:, -1]
1.62 logistic.py(289):         w = w[:, :-1]
1.62 logistic.py(292):     p = safe_sparse_dot(X, w.T)
1.62 logistic.py(293):     p += intercept
1.62 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
1.62 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
1.62 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
1.62 logistic.py(297):     p = np.exp(p, p)
1.62 logistic.py(298):     return loss, p, w
1.62 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
1.62 logistic.py(346):     diff = sample_weight * (p - Y)
1.62 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
1.62 logistic.py(348):     grad[:, :n_features] += alpha * w
1.62 logistic.py(349):     if fit_intercept:
1.62 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
1.62 logistic.py(351):     return loss, grad.ravel(), p
1.62 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
1.62 logistic.py(339):     n_classes = Y.shape[1]
1.62 logistic.py(340):     n_features = X.shape[1]
1.62 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
1.62 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
1.62 logistic.py(343):                     dtype=X.dtype)
1.62 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
1.62 logistic.py(282):     n_classes = Y.shape[1]
1.62 logistic.py(283):     n_features = X.shape[1]
1.62 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
1.62 logistic.py(285):     w = w.reshape(n_classes, -1)
1.62 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
1.62 logistic.py(287):     if fit_intercept:
1.62 logistic.py(288):         intercept = w[:, -1]
1.62 logistic.py(289):         w = w[:, :-1]
1.62 logistic.py(292):     p = safe_sparse_dot(X, w.T)
1.62 logistic.py(293):     p += intercept
1.62 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
1.62 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
1.62 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
1.62 logistic.py(297):     p = np.exp(p, p)
1.62 logistic.py(298):     return loss, p, w
1.62 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
1.62 logistic.py(346):     diff = sample_weight * (p - Y)
1.62 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
1.62 logistic.py(348):     grad[:, :n_features] += alpha * w
1.62 logistic.py(349):     if fit_intercept:
1.62 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
1.62 logistic.py(351):     return loss, grad.ravel(), p
1.62 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
1.62 logistic.py(339):     n_classes = Y.shape[1]
1.62 logistic.py(340):     n_features = X.shape[1]
1.62 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
1.62 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
1.62 logistic.py(343):                     dtype=X.dtype)
1.62 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
1.62 logistic.py(282):     n_classes = Y.shape[1]
1.62 logistic.py(283):     n_features = X.shape[1]
1.62 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
1.62 logistic.py(285):     w = w.reshape(n_classes, -1)
1.62 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
1.62 logistic.py(287):     if fit_intercept:
1.62 logistic.py(288):         intercept = w[:, -1]
1.62 logistic.py(289):         w = w[:, :-1]
1.62 logistic.py(292):     p = safe_sparse_dot(X, w.T)
1.62 logistic.py(293):     p += intercept
1.62 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
1.62 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
1.62 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
1.62 logistic.py(297):     p = np.exp(p, p)
1.62 logistic.py(298):     return loss, p, w
1.62 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
1.62 logistic.py(346):     diff = sample_weight * (p - Y)
1.62 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
1.62 logistic.py(348):     grad[:, :n_features] += alpha * w
1.62 logistic.py(349):     if fit_intercept:
1.62 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
1.62 logistic.py(351):     return loss, grad.ravel(), p
1.62 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
1.62 logistic.py(339):     n_classes = Y.shape[1]
1.62 logistic.py(340):     n_features = X.shape[1]
1.62 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
1.62 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
1.62 logistic.py(343):                     dtype=X.dtype)
1.62 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
1.62 logistic.py(282):     n_classes = Y.shape[1]
1.62 logistic.py(283):     n_features = X.shape[1]
1.62 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
1.62 logistic.py(285):     w = w.reshape(n_classes, -1)
1.62 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
1.62 logistic.py(287):     if fit_intercept:
1.62 logistic.py(288):         intercept = w[:, -1]
1.62 logistic.py(289):         w = w[:, :-1]
1.62 logistic.py(292):     p = safe_sparse_dot(X, w.T)
1.62 logistic.py(293):     p += intercept
1.62 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
1.62 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
1.62 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
1.62 logistic.py(297):     p = np.exp(p, p)
1.62 logistic.py(298):     return loss, p, w
1.62 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
1.62 logistic.py(346):     diff = sample_weight * (p - Y)
1.62 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
1.62 logistic.py(348):     grad[:, :n_features] += alpha * w
1.62 logistic.py(349):     if fit_intercept:
1.62 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
1.62 logistic.py(351):     return loss, grad.ravel(), p
1.62 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
1.62 logistic.py(339):     n_classes = Y.shape[1]
1.62 logistic.py(340):     n_features = X.shape[1]
1.62 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
1.62 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
1.62 logistic.py(343):                     dtype=X.dtype)
1.62 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
1.62 logistic.py(282):     n_classes = Y.shape[1]
1.62 logistic.py(283):     n_features = X.shape[1]
1.62 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
1.62 logistic.py(285):     w = w.reshape(n_classes, -1)
1.62 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
1.62 logistic.py(287):     if fit_intercept:
1.62 logistic.py(288):         intercept = w[:, -1]
1.62 logistic.py(289):         w = w[:, :-1]
1.62 logistic.py(292):     p = safe_sparse_dot(X, w.T)
1.62 logistic.py(293):     p += intercept
1.62 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
1.62 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
1.62 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
1.62 logistic.py(297):     p = np.exp(p, p)
1.62 logistic.py(298):     return loss, p, w
1.62 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
1.62 logistic.py(346):     diff = sample_weight * (p - Y)
1.62 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
1.62 logistic.py(348):     grad[:, :n_features] += alpha * w
1.63 logistic.py(349):     if fit_intercept:
1.63 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
1.63 logistic.py(351):     return loss, grad.ravel(), p
1.63 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
1.63 logistic.py(339):     n_classes = Y.shape[1]
1.63 logistic.py(340):     n_features = X.shape[1]
1.63 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
1.63 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
1.63 logistic.py(343):                     dtype=X.dtype)
1.63 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
1.63 logistic.py(282):     n_classes = Y.shape[1]
1.63 logistic.py(283):     n_features = X.shape[1]
1.63 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
1.63 logistic.py(285):     w = w.reshape(n_classes, -1)
1.63 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
1.63 logistic.py(287):     if fit_intercept:
1.63 logistic.py(288):         intercept = w[:, -1]
1.63 logistic.py(289):         w = w[:, :-1]
1.63 logistic.py(292):     p = safe_sparse_dot(X, w.T)
1.63 logistic.py(293):     p += intercept
1.63 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
1.63 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
1.63 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
1.63 logistic.py(297):     p = np.exp(p, p)
1.63 logistic.py(298):     return loss, p, w
1.63 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
1.63 logistic.py(346):     diff = sample_weight * (p - Y)
1.63 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
1.63 logistic.py(348):     grad[:, :n_features] += alpha * w
1.63 logistic.py(349):     if fit_intercept:
1.63 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
1.63 logistic.py(351):     return loss, grad.ravel(), p
1.63 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
1.63 logistic.py(339):     n_classes = Y.shape[1]
1.63 logistic.py(340):     n_features = X.shape[1]
1.63 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
1.63 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
1.63 logistic.py(343):                     dtype=X.dtype)
1.63 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
1.63 logistic.py(282):     n_classes = Y.shape[1]
1.63 logistic.py(283):     n_features = X.shape[1]
1.63 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
1.63 logistic.py(285):     w = w.reshape(n_classes, -1)
1.63 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
1.63 logistic.py(287):     if fit_intercept:
1.63 logistic.py(288):         intercept = w[:, -1]
1.63 logistic.py(289):         w = w[:, :-1]
1.63 logistic.py(292):     p = safe_sparse_dot(X, w.T)
1.63 logistic.py(293):     p += intercept
1.63 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
1.63 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
1.63 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
1.63 logistic.py(297):     p = np.exp(p, p)
1.63 logistic.py(298):     return loss, p, w
1.63 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
1.63 logistic.py(346):     diff = sample_weight * (p - Y)
1.63 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
1.63 logistic.py(348):     grad[:, :n_features] += alpha * w
1.63 logistic.py(349):     if fit_intercept:
1.63 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
1.63 logistic.py(351):     return loss, grad.ravel(), p
1.63 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
1.63 logistic.py(339):     n_classes = Y.shape[1]
1.63 logistic.py(340):     n_features = X.shape[1]
1.63 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
1.63 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
1.63 logistic.py(343):                     dtype=X.dtype)
1.63 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
1.63 logistic.py(282):     n_classes = Y.shape[1]
1.63 logistic.py(283):     n_features = X.shape[1]
1.63 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
1.63 logistic.py(285):     w = w.reshape(n_classes, -1)
1.63 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
1.63 logistic.py(287):     if fit_intercept:
1.63 logistic.py(288):         intercept = w[:, -1]
1.63 logistic.py(289):         w = w[:, :-1]
1.63 logistic.py(292):     p = safe_sparse_dot(X, w.T)
1.63 logistic.py(293):     p += intercept
1.63 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
1.63 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
1.63 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
1.63 logistic.py(297):     p = np.exp(p, p)
1.63 logistic.py(298):     return loss, p, w
1.63 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
1.63 logistic.py(346):     diff = sample_weight * (p - Y)
1.63 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
1.63 logistic.py(348):     grad[:, :n_features] += alpha * w
1.63 logistic.py(349):     if fit_intercept:
1.63 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
1.63 logistic.py(351):     return loss, grad.ravel(), p
1.63 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
1.63 logistic.py(339):     n_classes = Y.shape[1]
1.63 logistic.py(340):     n_features = X.shape[1]
1.63 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
1.63 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
1.63 logistic.py(343):                     dtype=X.dtype)
1.63 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
1.63 logistic.py(282):     n_classes = Y.shape[1]
1.63 logistic.py(283):     n_features = X.shape[1]
1.63 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
1.63 logistic.py(285):     w = w.reshape(n_classes, -1)
1.63 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
1.63 logistic.py(287):     if fit_intercept:
1.63 logistic.py(288):         intercept = w[:, -1]
1.63 logistic.py(289):         w = w[:, :-1]
1.63 logistic.py(292):     p = safe_sparse_dot(X, w.T)
1.63 logistic.py(293):     p += intercept
1.63 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
1.63 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
1.63 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
1.63 logistic.py(297):     p = np.exp(p, p)
1.63 logistic.py(298):     return loss, p, w
1.63 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
1.63 logistic.py(346):     diff = sample_weight * (p - Y)
1.63 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
1.63 logistic.py(348):     grad[:, :n_features] += alpha * w
1.63 logistic.py(349):     if fit_intercept:
1.63 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
1.63 logistic.py(351):     return loss, grad.ravel(), p
1.63 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
1.63 logistic.py(339):     n_classes = Y.shape[1]
1.63 logistic.py(340):     n_features = X.shape[1]
1.63 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
1.63 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
1.63 logistic.py(343):                     dtype=X.dtype)
1.63 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
1.63 logistic.py(282):     n_classes = Y.shape[1]
1.63 logistic.py(283):     n_features = X.shape[1]
1.63 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
1.63 logistic.py(285):     w = w.reshape(n_classes, -1)
1.63 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
1.63 logistic.py(287):     if fit_intercept:
1.63 logistic.py(288):         intercept = w[:, -1]
1.63 logistic.py(289):         w = w[:, :-1]
1.63 logistic.py(292):     p = safe_sparse_dot(X, w.T)
1.63 logistic.py(293):     p += intercept
1.63 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
1.63 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
1.63 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
1.63 logistic.py(297):     p = np.exp(p, p)
1.63 logistic.py(298):     return loss, p, w
1.63 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
1.63 logistic.py(346):     diff = sample_weight * (p - Y)
1.63 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
1.63 logistic.py(348):     grad[:, :n_features] += alpha * w
1.63 logistic.py(349):     if fit_intercept:
1.63 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
1.63 logistic.py(351):     return loss, grad.ravel(), p
1.63 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
1.63 logistic.py(339):     n_classes = Y.shape[1]
1.63 logistic.py(340):     n_features = X.shape[1]
1.63 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
1.63 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
1.63 logistic.py(343):                     dtype=X.dtype)
1.63 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
1.63 logistic.py(282):     n_classes = Y.shape[1]
1.63 logistic.py(283):     n_features = X.shape[1]
1.63 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
1.63 logistic.py(285):     w = w.reshape(n_classes, -1)
1.63 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
1.63 logistic.py(287):     if fit_intercept:
1.63 logistic.py(288):         intercept = w[:, -1]
1.63 logistic.py(289):         w = w[:, :-1]
1.63 logistic.py(292):     p = safe_sparse_dot(X, w.T)
1.63 logistic.py(293):     p += intercept
1.63 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
1.63 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
1.63 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
1.63 logistic.py(297):     p = np.exp(p, p)
1.63 logistic.py(298):     return loss, p, w
1.63 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
1.63 logistic.py(346):     diff = sample_weight * (p - Y)
1.63 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
1.63 logistic.py(348):     grad[:, :n_features] += alpha * w
1.63 logistic.py(349):     if fit_intercept:
1.63 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
1.63 logistic.py(351):     return loss, grad.ravel(), p
1.63 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
1.63 logistic.py(339):     n_classes = Y.shape[1]
1.63 logistic.py(340):     n_features = X.shape[1]
1.63 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
1.63 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
1.63 logistic.py(343):                     dtype=X.dtype)
1.63 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
1.63 logistic.py(282):     n_classes = Y.shape[1]
1.63 logistic.py(283):     n_features = X.shape[1]
1.63 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
1.63 logistic.py(285):     w = w.reshape(n_classes, -1)
1.63 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
1.63 logistic.py(287):     if fit_intercept:
1.63 logistic.py(288):         intercept = w[:, -1]
1.63 logistic.py(289):         w = w[:, :-1]
1.63 logistic.py(292):     p = safe_sparse_dot(X, w.T)
1.63 logistic.py(293):     p += intercept
1.63 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
1.63 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
1.63 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
1.63 logistic.py(297):     p = np.exp(p, p)
1.63 logistic.py(298):     return loss, p, w
1.63 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
1.63 logistic.py(346):     diff = sample_weight * (p - Y)
1.63 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
1.63 logistic.py(348):     grad[:, :n_features] += alpha * w
1.63 logistic.py(349):     if fit_intercept:
1.63 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
1.63 logistic.py(351):     return loss, grad.ravel(), p
1.63 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
1.63 logistic.py(339):     n_classes = Y.shape[1]
1.63 logistic.py(340):     n_features = X.shape[1]
1.63 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
1.63 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
1.63 logistic.py(343):                     dtype=X.dtype)
1.63 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
1.63 logistic.py(282):     n_classes = Y.shape[1]
1.63 logistic.py(283):     n_features = X.shape[1]
1.63 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
1.63 logistic.py(285):     w = w.reshape(n_classes, -1)
1.63 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
1.63 logistic.py(287):     if fit_intercept:
1.63 logistic.py(288):         intercept = w[:, -1]
1.63 logistic.py(289):         w = w[:, :-1]
1.63 logistic.py(292):     p = safe_sparse_dot(X, w.T)
1.63 logistic.py(293):     p += intercept
1.63 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
1.63 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
1.63 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
1.63 logistic.py(297):     p = np.exp(p, p)
1.63 logistic.py(298):     return loss, p, w
1.63 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
1.63 logistic.py(346):     diff = sample_weight * (p - Y)
1.63 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
1.63 logistic.py(348):     grad[:, :n_features] += alpha * w
1.63 logistic.py(349):     if fit_intercept:
1.63 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
1.63 logistic.py(351):     return loss, grad.ravel(), p
1.63 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
1.63 logistic.py(339):     n_classes = Y.shape[1]
1.63 logistic.py(340):     n_features = X.shape[1]
1.63 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
1.63 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
1.63 logistic.py(343):                     dtype=X.dtype)
1.63 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
1.63 logistic.py(282):     n_classes = Y.shape[1]
1.63 logistic.py(283):     n_features = X.shape[1]
1.63 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
1.63 logistic.py(285):     w = w.reshape(n_classes, -1)
1.63 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
1.63 logistic.py(287):     if fit_intercept:
1.63 logistic.py(288):         intercept = w[:, -1]
1.63 logistic.py(289):         w = w[:, :-1]
1.63 logistic.py(292):     p = safe_sparse_dot(X, w.T)
1.63 logistic.py(293):     p += intercept
1.63 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
1.63 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
1.63 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
1.63 logistic.py(297):     p = np.exp(p, p)
1.63 logistic.py(298):     return loss, p, w
1.63 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
1.63 logistic.py(346):     diff = sample_weight * (p - Y)
1.63 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
1.63 logistic.py(348):     grad[:, :n_features] += alpha * w
1.63 logistic.py(349):     if fit_intercept:
1.63 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
1.63 logistic.py(351):     return loss, grad.ravel(), p
1.63 logistic.py(718):             if info["warnflag"] == 1:
1.63 logistic.py(723):             n_iter_i = min(info['nit'], max_iter)
1.63 logistic.py(760):         if multi_class == 'multinomial':
1.63 logistic.py(761):             multi_w0 = np.reshape(w0, (classes.size, -1))
1.63 logistic.py(762):             if classes.size == 2:
1.63 logistic.py(764):             coefs.append(multi_w0)
1.63 logistic.py(768):         n_iter[i] = n_iter_i
1.63 logistic.py(712):     for i, C in enumerate(Cs):
1.63 logistic.py(713):         if solver == 'lbfgs':
1.63 logistic.py(714):             w0, loss, info = optimize.fmin_l_bfgs_b(
1.63 logistic.py(715):                 func, w0, fprime=None,
1.63 logistic.py(716):                 args=(X, target, 1. / C, sample_weight),
1.63 logistic.py(717):                 iprint=(verbose > 0) - 1, pgtol=tol, maxiter=max_iter)
1.63 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
1.63 logistic.py(339):     n_classes = Y.shape[1]
1.63 logistic.py(340):     n_features = X.shape[1]
1.63 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
1.63 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
1.63 logistic.py(343):                     dtype=X.dtype)
1.63 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
1.63 logistic.py(282):     n_classes = Y.shape[1]
1.63 logistic.py(283):     n_features = X.shape[1]
1.63 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
1.63 logistic.py(285):     w = w.reshape(n_classes, -1)
1.63 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
1.63 logistic.py(287):     if fit_intercept:
1.63 logistic.py(288):         intercept = w[:, -1]
1.63 logistic.py(289):         w = w[:, :-1]
1.63 logistic.py(292):     p = safe_sparse_dot(X, w.T)
1.63 logistic.py(293):     p += intercept
1.63 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
1.63 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
1.63 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
1.63 logistic.py(297):     p = np.exp(p, p)
1.63 logistic.py(298):     return loss, p, w
1.63 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
1.63 logistic.py(346):     diff = sample_weight * (p - Y)
1.63 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
1.63 logistic.py(348):     grad[:, :n_features] += alpha * w
1.63 logistic.py(349):     if fit_intercept:
1.63 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
1.63 logistic.py(351):     return loss, grad.ravel(), p
1.63 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
1.63 logistic.py(339):     n_classes = Y.shape[1]
1.63 logistic.py(340):     n_features = X.shape[1]
1.63 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
1.63 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
1.63 logistic.py(343):                     dtype=X.dtype)
1.63 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
1.63 logistic.py(282):     n_classes = Y.shape[1]
1.63 logistic.py(283):     n_features = X.shape[1]
1.63 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
1.63 logistic.py(285):     w = w.reshape(n_classes, -1)
1.63 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
1.63 logistic.py(287):     if fit_intercept:
1.63 logistic.py(288):         intercept = w[:, -1]
1.63 logistic.py(289):         w = w[:, :-1]
1.63 logistic.py(292):     p = safe_sparse_dot(X, w.T)
1.63 logistic.py(293):     p += intercept
1.63 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
1.63 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
1.63 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
1.63 logistic.py(297):     p = np.exp(p, p)
1.63 logistic.py(298):     return loss, p, w
1.63 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
1.63 logistic.py(346):     diff = sample_weight * (p - Y)
1.63 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
1.63 logistic.py(348):     grad[:, :n_features] += alpha * w
1.63 logistic.py(349):     if fit_intercept:
1.63 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
1.63 logistic.py(351):     return loss, grad.ravel(), p
1.63 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
1.63 logistic.py(339):     n_classes = Y.shape[1]
1.63 logistic.py(340):     n_features = X.shape[1]
1.63 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
1.63 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
1.63 logistic.py(343):                     dtype=X.dtype)
1.63 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
1.63 logistic.py(282):     n_classes = Y.shape[1]
1.63 logistic.py(283):     n_features = X.shape[1]
1.63 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
1.63 logistic.py(285):     w = w.reshape(n_classes, -1)
1.63 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
1.63 logistic.py(287):     if fit_intercept:
1.63 logistic.py(288):         intercept = w[:, -1]
1.63 logistic.py(289):         w = w[:, :-1]
1.63 logistic.py(292):     p = safe_sparse_dot(X, w.T)
1.63 logistic.py(293):     p += intercept
1.63 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
1.63 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
1.63 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
1.63 logistic.py(297):     p = np.exp(p, p)
1.63 logistic.py(298):     return loss, p, w
1.63 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
1.63 logistic.py(346):     diff = sample_weight * (p - Y)
1.63 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
1.63 logistic.py(348):     grad[:, :n_features] += alpha * w
1.63 logistic.py(349):     if fit_intercept:
1.63 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
1.63 logistic.py(351):     return loss, grad.ravel(), p
1.63 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
1.63 logistic.py(339):     n_classes = Y.shape[1]
1.63 logistic.py(340):     n_features = X.shape[1]
1.63 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
1.63 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
1.63 logistic.py(343):                     dtype=X.dtype)
1.63 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
1.63 logistic.py(282):     n_classes = Y.shape[1]
1.63 logistic.py(283):     n_features = X.shape[1]
1.63 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
1.63 logistic.py(285):     w = w.reshape(n_classes, -1)
1.63 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
1.63 logistic.py(287):     if fit_intercept:
1.63 logistic.py(288):         intercept = w[:, -1]
1.63 logistic.py(289):         w = w[:, :-1]
1.63 logistic.py(292):     p = safe_sparse_dot(X, w.T)
1.63 logistic.py(293):     p += intercept
1.63 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
1.63 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
1.63 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
1.63 logistic.py(297):     p = np.exp(p, p)
1.63 logistic.py(298):     return loss, p, w
1.63 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
1.63 logistic.py(346):     diff = sample_weight * (p - Y)
1.63 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
1.63 logistic.py(348):     grad[:, :n_features] += alpha * w
1.63 logistic.py(349):     if fit_intercept:
1.63 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
1.63 logistic.py(351):     return loss, grad.ravel(), p
1.63 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
1.63 logistic.py(339):     n_classes = Y.shape[1]
1.63 logistic.py(340):     n_features = X.shape[1]
1.63 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
1.63 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
1.63 logistic.py(343):                     dtype=X.dtype)
1.63 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
1.63 logistic.py(282):     n_classes = Y.shape[1]
1.63 logistic.py(283):     n_features = X.shape[1]
1.63 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
1.63 logistic.py(285):     w = w.reshape(n_classes, -1)
1.63 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
1.63 logistic.py(287):     if fit_intercept:
1.63 logistic.py(288):         intercept = w[:, -1]
1.63 logistic.py(289):         w = w[:, :-1]
1.63 logistic.py(292):     p = safe_sparse_dot(X, w.T)
1.63 logistic.py(293):     p += intercept
1.63 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
1.63 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
1.63 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
1.63 logistic.py(297):     p = np.exp(p, p)
1.63 logistic.py(298):     return loss, p, w
1.63 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
1.63 logistic.py(346):     diff = sample_weight * (p - Y)
1.63 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
1.63 logistic.py(348):     grad[:, :n_features] += alpha * w
1.63 logistic.py(349):     if fit_intercept:
1.63 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
1.63 logistic.py(351):     return loss, grad.ravel(), p
1.63 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
1.63 logistic.py(339):     n_classes = Y.shape[1]
1.63 logistic.py(340):     n_features = X.shape[1]
1.63 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
1.63 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
1.63 logistic.py(343):                     dtype=X.dtype)
1.63 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
1.63 logistic.py(282):     n_classes = Y.shape[1]
1.63 logistic.py(283):     n_features = X.shape[1]
1.63 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
1.63 logistic.py(285):     w = w.reshape(n_classes, -1)
1.63 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
1.63 logistic.py(287):     if fit_intercept:
1.63 logistic.py(288):         intercept = w[:, -1]
1.63 logistic.py(289):         w = w[:, :-1]
1.63 logistic.py(292):     p = safe_sparse_dot(X, w.T)
1.63 logistic.py(293):     p += intercept
1.63 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
1.63 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
1.63 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
1.63 logistic.py(297):     p = np.exp(p, p)
1.63 logistic.py(298):     return loss, p, w
1.63 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
1.63 logistic.py(346):     diff = sample_weight * (p - Y)
1.63 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
1.63 logistic.py(348):     grad[:, :n_features] += alpha * w
1.63 logistic.py(349):     if fit_intercept:
1.63 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
1.63 logistic.py(351):     return loss, grad.ravel(), p
1.63 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
1.63 logistic.py(339):     n_classes = Y.shape[1]
1.63 logistic.py(340):     n_features = X.shape[1]
1.63 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
1.63 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
1.63 logistic.py(343):                     dtype=X.dtype)
1.63 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
1.63 logistic.py(282):     n_classes = Y.shape[1]
1.63 logistic.py(283):     n_features = X.shape[1]
1.63 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
1.63 logistic.py(285):     w = w.reshape(n_classes, -1)
1.63 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
1.63 logistic.py(287):     if fit_intercept:
1.63 logistic.py(288):         intercept = w[:, -1]
1.63 logistic.py(289):         w = w[:, :-1]
1.63 logistic.py(292):     p = safe_sparse_dot(X, w.T)
1.63 logistic.py(293):     p += intercept
1.63 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
1.63 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
1.63 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
1.63 logistic.py(297):     p = np.exp(p, p)
1.63 logistic.py(298):     return loss, p, w
1.63 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
1.63 logistic.py(346):     diff = sample_weight * (p - Y)
1.63 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
1.63 logistic.py(348):     grad[:, :n_features] += alpha * w
1.63 logistic.py(349):     if fit_intercept:
1.63 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
1.64 logistic.py(351):     return loss, grad.ravel(), p
1.64 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
1.64 logistic.py(339):     n_classes = Y.shape[1]
1.64 logistic.py(340):     n_features = X.shape[1]
1.64 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
1.64 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
1.64 logistic.py(343):                     dtype=X.dtype)
1.64 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
1.64 logistic.py(282):     n_classes = Y.shape[1]
1.64 logistic.py(283):     n_features = X.shape[1]
1.64 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
1.64 logistic.py(285):     w = w.reshape(n_classes, -1)
1.64 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
1.64 logistic.py(287):     if fit_intercept:
1.64 logistic.py(288):         intercept = w[:, -1]
1.64 logistic.py(289):         w = w[:, :-1]
1.64 logistic.py(292):     p = safe_sparse_dot(X, w.T)
1.64 logistic.py(293):     p += intercept
1.64 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
1.64 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
1.64 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
1.64 logistic.py(297):     p = np.exp(p, p)
1.64 logistic.py(298):     return loss, p, w
1.64 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
1.64 logistic.py(346):     diff = sample_weight * (p - Y)
1.64 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
1.64 logistic.py(348):     grad[:, :n_features] += alpha * w
1.64 logistic.py(349):     if fit_intercept:
1.64 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
1.64 logistic.py(351):     return loss, grad.ravel(), p
1.64 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
1.64 logistic.py(339):     n_classes = Y.shape[1]
1.64 logistic.py(340):     n_features = X.shape[1]
1.64 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
1.64 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
1.64 logistic.py(343):                     dtype=X.dtype)
1.64 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
1.64 logistic.py(282):     n_classes = Y.shape[1]
1.64 logistic.py(283):     n_features = X.shape[1]
1.64 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
1.64 logistic.py(285):     w = w.reshape(n_classes, -1)
1.64 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
1.64 logistic.py(287):     if fit_intercept:
1.64 logistic.py(288):         intercept = w[:, -1]
1.64 logistic.py(289):         w = w[:, :-1]
1.64 logistic.py(292):     p = safe_sparse_dot(X, w.T)
1.64 logistic.py(293):     p += intercept
1.64 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
1.64 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
1.64 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
1.64 logistic.py(297):     p = np.exp(p, p)
1.64 logistic.py(298):     return loss, p, w
1.64 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
1.64 logistic.py(346):     diff = sample_weight * (p - Y)
1.64 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
1.64 logistic.py(348):     grad[:, :n_features] += alpha * w
1.64 logistic.py(349):     if fit_intercept:
1.64 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
1.64 logistic.py(351):     return loss, grad.ravel(), p
1.64 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
1.64 logistic.py(339):     n_classes = Y.shape[1]
1.64 logistic.py(340):     n_features = X.shape[1]
1.64 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
1.64 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
1.64 logistic.py(343):                     dtype=X.dtype)
1.64 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
1.64 logistic.py(282):     n_classes = Y.shape[1]
1.64 logistic.py(283):     n_features = X.shape[1]
1.64 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
1.64 logistic.py(285):     w = w.reshape(n_classes, -1)
1.64 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
1.64 logistic.py(287):     if fit_intercept:
1.64 logistic.py(288):         intercept = w[:, -1]
1.64 logistic.py(289):         w = w[:, :-1]
1.64 logistic.py(292):     p = safe_sparse_dot(X, w.T)
1.64 logistic.py(293):     p += intercept
1.64 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
1.64 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
1.64 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
1.64 logistic.py(297):     p = np.exp(p, p)
1.64 logistic.py(298):     return loss, p, w
1.64 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
1.64 logistic.py(346):     diff = sample_weight * (p - Y)
1.64 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
1.64 logistic.py(348):     grad[:, :n_features] += alpha * w
1.64 logistic.py(349):     if fit_intercept:
1.64 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
1.64 logistic.py(351):     return loss, grad.ravel(), p
1.64 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
1.64 logistic.py(339):     n_classes = Y.shape[1]
1.64 logistic.py(340):     n_features = X.shape[1]
1.64 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
1.64 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
1.64 logistic.py(343):                     dtype=X.dtype)
1.64 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
1.64 logistic.py(282):     n_classes = Y.shape[1]
1.64 logistic.py(283):     n_features = X.shape[1]
1.64 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
1.64 logistic.py(285):     w = w.reshape(n_classes, -1)
1.64 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
1.64 logistic.py(287):     if fit_intercept:
1.64 logistic.py(288):         intercept = w[:, -1]
1.64 logistic.py(289):         w = w[:, :-1]
1.64 logistic.py(292):     p = safe_sparse_dot(X, w.T)
1.64 logistic.py(293):     p += intercept
1.64 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
1.64 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
1.64 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
1.64 logistic.py(297):     p = np.exp(p, p)
1.64 logistic.py(298):     return loss, p, w
1.64 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
1.64 logistic.py(346):     diff = sample_weight * (p - Y)
1.64 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
1.64 logistic.py(348):     grad[:, :n_features] += alpha * w
1.64 logistic.py(349):     if fit_intercept:
1.64 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
1.64 logistic.py(351):     return loss, grad.ravel(), p
1.64 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
1.64 logistic.py(339):     n_classes = Y.shape[1]
1.64 logistic.py(340):     n_features = X.shape[1]
1.64 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
1.64 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
1.64 logistic.py(343):                     dtype=X.dtype)
1.64 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
1.64 logistic.py(282):     n_classes = Y.shape[1]
1.64 logistic.py(283):     n_features = X.shape[1]
1.64 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
1.64 logistic.py(285):     w = w.reshape(n_classes, -1)
1.64 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
1.64 logistic.py(287):     if fit_intercept:
1.64 logistic.py(288):         intercept = w[:, -1]
1.64 logistic.py(289):         w = w[:, :-1]
1.64 logistic.py(292):     p = safe_sparse_dot(X, w.T)
1.64 logistic.py(293):     p += intercept
1.64 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
1.64 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
1.64 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
1.64 logistic.py(297):     p = np.exp(p, p)
1.64 logistic.py(298):     return loss, p, w
1.64 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
1.64 logistic.py(346):     diff = sample_weight * (p - Y)
1.64 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
1.64 logistic.py(348):     grad[:, :n_features] += alpha * w
1.64 logistic.py(349):     if fit_intercept:
1.64 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
1.64 logistic.py(351):     return loss, grad.ravel(), p
1.64 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
1.64 logistic.py(339):     n_classes = Y.shape[1]
1.64 logistic.py(340):     n_features = X.shape[1]
1.64 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
1.64 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
1.64 logistic.py(343):                     dtype=X.dtype)
1.64 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
1.64 logistic.py(282):     n_classes = Y.shape[1]
1.64 logistic.py(283):     n_features = X.shape[1]
1.64 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
1.64 logistic.py(285):     w = w.reshape(n_classes, -1)
1.64 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
1.64 logistic.py(287):     if fit_intercept:
1.64 logistic.py(288):         intercept = w[:, -1]
1.64 logistic.py(289):         w = w[:, :-1]
1.64 logistic.py(292):     p = safe_sparse_dot(X, w.T)
1.64 logistic.py(293):     p += intercept
1.64 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
1.64 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
1.64 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
1.64 logistic.py(297):     p = np.exp(p, p)
1.64 logistic.py(298):     return loss, p, w
1.64 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
1.64 logistic.py(346):     diff = sample_weight * (p - Y)
1.64 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
1.64 logistic.py(348):     grad[:, :n_features] += alpha * w
1.64 logistic.py(349):     if fit_intercept:
1.64 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
1.64 logistic.py(351):     return loss, grad.ravel(), p
1.64 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
1.64 logistic.py(339):     n_classes = Y.shape[1]
1.64 logistic.py(340):     n_features = X.shape[1]
1.64 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
1.64 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
1.64 logistic.py(343):                     dtype=X.dtype)
1.64 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
1.64 logistic.py(282):     n_classes = Y.shape[1]
1.64 logistic.py(283):     n_features = X.shape[1]
1.64 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
1.64 logistic.py(285):     w = w.reshape(n_classes, -1)
1.64 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
1.64 logistic.py(287):     if fit_intercept:
1.64 logistic.py(288):         intercept = w[:, -1]
1.64 logistic.py(289):         w = w[:, :-1]
1.64 logistic.py(292):     p = safe_sparse_dot(X, w.T)
1.64 logistic.py(293):     p += intercept
1.64 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
1.64 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
1.64 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
1.64 logistic.py(297):     p = np.exp(p, p)
1.64 logistic.py(298):     return loss, p, w
1.64 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
1.64 logistic.py(346):     diff = sample_weight * (p - Y)
1.64 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
1.64 logistic.py(348):     grad[:, :n_features] += alpha * w
1.64 logistic.py(349):     if fit_intercept:
1.64 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
1.64 logistic.py(351):     return loss, grad.ravel(), p
1.64 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
1.64 logistic.py(339):     n_classes = Y.shape[1]
1.64 logistic.py(340):     n_features = X.shape[1]
1.64 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
1.64 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
1.64 logistic.py(343):                     dtype=X.dtype)
1.64 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
1.64 logistic.py(282):     n_classes = Y.shape[1]
1.64 logistic.py(283):     n_features = X.shape[1]
1.64 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
1.64 logistic.py(285):     w = w.reshape(n_classes, -1)
1.64 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
1.64 logistic.py(287):     if fit_intercept:
1.64 logistic.py(288):         intercept = w[:, -1]
1.64 logistic.py(289):         w = w[:, :-1]
1.64 logistic.py(292):     p = safe_sparse_dot(X, w.T)
1.64 logistic.py(293):     p += intercept
1.64 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
1.64 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
1.64 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
1.64 logistic.py(297):     p = np.exp(p, p)
1.64 logistic.py(298):     return loss, p, w
1.64 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
1.64 logistic.py(346):     diff = sample_weight * (p - Y)
1.64 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
1.64 logistic.py(348):     grad[:, :n_features] += alpha * w
1.64 logistic.py(349):     if fit_intercept:
1.64 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
1.64 logistic.py(351):     return loss, grad.ravel(), p
1.64 logistic.py(718):             if info["warnflag"] == 1:
1.64 logistic.py(723):             n_iter_i = min(info['nit'], max_iter)
1.64 logistic.py(760):         if multi_class == 'multinomial':
1.64 logistic.py(761):             multi_w0 = np.reshape(w0, (classes.size, -1))
1.64 logistic.py(762):             if classes.size == 2:
1.64 logistic.py(764):             coefs.append(multi_w0)
1.64 logistic.py(768):         n_iter[i] = n_iter_i
1.64 logistic.py(712):     for i, C in enumerate(Cs):
1.64 logistic.py(713):         if solver == 'lbfgs':
1.64 logistic.py(714):             w0, loss, info = optimize.fmin_l_bfgs_b(
1.64 logistic.py(715):                 func, w0, fprime=None,
1.64 logistic.py(716):                 args=(X, target, 1. / C, sample_weight),
1.64 logistic.py(717):                 iprint=(verbose > 0) - 1, pgtol=tol, maxiter=max_iter)
1.64 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
1.64 logistic.py(339):     n_classes = Y.shape[1]
1.64 logistic.py(340):     n_features = X.shape[1]
1.64 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
1.64 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
1.64 logistic.py(343):                     dtype=X.dtype)
1.64 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
1.64 logistic.py(282):     n_classes = Y.shape[1]
1.64 logistic.py(283):     n_features = X.shape[1]
1.64 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
1.64 logistic.py(285):     w = w.reshape(n_classes, -1)
1.64 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
1.64 logistic.py(287):     if fit_intercept:
1.64 logistic.py(288):         intercept = w[:, -1]
1.64 logistic.py(289):         w = w[:, :-1]
1.64 logistic.py(292):     p = safe_sparse_dot(X, w.T)
1.64 logistic.py(293):     p += intercept
1.64 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
1.64 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
1.64 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
1.64 logistic.py(297):     p = np.exp(p, p)
1.64 logistic.py(298):     return loss, p, w
1.64 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
1.64 logistic.py(346):     diff = sample_weight * (p - Y)
1.64 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
1.64 logistic.py(348):     grad[:, :n_features] += alpha * w
1.64 logistic.py(349):     if fit_intercept:
1.64 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
1.64 logistic.py(351):     return loss, grad.ravel(), p
1.64 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
1.64 logistic.py(339):     n_classes = Y.shape[1]
1.64 logistic.py(340):     n_features = X.shape[1]
1.64 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
1.64 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
1.64 logistic.py(343):                     dtype=X.dtype)
1.64 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
1.64 logistic.py(282):     n_classes = Y.shape[1]
1.64 logistic.py(283):     n_features = X.shape[1]
1.64 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
1.64 logistic.py(285):     w = w.reshape(n_classes, -1)
1.64 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
1.64 logistic.py(287):     if fit_intercept:
1.64 logistic.py(288):         intercept = w[:, -1]
1.64 logistic.py(289):         w = w[:, :-1]
1.64 logistic.py(292):     p = safe_sparse_dot(X, w.T)
1.64 logistic.py(293):     p += intercept
1.64 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
1.64 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
1.64 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
1.64 logistic.py(297):     p = np.exp(p, p)
1.64 logistic.py(298):     return loss, p, w
1.64 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
1.64 logistic.py(346):     diff = sample_weight * (p - Y)
1.64 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
1.64 logistic.py(348):     grad[:, :n_features] += alpha * w
1.64 logistic.py(349):     if fit_intercept:
1.64 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
1.64 logistic.py(351):     return loss, grad.ravel(), p
1.64 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
1.64 logistic.py(339):     n_classes = Y.shape[1]
1.64 logistic.py(340):     n_features = X.shape[1]
1.64 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
1.64 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
1.64 logistic.py(343):                     dtype=X.dtype)
1.64 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
1.64 logistic.py(282):     n_classes = Y.shape[1]
1.64 logistic.py(283):     n_features = X.shape[1]
1.64 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
1.64 logistic.py(285):     w = w.reshape(n_classes, -1)
1.64 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
1.64 logistic.py(287):     if fit_intercept:
1.64 logistic.py(288):         intercept = w[:, -1]
1.64 logistic.py(289):         w = w[:, :-1]
1.64 logistic.py(292):     p = safe_sparse_dot(X, w.T)
1.64 logistic.py(293):     p += intercept
1.64 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
1.64 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
1.64 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
1.64 logistic.py(297):     p = np.exp(p, p)
1.64 logistic.py(298):     return loss, p, w
1.64 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
1.64 logistic.py(346):     diff = sample_weight * (p - Y)
1.64 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
1.64 logistic.py(348):     grad[:, :n_features] += alpha * w
1.64 logistic.py(349):     if fit_intercept:
1.64 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
1.64 logistic.py(351):     return loss, grad.ravel(), p
1.64 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
1.64 logistic.py(339):     n_classes = Y.shape[1]
1.64 logistic.py(340):     n_features = X.shape[1]
1.64 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
1.64 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
1.64 logistic.py(343):                     dtype=X.dtype)
1.64 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
1.64 logistic.py(282):     n_classes = Y.shape[1]
1.64 logistic.py(283):     n_features = X.shape[1]
1.64 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
1.64 logistic.py(285):     w = w.reshape(n_classes, -1)
1.64 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
1.64 logistic.py(287):     if fit_intercept:
1.64 logistic.py(288):         intercept = w[:, -1]
1.64 logistic.py(289):         w = w[:, :-1]
1.64 logistic.py(292):     p = safe_sparse_dot(X, w.T)
1.64 logistic.py(293):     p += intercept
1.64 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
1.64 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
1.64 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
1.64 logistic.py(297):     p = np.exp(p, p)
1.64 logistic.py(298):     return loss, p, w
1.64 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
1.64 logistic.py(346):     diff = sample_weight * (p - Y)
1.64 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
1.64 logistic.py(348):     grad[:, :n_features] += alpha * w
1.64 logistic.py(349):     if fit_intercept:
1.64 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
1.64 logistic.py(351):     return loss, grad.ravel(), p
1.64 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
1.64 logistic.py(339):     n_classes = Y.shape[1]
1.64 logistic.py(340):     n_features = X.shape[1]
1.64 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
1.64 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
1.64 logistic.py(343):                     dtype=X.dtype)
1.64 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
1.64 logistic.py(282):     n_classes = Y.shape[1]
1.64 logistic.py(283):     n_features = X.shape[1]
1.64 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
1.64 logistic.py(285):     w = w.reshape(n_classes, -1)
1.64 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
1.64 logistic.py(287):     if fit_intercept:
1.64 logistic.py(288):         intercept = w[:, -1]
1.64 logistic.py(289):         w = w[:, :-1]
1.64 logistic.py(292):     p = safe_sparse_dot(X, w.T)
1.64 logistic.py(293):     p += intercept
1.64 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
1.64 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
1.64 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
1.64 logistic.py(297):     p = np.exp(p, p)
1.64 logistic.py(298):     return loss, p, w
1.64 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
1.64 logistic.py(346):     diff = sample_weight * (p - Y)
1.64 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
1.64 logistic.py(348):     grad[:, :n_features] += alpha * w
1.64 logistic.py(349):     if fit_intercept:
1.64 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
1.64 logistic.py(351):     return loss, grad.ravel(), p
1.64 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
1.64 logistic.py(339):     n_classes = Y.shape[1]
1.64 logistic.py(340):     n_features = X.shape[1]
1.64 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
1.64 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
1.64 logistic.py(343):                     dtype=X.dtype)
1.64 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
1.64 logistic.py(282):     n_classes = Y.shape[1]
1.64 logistic.py(283):     n_features = X.shape[1]
1.64 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
1.64 logistic.py(285):     w = w.reshape(n_classes, -1)
1.64 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
1.64 logistic.py(287):     if fit_intercept:
1.64 logistic.py(288):         intercept = w[:, -1]
1.64 logistic.py(289):         w = w[:, :-1]
1.64 logistic.py(292):     p = safe_sparse_dot(X, w.T)
1.64 logistic.py(293):     p += intercept
1.64 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
1.64 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
1.64 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
1.64 logistic.py(297):     p = np.exp(p, p)
1.64 logistic.py(298):     return loss, p, w
1.64 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
1.64 logistic.py(346):     diff = sample_weight * (p - Y)
1.64 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
1.64 logistic.py(348):     grad[:, :n_features] += alpha * w
1.64 logistic.py(349):     if fit_intercept:
1.64 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
1.64 logistic.py(351):     return loss, grad.ravel(), p
1.64 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
1.64 logistic.py(339):     n_classes = Y.shape[1]
1.64 logistic.py(340):     n_features = X.shape[1]
1.64 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
1.64 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
1.64 logistic.py(343):                     dtype=X.dtype)
1.64 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
1.64 logistic.py(282):     n_classes = Y.shape[1]
1.64 logistic.py(283):     n_features = X.shape[1]
1.64 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
1.64 logistic.py(285):     w = w.reshape(n_classes, -1)
1.64 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
1.64 logistic.py(287):     if fit_intercept:
1.64 logistic.py(288):         intercept = w[:, -1]
1.64 logistic.py(289):         w = w[:, :-1]
1.64 logistic.py(292):     p = safe_sparse_dot(X, w.T)
1.64 logistic.py(293):     p += intercept
1.64 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
1.64 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
1.64 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
1.64 logistic.py(297):     p = np.exp(p, p)
1.64 logistic.py(298):     return loss, p, w
1.64 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
1.64 logistic.py(346):     diff = sample_weight * (p - Y)
1.64 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
1.64 logistic.py(348):     grad[:, :n_features] += alpha * w
1.64 logistic.py(349):     if fit_intercept:
1.64 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
1.64 logistic.py(351):     return loss, grad.ravel(), p
1.64 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
1.64 logistic.py(339):     n_classes = Y.shape[1]
1.64 logistic.py(340):     n_features = X.shape[1]
1.64 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
1.64 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
1.64 logistic.py(343):                     dtype=X.dtype)
1.64 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
1.64 logistic.py(282):     n_classes = Y.shape[1]
1.64 logistic.py(283):     n_features = X.shape[1]
1.64 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
1.64 logistic.py(285):     w = w.reshape(n_classes, -1)
1.64 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
1.64 logistic.py(287):     if fit_intercept:
1.64 logistic.py(288):         intercept = w[:, -1]
1.64 logistic.py(289):         w = w[:, :-1]
1.64 logistic.py(292):     p = safe_sparse_dot(X, w.T)
1.64 logistic.py(293):     p += intercept
1.64 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
1.64 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
1.64 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
1.64 logistic.py(297):     p = np.exp(p, p)
1.64 logistic.py(298):     return loss, p, w
1.64 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
1.64 logistic.py(346):     diff = sample_weight * (p - Y)
1.64 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
1.64 logistic.py(348):     grad[:, :n_features] += alpha * w
1.65 logistic.py(349):     if fit_intercept:
1.65 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
1.65 logistic.py(351):     return loss, grad.ravel(), p
1.65 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
1.65 logistic.py(339):     n_classes = Y.shape[1]
1.65 logistic.py(340):     n_features = X.shape[1]
1.65 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
1.65 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
1.65 logistic.py(343):                     dtype=X.dtype)
1.65 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
1.65 logistic.py(282):     n_classes = Y.shape[1]
1.65 logistic.py(283):     n_features = X.shape[1]
1.65 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
1.65 logistic.py(285):     w = w.reshape(n_classes, -1)
1.65 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
1.65 logistic.py(287):     if fit_intercept:
1.65 logistic.py(288):         intercept = w[:, -1]
1.65 logistic.py(289):         w = w[:, :-1]
1.65 logistic.py(292):     p = safe_sparse_dot(X, w.T)
1.65 logistic.py(293):     p += intercept
1.65 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
1.65 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
1.65 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
1.65 logistic.py(297):     p = np.exp(p, p)
1.65 logistic.py(298):     return loss, p, w
1.65 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
1.65 logistic.py(346):     diff = sample_weight * (p - Y)
1.65 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
1.65 logistic.py(348):     grad[:, :n_features] += alpha * w
1.65 logistic.py(349):     if fit_intercept:
1.65 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
1.65 logistic.py(351):     return loss, grad.ravel(), p
1.65 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
1.65 logistic.py(339):     n_classes = Y.shape[1]
1.65 logistic.py(340):     n_features = X.shape[1]
1.65 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
1.65 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
1.65 logistic.py(343):                     dtype=X.dtype)
1.65 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
1.65 logistic.py(282):     n_classes = Y.shape[1]
1.65 logistic.py(283):     n_features = X.shape[1]
1.65 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
1.65 logistic.py(285):     w = w.reshape(n_classes, -1)
1.65 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
1.65 logistic.py(287):     if fit_intercept:
1.65 logistic.py(288):         intercept = w[:, -1]
1.65 logistic.py(289):         w = w[:, :-1]
1.65 logistic.py(292):     p = safe_sparse_dot(X, w.T)
1.65 logistic.py(293):     p += intercept
1.65 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
1.65 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
1.65 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
1.65 logistic.py(297):     p = np.exp(p, p)
1.65 logistic.py(298):     return loss, p, w
1.65 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
1.65 logistic.py(346):     diff = sample_weight * (p - Y)
1.65 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
1.65 logistic.py(348):     grad[:, :n_features] += alpha * w
1.65 logistic.py(349):     if fit_intercept:
1.65 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
1.65 logistic.py(351):     return loss, grad.ravel(), p
1.65 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
1.65 logistic.py(339):     n_classes = Y.shape[1]
1.65 logistic.py(340):     n_features = X.shape[1]
1.65 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
1.65 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
1.65 logistic.py(343):                     dtype=X.dtype)
1.65 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
1.65 logistic.py(282):     n_classes = Y.shape[1]
1.65 logistic.py(283):     n_features = X.shape[1]
1.65 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
1.65 logistic.py(285):     w = w.reshape(n_classes, -1)
1.65 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
1.65 logistic.py(287):     if fit_intercept:
1.65 logistic.py(288):         intercept = w[:, -1]
1.65 logistic.py(289):         w = w[:, :-1]
1.65 logistic.py(292):     p = safe_sparse_dot(X, w.T)
1.65 logistic.py(293):     p += intercept
1.65 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
1.65 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
1.65 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
1.65 logistic.py(297):     p = np.exp(p, p)
1.65 logistic.py(298):     return loss, p, w
1.65 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
1.65 logistic.py(346):     diff = sample_weight * (p - Y)
1.65 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
1.65 logistic.py(348):     grad[:, :n_features] += alpha * w
1.65 logistic.py(349):     if fit_intercept:
1.65 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
1.65 logistic.py(351):     return loss, grad.ravel(), p
1.65 logistic.py(718):             if info["warnflag"] == 1:
1.65 logistic.py(723):             n_iter_i = min(info['nit'], max_iter)
1.65 logistic.py(760):         if multi_class == 'multinomial':
1.65 logistic.py(761):             multi_w0 = np.reshape(w0, (classes.size, -1))
1.65 logistic.py(762):             if classes.size == 2:
1.65 logistic.py(764):             coefs.append(multi_w0)
1.65 logistic.py(768):         n_iter[i] = n_iter_i
1.65 logistic.py(712):     for i, C in enumerate(Cs):
1.65 logistic.py(713):         if solver == 'lbfgs':
1.65 logistic.py(714):             w0, loss, info = optimize.fmin_l_bfgs_b(
1.65 logistic.py(715):                 func, w0, fprime=None,
1.65 logistic.py(716):                 args=(X, target, 1. / C, sample_weight),
1.65 logistic.py(717):                 iprint=(verbose > 0) - 1, pgtol=tol, maxiter=max_iter)
1.65 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
1.65 logistic.py(339):     n_classes = Y.shape[1]
1.65 logistic.py(340):     n_features = X.shape[1]
1.65 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
1.65 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
1.65 logistic.py(343):                     dtype=X.dtype)
1.65 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
1.65 logistic.py(282):     n_classes = Y.shape[1]
1.65 logistic.py(283):     n_features = X.shape[1]
1.65 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
1.65 logistic.py(285):     w = w.reshape(n_classes, -1)
1.65 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
1.65 logistic.py(287):     if fit_intercept:
1.65 logistic.py(288):         intercept = w[:, -1]
1.65 logistic.py(289):         w = w[:, :-1]
1.65 logistic.py(292):     p = safe_sparse_dot(X, w.T)
1.65 logistic.py(293):     p += intercept
1.65 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
1.65 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
1.65 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
1.65 logistic.py(297):     p = np.exp(p, p)
1.65 logistic.py(298):     return loss, p, w
1.65 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
1.65 logistic.py(346):     diff = sample_weight * (p - Y)
1.65 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
1.65 logistic.py(348):     grad[:, :n_features] += alpha * w
1.65 logistic.py(349):     if fit_intercept:
1.65 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
1.65 logistic.py(351):     return loss, grad.ravel(), p
1.65 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
1.65 logistic.py(339):     n_classes = Y.shape[1]
1.65 logistic.py(340):     n_features = X.shape[1]
1.65 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
1.65 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
1.65 logistic.py(343):                     dtype=X.dtype)
1.65 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
1.65 logistic.py(282):     n_classes = Y.shape[1]
1.65 logistic.py(283):     n_features = X.shape[1]
1.65 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
1.65 logistic.py(285):     w = w.reshape(n_classes, -1)
1.65 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
1.65 logistic.py(287):     if fit_intercept:
1.65 logistic.py(288):         intercept = w[:, -1]
1.65 logistic.py(289):         w = w[:, :-1]
1.65 logistic.py(292):     p = safe_sparse_dot(X, w.T)
1.65 logistic.py(293):     p += intercept
1.65 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
1.65 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
1.65 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
1.65 logistic.py(297):     p = np.exp(p, p)
1.65 logistic.py(298):     return loss, p, w
1.65 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
1.65 logistic.py(346):     diff = sample_weight * (p - Y)
1.65 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
1.65 logistic.py(348):     grad[:, :n_features] += alpha * w
1.65 logistic.py(349):     if fit_intercept:
1.65 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
1.65 logistic.py(351):     return loss, grad.ravel(), p
1.65 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
1.65 logistic.py(339):     n_classes = Y.shape[1]
1.65 logistic.py(340):     n_features = X.shape[1]
1.65 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
1.65 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
1.65 logistic.py(343):                     dtype=X.dtype)
1.65 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
1.65 logistic.py(282):     n_classes = Y.shape[1]
1.65 logistic.py(283):     n_features = X.shape[1]
1.65 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
1.65 logistic.py(285):     w = w.reshape(n_classes, -1)
1.65 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
1.65 logistic.py(287):     if fit_intercept:
1.65 logistic.py(288):         intercept = w[:, -1]
1.65 logistic.py(289):         w = w[:, :-1]
1.65 logistic.py(292):     p = safe_sparse_dot(X, w.T)
1.65 logistic.py(293):     p += intercept
1.65 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
1.65 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
1.65 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
1.65 logistic.py(297):     p = np.exp(p, p)
1.65 logistic.py(298):     return loss, p, w
1.65 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
1.65 logistic.py(346):     diff = sample_weight * (p - Y)
1.65 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
1.65 logistic.py(348):     grad[:, :n_features] += alpha * w
1.65 logistic.py(349):     if fit_intercept:
1.65 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
1.65 logistic.py(351):     return loss, grad.ravel(), p
1.65 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
1.65 logistic.py(339):     n_classes = Y.shape[1]
1.65 logistic.py(340):     n_features = X.shape[1]
1.65 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
1.65 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
1.65 logistic.py(343):                     dtype=X.dtype)
1.65 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
1.65 logistic.py(282):     n_classes = Y.shape[1]
1.65 logistic.py(283):     n_features = X.shape[1]
1.65 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
1.65 logistic.py(285):     w = w.reshape(n_classes, -1)
1.65 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
1.65 logistic.py(287):     if fit_intercept:
1.65 logistic.py(288):         intercept = w[:, -1]
1.65 logistic.py(289):         w = w[:, :-1]
1.65 logistic.py(292):     p = safe_sparse_dot(X, w.T)
1.65 logistic.py(293):     p += intercept
1.65 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
1.65 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
1.65 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
1.65 logistic.py(297):     p = np.exp(p, p)
1.65 logistic.py(298):     return loss, p, w
1.65 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
1.65 logistic.py(346):     diff = sample_weight * (p - Y)
1.65 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
1.65 logistic.py(348):     grad[:, :n_features] += alpha * w
1.65 logistic.py(349):     if fit_intercept:
1.65 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
1.65 logistic.py(351):     return loss, grad.ravel(), p
1.65 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
1.65 logistic.py(339):     n_classes = Y.shape[1]
1.65 logistic.py(340):     n_features = X.shape[1]
1.65 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
1.65 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
1.65 logistic.py(343):                     dtype=X.dtype)
1.65 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
1.65 logistic.py(282):     n_classes = Y.shape[1]
1.65 logistic.py(283):     n_features = X.shape[1]
1.65 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
1.65 logistic.py(285):     w = w.reshape(n_classes, -1)
1.65 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
1.65 logistic.py(287):     if fit_intercept:
1.65 logistic.py(288):         intercept = w[:, -1]
1.65 logistic.py(289):         w = w[:, :-1]
1.65 logistic.py(292):     p = safe_sparse_dot(X, w.T)
1.65 logistic.py(293):     p += intercept
1.65 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
1.65 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
1.65 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
1.65 logistic.py(297):     p = np.exp(p, p)
1.65 logistic.py(298):     return loss, p, w
1.65 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
1.65 logistic.py(346):     diff = sample_weight * (p - Y)
1.65 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
1.65 logistic.py(348):     grad[:, :n_features] += alpha * w
1.65 logistic.py(349):     if fit_intercept:
1.65 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
1.65 logistic.py(351):     return loss, grad.ravel(), p
1.65 logistic.py(718):             if info["warnflag"] == 1:
1.65 logistic.py(723):             n_iter_i = min(info['nit'], max_iter)
1.65 logistic.py(760):         if multi_class == 'multinomial':
1.65 logistic.py(761):             multi_w0 = np.reshape(w0, (classes.size, -1))
1.65 logistic.py(762):             if classes.size == 2:
1.65 logistic.py(764):             coefs.append(multi_w0)
1.65 logistic.py(768):         n_iter[i] = n_iter_i
1.65 logistic.py(712):     for i, C in enumerate(Cs):
1.65 logistic.py(713):         if solver == 'lbfgs':
1.65 logistic.py(714):             w0, loss, info = optimize.fmin_l_bfgs_b(
1.65 logistic.py(715):                 func, w0, fprime=None,
1.65 logistic.py(716):                 args=(X, target, 1. / C, sample_weight),
1.65 logistic.py(717):                 iprint=(verbose > 0) - 1, pgtol=tol, maxiter=max_iter)
1.65 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
1.65 logistic.py(339):     n_classes = Y.shape[1]
1.65 logistic.py(340):     n_features = X.shape[1]
1.65 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
1.65 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
1.65 logistic.py(343):                     dtype=X.dtype)
1.65 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
1.65 logistic.py(282):     n_classes = Y.shape[1]
1.65 logistic.py(283):     n_features = X.shape[1]
1.65 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
1.65 logistic.py(285):     w = w.reshape(n_classes, -1)
1.65 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
1.65 logistic.py(287):     if fit_intercept:
1.65 logistic.py(288):         intercept = w[:, -1]
1.65 logistic.py(289):         w = w[:, :-1]
1.65 logistic.py(292):     p = safe_sparse_dot(X, w.T)
1.65 logistic.py(293):     p += intercept
1.65 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
1.65 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
1.65 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
1.65 logistic.py(297):     p = np.exp(p, p)
1.65 logistic.py(298):     return loss, p, w
1.65 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
1.65 logistic.py(346):     diff = sample_weight * (p - Y)
1.65 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
1.65 logistic.py(348):     grad[:, :n_features] += alpha * w
1.65 logistic.py(349):     if fit_intercept:
1.65 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
1.65 logistic.py(351):     return loss, grad.ravel(), p
1.65 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
1.65 logistic.py(339):     n_classes = Y.shape[1]
1.65 logistic.py(340):     n_features = X.shape[1]
1.65 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
1.65 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
1.65 logistic.py(343):                     dtype=X.dtype)
1.65 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
1.65 logistic.py(282):     n_classes = Y.shape[1]
1.65 logistic.py(283):     n_features = X.shape[1]
1.65 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
1.65 logistic.py(285):     w = w.reshape(n_classes, -1)
1.65 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
1.65 logistic.py(287):     if fit_intercept:
1.65 logistic.py(288):         intercept = w[:, -1]
1.65 logistic.py(289):         w = w[:, :-1]
1.65 logistic.py(292):     p = safe_sparse_dot(X, w.T)
1.65 logistic.py(293):     p += intercept
1.65 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
1.65 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
1.65 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
1.65 logistic.py(297):     p = np.exp(p, p)
1.65 logistic.py(298):     return loss, p, w
1.65 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
1.65 logistic.py(346):     diff = sample_weight * (p - Y)
1.65 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
1.65 logistic.py(348):     grad[:, :n_features] += alpha * w
1.65 logistic.py(349):     if fit_intercept:
1.65 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
1.65 logistic.py(351):     return loss, grad.ravel(), p
1.65 logistic.py(694):             func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]
1.65 logistic.py(339):     n_classes = Y.shape[1]
1.65 logistic.py(340):     n_features = X.shape[1]
1.65 logistic.py(341):     fit_intercept = (w.size == n_classes * (n_features + 1))
1.65 logistic.py(342):     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),
1.65 logistic.py(343):                     dtype=X.dtype)
1.65 logistic.py(344):     loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)
1.65 logistic.py(282):     n_classes = Y.shape[1]
1.65 logistic.py(283):     n_features = X.shape[1]
1.65 logistic.py(284):     fit_intercept = w.size == (n_classes * (n_features + 1))
1.65 logistic.py(285):     w = w.reshape(n_classes, -1)
1.65 logistic.py(286):     sample_weight = sample_weight[:, np.newaxis]
1.65 logistic.py(287):     if fit_intercept:
1.65 logistic.py(288):         intercept = w[:, -1]
1.65 logistic.py(289):         w = w[:, :-1]
1.65 logistic.py(292):     p = safe_sparse_dot(X, w.T)
1.65 logistic.py(293):     p += intercept
1.65 logistic.py(294):     p -= logsumexp(p, axis=1)[:, np.newaxis]
1.65 logistic.py(295):     loss = -(sample_weight * Y * p).sum()
1.65 logistic.py(296):     loss += 0.5 * alpha * squared_norm(w)
1.65 logistic.py(297):     p = np.exp(p, p)
1.65 logistic.py(298):     return loss, p, w
1.65 logistic.py(345):     sample_weight = sample_weight[:, np.newaxis]
1.65 logistic.py(346):     diff = sample_weight * (p - Y)
1.65 logistic.py(347):     grad[:, :n_features] = safe_sparse_dot(diff.T, X)
1.65 logistic.py(348):     grad[:, :n_features] += alpha * w
1.65 logistic.py(349):     if fit_intercept:
1.65 logistic.py(350):         grad[:, -1] = diff.sum(axis=0)
1.65 logistic.py(351):     return loss, grad.ravel(), p
1.65 logistic.py(718):             if info["warnflag"] == 1:
1.65 logistic.py(723):             n_iter_i = min(info['nit'], max_iter)
1.65 logistic.py(760):         if multi_class == 'multinomial':
1.65 logistic.py(761):             multi_w0 = np.reshape(w0, (classes.size, -1))
1.65 logistic.py(762):             if classes.size == 2:
1.65 logistic.py(764):             coefs.append(multi_w0)
1.65 logistic.py(768):         n_iter[i] = n_iter_i
1.65 logistic.py(712):     for i, C in enumerate(Cs):
1.65 logistic.py(770):     return coefs, np.array(Cs), n_iter
1.65 logistic.py(925):     log_reg = LogisticRegression(multi_class=multi_class)
1.65 logistic.py(1171):         self.penalty = penalty
1.65 logistic.py(1172):         self.dual = dual
1.65 logistic.py(1173):         self.tol = tol
1.65 logistic.py(1174):         self.C = C
1.65 logistic.py(1175):         self.fit_intercept = fit_intercept
1.65 logistic.py(1176):         self.intercept_scaling = intercept_scaling
1.65 logistic.py(1177):         self.class_weight = class_weight
1.65 logistic.py(1178):         self.random_state = random_state
1.65 logistic.py(1179):         self.solver = solver
1.65 logistic.py(1180):         self.max_iter = max_iter
1.65 logistic.py(1181):         self.multi_class = multi_class
1.65 logistic.py(1182):         self.verbose = verbose
1.65 logistic.py(1183):         self.warm_start = warm_start
1.65 logistic.py(1184):         self.n_jobs = n_jobs
1.65 logistic.py(928):     if multi_class == 'ovr':
1.65 logistic.py(930):     elif multi_class == 'multinomial':
1.65 logistic.py(931):         log_reg.classes_ = np.unique(y_train)
1.65 logistic.py(936):     if pos_class is not None:
1.65 logistic.py(941):     scores = list()
1.65 logistic.py(943):     if isinstance(scoring, six.string_types):
1.65 logistic.py(944):         scoring = get_scorer(scoring)
1.65 logistic.py(945):     for w in coefs:
1.65 logistic.py(946):         if multi_class == 'ovr':
1.65 logistic.py(948):         if fit_intercept:
1.65 logistic.py(949):             log_reg.coef_ = w[:, :-1]
1.65 logistic.py(950):             log_reg.intercept_ = w[:, -1]
1.65 logistic.py(955):         if scoring is None:
1.65 logistic.py(958):             scores.append(scoring(log_reg, X_test, y_test))
1.65 logistic.py(1341):         if not hasattr(self, "coef_"):
1.65 logistic.py(1343):         if self.multi_class == "ovr":
1.65 logistic.py(1346):             decision = self.decision_function(X)
1.65 logistic.py(1347):             if decision.ndim == 1:
1.65 logistic.py(1352):                 decision_2d = decision
1.65 logistic.py(1353):             return softmax(decision_2d, copy=False)
1.65 logistic.py(945):     for w in coefs:
1.65 logistic.py(946):         if multi_class == 'ovr':
1.65 logistic.py(948):         if fit_intercept:
1.65 logistic.py(949):             log_reg.coef_ = w[:, :-1]
1.65 logistic.py(950):             log_reg.intercept_ = w[:, -1]
1.65 logistic.py(955):         if scoring is None:
1.65 logistic.py(958):             scores.append(scoring(log_reg, X_test, y_test))
1.65 logistic.py(1341):         if not hasattr(self, "coef_"):
1.65 logistic.py(1343):         if self.multi_class == "ovr":
1.65 logistic.py(1346):             decision = self.decision_function(X)
1.65 logistic.py(1347):             if decision.ndim == 1:
1.65 logistic.py(1352):                 decision_2d = decision
1.65 logistic.py(1353):             return softmax(decision_2d, copy=False)
1.65 logistic.py(945):     for w in coefs:
1.66 logistic.py(946):         if multi_class == 'ovr':
1.66 logistic.py(948):         if fit_intercept:
1.66 logistic.py(949):             log_reg.coef_ = w[:, :-1]
1.66 logistic.py(950):             log_reg.intercept_ = w[:, -1]
1.66 logistic.py(955):         if scoring is None:
1.66 logistic.py(958):             scores.append(scoring(log_reg, X_test, y_test))
1.66 logistic.py(1341):         if not hasattr(self, "coef_"):
1.66 logistic.py(1343):         if self.multi_class == "ovr":
1.66 logistic.py(1346):             decision = self.decision_function(X)
1.66 logistic.py(1347):             if decision.ndim == 1:
1.66 logistic.py(1352):                 decision_2d = decision
1.66 logistic.py(1353):             return softmax(decision_2d, copy=False)
1.66 logistic.py(945):     for w in coefs:
1.66 logistic.py(946):         if multi_class == 'ovr':
1.66 logistic.py(948):         if fit_intercept:
1.66 logistic.py(949):             log_reg.coef_ = w[:, :-1]
1.66 logistic.py(950):             log_reg.intercept_ = w[:, -1]
1.66 logistic.py(955):         if scoring is None:
1.66 logistic.py(958):             scores.append(scoring(log_reg, X_test, y_test))
1.66 logistic.py(1341):         if not hasattr(self, "coef_"):
1.66 logistic.py(1343):         if self.multi_class == "ovr":
1.66 logistic.py(1346):             decision = self.decision_function(X)
1.66 logistic.py(1347):             if decision.ndim == 1:
1.66 logistic.py(1352):                 decision_2d = decision
1.66 logistic.py(1353):             return softmax(decision_2d, copy=False)
1.66 logistic.py(945):     for w in coefs:
1.66 logistic.py(946):         if multi_class == 'ovr':
1.66 logistic.py(948):         if fit_intercept:
1.66 logistic.py(949):             log_reg.coef_ = w[:, :-1]
1.66 logistic.py(950):             log_reg.intercept_ = w[:, -1]
1.66 logistic.py(955):         if scoring is None:
1.66 logistic.py(958):             scores.append(scoring(log_reg, X_test, y_test))
1.66 logistic.py(1341):         if not hasattr(self, "coef_"):
1.66 logistic.py(1343):         if self.multi_class == "ovr":
1.66 logistic.py(1346):             decision = self.decision_function(X)
1.66 logistic.py(1347):             if decision.ndim == 1:
1.66 logistic.py(1352):                 decision_2d = decision
1.66 logistic.py(1353):             return softmax(decision_2d, copy=False)
1.66 logistic.py(945):     for w in coefs:
1.66 logistic.py(946):         if multi_class == 'ovr':
1.66 logistic.py(948):         if fit_intercept:
1.66 logistic.py(949):             log_reg.coef_ = w[:, :-1]
1.66 logistic.py(950):             log_reg.intercept_ = w[:, -1]
1.66 logistic.py(955):         if scoring is None:
1.66 logistic.py(958):             scores.append(scoring(log_reg, X_test, y_test))
1.66 logistic.py(1341):         if not hasattr(self, "coef_"):
1.66 logistic.py(1343):         if self.multi_class == "ovr":
1.66 logistic.py(1346):             decision = self.decision_function(X)
1.66 logistic.py(1347):             if decision.ndim == 1:
1.66 logistic.py(1352):                 decision_2d = decision
1.66 logistic.py(1353):             return softmax(decision_2d, copy=False)
1.66 logistic.py(945):     for w in coefs:
1.66 logistic.py(946):         if multi_class == 'ovr':
1.66 logistic.py(948):         if fit_intercept:
1.66 logistic.py(949):             log_reg.coef_ = w[:, :-1]
1.66 logistic.py(950):             log_reg.intercept_ = w[:, -1]
1.66 logistic.py(955):         if scoring is None:
1.66 logistic.py(958):             scores.append(scoring(log_reg, X_test, y_test))
1.66 logistic.py(1341):         if not hasattr(self, "coef_"):
1.66 logistic.py(1343):         if self.multi_class == "ovr":
1.66 logistic.py(1346):             decision = self.decision_function(X)
1.66 logistic.py(1347):             if decision.ndim == 1:
1.66 logistic.py(1352):                 decision_2d = decision
1.66 logistic.py(1353):             return softmax(decision_2d, copy=False)
1.66 logistic.py(945):     for w in coefs:
1.66 logistic.py(946):         if multi_class == 'ovr':
1.66 logistic.py(948):         if fit_intercept:
1.66 logistic.py(949):             log_reg.coef_ = w[:, :-1]
1.66 logistic.py(950):             log_reg.intercept_ = w[:, -1]
1.66 logistic.py(955):         if scoring is None:
1.66 logistic.py(958):             scores.append(scoring(log_reg, X_test, y_test))
1.66 logistic.py(1341):         if not hasattr(self, "coef_"):
1.66 logistic.py(1343):         if self.multi_class == "ovr":
1.66 logistic.py(1346):             decision = self.decision_function(X)
1.66 logistic.py(1347):             if decision.ndim == 1:
1.66 logistic.py(1352):                 decision_2d = decision
1.66 logistic.py(1353):             return softmax(decision_2d, copy=False)
1.66 logistic.py(945):     for w in coefs:
1.66 logistic.py(946):         if multi_class == 'ovr':
1.66 logistic.py(948):         if fit_intercept:
1.66 logistic.py(949):             log_reg.coef_ = w[:, :-1]
1.66 logistic.py(950):             log_reg.intercept_ = w[:, -1]
1.66 logistic.py(955):         if scoring is None:
1.66 logistic.py(958):             scores.append(scoring(log_reg, X_test, y_test))
1.66 logistic.py(1341):         if not hasattr(self, "coef_"):
1.66 logistic.py(1343):         if self.multi_class == "ovr":
1.66 logistic.py(1346):             decision = self.decision_function(X)
1.66 logistic.py(1347):             if decision.ndim == 1:
1.66 logistic.py(1352):                 decision_2d = decision
1.66 logistic.py(1353):             return softmax(decision_2d, copy=False)
1.66 logistic.py(945):     for w in coefs:
1.66 logistic.py(946):         if multi_class == 'ovr':
1.66 logistic.py(948):         if fit_intercept:
1.66 logistic.py(949):             log_reg.coef_ = w[:, :-1]
1.66 logistic.py(950):             log_reg.intercept_ = w[:, -1]
1.66 logistic.py(955):         if scoring is None:
1.66 logistic.py(958):             scores.append(scoring(log_reg, X_test, y_test))
1.66 logistic.py(1341):         if not hasattr(self, "coef_"):
1.66 logistic.py(1343):         if self.multi_class == "ovr":
1.66 logistic.py(1346):             decision = self.decision_function(X)
1.66 logistic.py(1347):             if decision.ndim == 1:
1.66 logistic.py(1352):                 decision_2d = decision
1.66 logistic.py(1353):             return softmax(decision_2d, copy=False)
1.66 logistic.py(945):     for w in coefs:
1.66 logistic.py(959):     return coefs, Cs, np.array(scores), n_iter
1.66 logistic.py(1171):         self.penalty = penalty
1.66 logistic.py(1172):         self.dual = dual
1.66 logistic.py(1173):         self.tol = tol
1.66 logistic.py(1174):         self.C = C
1.66 logistic.py(1175):         self.fit_intercept = fit_intercept
1.66 logistic.py(1176):         self.intercept_scaling = intercept_scaling
1.66 logistic.py(1177):         self.class_weight = class_weight
1.66 logistic.py(1178):         self.random_state = random_state
1.66 logistic.py(1179):         self.solver = solver
1.66 logistic.py(1180):         self.max_iter = max_iter
1.66 logistic.py(1181):         self.multi_class = multi_class
1.66 logistic.py(1182):         self.verbose = verbose
1.66 logistic.py(1183):         self.warm_start = warm_start
1.66 logistic.py(1184):         self.n_jobs = n_jobs
1.66 logistic.py(1341):         if not hasattr(self, "coef_"):
1.66 logistic.py(1343):         if self.multi_class == "ovr":
1.66 logistic.py(1346):             decision = self.decision_function(X)
1.66 logistic.py(1347):             if decision.ndim == 1:
1.66 logistic.py(1352):                 decision_2d = decision
1.66 logistic.py(1353):             return softmax(decision_2d, copy=False)
=========================== short test summary info ============================
FAILED sklearn/tests/test_coverup_scikit-learn__scikit-learn-11578.py::test_log_reg_scoring_path_multinomial_bug
============================== 1 failed in 1.63s ===============================
+ cat coverage.cover
{"/testbed/sklearn/linear_model/logistic.py": {"13": 1, "14": 1, "16": 1, "17": 1, "18": 1, "20": 1, "21": 1, "22": 1, "23": 1, "24": 1, "25": 1, "26": 1, "28": 1, "29": 1, "30": 1, "31": 1, "32": 1, "34": 1, "35": 1, "36": 1, "37": 1, "38": 1, "42": 1, "80": 1, "131": 1, "167": 1, "244": 1, "301": 1, "354": 1, "427": 1, "457": 1, "780": 1, "962": 2, "963": 1, "1374": 2, "1375": 1, "70": 0, "71": 0, "72": 0, "73": 0, "75": 0, "76": 0, "77": 0, "109": 0, "110": 0, "112": 0, "114": 0, "115": 0, "118": 0, "120": 0, "121": 0, "123": 0, "126": 0, "127": 0, "128": 0, "157": 0, "159": 0, "160": 0, "163": 0, "164": 0, "197": 0, "198": 0, "199": 0, "201": 0, "203": 0, "204": 0, "206": 0, "207": 0, "209": 0, "212": 0, "213": 0, "216": 0, "217": 0, "218": 0, "219": 0, "222": 0, "224": 0, "227": 0, "229": 0, "241": 0, "230": 0, "231": 0, "232": 0, "235": 0, "236": 0, "237": 0, "238": 0, "239": 0, "282": 1298, "283": 1298, "284": 1298, "285": 1298, "286": 1298, "287": 1298, "288": 1298, "289": 1298, "291": 0, "292": 1298, "293": 1298, "294": 1298, "295": 1298, "296": 1298, "297": 1298, "298": 1298, "339": 1298, "340": 1298, "341": 1298, "342": 1298, "343": 1298, "344": 1298, "345": 1298, "346": 1298, "347": 1298, "348": 1298, "349": 1298, "350": 1298, "351": 1298, "392": 0, "393": 0, "394": 0, "398": 0, "399": 0, "403": 0, "424": 0, "404": 0, "405": 0, "406": 0, "407": 0, "409": 0, "412": 0, "413": 0, "414": 0, "415": 0, "416": 0, "417": 0, "418": 0, "419": 0, "420": 0, "421": 0, "422": 0, "428": 24, "429": 0, "431": 0, "433": 24, "434": 0, "435": 0, "437": 24, "438": 0, "439": 0, "441": 24, "442": 24, "443": 0, "444": 0, "445": 24, "446": 24, "447": 0, "448": 0, "591": 12, "592": 11, "594": 12, "597": 12, "598": 0, "599": 0, "600": 0, "601": 0, "602": 12, "603": 12, "604": 12, "606": 12, "607": 0, "608": 0, "610": 0, "615": 12, "616": 0, "617": 0, "619": 12, "624": 12, "625": 12, "626": 12, "627": 12, "631": 12, "632": 0, "633": 0, "634": 0, "635": 0, "636": 0, "639": 0, "640": 0, "641": 0, "642": 0, "645": 12, "646": 12, "647": 12, "648": 12, "649": 0, "652": 0, "653": 0, "655": 12, "656": 12, "658": 12, "660": 1, "661": 0, "662": 0, "663": 0, "664": 0, "665": 0, "669": 1, "670": 1, "671": 0, "673": 1, "674": 1, "675": 0, "676": 0, "678": 0, "679": 0, "681": 1, "682": 0, "683": 0, "685": 1, "688": 12, "690": 12, "691": 12, "692": 12, "693": 12, "694": 1310, "695": 0, "696": 0, "697": 0, "698": 0, "699": 12, "701": 0, "702": 0, "703": 0, "704": 0, "705": 0, "706": 0, "707": 0, "708": 0, "710": 12, "711": 12, "712": 123, "713": 111, "714": 111, "715": 111, "716": 111, "717": 111, "718": 111, "719": 0, "720": 0, "723": 111, "724": 0, "725": 0, "726": 0, "727": 0, "728": 0, "729": 0, "730": 0, "731": 0, "732": 0, "733": 0, "734": 0, "736": 0, "738": 0, "739": 0, "740": 0, "741": 0, "743": 0, "744": 0, "745": 0, "746": 0, "748": 0, "749": 0, "750": 0, "751": 0, "752": 0, "753": 0, "754": 0, "757": 0, "758": 0, "760": 111, "761": 111, "762": 111, "763": 0, "764": 111, "766": 0, "768": 111, "770": 12, "903": 11, "905": 11, "906": 11, "907": 11, "908": 11, "910": 11, "911": 0, "912": 0, "914": 0, "916": 11, "917": 11, "918": 11, "919": 11, "920": 11, "921": 11, "922": 11, "923": 11, "925": 11, "928": 11, "929": 0, "930": 11, "931": 11, "933": 0, "934": 0, "936": 11, "937": 0, "938": 0, "939": 0, "941": 11, "943": 11, "944": 1, "945": 121, "946": 110, "947": 0, "948": 110, "949": 110, "950": 110, "952": 0, "953": 0, "955": 110, "956": 100, "958": 10, "959": 11, "1169": 1, "1186": 1, "1318": 1, "1355": 1, "1171": 12, "1172": 12, "1173": 12, "1174": 12, "1175": 12, "1176": 12, "1177": 12, "1178": 12, "1179": 12, "1180": 12, "1181": 12, "1182": 12, "1183": 12, "1184": 12, "1209": 0, "1210": 0, "1211": 0, "1212": 0, "1213": 0, "1214": 0, "1215": 0, "1216": 0, "1217": 0, "1219": 0, "1220": 0, "1222": 0, "1224": 0, "1225": 0, "1226": 0, "1227": 0, "1228": 0, "1230": 0, "1231": 0, "1233": 0, "1234": 0, "1235": 0, "1237": 0, "1238": 0, "1239": 0, "1240": 0, "1241": 0, "1242": 0, "1243": 0, "1244": 0, "1246": 0, "1247": 0, "1249": 0, "1251": 0, "1252": 0, "1253": 0, "1254": 0, "1256": 0, "1258": 0, "1259": 0, "1260": 0, "1262": 0, "1263": 0, "1265": 0, "1266": 0, "1267": 0, "1268": 0, "1269": 0, "1271": 0, "1272": 0, "1275": 0, "1276": 0, "1277": 0, "1278": 0, "1279": 0, "1281": 0, "1285": 0, "1286": 0, "1288": 0, "1289": 0, "1290": 0, "1291": 0, "1300": 0, "1302": 0, "1303": 0, "1305": 0, "1306": 0, "1308": 0, "1309": 0, "1310": 0, "1312": 0, "1313": 0, "1314": 0, "1316": 0, "1341": 12, "1342": 0, "1343": 12, "1344": 0, "1346": 12, "1347": 12, "1350": 0, "1352": 12, "1353": 12, "1371": 0, "1576": 1, "1594": 1, "1802": 1, "1577": 1, "1578": 1, "1579": 1, "1580": 1, "1581": 1, "1582": 1, "1583": 1, "1584": 1, "1585": 1, "1586": 1, "1587": 1, "1588": 1, "1589": 1, "1590": 1, "1591": 1, "1592": 1, "1614": 1, "1615": 1, "1617": 1, "1618": 0, "1619": 0, "1620": 1, "1621": 0, "1622": 0, "1624": 1, "1625": 1, "1626": 1, "1627": 1, "1629": 1, "1632": 1, "1633": 1, "1634": 1, "1635": 0, "1636": 0, "1639": 1, "1640": 1, "1642": 1, "1643": 0, "1645": 1, "1648": 1, "1649": 1, "1652": 1, "1654": 1, "1655": 0, "1657": 0, "1659": 1, "1662": 0, "1663": 0, "1664": 0, "1668": 1, "1669": 1, "1671": 0, "1672": 0, "1675": 1, "1676": 0, "1677": 0, "1678": 0, "1679": 0, "1681": 1, "1685": 1, "1686": 0, "1688": 1, "1689": 1, "1690": 1, "1691": 3, "1702": 2, "1705": 1, "1706": 1, "1707": 1, "1708": 1, "1715": 1, "1720": 1, "1721": 1, "1722": 1, "1723": 1, "1726": 0, "1727": 0, "1728": 0, "1729": 0, "1730": 0, "1731": 0, "1733": 1, "1734": 1, "1735": 1, "1737": 1, "1738": 1, "1739": 1, "1742": 1, "1743": 1, "1744": 1, "1746": 1, "1747": 2, "1749": 1, "1752": 0, "1753": 0, "1755": 1, "1756": 1, "1758": 1, "1759": 1, "1760": 1, "1761": 1, "1762": 1, "1764": 0, "1768": 1, "1769": 1, "1770": 1, "1771": 1, "1772": 1, "1773": 1, "1774": 1, "1775": 1, "1776": 1, "1777": 1, "1778": 1, "1779": 1, "1784": 0, "1785": 0, "1786": 0, "1787": 0, "1789": 1, "1790": 1, "1791": 1, "1792": 1, "1793": 1, "1795": 0, "1796": 0, "1797": 0, "1799": 1, "1800": 1, "1703": 11, "1824": 0, "1825": 0, "1829": 0, "1830": 0, "1831": 0, "1832": 0, "1834": 0}}
+ git checkout dd69361a0d9c6ccde0d2353b00b86e0e7541a3e3
Note: switching to 'dd69361a0d9c6ccde0d2353b00b86e0e7541a3e3'.

You are in 'detached HEAD' state. You can look around, make experimental
changes and commit them, and you can discard any commits you make in this
state without impacting any branches by switching back to a branch.

If you want to create a new branch to retain commits you create, you may
do so (now or later) by using -c with the switch command. Example:

  git switch -c <new-branch-name>

Or undo this operation with:

  git switch -

Turn off this advice by setting config variable advice.detachedHead to false

HEAD is now at dd69361a0d [MRG+2] ENH&BUG Add pos_label parameter and fix a bug in average_precision_score (#9980)
M	sklearn/linear_model/logistic.py
+ git apply /root/pre_state.patch
error: unrecognized input
