+ source /opt/miniconda3/bin/activate
++ _CONDA_ROOT=/opt/miniconda3
++ . /opt/miniconda3/etc/profile.d/conda.sh
+++ export CONDA_EXE=/opt/miniconda3/bin/conda
+++ CONDA_EXE=/opt/miniconda3/bin/conda
+++ export _CE_M=
+++ _CE_M=
+++ export _CE_CONDA=
+++ _CE_CONDA=
+++ export CONDA_PYTHON_EXE=/opt/miniconda3/bin/python
+++ CONDA_PYTHON_EXE=/opt/miniconda3/bin/python
+++ '[' -z '' ']'
+++ export CONDA_SHLVL=0
+++ CONDA_SHLVL=0
+++ '[' -n '' ']'
+++++ dirname /opt/miniconda3/bin/conda
++++ dirname /opt/miniconda3/bin
+++ PATH=/opt/miniconda3/condabin:/opt/miniconda3/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin
+++ export PATH
+++ '[' -z '' ']'
+++ PS1=
++ conda activate
++ local cmd=activate
++ case "$cmd" in
++ __conda_activate activate
++ '[' -n '' ']'
++ local ask_conda
+++ PS1=
+++ __conda_exe shell.posix activate
+++ /opt/miniconda3/bin/conda shell.posix activate
++ ask_conda='PS1='\''(base) '\''
export PATH='\''/opt/miniconda3/bin:/opt/miniconda3/condabin:/opt/miniconda3/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin'\''
export CONDA_PREFIX='\''/opt/miniconda3'\''
export CONDA_SHLVL='\''1'\''
export CONDA_DEFAULT_ENV='\''base'\''
export CONDA_PROMPT_MODIFIER='\''(base) '\''
export CONDA_EXE='\''/opt/miniconda3/bin/conda'\''
export _CE_M='\'''\''
export _CE_CONDA='\'''\''
export CONDA_PYTHON_EXE='\''/opt/miniconda3/bin/python'\'''
++ eval 'PS1='\''(base) '\''
export PATH='\''/opt/miniconda3/bin:/opt/miniconda3/condabin:/opt/miniconda3/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin'\''
export CONDA_PREFIX='\''/opt/miniconda3'\''
export CONDA_SHLVL='\''1'\''
export CONDA_DEFAULT_ENV='\''base'\''
export CONDA_PROMPT_MODIFIER='\''(base) '\''
export CONDA_EXE='\''/opt/miniconda3/bin/conda'\''
export _CE_M='\'''\''
export _CE_CONDA='\'''\''
export CONDA_PYTHON_EXE='\''/opt/miniconda3/bin/python'\'''
+++ PS1='(base) '
+++ export PATH=/opt/miniconda3/bin:/opt/miniconda3/condabin:/opt/miniconda3/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin
+++ PATH=/opt/miniconda3/bin:/opt/miniconda3/condabin:/opt/miniconda3/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin
+++ export CONDA_PREFIX=/opt/miniconda3
+++ CONDA_PREFIX=/opt/miniconda3
+++ export CONDA_SHLVL=1
+++ CONDA_SHLVL=1
+++ export CONDA_DEFAULT_ENV=base
+++ CONDA_DEFAULT_ENV=base
+++ export 'CONDA_PROMPT_MODIFIER=(base) '
+++ CONDA_PROMPT_MODIFIER='(base) '
+++ export CONDA_EXE=/opt/miniconda3/bin/conda
+++ CONDA_EXE=/opt/miniconda3/bin/conda
+++ export _CE_M=
+++ _CE_M=
+++ export _CE_CONDA=
+++ _CE_CONDA=
+++ export CONDA_PYTHON_EXE=/opt/miniconda3/bin/python
+++ CONDA_PYTHON_EXE=/opt/miniconda3/bin/python
++ __conda_hashr
++ '[' -n '' ']'
++ '[' -n '' ']'
++ hash -r
+ conda activate testbed
+ local cmd=activate
+ case "$cmd" in
+ __conda_activate activate testbed
+ '[' -n '' ']'
+ local ask_conda
++ PS1='(base) '
++ __conda_exe shell.posix activate testbed
++ /opt/miniconda3/bin/conda shell.posix activate testbed
+ ask_conda='PS1='\''(testbed) '\''
export PATH='\''/opt/miniconda3/envs/testbed/bin:/opt/miniconda3/condabin:/opt/miniconda3/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin'\''
export CONDA_PREFIX='\''/opt/miniconda3/envs/testbed'\''
export CONDA_SHLVL='\''2'\''
export CONDA_DEFAULT_ENV='\''testbed'\''
export CONDA_PROMPT_MODIFIER='\''(testbed) '\''
export CONDA_PREFIX_1='\''/opt/miniconda3'\''
export CONDA_EXE='\''/opt/miniconda3/bin/conda'\''
export _CE_M='\'''\''
export _CE_CONDA='\'''\''
export CONDA_PYTHON_EXE='\''/opt/miniconda3/bin/python'\''
. "/opt/miniconda3/envs/testbed/etc/conda/activate.d/esmf-activate.sh"
. "/opt/miniconda3/envs/testbed/etc/conda/activate.d/esmpy-activate.sh"
. "/opt/miniconda3/envs/testbed/etc/conda/activate.d/gdal-activate.sh"
. "/opt/miniconda3/envs/testbed/etc/conda/activate.d/geotiff-activate.sh"
. "/opt/miniconda3/envs/testbed/etc/conda/activate.d/libarrow_activate.sh"
. "/opt/miniconda3/envs/testbed/etc/conda/activate.d/libxml2_activate.sh"
. "/opt/miniconda3/envs/testbed/etc/conda/activate.d/proj4-activate.sh"
. "/opt/miniconda3/envs/testbed/etc/conda/activate.d/udunits2-activate.sh"'
+ eval 'PS1='\''(testbed) '\''
export PATH='\''/opt/miniconda3/envs/testbed/bin:/opt/miniconda3/condabin:/opt/miniconda3/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin'\''
export CONDA_PREFIX='\''/opt/miniconda3/envs/testbed'\''
export CONDA_SHLVL='\''2'\''
export CONDA_DEFAULT_ENV='\''testbed'\''
export CONDA_PROMPT_MODIFIER='\''(testbed) '\''
export CONDA_PREFIX_1='\''/opt/miniconda3'\''
export CONDA_EXE='\''/opt/miniconda3/bin/conda'\''
export _CE_M='\'''\''
export _CE_CONDA='\'''\''
export CONDA_PYTHON_EXE='\''/opt/miniconda3/bin/python'\''
. "/opt/miniconda3/envs/testbed/etc/conda/activate.d/esmf-activate.sh"
. "/opt/miniconda3/envs/testbed/etc/conda/activate.d/esmpy-activate.sh"
. "/opt/miniconda3/envs/testbed/etc/conda/activate.d/gdal-activate.sh"
. "/opt/miniconda3/envs/testbed/etc/conda/activate.d/geotiff-activate.sh"
. "/opt/miniconda3/envs/testbed/etc/conda/activate.d/libarrow_activate.sh"
. "/opt/miniconda3/envs/testbed/etc/conda/activate.d/libxml2_activate.sh"
. "/opt/miniconda3/envs/testbed/etc/conda/activate.d/proj4-activate.sh"
. "/opt/miniconda3/envs/testbed/etc/conda/activate.d/udunits2-activate.sh"'
++ PS1='(testbed) '
++ export PATH=/opt/miniconda3/envs/testbed/bin:/opt/miniconda3/condabin:/opt/miniconda3/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin
++ PATH=/opt/miniconda3/envs/testbed/bin:/opt/miniconda3/condabin:/opt/miniconda3/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin
++ export CONDA_PREFIX=/opt/miniconda3/envs/testbed
++ CONDA_PREFIX=/opt/miniconda3/envs/testbed
++ export CONDA_SHLVL=2
++ CONDA_SHLVL=2
++ export CONDA_DEFAULT_ENV=testbed
++ CONDA_DEFAULT_ENV=testbed
++ export 'CONDA_PROMPT_MODIFIER=(testbed) '
++ CONDA_PROMPT_MODIFIER='(testbed) '
++ export CONDA_PREFIX_1=/opt/miniconda3
++ CONDA_PREFIX_1=/opt/miniconda3
++ export CONDA_EXE=/opt/miniconda3/bin/conda
++ CONDA_EXE=/opt/miniconda3/bin/conda
++ export _CE_M=
++ _CE_M=
++ export _CE_CONDA=
++ _CE_CONDA=
++ export CONDA_PYTHON_EXE=/opt/miniconda3/bin/python
++ CONDA_PYTHON_EXE=/opt/miniconda3/bin/python
++ . /opt/miniconda3/envs/testbed/etc/conda/activate.d/esmf-activate.sh
+++ '[' -n '' ']'
+++ '[' -f /opt/miniconda3/envs/testbed/lib/esmf.mk ']'
+++ export ESMFMKFILE=/opt/miniconda3/envs/testbed/lib/esmf.mk
+++ ESMFMKFILE=/opt/miniconda3/envs/testbed/lib/esmf.mk
++ . /opt/miniconda3/envs/testbed/etc/conda/activate.d/esmpy-activate.sh
+++ '[' -n /opt/miniconda3/envs/testbed/lib/esmf.mk ']'
+++ export _CONDA_SET_ESMFMKFILE=/opt/miniconda3/envs/testbed/lib/esmf.mk
+++ _CONDA_SET_ESMFMKFILE=/opt/miniconda3/envs/testbed/lib/esmf.mk
+++ '[' -f /opt/miniconda3/envs/testbed/lib/esmf.mk ']'
+++ export ESMFMKFILE=/opt/miniconda3/envs/testbed/lib/esmf.mk
+++ ESMFMKFILE=/opt/miniconda3/envs/testbed/lib/esmf.mk
++ . /opt/miniconda3/envs/testbed/etc/conda/activate.d/gdal-activate.sh
+++ '[' -n '' ']'
+++ '[' -n '' ']'
+++ '[' -d /opt/miniconda3/envs/testbed/share/gdal ']'
+++ export GDAL_DATA=/opt/miniconda3/envs/testbed/share/gdal
+++ GDAL_DATA=/opt/miniconda3/envs/testbed/share/gdal
+++ export GDAL_DRIVER_PATH=/opt/miniconda3/envs/testbed/lib/gdalplugins
+++ GDAL_DRIVER_PATH=/opt/miniconda3/envs/testbed/lib/gdalplugins
+++ '[' '!' -d /opt/miniconda3/envs/testbed/lib/gdalplugins ']'
+++ export CPL_ZIP_ENCODING=UTF-8
+++ CPL_ZIP_ENCODING=UTF-8
+++ '[' -n '5.1.16(1)-release' ']'
+++ '[' -f /opt/miniconda3/envs/testbed/share/bash-completion/completions/gdalinfo ']'
+++ source /opt/miniconda3/envs/testbed/share/bash-completion/completions/gdalinfo
++++ function_exists _get_comp_words_by_ref
++++ declare -f -F _get_comp_words_by_ref
++++ return 1
++++ return 0
++ . /opt/miniconda3/envs/testbed/etc/conda/activate.d/geotiff-activate.sh
+++ '[' -n '' ']'
+++ '[' -d /opt/miniconda3/envs/testbed/share/epsg_csv ']'
+++ '[' -d /opt/miniconda3/envs/testbed/Library/share/epsg_csv ']'
++ . /opt/miniconda3/envs/testbed/etc/conda/activate.d/libarrow_activate.sh
+++ '[' -n '' ']'
+++ _la_log 'Beginning libarrow activation.'
+++ '[' '' = 1 ']'
+++ _la_gdb_prefix=/opt/miniconda3/envs/testbed/share/gdb/auto-load
+++ '[' '!' -w /opt/miniconda3/envs/testbed/share/gdb/auto-load ']'
+++ _la_placeholder=replace_this_section_with_absolute_slashed_path_to_CONDA_PREFIX
+++ _la_symlink_dir=/opt/miniconda3/envs/testbed/share/gdb/auto-load//opt/miniconda3/envs/testbed/lib
+++ _la_orig_install_dir=/opt/miniconda3/envs/testbed/share/gdb/auto-load/replace_this_section_with_absolute_slashed_path_to_CONDA_PREFIX/lib
+++ _la_log '          _la_gdb_prefix: /opt/miniconda3/envs/testbed/share/gdb/auto-load'
+++ '[' '' = 1 ']'
+++ _la_log '         _la_placeholder: replace_this_section_with_absolute_slashed_path_to_CONDA_PREFIX'
+++ '[' '' = 1 ']'
+++ _la_log '         _la_symlink_dir: /opt/miniconda3/envs/testbed/share/gdb/auto-load//opt/miniconda3/envs/testbed/lib'
+++ '[' '' = 1 ']'
+++ _la_log '    _la_orig_install_dir: /opt/miniconda3/envs/testbed/share/gdb/auto-load/replace_this_section_with_absolute_slashed_path_to_CONDA_PREFIX/lib'
+++ '[' '' = 1 ']'
+++ _la_log '  content of that folder:'
+++ '[' '' = 1 ']'
++++ ls -al /opt/miniconda3/envs/testbed/share/gdb/auto-load/replace_this_section_with_absolute_slashed_path_to_CONDA_PREFIX/lib
++++ sed 's/^/      /'
+++ _la_log '      total 12
      drwxr-xr-x 2 root root 4096 Aug 25 05:38 .
      drwxr-xr-x 3 root root 4096 Aug 25 05:38 ..
      -rw-r--r-- 1 root root  971 Aug 25 05:38 libarrow.so.2100.0.0-gdb.py'
+++ '[' '' = 1 ']'
+++ for _la_target in "$_la_orig_install_dir/"*.py
+++ '[' '!' -e /opt/miniconda3/envs/testbed/share/gdb/auto-load/replace_this_section_with_absolute_slashed_path_to_CONDA_PREFIX/lib/libarrow.so.2100.0.0-gdb.py ']'
++++ basename /opt/miniconda3/envs/testbed/share/gdb/auto-load/replace_this_section_with_absolute_slashed_path_to_CONDA_PREFIX/lib/libarrow.so.2100.0.0-gdb.py
+++ _la_symlink=/opt/miniconda3/envs/testbed/share/gdb/auto-load//opt/miniconda3/envs/testbed/lib/libarrow.so.2100.0.0-gdb.py
+++ _la_log '   _la_target: /opt/miniconda3/envs/testbed/share/gdb/auto-load/replace_this_section_with_absolute_slashed_path_to_CONDA_PREFIX/lib/libarrow.so.2100.0.0-gdb.py'
+++ '[' '' = 1 ']'
+++ _la_log '  _la_symlink: /opt/miniconda3/envs/testbed/share/gdb/auto-load//opt/miniconda3/envs/testbed/lib/libarrow.so.2100.0.0-gdb.py'
+++ '[' '' = 1 ']'
+++ '[' -L /opt/miniconda3/envs/testbed/share/gdb/auto-load//opt/miniconda3/envs/testbed/lib/libarrow.so.2100.0.0-gdb.py ']'
++++ readlink /opt/miniconda3/envs/testbed/share/gdb/auto-load//opt/miniconda3/envs/testbed/lib/libarrow.so.2100.0.0-gdb.py
+++ '[' /opt/miniconda3/envs/testbed/share/gdb/auto-load/replace_this_section_with_absolute_slashed_path_to_CONDA_PREFIX/lib/libarrow.so.2100.0.0-gdb.py = /opt/miniconda3/envs/testbed/share/gdb/auto-load/replace_this_section_with_absolute_slashed_path_to_CONDA_PREFIX/lib/libarrow.so.2100.0.0-gdb.py ']'
+++ _la_log 'symlink $_la_symlink already exists and points to $_la_target, skipping.'
+++ '[' '' = 1 ']'
+++ continue
+++ _la_log 'Libarrow activation complete.'
+++ '[' '' = 1 ']'
+++ unset _la_gdb_prefix
+++ unset _la_log
+++ unset _la_orig_install_dir
+++ unset _la_placeholder
+++ unset _la_symlink
+++ unset _la_symlink_dir
+++ unset _la_target
++ . /opt/miniconda3/envs/testbed/etc/conda/activate.d/libxml2_activate.sh
+++ test -n ''
+++ xml_catalog_files_libxml2=
+++ XML_CATALOG_FILES=
+++ conda_catalog_files=
+++ ifs_libxml2=' 	
'
+++ IFS=' '
+++ rem=/opt/miniconda3/envs/testbed
+++ for pre in ${rem}
+++ test '' = /opt/miniconda3/envs/testbed
+++ conda_catalog_files=/opt/miniconda3/envs/testbed
+++ rem=
+++ IFS=' 	
'
+++ conda_catalog_files='file:///opt/miniconda3/envs/testbed/etc/xml/catalog file:///etc/xml/catalog'
+++ export 'XML_CATALOG_FILES=file:///opt/miniconda3/envs/testbed/etc/xml/catalog file:///etc/xml/catalog'
+++ XML_CATALOG_FILES='file:///opt/miniconda3/envs/testbed/etc/xml/catalog file:///etc/xml/catalog'
+++ unset conda_catalog_files ifs_libxml2 rem
++ . /opt/miniconda3/envs/testbed/etc/conda/activate.d/proj4-activate.sh
+++ '[' -n '' ']'
+++ '[' -d /opt/miniconda3/envs/testbed/share/proj ']'
+++ export PROJ_DATA=/opt/miniconda3/envs/testbed/share/proj
+++ PROJ_DATA=/opt/miniconda3/envs/testbed/share/proj
+++ '[' -f /opt/miniconda3/envs/testbed/share/proj/copyright_and_licenses.csv ']'
+++ export PROJ_NETWORK=ON
+++ PROJ_NETWORK=ON
++ . /opt/miniconda3/envs/testbed/etc/conda/activate.d/udunits2-activate.sh
+++ '[' -n '' ']'
+++ '[' -d /opt/miniconda3/envs/testbed/share/udunits ']'
+++ export UDUNITS2_XML_PATH=/opt/miniconda3/envs/testbed/share/udunits/udunits2.xml
+++ UDUNITS2_XML_PATH=/opt/miniconda3/envs/testbed/share/udunits/udunits2.xml
+ __conda_hashr
+ '[' -n '' ']'
+ '[' -n '' ']'
+ hash -r
+ cd /testbed
+ git diff HEAD 1757dffac2fa493d7b9a074b84cf8c830a706688
+ git config --global --add safe.directory /testbed
+ cd /testbed
+ git status
On branch main
nothing to commit, working tree clean
+ git show
commit 1757dffac2fa493d7b9a074b84cf8c830a706688
Author: crusaderky <crusaderky@gmail.com>
Date:   Wed Jul 31 18:47:59 2019 +0100

    More annotations in Dataset (#3112)
    
    * Let mypy discover xarray typing hints
    
    * Docs tweak
    
    * More annotations in Dataset
    
    * Fix Python 3.5-specific check for OrderedDict
    
    * typing in merge
    
    * broadcast_like typing tweaks
    
    * Changed redundant x: Optional[MyType] = None to x: MyType = None
    
    * flake8
    
    * What's New

diff --git a/doc/whats-new.rst b/doc/whats-new.rst
index be12961c..afb8a9b2 100644
--- a/doc/whats-new.rst
+++ b/doc/whats-new.rst
@@ -28,6 +28,15 @@ New functions/methods
   By `Deepak Cherian <https://github.com/dcherian>`_ and `David Mertz 
   <http://github.com/DavidMertz>`_.
 
+- The xarray package is now discoverably by mypy (although typing hints
+  coverage is not complete yet). mypy users can now remove from their setup.cfg
+  the lines::
+
+    [mypy-xarray]
+    ignore_missing_imports = True
+
+   By `Guido Imperiale <https://github.com/crusaderky>`_
+
 Enhancements
 ~~~~~~~~~~~~
 
@@ -70,7 +79,6 @@ New functions/methods
   (:issue:`3026`).
   By `Julia Kent <https://github.com/jukent>`_.
 
-
 Enhancements
 ~~~~~~~~~~~~
 
diff --git a/setup.py b/setup.py
index 6998f001..977ad2e1 100644
--- a/setup.py
+++ b/setup.py
@@ -104,4 +104,4 @@ setup(name=DISTNAME,
       tests_require=TESTS_REQUIRE,
       url=URL,
       packages=find_packages(),
-      package_data={'xarray': ['tests/data/*']})
+      package_data={'xarray': ['py.typed', 'tests/data/*']})
diff --git a/xarray/core/alignment.py b/xarray/core/alignment.py
index 6d7fcb98..711634a9 100644
--- a/xarray/core/alignment.py
+++ b/xarray/core/alignment.py
@@ -3,7 +3,16 @@ import operator
 import warnings
 from collections import OrderedDict, defaultdict
 from contextlib import suppress
-from typing import Any, Mapping, Optional, Tuple
+from typing import (
+    Any,
+    Dict,
+    Hashable,
+    Mapping,
+    Optional,
+    Tuple,
+    Union,
+    TYPE_CHECKING,
+)
 
 import numpy as np
 import pandas as pd
@@ -13,6 +22,10 @@ from .indexing import get_indexer_nd
 from .utils import is_dict_like, is_full_slice
 from .variable import IndexVariable, Variable
 
+if TYPE_CHECKING:
+    from .dataarray import DataArray
+    from .dataset import Dataset
+
 
 def _get_joiner(join):
     if join == 'outer':
@@ -169,8 +182,8 @@ def deep_align(objects, join='inner', copy=True, indexes=None,
 
     This function is not public API.
     """
-    from .dataarray import DataArray
-    from .dataset import Dataset
+    from .dataarray import DataArray  # noqa: F811
+    from .dataset import Dataset  # noqa: F811
 
     if indexes is None:
         indexes = {}
@@ -222,7 +235,10 @@ def deep_align(objects, join='inner', copy=True, indexes=None,
     return out
 
 
-def reindex_like_indexers(target, other):
+def reindex_like_indexers(
+    target: Union['DataArray', 'Dataset'],
+    other: Union['DataArray', 'Dataset'],
+) -> Dict[Hashable, pd.Index]:
     """Extract indexers to align target with other.
 
     Not public API.
@@ -236,7 +252,8 @@ def reindex_like_indexers(target, other):
 
     Returns
     -------
-    Dict[Any, pandas.Index] providing indexes for reindex keyword arguments.
+    Dict[Hashable, pandas.Index] providing indexes for reindex keyword
+    arguments.
 
     Raises
     ------
@@ -310,7 +327,7 @@ def reindex_variables(
     new_indexes : OrderedDict
         Dict of indexes associated with the reindexed variables.
     """
-    from .dataarray import DataArray
+    from .dataarray import DataArray  # noqa: F811
 
     # create variables for the new dataset
     reindexed = OrderedDict()  # type: OrderedDict[Any, Variable]
@@ -407,8 +424,8 @@ def _get_broadcast_dims_map_common_coords(args, exclude):
 
 def _broadcast_helper(arg, exclude, dims_map, common_coords):
 
-    from .dataarray import DataArray
-    from .dataset import Dataset
+    from .dataarray import DataArray  # noqa: F811
+    from .dataset import Dataset  # noqa: F811
 
     def _set_dims(var):
         # Add excluded dims to a copy of dims_map
diff --git a/xarray/core/common.py b/xarray/core/common.py
index c4b13b1a..bae3b6cd 100644
--- a/xarray/core/common.py
+++ b/xarray/core/common.py
@@ -3,7 +3,7 @@ from contextlib import suppress
 from textwrap import dedent
 from typing import (
     Any, Callable, Hashable, Iterable, Iterator, List, Mapping, MutableMapping,
-    Optional, Tuple, TypeVar, Union)
+    Tuple, TypeVar, Union)
 
 import numpy as np
 import pandas as pd
@@ -101,7 +101,7 @@ class AbstractArray(ImplementsArrayReduce):
     def __complex__(self: Any) -> complex:
         return complex(self.values)
 
-    def __array__(self: Any, dtype: Optional[DTypeLike] = None) -> np.ndarray:
+    def __array__(self: Any, dtype: DTypeLike = None) -> np.ndarray:
         return np.asarray(self.values, dtype=dtype)
 
     def __repr__(self) -> str:
@@ -448,7 +448,7 @@ class DataWithCoords(SupportsArithmetic, AttrAccessMixin):
             return func(self, *args, **kwargs)
 
     def groupby(self, group, squeeze: bool = True,
-                restore_coord_dims: Optional[bool] = None):
+                restore_coord_dims: bool = None):
         """Returns a GroupBy object for performing grouped operations.
 
         Parameters
@@ -501,7 +501,7 @@ class DataWithCoords(SupportsArithmetic, AttrAccessMixin):
     def groupby_bins(self, group, bins, right: bool = True, labels=None,
                      precision: int = 3, include_lowest: bool = False,
                      squeeze: bool = True,
-                     restore_coord_dims: Optional[bool] = None):
+                     restore_coord_dims: bool = None):
         """Returns a GroupBy object for performing grouped operations.
 
         Rather than using all unique values of `group`, the values are discretized
@@ -557,8 +557,8 @@ class DataWithCoords(SupportsArithmetic, AttrAccessMixin):
                                              'include_lowest':
                                                  include_lowest})
 
-    def rolling(self, dim: Optional[Mapping[Hashable, int]] = None,
-                min_periods: Optional[int] = None, center: bool = False,
+    def rolling(self, dim: Mapping[Hashable, int] = None,
+                min_periods: int = None, center: bool = False,
                 **window_kwargs: int):
         """
         Rolling window object.
@@ -621,7 +621,7 @@ class DataWithCoords(SupportsArithmetic, AttrAccessMixin):
 
     def rolling_exp(
         self,
-        window: Optional[Mapping[Hashable, int]] = None,
+        window: Mapping[Hashable, int] = None,
         window_type: str = 'span',
         **window_kwargs
     ):
@@ -658,7 +658,7 @@ class DataWithCoords(SupportsArithmetic, AttrAccessMixin):
 
         return self._rolling_exp_cls(self, window, window_type)
 
-    def coarsen(self, dim: Optional[Mapping[Hashable, int]] = None,
+    def coarsen(self, dim: Mapping[Hashable, int] = None,
                 boundary: str = 'exact',
                 side: Union[str, Mapping[Hashable, str]] = 'left',
                 coord_func: str = 'mean',
@@ -721,11 +721,11 @@ class DataWithCoords(SupportsArithmetic, AttrAccessMixin):
             self, dim, boundary=boundary, side=side,
             coord_func=coord_func)
 
-    def resample(self, indexer: Optional[Mapping[Hashable, str]] = None,
-                 skipna=None, closed: Optional[str] = None,
-                 label: Optional[str] = None,
-                 base: int = 0, keep_attrs: Optional[bool] = None,
-                 loffset=None, restore_coord_dims: Optional[bool] = None,
+    def resample(self, indexer: Mapping[Hashable, str] = None,
+                 skipna=None, closed: str = None,
+                 label: str = None,
+                 base: int = 0, keep_attrs: bool = None,
+                 loffset=None, restore_coord_dims: bool = None,
                  **indexer_kwargs: str):
         """Returns a Resample object for performing resampling operations.
 
@@ -1003,7 +1003,7 @@ class DataWithCoords(SupportsArithmetic, AttrAccessMixin):
         raise NotImplementedError
 
 
-def full_like(other, fill_value, dtype: Optional[DTypeLike] = None):
+def full_like(other, fill_value, dtype: DTypeLike = None):
     """Return a new object with the same shape and type as a given object.
 
     Parameters
@@ -1044,7 +1044,7 @@ def full_like(other, fill_value, dtype: Optional[DTypeLike] = None):
 
 
 def _full_like_variable(other, fill_value,
-                        dtype: Optional[DTypeLike] = None):
+                        dtype: DTypeLike = None):
     """Inner function of full_like, where other must be a variable
     """
     from .variable import Variable
@@ -1061,13 +1061,13 @@ def _full_like_variable(other, fill_value,
     return Variable(dims=other.dims, data=data, attrs=other.attrs)
 
 
-def zeros_like(other, dtype: Optional[DTypeLike] = None):
+def zeros_like(other, dtype: DTypeLike = None):
     """Shorthand for full_like(other, 0, dtype)
     """
     return full_like(other, 0, dtype)
 
 
-def ones_like(other, dtype: Optional[DTypeLike] = None):
+def ones_like(other, dtype: DTypeLike = None):
     """Shorthand for full_like(other, 1, dtype)
     """
     return full_like(other, 1, dtype)
diff --git a/xarray/core/computation.py b/xarray/core/computation.py
index ef8a3e62..7ccfeae2 100644
--- a/xarray/core/computation.py
+++ b/xarray/core/computation.py
@@ -683,7 +683,7 @@ def apply_array_ufunc(func, *args, dask='forbidden'):
 def apply_ufunc(
     func: Callable,
     *args: Any,
-    input_core_dims: Optional[Sequence[Sequence]] = None,
+    input_core_dims: Sequence[Sequence] = None,
     output_core_dims: Optional[Sequence[Sequence]] = ((),),
     exclude_dims: AbstractSet = frozenset(),
     vectorize: bool = False,
@@ -693,8 +693,8 @@ def apply_ufunc(
     keep_attrs: bool = False,
     kwargs: Mapping = None,
     dask: str = 'forbidden',
-    output_dtypes: Optional[Sequence] = None,
-    output_sizes: Optional[Mapping[Any, int]] = None
+    output_dtypes: Sequence = None,
+    output_sizes: Mapping[Any, int] = None
 ) -> Any:
     """Apply a vectorized function for unlabeled arrays on xarray objects.
 
diff --git a/xarray/core/dataarray.py b/xarray/core/dataarray.py
index 99014e9e..f993888c 100644
--- a/xarray/core/dataarray.py
+++ b/xarray/core/dataarray.py
@@ -197,8 +197,8 @@ class DataArray(AbstractArray, DataWithCoords):
             None,
         ] = None,
         dims: Union[Hashable, Sequence[Hashable], None] = None,
-        name: Optional[Hashable] = None,
-        attrs: Optional[Mapping] = None,
+        name: Hashable = None,
+        attrs: Mapping = None,
         # deprecated parameters
         encoding=None,
         # internal parameters
@@ -298,7 +298,7 @@ class DataArray(AbstractArray, DataWithCoords):
 
     def _replace(
         self,
-        variable: Optional[Variable] = None,
+        variable: Variable = None,
         coords=None,
         name: Union[Hashable, None, ReprObject] = __default,
     ) -> 'DataArray':
@@ -379,7 +379,7 @@ class DataArray(AbstractArray, DataWithCoords):
 
     def _to_dataset_whole(
             self,
-            name: Optional[Hashable] = None,
+            name: Hashable = None,
             shallow_copy: bool = True
     ) -> Dataset:
         if name is None:
@@ -403,8 +403,8 @@ class DataArray(AbstractArray, DataWithCoords):
 
     def to_dataset(
         self,
-        dim: Optional[Hashable] = None,
-        name: Optional[Hashable] = None,
+        dim: Hashable = None,
+        name: Hashable = None,
     ) -> Dataset:
         """Convert a DataArray to a Dataset.
 
@@ -636,7 +636,7 @@ class DataArray(AbstractArray, DataWithCoords):
 
     def reset_coords(self,
                      names: Union[Iterable[Hashable], Hashable, None] = None,
-                     drop: bool = False, inplace: Optional[bool] = None
+                     drop: bool = False, inplace: bool = None
                      ) -> Union[None, 'DataArray', Dataset]:
         """Given names of coordinates, reset them to become variables.
 
@@ -777,7 +777,7 @@ class DataArray(AbstractArray, DataWithCoords):
     def copy(
         self,
         deep: bool = True,
-        data: Optional[Any] = None,
+        data: Any = None,
     ) -> 'DataArray':
         """Returns a copy of this array.
 
@@ -882,7 +882,7 @@ class DataArray(AbstractArray, DataWithCoords):
             Mapping[Hashable, Union[None, Number, Tuple[Number, ...]]],
         ] = None,
         name_prefix: str = 'xarray-',
-        token: Optional[str] = None,
+        token: str = None,
         lock: bool = False
     ) -> 'DataArray':
         """Coerce this array's data into a dask arrays with the given chunks.
@@ -921,7 +921,7 @@ class DataArray(AbstractArray, DataWithCoords):
 
     def isel(
         self,
-        indexers: Optional[Mapping[Hashable, Any]] = None,
+        indexers: Mapping[Hashable, Any] = None,
         drop: bool = False,
         **indexers_kwargs: Any
     ) -> 'DataArray':
@@ -939,8 +939,8 @@ class DataArray(AbstractArray, DataWithCoords):
 
     def sel(
         self,
-        indexers: Optional[Mapping[Hashable, Any]] = None,
-        method: Optional[str] = None,
+        indexers: Mapping[Hashable, Any] = None,
+        method: str = None,
         tolerance=None,
         drop: bool = False,
         **indexers_kwargs: Any
@@ -997,8 +997,8 @@ class DataArray(AbstractArray, DataWithCoords):
 
     def broadcast_like(self,
                        other: Union['DataArray', Dataset],
-                       exclude=None) -> 'DataArray':
-        """Broadcast a DataArray to the shape of another DataArray or Dataset
+                       exclude: Iterable[Hashable] = None) -> 'DataArray':
+        """Broadcast this DataArray against another Dataset or DataArray.
 
         This is equivalent to xr.broadcast(other, self)[1]
 
@@ -1015,8 +1015,7 @@ class DataArray(AbstractArray, DataWithCoords):
         ----------
         other : Dataset or DataArray
             Object against which to broadcast this array.
-
-        exclude : sequence of str, optional
+        exclude : iterable of hashable, optional
             Dimensions that must not be broadcasted
 
         Returns
@@ -1052,6 +1051,8 @@ class DataArray(AbstractArray, DataWithCoords):
         """
         if exclude is None:
             exclude = set()
+        else:
+            exclude = set(exclude)
         args = align(other, self, join='outer', copy=False, exclude=exclude)
 
         dims_map, common_coords = _get_broadcast_dims_map_common_coords(
@@ -1060,7 +1061,7 @@ class DataArray(AbstractArray, DataWithCoords):
         return _broadcast_helper(args[1], exclude, dims_map, common_coords)
 
     def reindex_like(self, other: Union['DataArray', Dataset],
-                     method: Optional[str] = None, tolerance=None,
+                     method: str = None, tolerance=None,
                      copy: bool = True, fill_value=dtypes.NA) -> 'DataArray':
         """Conform this object onto the indexes of another object, filling in
         missing values with ``fill_value``. The default fill value is NaN.
@@ -1107,11 +1108,16 @@ class DataArray(AbstractArray, DataWithCoords):
         align
         """
         indexers = reindex_like_indexers(self, other)
-        return self.reindex(method=method, tolerance=tolerance, copy=copy,
-                            fill_value=fill_value, **indexers)
-
-    def reindex(self, indexers: Optional[Mapping[Hashable, Any]] = None,
-                method: Optional[str] = None, tolerance=None,
+        return self.reindex(
+            indexers=indexers,
+            method=method,
+            tolerance=tolerance,
+            copy=copy,
+            fill_value=fill_value,
+        )
+
+    def reindex(self, indexers: Mapping[Hashable, Any] = None,
+                method: str = None, tolerance=None,
                 copy: bool = True, fill_value=dtypes.NA, **indexers_kwargs: Any
                 ) -> 'DataArray':
         """Conform this object onto the indexes of another object, filling in
@@ -1166,9 +1172,9 @@ class DataArray(AbstractArray, DataWithCoords):
             fill_value=fill_value)
         return self._from_temp_dataset(ds)
 
-    def interp(self, coords: Optional[Mapping[Hashable, Any]] = None,
+    def interp(self, coords: Mapping[Hashable, Any] = None,
                method: str = 'linear', assume_sorted: bool = False,
-               kwargs: Optional[Mapping[str, Any]] = None,
+               kwargs: Mapping[str, Any] = None,
                **coords_kwargs: Any) -> 'DataArray':
         """ Multidimensional interpolation of variables.
 
@@ -1223,7 +1229,7 @@ class DataArray(AbstractArray, DataWithCoords):
 
     def interp_like(self, other: Union['DataArray', Dataset],
                     method: str = 'linear', assume_sorted: bool = False,
-                    kwargs: Optional[Mapping[str, Any]] = None) -> 'DataArray':
+                    kwargs: Mapping[str, Any] = None) -> 'DataArray':
         """Interpolate this object onto the coordinates of another object,
         filling out of range values with NaN.
 
@@ -1272,7 +1278,7 @@ class DataArray(AbstractArray, DataWithCoords):
     def rename(
         self,
         new_name_or_name_dict:
-            Optional[Union[Hashable, Mapping[Hashable, Hashable]]] = None,
+            Union[Hashable, Mapping[Hashable, Hashable]] = None,
         **names: Hashable
     ) -> 'DataArray':
         """Returns a new DataArray with renamed coordinates or a new name.
@@ -1397,9 +1403,13 @@ class DataArray(AbstractArray, DataWithCoords):
         ds = self._to_temp_dataset().expand_dims(dim, axis)
         return self._from_temp_dataset(ds)
 
-    def set_index(self, indexes: Optional[Mapping[Hashable, Any]] = None,
-                  append: bool = False, inplace: Optional[bool] = None,
-                  **indexes_kwargs: Any) -> Optional['DataArray']:
+    def set_index(
+        self,
+        indexes: Mapping[Hashable, Union[Hashable, Sequence[Hashable]]] = None,
+        append: bool = False,
+        inplace: bool = None,
+        **indexes_kwargs: Union[Hashable, Sequence[Hashable]]
+    ) -> Optional['DataArray']:
         """Set DataArray (multi-)indexes using one or more existing
         coordinates.
 
@@ -1438,9 +1448,12 @@ class DataArray(AbstractArray, DataWithCoords):
         else:
             return self._replace(coords=coords)
 
-    def reset_index(self, dims_or_levels: Union[Hashable, Sequence[Hashable]],
-                    drop: bool = False, inplace: Optional[bool] = None
-                    ) -> Optional['DataArray']:
+    def reset_index(
+        self,
+        dims_or_levels: Union[Hashable, Sequence[Hashable]],
+        drop: bool = False,
+        inplace: bool = None
+    ) -> Optional['DataArray']:
         """Reset the specified index(es) or multi-index level(s).
 
         Parameters
@@ -1474,12 +1487,12 @@ class DataArray(AbstractArray, DataWithCoords):
         else:
             return self._replace(coords=coords)
 
-    def reorder_levels(self,
-                       dim_order: Optional[
-                           Mapping[Hashable, Sequence[int]]] = None,
-                       inplace: Optional[bool] = None,
-                       **dim_order_kwargs: Sequence[int]
-                       ) -> Optional['DataArray']:
+    def reorder_levels(
+        self,
+        dim_order: Mapping[Hashable, Sequence[int]] = None,
+        inplace: bool = None,
+        **dim_order_kwargs: Sequence[int]
+    ) -> Optional['DataArray']:
         """Rearrange index levels using input order.
 
         Parameters
@@ -1520,9 +1533,11 @@ class DataArray(AbstractArray, DataWithCoords):
         else:
             return self._replace(coords=coords)
 
-    def stack(self, dimensions: Optional[
-            Mapping[Hashable, Sequence[Hashable]]] = None,
-            **dimensions_kwargs: Sequence[Hashable]) -> 'DataArray':
+    def stack(
+        self,
+        dimensions: Mapping[Hashable, Sequence[Hashable]] = None,
+        **dimensions_kwargs: Sequence[Hashable]
+    ) -> 'DataArray':
         """
         Stack any number of existing dimensions into a single new dimension.
 
@@ -1683,7 +1698,7 @@ class DataArray(AbstractArray, DataWithCoords):
 
     def transpose(self,
                   *dims: Hashable,
-                  transpose_coords: Optional[bool] = None) -> 'DataArray':
+                  transpose_coords: bool = None) -> 'DataArray':
         """Return a new DataArray object with transposed dimensions.
 
         Parameters
@@ -1739,7 +1754,7 @@ class DataArray(AbstractArray, DataWithCoords):
 
     def drop(self,
              labels: Union[Hashable, Sequence[Hashable]],
-             dim: Optional[Hashable] = None,
+             dim: Hashable = None,
              *,
              errors: str = 'raise') -> 'DataArray':
         """Drop coordinates or index labels from this DataArray.
@@ -1766,7 +1781,7 @@ class DataArray(AbstractArray, DataWithCoords):
         return self._from_temp_dataset(ds)
 
     def dropna(self, dim: Hashable, how: str = 'any',
-               thresh: Optional[int] = None) -> 'DataArray':
+               thresh: int = None) -> 'DataArray':
         """Returns a new array with dropped labels for missing values along
         the provided dimension.
 
@@ -1814,7 +1829,7 @@ class DataArray(AbstractArray, DataWithCoords):
         return out
 
     def interpolate_na(self, dim=None, method: str = 'linear',
-                       limit: Optional[int] = None,
+                       limit: int = None,
                        use_coordinate: Union[bool, str] = True,
                        **kwargs: Any) -> 'DataArray':
         """Interpolate values according to different methods.
@@ -1859,7 +1874,7 @@ class DataArray(AbstractArray, DataWithCoords):
         return interp_na(self, dim=dim, method=method, limit=limit,
                          use_coordinate=use_coordinate, **kwargs)
 
-    def ffill(self, dim: Hashable, limit: Optional[int] = None) -> 'DataArray':
+    def ffill(self, dim: Hashable, limit: int = None) -> 'DataArray':
         """Fill NaN values by propogating values forward
 
         *Requires bottleneck.*
@@ -1882,7 +1897,7 @@ class DataArray(AbstractArray, DataWithCoords):
         from .missing import ffill
         return ffill(self, dim, limit=limit)
 
-    def bfill(self, dim: Hashable, limit: Optional[int] = None) -> 'DataArray':
+    def bfill(self, dim: Hashable, limit: int = None) -> 'DataArray':
         """Fill NaN values by propogating values backward
 
         *Requires bottleneck.*
@@ -1926,7 +1941,7 @@ class DataArray(AbstractArray, DataWithCoords):
     def reduce(self, func: Callable[..., Any],
                dim: Union[None, Hashable, Sequence[Hashable]] = None,
                axis: Union[None, int, Sequence[int]] = None,
-               keep_attrs: Optional[bool] = None,
+               keep_attrs: bool = None,
                keepdims: bool = False,
                **kwargs: Any) -> 'DataArray':
         """Reduce this array by applying `func` along some dimension(s).
@@ -1997,7 +2012,7 @@ class DataArray(AbstractArray, DataWithCoords):
 
     def to_dataframe(
         self,
-        name: Optional[Hashable] = None,
+        name: Hashable = None,
     ) -> pd.DataFrame:
         """Convert this array and its coordinates into a tidy pandas.DataFrame.
 
@@ -2295,7 +2310,7 @@ class DataArray(AbstractArray, DataWithCoords):
     @staticmethod
     def _binary_op(f: Callable[..., Any],
                    reflexive: bool = False,
-                   join: Optional[str] = None,  # see xarray.align
+                   join: str = None,  # see xarray.align
                    **ignored_kwargs
                    ) -> Callable[..., 'DataArray']:
         @functools.wraps(f)
@@ -2429,7 +2444,7 @@ class DataArray(AbstractArray, DataWithCoords):
         ds = self._to_temp_dataset().diff(n=n, dim=dim, label=label)
         return self._from_temp_dataset(ds)
 
-    def shift(self, shifts: Optional[Mapping[Hashable, int]] = None,
+    def shift(self, shifts: Mapping[Hashable, int] = None,
               fill_value: Any = dtypes.NA, **shifts_kwargs: int
               ) -> 'DataArray':
         """Shift this array by an offset along one or more dimensions.
@@ -2474,8 +2489,8 @@ class DataArray(AbstractArray, DataWithCoords):
             shifts=shifts, fill_value=fill_value, **shifts_kwargs)
         return self._replace(variable=variable)
 
-    def roll(self, shifts: Optional[Mapping[Hashable, int]] = None,
-             roll_coords: Optional[bool] = None,
+    def roll(self, shifts: Mapping[Hashable, int] = None,
+             roll_coords: bool = None,
              **shifts_kwargs: int) -> 'DataArray':
         """Roll this array by an offset along one or more dimensions.
 
@@ -2635,7 +2650,7 @@ class DataArray(AbstractArray, DataWithCoords):
     def quantile(self, q: Any,
                  dim: Union[Hashable, Sequence[Hashable], None] = None,
                  interpolation: str = 'linear',
-                 keep_attrs: Optional[bool] = None) -> 'DataArray':
+                 keep_attrs: bool = None) -> 'DataArray':
         """Compute the qth quantile of the data along the specified dimension.
 
         Returns the qth quantiles(s) of the array elements.
@@ -2723,7 +2738,7 @@ class DataArray(AbstractArray, DataWithCoords):
         return self._from_temp_dataset(ds)
 
     def differentiate(self, coord: Hashable, edge_order: int = 1,
-                      datetime_unit: Optional[str] = None) -> 'DataArray':
+                      datetime_unit: str = None) -> 'DataArray':
         """ Differentiate the array with the second order accurate central
         differences.
 
@@ -2779,7 +2794,7 @@ class DataArray(AbstractArray, DataWithCoords):
         return self._from_temp_dataset(ds)
 
     def integrate(self, dim: Union[Hashable, Sequence[Hashable]],
-                  datetime_unit: Optional[str] = None) -> 'DataArray':
+                  datetime_unit: str = None) -> 'DataArray':
         """ integrate the array with the trapezoidal rule.
 
         .. note::
diff --git a/xarray/core/dataset.py b/xarray/core/dataset.py
index f37d87ac..b00dad96 100644
--- a/xarray/core/dataset.py
+++ b/xarray/core/dataset.py
@@ -40,6 +40,7 @@ from .variable import IndexVariable, Variable, as_variable, broadcast_variables
 if TYPE_CHECKING:
     from ..backends import AbstractDataStore, ZarrStore
     from .dataarray import DataArray
+    from .merge import DatasetLike
     try:
         from dask.delayed import Delayed
     except ImportError:
@@ -54,8 +55,8 @@ _DATETIMEINDEX_COMPONENTS = ['year', 'month', 'day', 'hour', 'minute',
 
 
 def _get_virtual_variable(variables, key: Hashable,
-                          level_vars: Optional[Mapping] = None,
-                          dim_sizes: Optional[Mapping] = None,
+                          level_vars: Mapping = None,
+                          dim_sizes: Mapping = None,
                           ) -> Tuple[Hashable, Hashable, Variable]:
     """Get a virtual variable (e.g., 'time.year' or a MultiIndex level)
     from a dict of xarray.Variable objects (if possible)
@@ -342,14 +343,14 @@ class Dataset(Mapping, ImplementsDatasetReduce, DataWithCoords):
         self,
         # could make a VariableArgs to use more generally, and refine these
         # categories
-        data_vars: Optional[Mapping[Hashable, Union[
+        data_vars: Mapping[Hashable, Union[
             'DataArray',
             Variable,
             Tuple[Hashable, Any],
             Tuple[Sequence[Hashable], Any],
-        ]]] = None,
-        coords: Optional[Mapping[Hashable, Any]] = None,
-        attrs: Optional[Mapping] = None,
+        ]] = None,
+        coords: Mapping[Hashable, Any] = None,
+        attrs: Mapping = None,
         compat=None,
     ):
         """To load data from a file or file-like object, use the `open_dataset`
@@ -741,7 +742,7 @@ class Dataset(Mapping, ImplementsDatasetReduce, DataWithCoords):
     def _replace(  # type: ignore
         self,
         variables: 'OrderedDict[Any, Variable]' = None,
-        coord_names: Optional[Set[Hashable]] = None,
+        coord_names: Set[Hashable] = None,
         dims: Dict[Any, int] = None,
         attrs: 'Optional[OrderedDict]' = __default,
         indexes: 'Optional[OrderedDict[Any, pd.Index]]' = __default,
@@ -791,8 +792,8 @@ class Dataset(Mapping, ImplementsDatasetReduce, DataWithCoords):
         self,
         variables: 'OrderedDict[Any, Variable]',
         coord_names: set = None,
-        attrs: 'Optional[OrderedDict]' = __default,
-        indexes: 'Optional[OrderedDict[Any, pd.Index]]' = __default,
+        attrs: 'OrderedDict' = __default,
+        indexes: 'OrderedDict[Any, pd.Index]' = __default,
         inplace: bool = False,
     ) -> 'Dataset':
         """Replace variables with recalculated dimensions."""
@@ -804,8 +805,8 @@ class Dataset(Mapping, ImplementsDatasetReduce, DataWithCoords):
         self,
         variables: 'OrderedDict[Any, Variable]',
         coord_names: set = None,
-        dims: 'Optional[Dict[Any, int]]' = None,
-        attrs: 'Optional[OrderedDict]' = __default,
+        dims: Dict[Any, int] = None,
+        attrs: 'OrderedDict' = __default,
         inplace: bool = False,
     ) -> 'Dataset':
         """Deprecated version of _replace_with_new_dims().
@@ -1685,7 +1686,12 @@ class Dataset(Mapping, ImplementsDatasetReduce, DataWithCoords):
         )
         return attached_coords, attached_indexes
 
-    def isel(self, indexers=None, drop=False, **indexers_kwargs):
+    def isel(
+        self,
+        indexers: Mapping[Hashable, Any] = None,
+        drop: bool = False,
+        **indexers_kwargs: Any
+    ) -> 'Dataset':
         """Returns a new dataset with each array indexed along the specified
         dimension(s).
 
@@ -1730,8 +1736,9 @@ class Dataset(Mapping, ImplementsDatasetReduce, DataWithCoords):
 
         indexers_list = self._validate_indexers(indexers)
 
-        variables = OrderedDict()
-        indexes = OrderedDict()
+        variables = OrderedDict()  # type: OrderedDict[Hashable, Variable]
+        indexes = OrderedDict()  # type: OrderedDict[Hashable, pd.Index]
+
         for name, var in self.variables.items():
             var_indexers = {k: v for k, v in indexers_list if k in var.dims}
             if drop and name in var_indexers:
@@ -1762,8 +1769,14 @@ class Dataset(Mapping, ImplementsDatasetReduce, DataWithCoords):
         return self._replace_with_new_dims(
             variables, coord_names, indexes=indexes)
 
-    def sel(self, indexers=None, method=None, tolerance=None, drop=False,
-            **indexers_kwargs):
+    def sel(
+        self,
+        indexers: Mapping[Hashable, Any] = None,
+        method: str = None,
+        tolerance: Number = None,
+        drop: bool = False,
+        **indexers_kwargs: Any
+    ) -> 'Dataset':
         """Returns a new dataset with each array indexed by tick labels
         along the specified dimension(s).
 
@@ -1832,8 +1845,7 @@ class Dataset(Mapping, ImplementsDatasetReduce, DataWithCoords):
         result = self.isel(indexers=pos_indexers, drop=drop)
         return result._overwrite_indexes(new_indexes)
 
-    def isel_points(self, dim='points', **indexers):
-        # type: (...) -> Dataset
+    def isel_points(self, dim: Any = 'points', **indexers: Any) -> 'Dataset':
         """Returns a new dataset with each array indexed pointwise along the
         specified dimension(s).
 
@@ -1843,9 +1855,10 @@ class Dataset(Mapping, ImplementsDatasetReduce, DataWithCoords):
 
         Parameters
         ----------
-        dim : str or DataArray or pandas.Index or other list-like object, optional
+        dim : hashable or DataArray or pandas.Index or other list-like object, 
+              optional
             Name of the dimension to concatenate along. If dim is provided as a
-            string, it must be a new dimension name, in which case it is added
+            hashable, it must be a new dimension name, in which case it is added
             along axis=0. If dim is provided as a DataArray or Index or
             list-like object, its name, which must not be present in the
             dataset, is used as the dimension to concatenate along and the
@@ -1970,8 +1983,9 @@ class Dataset(Mapping, ImplementsDatasetReduce, DataWithCoords):
             dset.coords[dim_name] = dim_coord
         return dset
 
-    def sel_points(self, dim='points', method=None, tolerance=None,
-                   **indexers):
+    def sel_points(self, dim: Any = 'points', method: str = None,
+                   tolerance: Number = None,
+                   **indexers: Any):
         """Returns a new dataset with each array indexed pointwise by tick
         labels along the specified dimension(s).
 
@@ -1983,9 +1997,10 @@ class Dataset(Mapping, ImplementsDatasetReduce, DataWithCoords):
 
         Parameters
         ----------
-        dim : str or DataArray or pandas.Index or other list-like object, optional
+        dim : hashable or DataArray or pandas.Index or other list-like object, 
+              optional
             Name of the dimension to concatenate along. If dim is provided as a
-            string, it must be a new dimension name, in which case it is added
+            hashable, it must be a new dimension name, in which case it is added
             along axis=0. If dim is provided as a DataArray or Index or
             list-like object, its name, which must not be present in the
             dataset, is used as the dimension to concatenate along and the
@@ -2032,7 +2047,7 @@ class Dataset(Mapping, ImplementsDatasetReduce, DataWithCoords):
 
     def broadcast_like(self,
                        other: Union['Dataset', 'DataArray'],
-                       exclude=None) -> 'Dataset':
+                       exclude: Iterable[Hashable] = None) -> 'Dataset':
         """Broadcast this DataArray against another Dataset or DataArray.
         This is equivalent to xr.broadcast(other, self)[1]
 
@@ -2040,13 +2055,14 @@ class Dataset(Mapping, ImplementsDatasetReduce, DataWithCoords):
         ----------
         other : Dataset or DataArray
             Object against which to broadcast this array.
-        exclude : sequence of str, optional
+        exclude : iterable of hashable, optional
             Dimensions that must not be broadcasted
 
         """
-
         if exclude is None:
             exclude = set()
+        else:
+            exclude = set(exclude)
         args = align(other, self, join='outer', copy=False, exclude=exclude)
 
         dims_map, common_coords = _get_broadcast_dims_map_common_coords(
@@ -2054,8 +2070,14 @@ class Dataset(Mapping, ImplementsDatasetReduce, DataWithCoords):
 
         return _broadcast_helper(args[1], exclude, dims_map, common_coords)
 
-    def reindex_like(self, other, method=None, tolerance=None, copy=True,
-                     fill_value=dtypes.NA):
+    def reindex_like(
+            self,
+            other: Union['Dataset', 'DataArray'],
+            method: str = None,
+            tolerance: Number = None,
+            copy: bool = True,
+            fill_value: Any = dtypes.NA
+    ) -> 'Dataset':
         """Conform this object onto the indexes of another object, filling in
         missing values with ``fill_value``. The default fill value is NaN.
 
@@ -2104,8 +2126,15 @@ class Dataset(Mapping, ImplementsDatasetReduce, DataWithCoords):
         return self.reindex(indexers=indexers, method=method, copy=copy,
                             fill_value=fill_value, tolerance=tolerance)
 
-    def reindex(self, indexers=None, method=None, tolerance=None, copy=True,
-                fill_value=dtypes.NA, **indexers_kwargs):
+    def reindex(
+        self,
+        indexers: Mapping[Hashable, Any] = None,
+        method: str = None,
+        tolerance: Number = None,
+        copy: bool = True,
+        fill_value: Any = dtypes.NA,
+        **indexers_kwargs: Any
+    ) -> 'Dataset':
         """Conform this object onto a new set of indexes, filling in
         missing values with ``fill_value``. The default fill value is NaN.
 
@@ -2167,8 +2196,14 @@ class Dataset(Mapping, ImplementsDatasetReduce, DataWithCoords):
         return self._replace_with_new_dims(
             variables, coord_names, indexes=indexes)
 
-    def interp(self, coords=None, method='linear', assume_sorted=False,
-               kwargs=None, **coords_kwargs):
+    def interp(
+        self,
+        coords: Mapping[Hashable, Any] = None,
+        method: str = 'linear',
+        assume_sorted: bool = False,
+        kwargs: Mapping[str, Any] = None,
+        **coords_kwargs: Any
+    ) -> 'Dataset':
         """ Multidimensional interpolation of Dataset.
 
         Parameters
@@ -2237,7 +2272,7 @@ class Dataset(Mapping, ImplementsDatasetReduce, DataWithCoords):
             else:
                 return (x, new_x)
 
-        variables = OrderedDict()
+        variables = OrderedDict()  # type: OrderedDict[Hashable, Variable]
         for name, var in obj._variables.items():
             if name not in indexers:
                 if var.dtype.kind in 'uifc':
@@ -2260,9 +2295,10 @@ class Dataset(Mapping, ImplementsDatasetReduce, DataWithCoords):
 
         # attach indexer as coordinate
         variables.update(indexers)
-        indexes.update(
-            (k, v.to_index()) for k, v in indexers.items() if v.dims == (k,)
-        )
+        for k, v in indexers.items():
+            assert isinstance(v, Variable)
+            if v.dims == (k,):
+                indexes[k] = v.to_index()
 
         # Extract coordinates from indexers
         coord_vars, new_indexes = (
@@ -2276,8 +2312,13 @@ class Dataset(Mapping, ImplementsDatasetReduce, DataWithCoords):
         return self._replace_with_new_dims(
             variables, coord_names, indexes=indexes)
 
-    def interp_like(self, other, method='linear', assume_sorted=False,
-                    kwargs=None):
+    def interp_like(
+        self,
+        other: Union['Dataset', 'DataArray'],
+        method: str = 'linear',
+        assume_sorted: bool = False,
+        kwargs: Mapping[str, Any] = None
+    ) -> 'Dataset':
         """Interpolate this object onto the coordinates of another object,
         filling the out of range values with NaN.
 
@@ -2320,8 +2361,8 @@ class Dataset(Mapping, ImplementsDatasetReduce, DataWithCoords):
             kwargs = {}
         coords = alignment.reindex_like_indexers(self, other)
 
-        numeric_coords = OrderedDict()
-        object_coords = OrderedDict()
+        numeric_coords = OrderedDict()  # type: OrderedDict[Hashable, pd.Index]
+        object_coords = OrderedDict()  # type: OrderedDict[Hashable, pd.Index]
         for k, v in coords.items():
             if v.dtype.kind in 'uifcMm':
                 numeric_coords[k] = v
@@ -2374,7 +2415,12 @@ class Dataset(Mapping, ImplementsDatasetReduce, DataWithCoords):
         indexes = self._rename_indexes(name_dict)
         return variables, coord_names, dims, indexes
 
-    def rename(self, name_dict=None, inplace=None, **names):
+    def rename(
+        self,
+        name_dict: Mapping[Hashable, Hashable] = None,
+        inplace: bool = None,
+        **names: Hashable
+    ) -> 'Dataset':
         """Returns a new object with renamed variables and dimensions.
 
         Parameters
@@ -2413,7 +2459,11 @@ class Dataset(Mapping, ImplementsDatasetReduce, DataWithCoords):
         return self._replace(variables, coord_names, dims=dims,
                              indexes=indexes, inplace=inplace)
 
-    def rename_dims(self, dims_dict=None, **dims):
+    def rename_dims(
+        self,
+        dims_dict: Mapping[Hashable, Hashable] = None,
+        **dims: Hashable
+    ) -> 'Dataset':
         """Returns a new object with renamed dimensions only.
 
         Parameters
@@ -2443,12 +2493,16 @@ class Dataset(Mapping, ImplementsDatasetReduce, DataWithCoords):
                 raise ValueError("cannot rename %r because it is not a "
                                  "dimension in this dataset" % k)
 
-        variables, coord_names, dims, indexes = self._rename_all(
+        variables, coord_names, sizes, indexes = self._rename_all(
             name_dict={}, dims_dict=dims_dict)
-        return self._replace(variables, coord_names, dims=dims,
-                             indexes=indexes)
+        return self._replace(
+            variables, coord_names, dims=sizes, indexes=indexes)
 
-    def rename_vars(self, name_dict=None, **names):
+    def rename_vars(
+        self,
+        name_dict: Mapping[Hashable, Hashable] = None,
+        **names: Hashable
+    ) -> 'Dataset':
         """Returns a new object with renamed variables including coordinates
 
         Parameters
@@ -2482,7 +2536,11 @@ class Dataset(Mapping, ImplementsDatasetReduce, DataWithCoords):
         return self._replace(variables, coord_names, dims=dims,
                              indexes=indexes)
 
-    def swap_dims(self, dims_dict, inplace=None):
+    def swap_dims(
+        self,
+        dims_dict: Mapping[Hashable, Hashable],
+        inplace: bool = None
+    ) -> 'Dataset':
         """Returns a new object with swapped dimensions.
 
         Parameters
@@ -2523,8 +2581,8 @@ class Dataset(Mapping, ImplementsDatasetReduce, DataWithCoords):
         coord_names = self._coord_names.copy()
         coord_names.update(dims_dict.values())
 
-        variables = OrderedDict()
-        indexes = OrderedDict()
+        variables = OrderedDict()  # type: OrderedDict[Hashable, Variable]
+        indexes = OrderedDict()  # type: OrderedDict[Hashable, pd.Index]
         for k, v in self.variables.items():
             dims = tuple(dims_dict.get(dim, dim) for dim in v.dims)
             if k in result_dims:
@@ -2541,7 +2599,13 @@ class Dataset(Mapping, ImplementsDatasetReduce, DataWithCoords):
         return self._replace_with_new_dims(variables, coord_names,
                                            indexes=indexes, inplace=inplace)
 
-    def expand_dims(self, dim=None, axis=None, **dim_kwargs):
+    def expand_dims(
+        self,
+        dim: Union[None, Hashable, Sequence[Hashable],
+                   Mapping[Hashable, Any]] = None,
+        axis: Union[None, int, Sequence[int]] = None,
+        **dim_kwargs: Any
+    ) -> 'Dataset':
         """Return a new object with an additional axis (or axes) inserted at
         the corresponding position in the array shape.  The new object is a
         view into the underlying array, not a copy.
@@ -2551,16 +2615,21 @@ class Dataset(Mapping, ImplementsDatasetReduce, DataWithCoords):
 
         Parameters
         ----------
-        dim : str, sequence of str, dict, or None
-            Dimensions to include on the new variable.
-            If provided as str or sequence of str, then dimensions are inserted
-            with length 1. If provided as a dict, then the keys are the new
-            dimensions and the values are either integers (giving the length of
-            the new dimensions) or sequence/ndarray (giving the coordinates of
-            the new dimensions). **WARNING** for python 3.5, if ``dim`` is
-            dict-like, then it must be an ``OrderedDict``. This is to ensure
-            that the order in which the dims are given is maintained.
-        axis : integer, list (or tuple) of integers, or None
+        dim : hashable, sequence of hashable, mapping, or None
+            Dimensions to include on the new variable. If provided as hashable
+            or sequence of hashable, then dimensions are inserted with length
+            1. If provided as a mapping, then the keys are the new dimensions
+            and the values are either integers (giving the length of the new
+            dimensions) or array-like (giving the coordinates of the new
+            dimensions).
+
+            .. note::
+
+               For Python 3.5, if ``dim`` is a mapping, then it must be an
+               ``OrderedDict``. This is to ensure that the order in which the
+               dims are given is maintained.
+
+        axis : integer, sequence of integers, or None
             Axis position(s) where new axis is to be inserted (position(s) on
             the result array). If a list (or tuple) of integers is passed,
             multiple axes are inserted. In this case, dim arguments should be
@@ -2570,39 +2639,49 @@ class Dataset(Mapping, ImplementsDatasetReduce, DataWithCoords):
             The keywords are arbitrary dimensions being inserted and the values
             are either the lengths of the new dims (if int is given), or their
             coordinates. Note, this is an alternative to passing a dict to the
-            dim kwarg and will only be used if dim is None. **WARNING** for
-            python 3.5 ``dim_kwargs`` is not available.
+            dim kwarg and will only be used if dim is None.
+
+            .. note::
+
+               For Python 3.5, ``dim_kwargs`` is not available.
 
         Returns
         -------
         expanded : same type as caller
             This object, but with an additional dimension(s).
         """
-        if isinstance(dim, int):
-            raise TypeError('dim should be str or sequence of strs or dict')
-        elif isinstance(dim, str):
+        # TODO: get rid of the below code block when python 3.5 is no longer
+        #   supported.
+        if sys.version < '3.6':
+            if isinstance(dim, Mapping) and not isinstance(dim, OrderedDict):
+                raise TypeError("dim must be an OrderedDict for python <3.6")
+            if dim_kwargs:
+                raise ValueError("dim_kwargs isn't available for python <3.6")
+
+        if dim is None:
+            pass
+        elif isinstance(dim, Mapping):
+            # We're later going to modify dim in place; don't tamper with
+            # the input
+            dim = OrderedDict(dim)
+        elif isinstance(dim, int):
+            raise TypeError(
+                "dim should be hashable or sequence of hashables or mapping"
+            )
+        elif isinstance(dim, str) or not isinstance(dim, Sequence):
             dim = OrderedDict(((dim, 1),))
-        elif isinstance(dim, (list, tuple)):
+        elif isinstance(dim, Sequence):
             if len(dim) != len(set(dim)):
                 raise ValueError('dims should not contain duplicate values.')
             dim = OrderedDict(((d, 1) for d in dim))
 
-        # TODO: get rid of the below code block when python 3.5 is no longer
-        #   supported.
-        python36_plus = sys.version_info[0] == 3 and sys.version_info[1] > 5
-        not_ordereddict = dim is not None and not isinstance(dim, OrderedDict)
-        if not python36_plus and not_ordereddict:
-            raise TypeError("dim must be an OrderedDict for python <3.6")
-        elif not python36_plus and dim_kwargs:
-            raise ValueError("dim_kwargs isn't available for python <3.6")
-
         dim = either_dict_or_kwargs(dim, dim_kwargs, 'expand_dims')
-
-        if axis is not None and not isinstance(axis, (list, tuple)):
-            axis = [axis]
+        assert isinstance(dim, MutableMapping)
 
         if axis is None:
             axis = list(range(len(dim)))
+        elif not isinstance(axis, Sequence):
+            axis = [axis]
 
         if len(dim) != len(axis):
             raise ValueError('lengths of dim and axis should be identical.')
@@ -2616,7 +2695,7 @@ class Dataset(Mapping, ImplementsDatasetReduce, DataWithCoords):
                     '{dim} already exists as coordinate or'
                     ' variable name.'.format(dim=d))
 
-        variables = OrderedDict()
+        variables = OrderedDict()  # type: OrderedDict[Hashable, Variable]
         coord_names = self._coord_names.copy()
         # If dim is a dict, then ensure that the values are either integers
         # or iterables.
@@ -2660,9 +2739,7 @@ class Dataset(Mapping, ImplementsDatasetReduce, DataWithCoords):
                     all_dims = list(zip(v.dims, v.shape))
                     for d, c in zip_axis_dim:
                         all_dims.insert(d, c)
-                    all_dims = OrderedDict(all_dims)
-
-                    variables[k] = v.set_dims(all_dims)
+                    variables[k] = v.set_dims(OrderedDict(all_dims))
             else:
                 # If dims includes a label of a non-dimension coordinate,
                 # it will be promoted to a 1D coordinate with a single value.
@@ -2674,8 +2751,13 @@ class Dataset(Mapping, ImplementsDatasetReduce, DataWithCoords):
         return self._replace_vars_and_dims(
             variables, dims=new_dims, coord_names=coord_names)
 
-    def set_index(self, indexes=None, append=False, inplace=None,
-                  **indexes_kwargs):
+    def set_index(
+        self,
+        indexes: Mapping[Hashable, Union[Hashable, Sequence[Hashable]]] = None,
+        append: bool = False,
+        inplace: bool = None,
+        **indexes_kwargs: Union[Hashable, Sequence[Hashable]]
+    ) -> 'Dataset':
         """Set Dataset (multi-)indexes using one or more existing coordinates
         or variables.
 
@@ -2713,7 +2795,12 @@ class Dataset(Mapping, ImplementsDatasetReduce, DataWithCoords):
         return self._replace_vars_and_dims(variables, coord_names=coord_names,
                                            inplace=inplace)
 
-    def reset_index(self, dims_or_levels, drop=False, inplace=None):
+    def reset_index(
+        self,
+        dims_or_levels: Union[Hashable, Sequence[Hashable]],
+        drop: bool = False,
+        inplace: bool = None,
+    ) -> 'Dataset':
         """Reset the specified index(es) or multi-index level(s).
 
         Parameters
@@ -2738,14 +2825,22 @@ class Dataset(Mapping, ImplementsDatasetReduce, DataWithCoords):
         Dataset.set_index
         """
         inplace = _check_inplace(inplace)
-        variables, coord_names = split_indexes(dims_or_levels, self._variables,
-                                               self._coord_names,
-                                               self._level_coords, drop=drop)
+        variables, coord_names = split_indexes(
+            dims_or_levels,
+            self._variables,
+            self._coord_names,
+            cast(Mapping[Hashable, Hashable], self._level_coords),
+            drop=drop,
+        )
         return self._replace_vars_and_dims(variables, coord_names=coord_names,
                                            inplace=inplace)
 
-    def reorder_levels(self, dim_order=None, inplace=None,
-                       **dim_order_kwargs):
+    def reorder_levels(
+        self,
+        dim_order: Mapping[Hashable, Sequence[int]] = None,
+        inplace: bool = None,
+        **dim_order_kwargs: Sequence[int]
+    ) -> 'Dataset':
         """Rearrange index levels using input order.
 
         Parameters
@@ -2818,7 +2913,11 @@ class Dataset(Mapping, ImplementsDatasetReduce, DataWithCoords):
         return self._replace_with_new_dims(
             variables, coord_names=coord_names, indexes=indexes)
 
-    def stack(self, dimensions=None, **dimensions_kwargs):
+    def stack(
+        self,
+        dimensions: Mapping[Hashable, Sequence[Hashable]] = None,
+        **dimensions_kwargs: Sequence[Hashable]
+    ) -> 'Dataset':
         """
         Stack any number of existing dimensions into a single new dimension.
 
@@ -2850,8 +2949,13 @@ class Dataset(Mapping, ImplementsDatasetReduce, DataWithCoords):
             result = result._stack_once(dims, new_dim)
         return result
 
-    def to_stacked_array(self, new_dim, sample_dims, variable_dim='variable',
-                         name=None):
+    def to_stacked_array(
+        self,
+        new_dim: Hashable,
+        sample_dims: Sequence[Hashable],
+        variable_dim: str = 'variable',
+        name: Hashable = None
+    ) -> 'DataArray':
         """Combine variables of differing dimensionality into a DataArray
         without broadcasting.
 
@@ -2860,9 +2964,9 @@ class Dataset(Mapping, ImplementsDatasetReduce, DataWithCoords):
 
         Parameters
         ----------
-        new_dim : str
+        new_dim : Hashable
             Name of the new stacked coordinate
-        sample_dims : Sequence[str]
+        sample_dims : Sequence[Hashable]
             Dimensions that **will not** be stacked. Each array in the dataset
             must share these dimensions. For machine learning applications,
             these define the dimensions over which samples are drawn.
@@ -2963,7 +3067,7 @@ class Dataset(Mapping, ImplementsDatasetReduce, DataWithCoords):
 
         return data_array
 
-    def _unstack_once(self, dim):
+    def _unstack_once(self, dim: Hashable) -> 'Dataset':
         index = self.get_index(dim)
         # GH2619. For MultiIndex, we need to call remove_unused.
         if LooseVersion(pd.__version__) >= "0.20":
@@ -2982,7 +3086,7 @@ class Dataset(Mapping, ImplementsDatasetReduce, DataWithCoords):
         new_dim_names = index.names
         new_dim_sizes = [lev.size for lev in index.levels]
 
-        variables = OrderedDict()
+        variables = OrderedDict()  # type: OrderedDict[Hashable, Variable]
         indexes = OrderedDict(
             (k, v) for k, v in self.indexes.items() if k != dim)
 
@@ -3003,7 +3107,10 @@ class Dataset(Mapping, ImplementsDatasetReduce, DataWithCoords):
         return self._replace_with_new_dims(
             variables, coord_names=coord_names, indexes=indexes)
 
-    def unstack(self, dim=None):
+    def unstack(
+        self,
+        dim: Union[Hashable, Iterable[Hashable]] = None
+    ) -> 'Dataset':
         """
         Unstack existing dimensions corresponding to MultiIndexes into
         multiple new dimensions.
@@ -3012,7 +3119,7 @@ class Dataset(Mapping, ImplementsDatasetReduce, DataWithCoords):
 
         Parameters
         ----------
-        dim : str or sequence of str, optional
+        dim : Hashable or iterable of Hashable, optional
             Dimension(s) over which to unstack. By default unstacks all
             MultiIndexes.
 
@@ -3025,12 +3132,16 @@ class Dataset(Mapping, ImplementsDatasetReduce, DataWithCoords):
         --------
         Dataset.stack
         """
-
         if dim is None:
-            dims = [d for d in self.dims if isinstance(self.get_index(d),
-                                                       pd.MultiIndex)]
+            dims = [
+                d for d in self.dims
+                if isinstance(self.get_index(d), pd.MultiIndex)
+            ]
         else:
-            dims = [dim] if isinstance(dim, str) else dim
+            if isinstance(dim, str) or not isinstance(dim, Iterable):
+                dims = [dim]
+            else:
+                dims = list(dim)
 
             missing_dims = [d for d in dims if d not in self.dims]
             if missing_dims:
@@ -3048,19 +3159,7 @@ class Dataset(Mapping, ImplementsDatasetReduce, DataWithCoords):
             result = result._unstack_once(dim)
         return result
 
-    def update(
-        self,
-        other: Union[
-            'Dataset',
-            Mapping[Hashable, Union[
-                'DataArray',
-                Variable,
-                Tuple[Hashable, Any],
-                Tuple[Sequence[Hashable], Any],
-            ]],
-        ],
-        inplace: bool = None
-    ) -> 'Dataset':
+    def update(self, other: 'DatasetLike', inplace: bool = None) -> 'Dataset':
         """Update this dataset's variables with those from another dataset.
 
         Parameters
@@ -3095,8 +3194,15 @@ class Dataset(Mapping, ImplementsDatasetReduce, DataWithCoords):
         return self._replace_vars_and_dims(variables, coord_names, dims,
                                            inplace=inplace)
 
-    def merge(self, other, inplace=None, overwrite_vars=frozenset(),
-              compat='no_conflicts', join='outer', fill_value=dtypes.NA):
+    def merge(
+        self,
+        other: 'DatasetLike',
+        inplace: bool = None,
+        overwrite_vars: Union[Hashable, Iterable[Hashable]] = frozenset(),
+        compat: str = 'no_conflicts',
+        join: str = 'outer',
+        fill_value: Any = dtypes.NA
+    ) -> 'Dataset':
         """Merge the arrays of two datasets into a single dataset.
 
         This method generally does not allow for overriding data, with the
@@ -3111,7 +3217,7 @@ class Dataset(Mapping, ImplementsDatasetReduce, DataWithCoords):
         inplace : bool, optional
             If True, merge the other dataset into this dataset in-place.
             Otherwise, return a new dataset object.
-        overwrite_vars : str or sequence, optional
+        overwrite_vars : Hashable or iterable of Hashable, optional
             If provided, update variables of these name(s) without checking for
             conflicts in this dataset.
         compat : {'broadcast_equals', 'equals', 'identical',
diff --git a/xarray/core/merge.py b/xarray/core/merge.py
index e71cf16a..9c909aa1 100644
--- a/xarray/core/merge.py
+++ b/xarray/core/merge.py
@@ -1,6 +1,18 @@
 from collections import OrderedDict
-from typing import (Any, Dict, Hashable, List, Mapping, Optional, Sequence,
-                    Set, Tuple, Union, TYPE_CHECKING)
+from typing import (
+    Any,
+    Dict,
+    Hashable,
+    Iterable,
+    List,
+    Mapping,
+    MutableMapping,
+    Sequence,
+    Set,
+    Tuple,
+    Union,
+    TYPE_CHECKING
+)
 
 import pandas as pd
 
@@ -14,6 +26,21 @@ if TYPE_CHECKING:
     from .dataarray import DataArray
     from .dataset import Dataset
 
+    DatasetLikeValue = Union[
+        DataArray,
+        Variable,
+        Tuple[Hashable, Any],
+        Tuple[Sequence[Hashable], Any],
+    ]
+    DatasetLike = Union[Dataset, Mapping[Hashable, DatasetLikeValue]]
+    """Any object type that can be used on the rhs of Dataset.update,
+    Dataset.merge, etc.
+    """
+    MutableDatasetLike = Union[
+        Dataset,
+        MutableMapping[Hashable, DatasetLikeValue],
+    ]
+
 
 PANDAS_TYPES = (pd.Series, pd.DataFrame, pdcompat.Panel)
 
@@ -116,11 +143,10 @@ class OrderedDefaultDict(OrderedDict):
 
 
 def merge_variables(
-        list_of_variables_dicts,  # type: List[Mapping[Any, Variable]]
-        priority_vars=None,       # type: Optional[Mapping[Any, Variable]]
-        compat='minimal',         # type: str
-):
-    # type: (...) -> OrderedDict[Any, Variable]
+        list_of_variables_dicts: List[Mapping[Any, Variable]],
+        priority_vars: Mapping[Any, Variable] = None,
+        compat: str = 'minimal',
+) -> 'OrderedDict[Any, Variable]':
     """Merge dicts of variables, while resolving conflicts appropriately.
 
     Parameters
@@ -228,8 +254,9 @@ def expand_variable_dicts(
     return var_dicts
 
 
-def determine_coords(list_of_variable_dicts):
-    # type: (List[Dict]) -> Tuple[Set, Set]
+def determine_coords(
+    list_of_variable_dicts: Iterable['DatasetLike']
+) -> Tuple[Set[Hashable], Set[Hashable]]:
     """Given a list of dicts with xarray object values, identify coordinates.
 
     Parameters
@@ -265,7 +292,9 @@ def determine_coords(list_of_variable_dicts):
     return coord_names, noncoord_names
 
 
-def coerce_pandas_values(objects):
+def coerce_pandas_values(
+    objects: Iterable['DatasetLike']
+) -> List['DatasetLike']:
     """Convert pandas values found in a list of labeled objects.
 
     Parameters
@@ -285,7 +314,7 @@ def coerce_pandas_values(objects):
     out = []
     for obj in objects:
         if isinstance(obj, Dataset):
-            variables = obj
+            variables = obj  # type: DatasetLike
         else:
             variables = OrderedDict()
             if isinstance(obj, PANDAS_TYPES):
@@ -555,17 +584,28 @@ def merge(objects, compat='no_conflicts', join='outer', fill_value=dtypes.NA):
     return merged
 
 
-def dataset_merge_method(dataset, other, overwrite_vars, compat, join,
-                         fill_value=dtypes.NA):
-    """Guts of the Dataset.merge method."""
+def dataset_merge_method(
+    dataset: 'Dataset',
+    other: 'DatasetLike',
+    overwrite_vars: Union[Hashable, Iterable[Hashable]],
+    compat: str,
+    join: str,
+    fill_value: Any
+) -> Tuple['OrderedDict[Hashable, Variable]',
+           Set[Hashable],
+           Dict[Hashable, int]]:
+    """Guts of the Dataset.merge method.
+    """
 
     # we are locked into supporting overwrite_vars for the Dataset.merge
     # method due for backwards compatibility
     # TODO: consider deprecating it?
 
-    if isinstance(overwrite_vars, str):
-        overwrite_vars = set([overwrite_vars])
-    overwrite_vars = set(overwrite_vars)
+    if isinstance(overwrite_vars, Iterable) and not isinstance(
+            overwrite_vars, str):
+        overwrite_vars = set(overwrite_vars)
+    else:
+        overwrite_vars = {overwrite_vars}
 
     if not overwrite_vars:
         objs = [dataset, other]
@@ -574,8 +614,8 @@ def dataset_merge_method(dataset, other, overwrite_vars, compat, join,
         objs = [dataset, other]
         priority_arg = 1
     else:
-        other_overwrite = OrderedDict()
-        other_no_overwrite = OrderedDict()
+        other_overwrite = OrderedDict()  # type: MutableDatasetLike
+        other_no_overwrite = OrderedDict()  # type: MutableDatasetLike
         for k, v in other.items():
             if k in overwrite_vars:
                 other_overwrite[k] = v
@@ -590,15 +630,7 @@ def dataset_merge_method(dataset, other, overwrite_vars, compat, join,
 
 def dataset_update_method(
     dataset: 'Dataset',
-    other: Union[
-        'Dataset',
-        Mapping[Hashable, Union[
-            'DataArray',
-            Variable,
-            Tuple[Hashable, Any],
-            Tuple[Sequence[Hashable], Any],
-        ]],
-    ],
+    other: 'DatasetLike',
 ) -> Tuple['OrderedDict[Hashable, Variable]',
            Set[Hashable],
            Dict[Hashable, int]]:
diff --git a/xarray/core/utils.py b/xarray/core/utils.py
index 811d71b7..8ce30a3c 100644
--- a/xarray/core/utils.py
+++ b/xarray/core/utils.py
@@ -88,7 +88,7 @@ def safe_cast_to_index(array: Any) -> pd.Index:
 
 
 def multiindex_from_product_levels(levels: Sequence[pd.Index],
-                                   names: Optional[Sequence[str]] = None
+                                   names: Sequence[str] = None
                                    ) -> pd.MultiIndex:
     """Creating a MultiIndex from a product without refactorizing levels.
 
@@ -351,7 +351,7 @@ class SortedKeysDict(MutableMapping[K, V]):
     """
     __slots__ = ['mapping']
 
-    def __init__(self, mapping: Optional[MutableMapping[K, V]] = None):
+    def __init__(self, mapping: MutableMapping[K, V] = None):
         self.mapping = {} if mapping is None else mapping
 
     def __getitem__(self, key: K) -> V:
@@ -382,7 +382,7 @@ class OrderedSet(MutableSet[T]):
     The API matches the builtin set, but it preserves insertion order of
     elements, like an OrderedDict.
     """
-    def __init__(self, values: Optional[AbstractSet[T]] = None):
+    def __init__(self, values: AbstractSet[T] = None):
         self._ordered_dict = OrderedDict()  # type: MutableMapping[T, None]
         if values is not None:
             # Disable type checking - both mypy and PyCharm believes that
diff --git a/xarray/py.typed b/xarray/py.typed
new file mode 100644
index 00000000..e69de29b
+ git diff 1757dffac2fa493d7b9a074b84cf8c830a706688
+ source /opt/miniconda3/bin/activate
++ _CONDA_ROOT=/opt/miniconda3
++ . /opt/miniconda3/etc/profile.d/conda.sh
+++ export CONDA_EXE=/opt/miniconda3/bin/conda
+++ CONDA_EXE=/opt/miniconda3/bin/conda
+++ export _CE_M=
+++ _CE_M=
+++ export _CE_CONDA=
+++ _CE_CONDA=
+++ export CONDA_PYTHON_EXE=/opt/miniconda3/bin/python
+++ CONDA_PYTHON_EXE=/opt/miniconda3/bin/python
+++ '[' -z x ']'
++ conda activate
++ local cmd=activate
++ case "$cmd" in
++ __conda_activate activate
++ '[' -n '' ']'
++ local ask_conda
+++ PS1='(testbed) '
+++ __conda_exe shell.posix activate
+++ /opt/miniconda3/bin/conda shell.posix activate
++ ask_conda='. "/opt/miniconda3/envs/testbed/etc/conda/deactivate.d/udunits2-deactivate.sh"
. "/opt/miniconda3/envs/testbed/etc/conda/deactivate.d/proj4-deactivate.sh"
. "/opt/miniconda3/envs/testbed/etc/conda/deactivate.d/libxml2_deactivate.sh"
. "/opt/miniconda3/envs/testbed/etc/conda/deactivate.d/geotiff-deactivate.sh"
. "/opt/miniconda3/envs/testbed/etc/conda/deactivate.d/gdal-deactivate.sh"
. "/opt/miniconda3/envs/testbed/etc/conda/deactivate.d/esmpy-deactivate.sh"
. "/opt/miniconda3/envs/testbed/etc/conda/deactivate.d/esmf-deactivate.sh"
PS1='\''(base) '\''
export PATH='\''/opt/miniconda3/bin:/opt/miniconda3/condabin:/opt/miniconda3/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin'\''
export CONDA_PREFIX='\''/opt/miniconda3'\''
export CONDA_SHLVL='\''3'\''
export CONDA_DEFAULT_ENV='\''base'\''
export CONDA_PROMPT_MODIFIER='\''(base) '\''
export CONDA_PREFIX_2='\''/opt/miniconda3/envs/testbed'\''
export CONDA_EXE='\''/opt/miniconda3/bin/conda'\''
export _CE_M='\'''\''
export _CE_CONDA='\'''\''
export CONDA_PYTHON_EXE='\''/opt/miniconda3/bin/python'\'''
++ eval '. "/opt/miniconda3/envs/testbed/etc/conda/deactivate.d/udunits2-deactivate.sh"
. "/opt/miniconda3/envs/testbed/etc/conda/deactivate.d/proj4-deactivate.sh"
. "/opt/miniconda3/envs/testbed/etc/conda/deactivate.d/libxml2_deactivate.sh"
. "/opt/miniconda3/envs/testbed/etc/conda/deactivate.d/geotiff-deactivate.sh"
. "/opt/miniconda3/envs/testbed/etc/conda/deactivate.d/gdal-deactivate.sh"
. "/opt/miniconda3/envs/testbed/etc/conda/deactivate.d/esmpy-deactivate.sh"
. "/opt/miniconda3/envs/testbed/etc/conda/deactivate.d/esmf-deactivate.sh"
PS1='\''(base) '\''
export PATH='\''/opt/miniconda3/bin:/opt/miniconda3/condabin:/opt/miniconda3/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin'\''
export CONDA_PREFIX='\''/opt/miniconda3'\''
export CONDA_SHLVL='\''3'\''
export CONDA_DEFAULT_ENV='\''base'\''
export CONDA_PROMPT_MODIFIER='\''(base) '\''
export CONDA_PREFIX_2='\''/opt/miniconda3/envs/testbed'\''
export CONDA_EXE='\''/opt/miniconda3/bin/conda'\''
export _CE_M='\'''\''
export _CE_CONDA='\'''\''
export CONDA_PYTHON_EXE='\''/opt/miniconda3/bin/python'\'''
+++ . /opt/miniconda3/envs/testbed/etc/conda/deactivate.d/udunits2-deactivate.sh
++++ unset UDUNITS2_XML_PATH
++++ '[' -n '' ']'
+++ . /opt/miniconda3/envs/testbed/etc/conda/deactivate.d/proj4-deactivate.sh
++++ unset PROJ_DATA
++++ unset PROJ_NETWORK
++++ '[' -n '' ']'
+++ . /opt/miniconda3/envs/testbed/etc/conda/deactivate.d/libxml2_deactivate.sh
++++ test -n ''
++++ unset XML_CATALOG_FILES
++++ unset xml_catalog_files_libxml2
+++ . /opt/miniconda3/envs/testbed/etc/conda/deactivate.d/geotiff-deactivate.sh
++++ unset GEOTIFF_CSV
++++ '[' -n '' ']'
+++ . /opt/miniconda3/envs/testbed/etc/conda/deactivate.d/gdal-deactivate.sh
++++ unset GDAL_DATA
++++ '[' -n '' ']'
++++ unset GDAL_DRIVER_PATH
++++ '[' -n '' ']'
++++ unset CPL_ZIP_ENCODING
+++ . /opt/miniconda3/envs/testbed/etc/conda/deactivate.d/esmpy-deactivate.sh
++++ unset ESMFMKFILE
++++ '[' -n /opt/miniconda3/envs/testbed/lib/esmf.mk ']'
++++ export ESMFMKFILE=/opt/miniconda3/envs/testbed/lib/esmf.mk
++++ ESMFMKFILE=/opt/miniconda3/envs/testbed/lib/esmf.mk
++++ unset _CONDA_SET_ESMFMKFILE
+++ . /opt/miniconda3/envs/testbed/etc/conda/deactivate.d/esmf-deactivate.sh
++++ unset ESMFMKFILE
++++ '[' -n '' ']'
+++ PS1='(base) '
+++ export PATH=/opt/miniconda3/bin:/opt/miniconda3/condabin:/opt/miniconda3/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin
+++ PATH=/opt/miniconda3/bin:/opt/miniconda3/condabin:/opt/miniconda3/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin
+++ export CONDA_PREFIX=/opt/miniconda3
+++ CONDA_PREFIX=/opt/miniconda3
+++ export CONDA_SHLVL=3
+++ CONDA_SHLVL=3
+++ export CONDA_DEFAULT_ENV=base
+++ CONDA_DEFAULT_ENV=base
+++ export 'CONDA_PROMPT_MODIFIER=(base) '
+++ CONDA_PROMPT_MODIFIER='(base) '
+++ export CONDA_PREFIX_2=/opt/miniconda3/envs/testbed
+++ CONDA_PREFIX_2=/opt/miniconda3/envs/testbed
+++ export CONDA_EXE=/opt/miniconda3/bin/conda
+++ CONDA_EXE=/opt/miniconda3/bin/conda
+++ export _CE_M=
+++ _CE_M=
+++ export _CE_CONDA=
+++ _CE_CONDA=
+++ export CONDA_PYTHON_EXE=/opt/miniconda3/bin/python
+++ CONDA_PYTHON_EXE=/opt/miniconda3/bin/python
++ __conda_hashr
++ '[' -n '' ']'
++ '[' -n '' ']'
++ hash -r
+ conda activate testbed
+ local cmd=activate
+ case "$cmd" in
+ __conda_activate activate testbed
+ '[' -n '' ']'
+ local ask_conda
++ PS1='(base) '
++ __conda_exe shell.posix activate testbed
++ /opt/miniconda3/bin/conda shell.posix activate testbed
+ ask_conda='PS1='\''(testbed) '\''
export PATH='\''/opt/miniconda3/envs/testbed/bin:/opt/miniconda3/condabin:/opt/miniconda3/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin'\''
export CONDA_PREFIX='\''/opt/miniconda3/envs/testbed'\''
export CONDA_SHLVL='\''4'\''
export CONDA_DEFAULT_ENV='\''testbed'\''
export CONDA_PROMPT_MODIFIER='\''(testbed) '\''
export CONDA_PREFIX_3='\''/opt/miniconda3'\''
export CONDA_EXE='\''/opt/miniconda3/bin/conda'\''
export _CE_M='\'''\''
export _CE_CONDA='\'''\''
export CONDA_PYTHON_EXE='\''/opt/miniconda3/bin/python'\''
. "/opt/miniconda3/envs/testbed/etc/conda/activate.d/esmf-activate.sh"
. "/opt/miniconda3/envs/testbed/etc/conda/activate.d/esmpy-activate.sh"
. "/opt/miniconda3/envs/testbed/etc/conda/activate.d/gdal-activate.sh"
. "/opt/miniconda3/envs/testbed/etc/conda/activate.d/geotiff-activate.sh"
. "/opt/miniconda3/envs/testbed/etc/conda/activate.d/libarrow_activate.sh"
. "/opt/miniconda3/envs/testbed/etc/conda/activate.d/libxml2_activate.sh"
. "/opt/miniconda3/envs/testbed/etc/conda/activate.d/proj4-activate.sh"
. "/opt/miniconda3/envs/testbed/etc/conda/activate.d/udunits2-activate.sh"'
+ eval 'PS1='\''(testbed) '\''
export PATH='\''/opt/miniconda3/envs/testbed/bin:/opt/miniconda3/condabin:/opt/miniconda3/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin'\''
export CONDA_PREFIX='\''/opt/miniconda3/envs/testbed'\''
export CONDA_SHLVL='\''4'\''
export CONDA_DEFAULT_ENV='\''testbed'\''
export CONDA_PROMPT_MODIFIER='\''(testbed) '\''
export CONDA_PREFIX_3='\''/opt/miniconda3'\''
export CONDA_EXE='\''/opt/miniconda3/bin/conda'\''
export _CE_M='\'''\''
export _CE_CONDA='\'''\''
export CONDA_PYTHON_EXE='\''/opt/miniconda3/bin/python'\''
. "/opt/miniconda3/envs/testbed/etc/conda/activate.d/esmf-activate.sh"
. "/opt/miniconda3/envs/testbed/etc/conda/activate.d/esmpy-activate.sh"
. "/opt/miniconda3/envs/testbed/etc/conda/activate.d/gdal-activate.sh"
. "/opt/miniconda3/envs/testbed/etc/conda/activate.d/geotiff-activate.sh"
. "/opt/miniconda3/envs/testbed/etc/conda/activate.d/libarrow_activate.sh"
. "/opt/miniconda3/envs/testbed/etc/conda/activate.d/libxml2_activate.sh"
. "/opt/miniconda3/envs/testbed/etc/conda/activate.d/proj4-activate.sh"
. "/opt/miniconda3/envs/testbed/etc/conda/activate.d/udunits2-activate.sh"'
++ PS1='(testbed) '
++ export PATH=/opt/miniconda3/envs/testbed/bin:/opt/miniconda3/condabin:/opt/miniconda3/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin
++ PATH=/opt/miniconda3/envs/testbed/bin:/opt/miniconda3/condabin:/opt/miniconda3/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin
++ export CONDA_PREFIX=/opt/miniconda3/envs/testbed
++ CONDA_PREFIX=/opt/miniconda3/envs/testbed
++ export CONDA_SHLVL=4
++ CONDA_SHLVL=4
++ export CONDA_DEFAULT_ENV=testbed
++ CONDA_DEFAULT_ENV=testbed
++ export 'CONDA_PROMPT_MODIFIER=(testbed) '
++ CONDA_PROMPT_MODIFIER='(testbed) '
++ export CONDA_PREFIX_3=/opt/miniconda3
++ CONDA_PREFIX_3=/opt/miniconda3
++ export CONDA_EXE=/opt/miniconda3/bin/conda
++ CONDA_EXE=/opt/miniconda3/bin/conda
++ export _CE_M=
++ _CE_M=
++ export _CE_CONDA=
++ _CE_CONDA=
++ export CONDA_PYTHON_EXE=/opt/miniconda3/bin/python
++ CONDA_PYTHON_EXE=/opt/miniconda3/bin/python
++ . /opt/miniconda3/envs/testbed/etc/conda/activate.d/esmf-activate.sh
+++ '[' -n '' ']'
+++ '[' -f /opt/miniconda3/envs/testbed/lib/esmf.mk ']'
+++ export ESMFMKFILE=/opt/miniconda3/envs/testbed/lib/esmf.mk
+++ ESMFMKFILE=/opt/miniconda3/envs/testbed/lib/esmf.mk
++ . /opt/miniconda3/envs/testbed/etc/conda/activate.d/esmpy-activate.sh
+++ '[' -n /opt/miniconda3/envs/testbed/lib/esmf.mk ']'
+++ export _CONDA_SET_ESMFMKFILE=/opt/miniconda3/envs/testbed/lib/esmf.mk
+++ _CONDA_SET_ESMFMKFILE=/opt/miniconda3/envs/testbed/lib/esmf.mk
+++ '[' -f /opt/miniconda3/envs/testbed/lib/esmf.mk ']'
+++ export ESMFMKFILE=/opt/miniconda3/envs/testbed/lib/esmf.mk
+++ ESMFMKFILE=/opt/miniconda3/envs/testbed/lib/esmf.mk
++ . /opt/miniconda3/envs/testbed/etc/conda/activate.d/gdal-activate.sh
+++ '[' -n '' ']'
+++ '[' -n '' ']'
+++ '[' -d /opt/miniconda3/envs/testbed/share/gdal ']'
+++ export GDAL_DATA=/opt/miniconda3/envs/testbed/share/gdal
+++ GDAL_DATA=/opt/miniconda3/envs/testbed/share/gdal
+++ export GDAL_DRIVER_PATH=/opt/miniconda3/envs/testbed/lib/gdalplugins
+++ GDAL_DRIVER_PATH=/opt/miniconda3/envs/testbed/lib/gdalplugins
+++ '[' '!' -d /opt/miniconda3/envs/testbed/lib/gdalplugins ']'
+++ export CPL_ZIP_ENCODING=UTF-8
+++ CPL_ZIP_ENCODING=UTF-8
+++ '[' -n '5.1.16(1)-release' ']'
+++ '[' -f /opt/miniconda3/envs/testbed/share/bash-completion/completions/gdalinfo ']'
+++ source /opt/miniconda3/envs/testbed/share/bash-completion/completions/gdalinfo
++++ function_exists _get_comp_words_by_ref
++++ declare -f -F _get_comp_words_by_ref
++++ return 1
++++ return 0
++ . /opt/miniconda3/envs/testbed/etc/conda/activate.d/geotiff-activate.sh
+++ '[' -n '' ']'
+++ '[' -d /opt/miniconda3/envs/testbed/share/epsg_csv ']'
+++ '[' -d /opt/miniconda3/envs/testbed/Library/share/epsg_csv ']'
++ . /opt/miniconda3/envs/testbed/etc/conda/activate.d/libarrow_activate.sh
+++ '[' -n '' ']'
+++ _la_log 'Beginning libarrow activation.'
+++ '[' '' = 1 ']'
+++ _la_gdb_prefix=/opt/miniconda3/envs/testbed/share/gdb/auto-load
+++ '[' '!' -w /opt/miniconda3/envs/testbed/share/gdb/auto-load ']'
+++ _la_placeholder=replace_this_section_with_absolute_slashed_path_to_CONDA_PREFIX
+++ _la_symlink_dir=/opt/miniconda3/envs/testbed/share/gdb/auto-load//opt/miniconda3/envs/testbed/lib
+++ _la_orig_install_dir=/opt/miniconda3/envs/testbed/share/gdb/auto-load/replace_this_section_with_absolute_slashed_path_to_CONDA_PREFIX/lib
+++ _la_log '          _la_gdb_prefix: /opt/miniconda3/envs/testbed/share/gdb/auto-load'
+++ '[' '' = 1 ']'
+++ _la_log '         _la_placeholder: replace_this_section_with_absolute_slashed_path_to_CONDA_PREFIX'
+++ '[' '' = 1 ']'
+++ _la_log '         _la_symlink_dir: /opt/miniconda3/envs/testbed/share/gdb/auto-load//opt/miniconda3/envs/testbed/lib'
+++ '[' '' = 1 ']'
+++ _la_log '    _la_orig_install_dir: /opt/miniconda3/envs/testbed/share/gdb/auto-load/replace_this_section_with_absolute_slashed_path_to_CONDA_PREFIX/lib'
+++ '[' '' = 1 ']'
+++ _la_log '  content of that folder:'
+++ '[' '' = 1 ']'
++++ ls -al /opt/miniconda3/envs/testbed/share/gdb/auto-load/replace_this_section_with_absolute_slashed_path_to_CONDA_PREFIX/lib
++++ sed 's/^/      /'
+++ _la_log '      total 12
      drwxr-xr-x 2 root root 4096 Aug 25 05:38 .
      drwxr-xr-x 3 root root 4096 Aug 25 05:38 ..
      -rw-r--r-- 1 root root  971 Aug 25 05:38 libarrow.so.2100.0.0-gdb.py'
+++ '[' '' = 1 ']'
+++ for _la_target in "$_la_orig_install_dir/"*.py
+++ '[' '!' -e /opt/miniconda3/envs/testbed/share/gdb/auto-load/replace_this_section_with_absolute_slashed_path_to_CONDA_PREFIX/lib/libarrow.so.2100.0.0-gdb.py ']'
++++ basename /opt/miniconda3/envs/testbed/share/gdb/auto-load/replace_this_section_with_absolute_slashed_path_to_CONDA_PREFIX/lib/libarrow.so.2100.0.0-gdb.py
+++ _la_symlink=/opt/miniconda3/envs/testbed/share/gdb/auto-load//opt/miniconda3/envs/testbed/lib/libarrow.so.2100.0.0-gdb.py
+++ _la_log '   _la_target: /opt/miniconda3/envs/testbed/share/gdb/auto-load/replace_this_section_with_absolute_slashed_path_to_CONDA_PREFIX/lib/libarrow.so.2100.0.0-gdb.py'
+++ '[' '' = 1 ']'
+++ _la_log '  _la_symlink: /opt/miniconda3/envs/testbed/share/gdb/auto-load//opt/miniconda3/envs/testbed/lib/libarrow.so.2100.0.0-gdb.py'
+++ '[' '' = 1 ']'
+++ '[' -L /opt/miniconda3/envs/testbed/share/gdb/auto-load//opt/miniconda3/envs/testbed/lib/libarrow.so.2100.0.0-gdb.py ']'
++++ readlink /opt/miniconda3/envs/testbed/share/gdb/auto-load//opt/miniconda3/envs/testbed/lib/libarrow.so.2100.0.0-gdb.py
+++ '[' /opt/miniconda3/envs/testbed/share/gdb/auto-load/replace_this_section_with_absolute_slashed_path_to_CONDA_PREFIX/lib/libarrow.so.2100.0.0-gdb.py = /opt/miniconda3/envs/testbed/share/gdb/auto-load/replace_this_section_with_absolute_slashed_path_to_CONDA_PREFIX/lib/libarrow.so.2100.0.0-gdb.py ']'
+++ _la_log 'symlink $_la_symlink already exists and points to $_la_target, skipping.'
+++ '[' '' = 1 ']'
+++ continue
+++ _la_log 'Libarrow activation complete.'
+++ '[' '' = 1 ']'
+++ unset _la_gdb_prefix
+++ unset _la_log
+++ unset _la_orig_install_dir
+++ unset _la_placeholder
+++ unset _la_symlink
+++ unset _la_symlink_dir
+++ unset _la_target
++ . /opt/miniconda3/envs/testbed/etc/conda/activate.d/libxml2_activate.sh
+++ test -n ''
+++ xml_catalog_files_libxml2=
+++ XML_CATALOG_FILES=
+++ conda_catalog_files=
+++ ifs_libxml2=' 	
'
+++ IFS=' '
+++ rem=/opt/miniconda3/envs/testbed
+++ for pre in ${rem}
+++ test '' = /opt/miniconda3/envs/testbed
+++ conda_catalog_files=/opt/miniconda3/envs/testbed
+++ rem=
+++ IFS=' 	
'
+++ conda_catalog_files='file:///opt/miniconda3/envs/testbed/etc/xml/catalog file:///etc/xml/catalog'
+++ export 'XML_CATALOG_FILES=file:///opt/miniconda3/envs/testbed/etc/xml/catalog file:///etc/xml/catalog'
+++ XML_CATALOG_FILES='file:///opt/miniconda3/envs/testbed/etc/xml/catalog file:///etc/xml/catalog'
+++ unset conda_catalog_files ifs_libxml2 rem
++ . /opt/miniconda3/envs/testbed/etc/conda/activate.d/proj4-activate.sh
+++ '[' -n '' ']'
+++ '[' -d /opt/miniconda3/envs/testbed/share/proj ']'
+++ export PROJ_DATA=/opt/miniconda3/envs/testbed/share/proj
+++ PROJ_DATA=/opt/miniconda3/envs/testbed/share/proj
+++ '[' -f /opt/miniconda3/envs/testbed/share/proj/copyright_and_licenses.csv ']'
+++ export PROJ_NETWORK=ON
+++ PROJ_NETWORK=ON
++ . /opt/miniconda3/envs/testbed/etc/conda/activate.d/udunits2-activate.sh
+++ '[' -n '' ']'
+++ '[' -d /opt/miniconda3/envs/testbed/share/udunits ']'
+++ export UDUNITS2_XML_PATH=/opt/miniconda3/envs/testbed/share/udunits/udunits2.xml
+++ UDUNITS2_XML_PATH=/opt/miniconda3/envs/testbed/share/udunits/udunits2.xml
+ __conda_hashr
+ '[' -n '' ']'
+ '[' -n '' ']'
+ hash -r
+ python -m pip install -e .
Obtaining file:///testbed
  Preparing metadata (setup.py): started
  Preparing metadata (setup.py): finished with status 'done'
Requirement already satisfied: numpy>=1.12 in /opt/miniconda3/envs/testbed/lib/python3.10/site-packages (from xarray==0.12.3+27.g1757dffa) (1.23.0)
Requirement already satisfied: pandas>=0.19.2 in /opt/miniconda3/envs/testbed/lib/python3.10/site-packages (from xarray==0.12.3+27.g1757dffa) (1.5.3)
Requirement already satisfied: python-dateutil>=2.8.1 in /opt/miniconda3/envs/testbed/lib/python3.10/site-packages (from pandas>=0.19.2->xarray==0.12.3+27.g1757dffa) (2.8.2)
Requirement already satisfied: pytz>=2020.1 in /opt/miniconda3/envs/testbed/lib/python3.10/site-packages (from pandas>=0.19.2->xarray==0.12.3+27.g1757dffa) (2023.3)
Requirement already satisfied: six>=1.5 in /opt/miniconda3/envs/testbed/lib/python3.10/site-packages (from python-dateutil>=2.8.1->pandas>=0.19.2->xarray==0.12.3+27.g1757dffa) (1.16.0)
Installing collected packages: xarray
  Attempting uninstall: xarray
    Found existing installation: xarray 0.12.3+27.g1757dffa
    Uninstalling xarray-0.12.3+27.g1757dffa:
      Successfully uninstalled xarray-0.12.3+27.g1757dffa
  DEPRECATION: Legacy editable install of xarray==0.12.3+27.g1757dffa from file:///testbed (setup.py develop) is deprecated. pip 25.3 will enforce this behaviour change. A possible replacement is to add a pyproject.toml or enable --use-pep517, and use setuptools >= 64. If the resulting installation is not behaving as expected, try using --config-settings editable_mode=compat. Please consult the setuptools documentation for more information. Discussion can be found at https://github.com/pypa/pip/issues/11457
  Running setup.py develop for xarray
Successfully installed xarray
WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.
+ git apply -v -
Checking patch xarray/core/indexing.py...
Checking patch xarray/core/variable.py...
Applied patch xarray/core/indexing.py cleanly.
Applied patch xarray/core/variable.py cleanly.
+ git apply -v -
<stdin>:16: trailing whitespace.
    
<stdin>:19: trailing whitespace.
    
<stdin>:23: trailing whitespace.
    
Checking patch xarray/tests/test_coverup_pydata__xarray-3095.py...
<stdin>:27: new blank line at EOF.
+
Applied patch xarray/tests/test_coverup_pydata__xarray-3095.py cleanly.
warning: 4 lines add whitespace errors.
+ python3 /root/trace.py --timing --trace --count -C coverage.cover --include-pattern '/testbed/(xarray/core/indexing\.py|xarray/core/variable\.py)' -m pytest --no-header -rA -p no:cacheprovider xarray/tests/test_coverup_pydata__xarray-3095.py
['--timing', '--trace', '--count', '-C', 'coverage.cover', '--include-pattern', '/testbed/(xarray/core/indexing\\.py|xarray/core/variable\\.py)']
============================= test session starts ==============================
collected 1 item

xarray/tests/test_coverup_pydata__xarray-3095.py .                       [100%]

=============================== warnings summary ===============================
xarray/core/dask_array_ops.py:11
xarray/core/dask_array_ops.py:11
  /testbed/xarray/core/dask_array_ops.py:11: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.
    if LooseVersion(dask.__version__) <= LooseVersion('0.18.2'):

xarray/core/npcompat.py:135
xarray/core/npcompat.py:135
  /testbed/xarray/core/npcompat.py:135: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.
    if LooseVersion(np.__version__) >= LooseVersion('1.13'):

xarray/core/dask_array_compat.py:43
xarray/core/dask_array_compat.py:43
  /testbed/xarray/core/dask_array_compat.py:43: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.
    if LooseVersion(dask_version) > LooseVersion('0.19.2'):

xarray/plot/utils.py:17
xarray/plot/utils.py:17
  /testbed/xarray/plot/utils.py:17: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.
    if LooseVersion(nc_time_axis.__version__) < LooseVersion('1.2.0'):

xarray/plot/plot.py:243
  /testbed/xarray/plot/plot.py:243: SyntaxWarning: "is" with a literal. Did you mean "=="?
    if args is ():

xarray/core/pdcompat.py:46
  /testbed/xarray/core/pdcompat.py:46: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.
    if LooseVersion(pd.__version__) < '0.25.0':

../opt/miniconda3/envs/testbed/lib/python3.10/site-packages/setuptools/_distutils/version.py:345
../opt/miniconda3/envs/testbed/lib/python3.10/site-packages/setuptools/_distutils/version.py:345
  /opt/miniconda3/envs/testbed/lib/python3.10/site-packages/setuptools/_distutils/version.py:345: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.
    other = LooseVersion(other)

xarray/tests/__init__.py:57: 15 warnings
  /testbed/xarray/tests/__init__.py:57: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.
    return version.LooseVersion(vstring)

-- Docs: https://docs.pytest.org/en/stable/how-to/capture-warnings.html
==================================== PASSES ====================================
_________________ test_deep_copy_unicode_index_cast_to_object __________________
----------------------------- Captured stdout call -----------------------------
3.50 variable.py(70):     from .dataarray import DataArray
3.50 variable.py(73):     if isinstance(obj, DataArray):
3.50 variable.py(77):     if isinstance(obj, Variable):
3.50 variable.py(79):     elif isinstance(obj, tuple):
3.50 variable.py(87):     elif utils.is_scalar(obj):
3.50 variable.py(89):     elif isinstance(obj, (pd.Index, IndexVariable)) and obj.name is not None:
3.50 variable.py(91):     elif isinstance(obj, (set, dict)):
3.50 variable.py(94):     elif name is not None:
3.50 variable.py(95):         data = as_compatible_data(obj)
3.50 variable.py(150):     if fastpath and getattr(data, 'ndim', 0) > 0:
3.50 variable.py(154):     if isinstance(data, Variable):
3.50 variable.py(157):     if isinstance(data, NON_NUMPY_SUPPORTED_ARRAY_TYPES):
3.50 variable.py(160):     if isinstance(data, tuple):
3.50 variable.py(163):     if isinstance(data, pd.Timestamp):
3.50 variable.py(167):     if isinstance(data, timedelta):
3.50 variable.py(171):     data = getattr(data, 'values', data)
3.50 variable.py(173):     if isinstance(data, np.ma.MaskedArray):
3.50 variable.py(183):     data = np.asarray(data)
3.50 variable.py(185):     if isinstance(data, np.ndarray):
3.50 variable.py(186):         if data.dtype.kind == 'O':
3.50 variable.py(188):         elif data.dtype.kind == 'M':
3.50 variable.py(190):         elif data.dtype.kind == 'm':
3.50 variable.py(193):     return _maybe_wrap_data(data)
3.50 variable.py(127):     if isinstance(data, pd.Index):
3.50 variable.py(129):     return data
3.50 variable.py(96):         if data.ndim != 1:
3.50 variable.py(101):         obj = Variable(name, data, fastpath=True)
3.50 variable.py(261):         self._data = as_compatible_data(data, fastpath=fastpath)
3.50 variable.py(150):     if fastpath and getattr(data, 'ndim', 0) > 0:
3.50 variable.py(152):         return _maybe_wrap_data(data)
3.50 variable.py(127):     if isinstance(data, pd.Index):
3.50 variable.py(129):     return data
3.50 variable.py(262):         self._dims = self._parse_dimensions(dims)
3.50 variable.py(434):         if isinstance(dims, str):
3.50 variable.py(435):             dims = (dims,)
3.50 variable.py(436):         dims = tuple(dims)
3.50 variable.py(437):         if len(dims) != self.ndim:
3.50 variable.py(276):         return self._data.shape
3.50 variable.py(441):         return dims
3.50 variable.py(263):         self._attrs = None
3.50 variable.py(264):         self._encoding = None
3.50 variable.py(265):         if attrs is not None:
3.50 variable.py(267):         if encoding is not None:
3.50 variable.py(106):     if name is not None and name in obj.dims:
3.50 variable.py(427):         return self._dims
3.50 variable.py(108):         if obj.ndim != 1:
3.50 variable.py(276):         return self._data.shape
3.50 variable.py(114):         obj = obj.to_index_variable()
3.50 variable.py(404):         return IndexVariable(self.dims, self._data, self._attrs,
3.50 variable.py(427):         return self._dims
3.50 variable.py(405):                              encoding=self._encoding, fastpath=True)
3.50 variable.py(404):         return IndexVariable(self.dims, self._data, self._attrs,
3.50 variable.py(1846):         super().__init__(dims, data, attrs, encoding, fastpath)
3.50 variable.py(261):         self._data = as_compatible_data(data, fastpath=fastpath)
3.50 variable.py(150):     if fastpath and getattr(data, 'ndim', 0) > 0:
3.50 variable.py(152):         return _maybe_wrap_data(data)
3.50 variable.py(127):     if isinstance(data, pd.Index):
3.50 variable.py(129):     return data
3.50 variable.py(262):         self._dims = self._parse_dimensions(dims)
3.50 variable.py(434):         if isinstance(dims, str):
3.50 variable.py(436):         dims = tuple(dims)
3.50 variable.py(437):         if len(dims) != self.ndim:
3.50 variable.py(276):         return self._data.shape
3.50 variable.py(441):         return dims
3.50 variable.py(263):         self._attrs = None
3.50 variable.py(264):         self._encoding = None
3.50 variable.py(265):         if attrs is not None:
3.50 variable.py(267):         if encoding is not None:
3.50 variable.py(1847):         if self.ndim != 1:
3.50 variable.py(276):         return self._data.shape
3.50 variable.py(1852):         if not isinstance(self._data, PandasIndexAdapter):
3.50 variable.py(1853):             self._data = PandasIndexAdapter(self._data)
3.50 indexing.py(1235):         self.array = utils.safe_cast_to_index(array)
3.50 indexing.py(1236):         if dtype is None:
3.50 indexing.py(1237):             if isinstance(array, pd.PeriodIndex):
3.50 indexing.py(1239):             elif hasattr(array, 'categories'):
3.50 indexing.py(1242):             elif not utils.is_valid_numpy_dtype(array.dtype):
3.50 indexing.py(1245):                 dtype = array.dtype
3.50 indexing.py(1248):         self._dtype = dtype
3.50 variable.py(116):     return obj
3.50 variable.py(427):         return self._dims
3.50 variable.py(1980):         assert self.ndim == 1
3.50 variable.py(276):         return self._data.shape
3.50 indexing.py(1267):         return (len(self.array),)
3.50 variable.py(1981):         index = self._data.array
3.50 variable.py(1982):         if isinstance(index, pd.MultiIndex):
3.50 variable.py(1989):             index = index.set_names(self.name)
3.50 variable.py(2012):         return self.dims[0]
3.50 variable.py(427):         return self._dims
3.50 variable.py(1990):         return index
3.50 variable.py(70):     from .dataarray import DataArray
3.50 variable.py(73):     if isinstance(obj, DataArray):
3.50 variable.py(77):     if isinstance(obj, Variable):
3.50 variable.py(79):     elif isinstance(obj, tuple):
3.50 variable.py(80):         try:
3.50 variable.py(81):             obj = Variable(*obj)
3.50 variable.py(261):         self._data = as_compatible_data(data, fastpath=fastpath)
3.50 variable.py(150):     if fastpath and getattr(data, 'ndim', 0) > 0:
3.50 variable.py(154):     if isinstance(data, Variable):
3.50 variable.py(157):     if isinstance(data, NON_NUMPY_SUPPORTED_ARRAY_TYPES):
3.50 variable.py(160):     if isinstance(data, tuple):
3.50 variable.py(163):     if isinstance(data, pd.Timestamp):
3.50 variable.py(167):     if isinstance(data, timedelta):
3.50 variable.py(171):     data = getattr(data, 'values', data)
3.50 variable.py(173):     if isinstance(data, np.ma.MaskedArray):
3.50 variable.py(183):     data = np.asarray(data)
3.50 variable.py(185):     if isinstance(data, np.ndarray):
3.50 variable.py(186):         if data.dtype.kind == 'O':
3.50 variable.py(188):         elif data.dtype.kind == 'M':
3.50 variable.py(190):         elif data.dtype.kind == 'm':
3.50 variable.py(193):     return _maybe_wrap_data(data)
3.50 variable.py(127):     if isinstance(data, pd.Index):
3.50 variable.py(129):     return data
3.50 variable.py(262):         self._dims = self._parse_dimensions(dims)
3.50 variable.py(434):         if isinstance(dims, str):
3.50 variable.py(435):             dims = (dims,)
3.50 variable.py(436):         dims = tuple(dims)
3.50 variable.py(437):         if len(dims) != self.ndim:
3.50 variable.py(276):         return self._data.shape
3.50 variable.py(441):         return dims
3.50 variable.py(263):         self._attrs = None
3.50 variable.py(264):         self._encoding = None
3.50 variable.py(265):         if attrs is not None:
3.50 variable.py(267):         if encoding is not None:
3.50 variable.py(106):     if name is not None and name in obj.dims:
3.50 variable.py(427):         return self._dims
3.50 variable.py(116):     return obj
3.50 variable.py(427):         return self._dims
3.50 variable.py(70):     from .dataarray import DataArray
3.50 variable.py(73):     if isinstance(obj, DataArray):
3.50 variable.py(77):     if isinstance(obj, Variable):
3.50 variable.py(79):     elif isinstance(obj, tuple):
3.50 variable.py(80):         try:
3.50 variable.py(81):             obj = Variable(*obj)
3.50 variable.py(261):         self._data = as_compatible_data(data, fastpath=fastpath)
3.50 variable.py(150):     if fastpath and getattr(data, 'ndim', 0) > 0:
3.50 variable.py(154):     if isinstance(data, Variable):
3.50 variable.py(157):     if isinstance(data, NON_NUMPY_SUPPORTED_ARRAY_TYPES):
3.50 variable.py(160):     if isinstance(data, tuple):
3.50 variable.py(163):     if isinstance(data, pd.Timestamp):
3.50 variable.py(167):     if isinstance(data, timedelta):
3.50 variable.py(171):     data = getattr(data, 'values', data)
3.50 variable.py(173):     if isinstance(data, np.ma.MaskedArray):
3.50 variable.py(183):     data = np.asarray(data)
3.50 variable.py(185):     if isinstance(data, np.ndarray):
3.50 variable.py(186):         if data.dtype.kind == 'O':
3.50 variable.py(188):         elif data.dtype.kind == 'M':
3.50 variable.py(190):         elif data.dtype.kind == 'm':
3.50 variable.py(193):     return _maybe_wrap_data(data)
3.50 variable.py(127):     if isinstance(data, pd.Index):
3.50 variable.py(129):     return data
3.50 variable.py(262):         self._dims = self._parse_dimensions(dims)
3.50 variable.py(434):         if isinstance(dims, str):
3.50 variable.py(435):             dims = (dims,)
3.50 variable.py(436):         dims = tuple(dims)
3.50 variable.py(437):         if len(dims) != self.ndim:
3.50 variable.py(276):         return self._data.shape
3.50 variable.py(441):         return dims
3.50 variable.py(263):         self._attrs = None
3.50 variable.py(264):         self._encoding = None
3.50 variable.py(265):         if attrs is not None:
3.50 variable.py(267):         if encoding is not None:
3.50 variable.py(106):     if name is not None and name in obj.dims:
3.50 variable.py(427):         return self._dims
3.50 variable.py(116):     return obj
3.50 variable.py(70):     from .dataarray import DataArray
3.50 variable.py(73):     if isinstance(obj, DataArray):
3.50 variable.py(77):     if isinstance(obj, Variable):
3.50 variable.py(79):     elif isinstance(obj, tuple):
3.50 variable.py(87):     elif utils.is_scalar(obj):
3.50 variable.py(89):     elif isinstance(obj, (pd.Index, IndexVariable)) and obj.name is not None:
3.50 variable.py(91):     elif isinstance(obj, (set, dict)):
3.50 variable.py(94):     elif name is not None:
3.50 variable.py(95):         data = as_compatible_data(obj)
3.50 variable.py(150):     if fastpath and getattr(data, 'ndim', 0) > 0:
3.50 variable.py(154):     if isinstance(data, Variable):
3.50 variable.py(157):     if isinstance(data, NON_NUMPY_SUPPORTED_ARRAY_TYPES):
3.50 variable.py(160):     if isinstance(data, tuple):
3.50 variable.py(163):     if isinstance(data, pd.Timestamp):
3.50 variable.py(167):     if isinstance(data, timedelta):
3.50 variable.py(171):     data = getattr(data, 'values', data)
3.50 variable.py(173):     if isinstance(data, np.ma.MaskedArray):
3.50 variable.py(183):     data = np.asarray(data)
3.50 variable.py(185):     if isinstance(data, np.ndarray):
3.50 variable.py(186):         if data.dtype.kind == 'O':
3.50 variable.py(188):         elif data.dtype.kind == 'M':
3.50 variable.py(190):         elif data.dtype.kind == 'm':
3.50 variable.py(193):     return _maybe_wrap_data(data)
3.50 variable.py(127):     if isinstance(data, pd.Index):
3.50 variable.py(129):     return data
3.50 variable.py(96):         if data.ndim != 1:
3.50 variable.py(101):         obj = Variable(name, data, fastpath=True)
3.50 variable.py(261):         self._data = as_compatible_data(data, fastpath=fastpath)
3.50 variable.py(150):     if fastpath and getattr(data, 'ndim', 0) > 0:
3.50 variable.py(152):         return _maybe_wrap_data(data)
3.50 variable.py(127):     if isinstance(data, pd.Index):
3.50 variable.py(129):     return data
3.50 variable.py(262):         self._dims = self._parse_dimensions(dims)
3.50 variable.py(434):         if isinstance(dims, str):
3.50 variable.py(435):             dims = (dims,)
3.50 variable.py(436):         dims = tuple(dims)
3.50 variable.py(437):         if len(dims) != self.ndim:
3.50 variable.py(276):         return self._data.shape
3.50 variable.py(441):         return dims
3.50 variable.py(263):         self._attrs = None
3.50 variable.py(264):         self._encoding = None
3.50 variable.py(265):         if attrs is not None:
3.50 variable.py(267):         if encoding is not None:
3.50 variable.py(106):     if name is not None and name in obj.dims:
3.50 variable.py(427):         return self._dims
3.50 variable.py(108):         if obj.ndim != 1:
3.50 variable.py(276):         return self._data.shape
3.50 variable.py(114):         obj = obj.to_index_variable()
3.50 variable.py(404):         return IndexVariable(self.dims, self._data, self._attrs,
3.50 variable.py(427):         return self._dims
3.50 variable.py(405):                              encoding=self._encoding, fastpath=True)
3.50 variable.py(404):         return IndexVariable(self.dims, self._data, self._attrs,
3.50 variable.py(1846):         super().__init__(dims, data, attrs, encoding, fastpath)
3.50 variable.py(261):         self._data = as_compatible_data(data, fastpath=fastpath)
3.50 variable.py(150):     if fastpath and getattr(data, 'ndim', 0) > 0:
3.50 variable.py(152):         return _maybe_wrap_data(data)
3.50 variable.py(127):     if isinstance(data, pd.Index):
3.50 variable.py(129):     return data
3.50 variable.py(262):         self._dims = self._parse_dimensions(dims)
3.50 variable.py(434):         if isinstance(dims, str):
3.50 variable.py(436):         dims = tuple(dims)
3.50 variable.py(437):         if len(dims) != self.ndim:
3.50 variable.py(276):         return self._data.shape
3.50 variable.py(441):         return dims
3.50 variable.py(263):         self._attrs = None
3.50 variable.py(264):         self._encoding = None
3.50 variable.py(265):         if attrs is not None:
3.50 variable.py(267):         if encoding is not None:
3.50 variable.py(1847):         if self.ndim != 1:
3.50 variable.py(276):         return self._data.shape
3.50 variable.py(1852):         if not isinstance(self._data, PandasIndexAdapter):
3.50 variable.py(1853):             self._data = PandasIndexAdapter(self._data)
3.50 indexing.py(1235):         self.array = utils.safe_cast_to_index(array)
3.50 indexing.py(1236):         if dtype is None:
3.50 indexing.py(1237):             if isinstance(array, pd.PeriodIndex):
3.50 indexing.py(1239):             elif hasattr(array, 'categories'):
3.50 indexing.py(1242):             elif not utils.is_valid_numpy_dtype(array.dtype):
3.50 indexing.py(1245):                 dtype = array.dtype
3.50 indexing.py(1248):         self._dtype = dtype
3.50 variable.py(116):     return obj
3.50 variable.py(70):     from .dataarray import DataArray
3.50 variable.py(73):     if isinstance(obj, DataArray):
3.50 variable.py(77):     if isinstance(obj, Variable):
3.50 variable.py(79):     elif isinstance(obj, tuple):
3.50 variable.py(80):         try:
3.50 variable.py(81):             obj = Variable(*obj)
3.50 variable.py(261):         self._data = as_compatible_data(data, fastpath=fastpath)
3.50 variable.py(150):     if fastpath and getattr(data, 'ndim', 0) > 0:
3.50 variable.py(154):     if isinstance(data, Variable):
3.50 variable.py(157):     if isinstance(data, NON_NUMPY_SUPPORTED_ARRAY_TYPES):
3.50 variable.py(160):     if isinstance(data, tuple):
3.50 variable.py(163):     if isinstance(data, pd.Timestamp):
3.50 variable.py(167):     if isinstance(data, timedelta):
3.50 variable.py(171):     data = getattr(data, 'values', data)
3.50 variable.py(173):     if isinstance(data, np.ma.MaskedArray):
3.50 variable.py(183):     data = np.asarray(data)
3.50 variable.py(185):     if isinstance(data, np.ndarray):
3.50 variable.py(186):         if data.dtype.kind == 'O':
3.50 variable.py(188):         elif data.dtype.kind == 'M':
3.50 variable.py(190):         elif data.dtype.kind == 'm':
3.50 variable.py(193):     return _maybe_wrap_data(data)
3.50 variable.py(127):     if isinstance(data, pd.Index):
3.50 variable.py(129):     return data
3.50 variable.py(262):         self._dims = self._parse_dimensions(dims)
3.50 variable.py(434):         if isinstance(dims, str):
3.50 variable.py(435):             dims = (dims,)
3.50 variable.py(436):         dims = tuple(dims)
3.50 variable.py(437):         if len(dims) != self.ndim:
3.50 variable.py(276):         return self._data.shape
3.50 variable.py(441):         return dims
3.50 variable.py(263):         self._attrs = None
3.50 variable.py(264):         self._encoding = None
3.50 variable.py(265):         if attrs is not None:
3.50 variable.py(267):         if encoding is not None:
3.50 variable.py(106):     if name is not None and name in obj.dims:
3.50 variable.py(427):         return self._dims
3.50 variable.py(116):     return obj
3.50 variable.py(427):         return self._dims
3.50 variable.py(427):         return self._dims
3.50 variable.py(427):         return self._dims
3.50 variable.py(2128):     level_names = defaultdict(list)
3.50 variable.py(2129):     all_level_names = set()
3.50 variable.py(2130):     for var_name, var in variables.items():
3.50 variable.py(2131):         if isinstance(var._data, PandasIndexAdapter):
3.50 variable.py(2130):     for var_name, var in variables.items():
3.50 variable.py(2131):         if isinstance(var._data, PandasIndexAdapter):
3.50 variable.py(2132):             idx_level_names = var.to_index_variable().level_names
3.50 variable.py(1972):         return self
3.50 variable.py(1997):         index = self.to_index()
3.50 variable.py(1980):         assert self.ndim == 1
3.50 variable.py(276):         return self._data.shape
3.50 indexing.py(1267):         return (len(self.array),)
3.50 variable.py(1981):         index = self._data.array
3.50 variable.py(1982):         if isinstance(index, pd.MultiIndex):
3.50 variable.py(1989):             index = index.set_names(self.name)
3.50 variable.py(2012):         return self.dims[0]
3.50 variable.py(427):         return self._dims
3.50 variable.py(1990):         return index
3.50 variable.py(1998):         if isinstance(index, pd.MultiIndex):
3.50 variable.py(2001):             return None
3.50 variable.py(2133):             if idx_level_names is not None:
3.50 variable.py(2136):             if idx_level_names:
3.50 variable.py(2130):     for var_name, var in variables.items():
3.50 variable.py(2131):         if isinstance(var._data, PandasIndexAdapter):
3.50 variable.py(2130):     for var_name, var in variables.items():
3.50 variable.py(2139):     for k, v in level_names.items():
3.50 variable.py(2143):     duplicate_names = [v for v in level_names.values() if len(v) > 1]
3.50 variable.py(2143):     duplicate_names = [v for v in level_names.values() if len(v) > 1]
3.50 variable.py(2144):     if duplicate_names:
3.50 variable.py(2149):     for k, v in variables.items():
3.50 variable.py(2150):         for d in v.dims:
3.50 variable.py(427):         return self._dims
3.50 variable.py(2151):             if d in all_level_names:
3.50 variable.py(2150):         for d in v.dims:
3.50 variable.py(2149):     for k, v in variables.items():
3.50 variable.py(2150):         for d in v.dims:
3.50 variable.py(427):         return self._dims
3.50 variable.py(2151):             if d in all_level_names:
3.50 variable.py(2150):         for d in v.dims:
3.50 variable.py(2149):     for k, v in variables.items():
3.50 variable.py(2150):         for d in v.dims:
3.50 variable.py(427):         return self._dims
3.50 variable.py(2151):             if d in all_level_names:
3.50 variable.py(2150):         for d in v.dims:
3.50 variable.py(2149):     for k, v in variables.items():
3.50 variable.py(427):         return self._dims
3.50 variable.py(427):         return self._dims
3.50 variable.py(427):         return self._dims
3.50 variable.py(427):         return self._dims
3.50 variable.py(276):         return self._data.shape
3.50 variable.py(427):         return self._dims
3.50 variable.py(276):         return self._data.shape
3.50 indexing.py(1267):         return (len(self.array),)
3.50 variable.py(427):         return self._dims
3.50 variable.py(276):         return self._data.shape
3.50 variable.py(427):         return self._dims
3.50 variable.py(800):         if data is None:
3.50 variable.py(801):             data = self._data
3.50 variable.py(803):             if isinstance(data, indexing.MemoryCachedArray):
3.50 variable.py(807):             if deep:
3.50 variable.py(808):                 if isinstance(data, dask_array_type):
3.50 variable.py(810):                 elif not isinstance(data, PandasIndexAdapter):
3.50 variable.py(812):                     data = np.array(data)
3.50 variable.py(822):         return type(self)(self.dims, data, self._attrs, self._encoding,
3.50 variable.py(427):         return self._dims
3.50 variable.py(823):                           fastpath=True)
3.50 variable.py(822):         return type(self)(self.dims, data, self._attrs, self._encoding,
3.50 variable.py(261):         self._data = as_compatible_data(data, fastpath=fastpath)
3.50 variable.py(150):     if fastpath and getattr(data, 'ndim', 0) > 0:
3.50 variable.py(152):         return _maybe_wrap_data(data)
3.50 variable.py(127):     if isinstance(data, pd.Index):
3.50 variable.py(129):     return data
3.50 variable.py(262):         self._dims = self._parse_dimensions(dims)
3.50 variable.py(434):         if isinstance(dims, str):
3.50 variable.py(436):         dims = tuple(dims)
3.50 variable.py(437):         if len(dims) != self.ndim:
3.50 variable.py(276):         return self._data.shape
3.50 variable.py(441):         return dims
3.50 variable.py(263):         self._attrs = None
3.50 variable.py(264):         self._encoding = None
3.50 variable.py(265):         if attrs is not None:
3.50 variable.py(267):         if encoding is not None:
3.50 variable.py(1944):         if data is None:
3.50 variable.py(1945):             data = self._data.copy(deep=deep)
3.50 indexing.py(1328):         array = self.array.copy(deep=True) if deep else self.array
3.51 indexing.py(1329):         return PandasIndexAdapter(array, self._dtype)
3.51 indexing.py(1235):         self.array = utils.safe_cast_to_index(array)
3.51 indexing.py(1236):         if dtype is None:
3.51 indexing.py(1247):             dtype = np.dtype(dtype)
3.51 indexing.py(1248):         self._dtype = dtype
3.51 variable.py(1951):         return type(self)(self.dims, data, self._attrs,
3.51 variable.py(427):         return self._dims
3.51 variable.py(1952):                           self._encoding, fastpath=True)
3.51 variable.py(1951):         return type(self)(self.dims, data, self._attrs,
3.51 variable.py(1846):         super().__init__(dims, data, attrs, encoding, fastpath)
3.51 variable.py(261):         self._data = as_compatible_data(data, fastpath=fastpath)
3.51 variable.py(150):     if fastpath and getattr(data, 'ndim', 0) > 0:
3.51 indexing.py(1267):         return (len(self.array),)
3.51 variable.py(152):         return _maybe_wrap_data(data)
3.51 variable.py(127):     if isinstance(data, pd.Index):
3.51 variable.py(129):     return data
3.51 variable.py(262):         self._dims = self._parse_dimensions(dims)
3.51 variable.py(434):         if isinstance(dims, str):
3.51 variable.py(436):         dims = tuple(dims)
3.51 variable.py(437):         if len(dims) != self.ndim:
3.51 variable.py(276):         return self._data.shape
3.51 indexing.py(1267):         return (len(self.array),)
3.51 variable.py(441):         return dims
3.51 variable.py(263):         self._attrs = None
3.51 variable.py(264):         self._encoding = None
3.51 variable.py(265):         if attrs is not None:
3.51 variable.py(267):         if encoding is not None:
3.51 variable.py(1847):         if self.ndim != 1:
3.51 variable.py(276):         return self._data.shape
3.51 indexing.py(1267):         return (len(self.array),)
3.51 variable.py(1852):         if not isinstance(self._data, PandasIndexAdapter):
3.51 variable.py(800):         if data is None:
3.51 variable.py(801):             data = self._data
3.51 variable.py(803):             if isinstance(data, indexing.MemoryCachedArray):
3.51 variable.py(807):             if deep:
3.51 variable.py(808):                 if isinstance(data, dask_array_type):
3.51 variable.py(810):                 elif not isinstance(data, PandasIndexAdapter):
3.51 variable.py(812):                     data = np.array(data)
3.51 variable.py(822):         return type(self)(self.dims, data, self._attrs, self._encoding,
3.51 variable.py(427):         return self._dims
3.51 variable.py(823):                           fastpath=True)
3.51 variable.py(822):         return type(self)(self.dims, data, self._attrs, self._encoding,
3.51 variable.py(261):         self._data = as_compatible_data(data, fastpath=fastpath)
3.51 variable.py(150):     if fastpath and getattr(data, 'ndim', 0) > 0:
3.51 variable.py(152):         return _maybe_wrap_data(data)
3.51 variable.py(127):     if isinstance(data, pd.Index):
3.51 variable.py(129):     return data
3.51 variable.py(262):         self._dims = self._parse_dimensions(dims)
3.51 variable.py(434):         if isinstance(dims, str):
3.51 variable.py(436):         dims = tuple(dims)
3.51 variable.py(437):         if len(dims) != self.ndim:
3.51 variable.py(276):         return self._data.shape
3.51 variable.py(441):         return dims
3.51 variable.py(263):         self._attrs = None
3.51 variable.py(264):         self._encoding = None
3.51 variable.py(265):         if attrs is not None:
3.51 variable.py(267):         if encoding is not None:
3.51 variable.py(427):         return self._dims
3.51 variable.py(427):         return self._dims
3.51 variable.py(427):         return self._dims
3.51 variable.py(272):         return self._data.dtype
3.51 indexing.py(1252):         return self._dtype
3.51 variable.py(427):         return self._dims
3.51 variable.py(427):         return self._dims
3.51 variable.py(427):         return self._dims
3.51 variable.py(272):         return self._data.dtype
3.51 variable.py(427):         return self._dims
3.51 variable.py(427):         return self._dims
3.51 variable.py(427):         return self._dims
3.51 variable.py(272):         return self._data.dtype
=========================== short test summary info ============================
PASSED xarray/tests/test_coverup_pydata__xarray-3095.py::test_deep_copy_unicode_index_cast_to_object
======================== 1 passed, 27 warnings in 4.37s ========================
+ cat coverage.cover
{"/testbed/xarray/core/indexing.py": {"1": 1, "2": 1, "3": 1, "4": 1, "5": 1, "6": 1, "8": 1, "9": 1, "11": 1, "12": 1, "13": 1, "14": 1, "17": 1, "47": 1, "51": 1, "72": 1, "90": 1, "96": 1, "107": 1, "112": 1, "122": 2, "123": 1, "196": 1, "229": 1, "261": 1, "283": 1, "298": 2, "322": 1, "326": 1, "333": 2, "359": 2, "395": 2, "434": 2, "438": 2, "445": 2, "466": 2, "542": 2, "592": 1, "600": 2, "624": 2, "646": 1, "663": 1, "699": 1, "723": 1, "750": 2, "761": 1, "792": 1, "800": 1, "816": 1, "887": 1, "1004": 1, "1025": 1, "1036": 1, "1040": 1, "1049": 1, "1095": 1, "1121": 1, "1144": 2, "1192": 2, "1230": 2, "25": 0, "27": 0, "28": 0, "31": 0, "32": 0, "33": 0, "34": 0, "35": 0, "36": 0, "38": 0, "40": 0, "41": 0, "42": 0, "43": 0, "44": 0, "48": 0, "52": 0, "53": 0, "55": 0, "56": 0, "58": 0, "59": 0, "60": 0, "61": 0, "62": 0, "64": 0, "67": 0, "69": 0, "79": 0, "80": 0, "87": 0, "82": 0, "83": 0, "84": 0, "85": 0, "91": 0, "92": 0, "93": 0, "99": 0, "100": 0, "101": 0, "102": 0, "103": 0, "104": 0, "108": 0, "109": 0, "114": 0, "116": 0, "117": 0, "118": 0, "119": 0, "129": 0, "131": 0, "132": 0, "133": 0, "134": 0, "136": 0, "137": 0, "138": 0, "139": 0, "142": 0, "144": 0, "193": 0, "146": 0, "147": 0, "148": 0, "149": 0, "151": 0, "152": 0, "154": 0, "156": 0, "157": 0, "158": 0, "159": 0, "160": 0, "163": 0, "164": 0, "166": 0, "167": 0, "168": 0, "169": 0, "170": 0, "172": 0, "173": 0, "176": 0, "177": 0, "178": 0, "179": 0, "180": 0, "182": 0, "183": 0, "184": 0, "186": 0, "187": 0, "188": 0, "189": 0, "190": 0, "191": 0, "192": 0, "204": 0, "206": 0, "207": 0, "208": 0, "210": 0, "211": 0, "212": 0, "213": 0, "214": 0, "216": 0, "218": 0, "220": 0, "221": 0, "222": 0, "223": 0, "224": 0, "226": 0, "205": 0, "234": 0, "235": 0, "237": 0, "238": 0, "240": 0, "241": 0, "242": 0, "243": 0, "244": 0, "246": 0, "247": 0, "250": 0, "252": 0, "253": 0, "254": 0, "255": 0, "256": 0, "258": 0, "266": 0, "271": 0, "272": 0, "273": 0, "274": 0, "275": 0, "276": 0, "278": 0, "279": 0, "280": 0, "284": 0, "285": 0, "287": 0, "288": 0, "289": 0, "290": 0, "295": 0, "292": 0, "294": 0, "309": 1, "314": 1, "315": 1, "318": 1, "310": 0, "311": 0, "312": 0, "316": 0, "319": 0, "323": 0, "327": 0, "328": 0, "329": 0, "330": 0, "341": 1, "342": 0, "343": 0, "345": 0, "346": 0, "347": 0, "348": 0, "349": 0, "350": 0, "352": 0, "353": 0, "354": 0, "356": 0, "368": 1, "369": 0, "370": 0, "372": 0, "373": 0, "374": 0, "375": 0, "376": 0, "377": 0, "378": 0, "379": 0, "380": 0, "381": 0, "382": 0, "383": 0, "385": 0, "386": 0, "388": 0, "389": 0, "390": 0, "392": 0, "405": 1, "406": 0, "407": 0, "409": 0, "410": 0, "411": 0, "412": 0, "413": 0, "414": 0, "415": 0, "416": 0, "417": 0, "418": 0, "419": 0, "420": 0, "421": 0, "422": 0, "424": 0, "425": 0, "427": 0, "428": 0, "429": 0, "431": 0, "440": 1, "441": 0, "442": 0, "448": 1, "452": 1, "455": 1, "449": 0, "450": 0, "453": 0, "456": 0, "457": 0, "458": 0, "459": 0, "463": 0, "470": 1, "491": 1, "505": 1, "506": 1, "515": 1, "519": 1, "523": 1, "529": 1, "537": 1, "480": 0, "482": 0, "483": 0, "485": 0, "486": 0, "488": 0, "489": 0, "492": 0, "493": 0, "494": 0, "495": 0, "496": 0, "498": 0, "499": 0, "501": 0, "502": 0, "503": 0, "507": 0, "508": 0, "509": 0, "510": 0, "511": 0, "512": 0, "513": 0, "516": 0, "517": 0, "520": 0, "521": 0, "524": 0, "525": 0, "526": 0, "527": 0, "530": 0, "531": 0, "532": 0, "534": 0, "535": 0, "538": 0, "539": 0, "546": 1, "560": 1, "561": 1, "564": 1, "567": 1, "570": 1, "577": 1, "582": 1, "587": 1, "554": 0, "555": 0, "557": 0, "558": 0, "562": 0, "565": 0, "568": 0, "572": 0, "573": 0, "574": 0, "575": 0, "578": 0, "579": 0, "580": 0, "583": 0, "584": 0, "588": 0, "589": 0, "594": 0, "595": 0, "597": 0, "601": 1, "605": 1, "610": 1, "613": 1, "616": 1, "619": 1, "602": 0, "603": 0, "606": 0, "607": 0, "608": 0, "611": 0, "614": 0, "617": 0, "620": 0, "621": 0, "625": 1, "628": 1, "632": 1, "636": 1, "639": 1, "642": 1, "626": 0, "629": 0, "630": 0, "633": 0, "634": 0, "637": 0, "640": 0, "643": 0, "652": 0, "653": 0, "654": 0, "655": 0, "656": 0, "657": 0, "658": 0, "659": 0, "660": 0, "680": 0, "682": 0, "683": 0, "684": 0, "685": 0, "686": 0, "687": 0, "689": 0, "690": 0, "691": 0, "692": 0, "693": 0, "694": 0, "695": 0, "696": 0, "714": 0, "718": 0, "720": 0, "735": 0, "736": 0, "737": 0, "738": 0, "740": 0, "741": 0, "742": 0, "744": 0, "746": 0, "747": 0, "752": 1, "754": 1, "756": 1, "758": 1, "784": 0, "785": 0, "786": 0, "788": 0, "789": 0, "793": 0, "794": 0, "795": 0, "796": 0, "797": 0, "804": 0, "805": 0, "807": 0, "811": 0, "812": 0, "813": 0, "847": 0, "849": 0, "850": 0, "852": 0, "853": 0, "855": 0, "856": 0, "858": 0, "859": 0, "863": 0, "864": 0, "865": 0, "869": 0, "870": 0, "871": 0, "873": 0, "874": 0, "876": 0, "877": 0, "881": 0, "882": 0, "883": 0, "884": 0, "918": 0, "919": 0, "920": 0, "922": 0, "923": 0, "925": 0, "926": 0, "927": 0, "928": 0, "929": 0, "930": 0, "932": 0, "933": 0, "935": 0, "938": 0, "939": 0, "940": 0, "942": 0, "943": 0, "946": 0, "947": 0, "948": 0, "950": 0, "951": 0, "952": 0, "953": 0, "954": 0, "956": 0, "957": 0, "958": 0, "960": 0, "961": 0, "963": 0, "964": 0, "965": 0, "967": 0, "968": 0, "969": 0, "970": 0, "971": 0, "972": 0, "973": 0, "974": 0, "977": 0, "978": 0, "979": 0, "981": 0, "982": 0, "985": 0, "987": 0, "988": 0, "991": 0, "992": 0, "993": 0, "994": 0, "996": 0, "997": 0, "998": 0, "1000": 0, "1001": 0, "1006": 0, "1007": 0, "1008": 0, "1010": 0, "1011": 0, "1012": 0, "1013": 0, "1014": 0, "1015": 0, "1016": 0, "1018": 0, "1019": 0, "1020": 0, "1021": 0, "1022": 0, "1027": 0, "1028": 0, "1029": 0, "1030": 0, "1031": 0, "1032": 0, "1033": 0, "1037": 0, "1041": 0, "1042": 0, "1043": 0, "1045": 0, "1046": 0, "1044": 0, "1070": 0, "1071": 0, "1072": 0, "1073": 0, "1092": 0, "1075": 0, "1076": 0, "1077": 0, "1078": 0, "1079": 0, "1081": 0, "1082": 0, "1083": 0, "1084": 0, "1086": 0, "1087": 0, "1090": 0, "1080": 0, "1109": 0, "1110": 0, "1111": 0, "1113": 0, "1114": 0, "1115": 0, "1116": 0, "1117": 0, "1118": 0, "1138": 0, "1140": 0, "1141": 0, "1139": 0, "1147": 1, "1154": 1, "1172": 1, "1175": 1, "1179": 1, "1149": 0, "1150": 0, "1151": 0, "1152": 0, "1155": 0, "1156": 0, "1157": 0, "1170": 0, "1158": 0, "1159": 0, "1160": 0, "1161": 0, "1162": 0, "1166": 0, "1168": 0, "1173": 0, "1176": 0, "1177": 0, "1180": 0, "1181": 0, "1182": 0, "1183": 0, "1185": 0, "1186": 0, "1189": 0, "1195": 1, "1201": 1, "1219": 1, "1226": 1, "1199": 0, "1202": 0, "1203": 0, "1204": 0, "1205": 0, "1207": 0, "1208": 0, "1209": 0, "1210": 0, "1211": 0, "1214": 0, "1215": 0, "1216": 0, "1217": 0, "1220": 0, "1227": 0, "1234": 1, "1250": 1, "1251": 1, "1254": 1, "1264": 1, "1265": 1, "1269": 2, "1271": 2, "1272": 2, "1273": 1, "1274": 1, "1275": 1, "1313": 1, "1316": 1, "1320": 1, "1235": 3, "1236": 3, "1237": 2, "1238": 0, "1239": 2, "1241": 0, "1242": 2, "1243": 0, "1245": 2, "1247": 1, "1248": 3, "1252": 1, "1255": 0, "1256": 0, "1257": 0, "1258": 0, "1259": 0, "1261": 0, "1262": 0, "1267": 6, "1277": 0, "1278": 0, "1281": 0, "1283": 0, "1284": 0, "1286": 0, "1288": 0, "1289": 0, "1311": 0, "1292": 0, "1297": 0, "1298": 0, "1299": 0, "1300": 0, "1303": 0, "1304": 0, "1305": 0, "1309": 0, "1314": 0, "1317": 0, "1318": 0, "1328": 1, "1329": 1}}
{"/testbed/xarray/core/variable.py": {"1": 1, "2": 1, "3": 1, "4": 1, "5": 1, "6": 1, "8": 1, "9": 1, "11": 1, "13": 1, "15": 1, "18": 1, "19": 1, "20": 1, "24": 1, "25": 1, "26": 0, "27": 0, "31": 2, "30": 2, "33": 1, "36": 2, "43": 1, "119": 1, "132": 1, "139": 1, "196": 1, "219": 3, "220": 1, "1831": 1, "1834": 2, "2020": 1, "2023": 1, "2041": 1, "2052": 1, "2068": 1, "2084": 1, "2121": 1, "70": 5, "73": 5, "75": 0, "77": 5, "78": 0, "79": 5, "80": 3, "81": 3, "82": 0, "84": 0, "86": 0, "87": 2, "88": 0, "89": 2, "90": 0, "91": 2, "92": 0, "93": 0, "94": 2, "95": 2, "96": 2, "97": 0, "98": 0, "100": 0, "101": 2, "103": 0, "104": 0, "106": 5, "108": 2, "109": 0, "110": 0, "113": 0, "114": 2, "116": 5, "127": 12, "128": 0, "129": 12, "136": 0, "150": 12, "152": 7, "154": 5, "155": 0, "157": 5, "158": 0, "160": 5, "161": 0, "163": 5, "165": 0, "167": 5, "168": 0, "171": 5, "173": 5, "174": 0, "175": 0, "176": 0, "177": 0, "178": 0, "180": 0, "183": 5, "185": 5, "186": 5, "187": 0, "193": 5, "188": 5, "189": 0, "190": 5, "191": 0, "210": 0, "211": 0, "212": 0, "213": 0, "216": 0, "214": 0, "215": 0, "241": 1, "270": 1, "271": 1, "274": 1, "275": 1, "278": 1, "279": 1, "282": 1, "283": 1, "289": 1, "290": 1, "296": 1, "297": 1, "304": 1, "327": 1, "348": 1, "354": 1, "357": 1, "360": 1, "361": 1, "364": 1, "365": 1, "368": 1, "373": 1, "378": 1, "379": 1, "386": 1, "387": 1, "391": 1, "392": 1, "395": 1, "400": 1, "402": 1, "407": 1, "409": 1, "413": 1, "423": 1, "424": 1, "429": 1, "430": 1, "433": 1, "443": 1, "449": 1, "507": 1, "512": 1, "540": 1, "557": 1, "565": 1, "619": 1, "639": 1, "645": 1, "684": 1, "716": 1, "717": 1, "724": 1, "725": 1, "728": 1, "729": 1, "736": 1, "737": 1, "743": 1, "825": 1, "828": 1, "835": 1, "837": 1, "838": 1, "844": 1, "846": 1, "911": 1, "940": 1, "963": 1, "1007": 1, "1034": 1, "1087": 1, "1109": 1, "1135": 1, "1169": 1, "1170": 1, "1173": 1, "1180": 1, "1228": 1, "1251": 1, "1283": 1, "1309": 1, "1341": 1, "1344": 1, "1347": 2, "1348": 1, "1418": 1, "1419": 2, "1420": 1, "1485": 1, "1503": 1, "1516": 1, "1525": 1, "1535": 1, "1600": 1, "1641": 2, "1642": 1, "1692": 1, "1708": 1, "1771": 1, "1772": 1, "1775": 1, "1776": 1, "1779": 1, "1782": 1, "1783": 1, "1790": 1, "1791": 1, "1807": 1, "1808": 1, "1822": 1, "261": 10, "262": 10, "263": 10, "264": 10, "265": 10, "266": 0, "267": 10, "268": 0, "272": 3, "276": 20, "280": 0, "284": 0, "285": 0, "286": 0, "287": 0, "291": 0, "292": 0, "294": 0, "298": 0, "299": 0, "300": 0, "301": 0, "302": 0, "321": 0, "322": 0, "325": 0, "323": 0, "324": 0, "345": 0, "346": 0, "349": 0, "350": 0, "352": 0, "355": 0, "358": 0, "362": 0, "366": 0, "369": 0, "370": 0, "371": 0, "374": 0, "375": 0, "376": 0, "380": 0, "381": 0, "382": 0, "383": 0, "384": 0, "389": 0, "393": 0, "397": 0, "398": 0, "404": 4, "405": 2, "411": 0, "415": 0, "416": 0, "417": 0, "418": 0, "421": 0, "420": 0, "427": 36, "431": 0, "434": 10, "435": 5, "436": 10, "437": 10, "438": 0, "440": 0, "441": 10, "444": 0, "445": 0, "447": 0, "470": 0, "472": 0, "474": 0, "476": 0, "478": 0, "480": 0, "482": 0, "483": 0, "485": 0, "489": 0, "490": 0, "494": 0, "495": 0, "496": 0, "497": 0, "498": 0, "499": 0, "500": 0, "501": 0, "502": 0, "503": 0, "505": 0, "475": 0, "479": 0, "508": 0, "510": 0, "509": 0, "514": 0, "515": 0, "516": 0, "518": 0, "519": 0, "520": 0, "521": 0, "522": 0, "523": 0, "524": 0, "525": 0, "526": 0, "527": 0, "528": 0, "529": 0, "530": 0, "531": 0, "532": 0, "533": 0, "534": 0, "535": 0, "538": 0, "541": 0, "542": 0, "545": 0, "546": 0, "547": 0, "548": 0, "549": 0, "550": 0, "551": 0, "552": 0, "553": 0, "555": 0, "543": 0, "561": 0, "562": 0, "563": 0, "566": 0, "567": 0, "568": 0, "569": 0, "570": 0, "572": 0, "573": 0, "574": 0, "575": 0, "577": 0, "578": 0, "580": 0, "581": 0, "582": 0, "584": 0, "585": 0, "586": 0, "587": 0, "593": 0, "594": 0, "596": 0, "598": 0, "599": 0, "600": 0, "601": 0, "603": 0, "604": 0, "605": 0, "606": 0, "607": 0, "608": 0, "609": 0, "611": 0, "612": 0, "615": 0, "617": 0, "613": 0, "632": 0, "633": 0, "634": 0, "635": 0, "636": 0, "637": 0, "642": 0, "643": 0, "655": 0, "656": 0, "658": 0, "660": 0, "661": 0, "665": 0, "667": 0, "669": 0, "670": 0, "671": 0, "672": 0, "676": 0, "677": 0, "679": 0, "680": 0, "681": 0, "682": 0, "690": 0, "692": 0, "693": 0, "694": 0, "695": 0, "696": 0, "698": 0, "699": 0, "700": 0, "702": 0, "704": 0, "706": 0, "707": 0, "708": 0, "709": 0, "710": 0, "711": 0, "713": 0, "714": 0, "720": 0, "721": 0, "722": 0, "726": 0, "732": 0, "733": 0, "734": 0, "738": 0, "739": 0, "740": 0, "741": 0, "800": 2, "801": 2, "803": 2, "805": 0, "807": 2, "808": 2, "809": 0, "810": 2, "812": 2, "814": 0, "815": 0, "816": 0, "817": 0, "822": 4, "823": 2, "826": 0, "831": 0, "842": 0, "873": 0, "874": 0, "876": 0, "877": 0, "878": 0, "880": 0, "881": 0, "883": 0, "884": 0, "885": 0, "887": 0, "888": 0, "889": 0, "894": 0, "895": 0, "900": 0, "901": 0, "903": 0, "905": 0, "906": 0, "908": 0, "909": 0, "928": 0, "930": 0, "931": 0, "932": 0, "934": 0, "935": 0, "936": 0, "937": 0, "938": 0, "960": 0, "961": 0, "964": 0, "966": 0, "967": 0, "968": 0, "969": 0, "971": 0, "973": 0, "975": 0, "976": 0, "978": 0, "980": 0, "981": 0, "983": 0, "984": 0, "985": 0, "986": 0, "988": 0, "990": 0, "992": 0, "993": 0, "995": 0, "997": 0, "999": 0, "1003": 0, "1005": 0, "1028": 0, "1029": 0, "1030": 0, "1031": 0, "1032": 0, "1046": 0, "1047": 0, "1049": 0, "1050": 0, "1052": 0, "1054": 0, "1055": 0, "1059": 0, "1060": 0, "1061": 0, "1062": 0, "1063": 0, "1064": 0, "1065": 0, "1066": 0, "1067": 0, "1068": 0, "1070": 0, "1071": 0, "1072": 0, "1073": 0, "1074": 0, "1075": 0, "1076": 0, "1077": 0, "1078": 0, "1079": 0, "1081": 0, "1082": 0, "1083": 0, "1084": 0, "1085": 0, "1088": 0, "1090": 0, "1091": 0, "1092": 0, "1094": 0, "1096": 0, "1097": 0, "1099": 0, "1101": 0, "1105": 0, "1107": 0, "1128": 0, "1130": 0, "1131": 0, "1132": 0, "1133": 0, "1159": 0, "1160": 0, "1161": 0, "1162": 0, "1163": 0, "1165": 0, "1166": 0, "1167": 0, "1171": 0, "1174": 0, "1175": 0, "1176": 0, "1177": 0, "1178": 0, "1197": 0, "1198": 0, "1200": 0, "1201": 0, "1203": 0, "1204": 0, "1205": 0, "1206": 0, "1208": 0, "1209": 0, "1210": 0, "1212": 0, "1215": 0, "1216": 0, "1217": 0, "1218": 0, "1219": 0, "1221": 0, "1222": 0, "1224": 0, "1225": 0, "1226": 0, "1229": 0, "1230": 0, "1232": 0, "1233": 0, "1236": 0, "1238": 0, "1240": 0, "1241": 0, "1242": 0, "1244": 0, "1245": 0, "1246": 0, "1248": 0, "1249": 0, "1276": 0, "1277": 0, "1278": 0, "1279": 0, "1280": 0, "1281": 0, "1284": 0, "1285": 0, "1287": 0, "1288": 0, "1290": 0, "1291": 0, "1294": 0, "1295": 0, "1298": 0, "1299": 0, "1300": 0, "1302": 0, "1303": 0, "1304": 0, "1306": 0, "1307": 0, "1334": 0, "1335": 0, "1336": 0, "1337": 0, "1338": 0, "1339": 0, "1342": 0, "1345": 0, "1380": 0, "1381": 0, "1382": 0, "1383": 0, "1385": 0, "1386": 0, "1387": 0, "1388": 0, "1389": 0, "1391": 0, "1393": 0, "1394": 0, "1396": 0, "1397": 0, "1398": 0, "1400": 0, "1401": 0, "1402": 0, "1404": 0, "1406": 0, "1407": 0, "1409": 0, "1412": 0, "1413": 0, "1414": 0, "1416": 0, "1410": 0, "1450": 0, "1451": 0, "1455": 0, "1456": 0, "1458": 0, "1460": 0, "1461": 0, "1462": 0, "1463": 0, "1464": 0, "1467": 0, "1468": 0, "1469": 0, "1471": 0, "1472": 0, "1473": 0, "1475": 0, "1476": 0, "1477": 0, "1478": 0, "1479": 0, "1480": 0, "1481": 0, "1483": 0, "1495": 0, "1496": 0, "1497": 0, "1498": 0, "1499": 0, "1500": 0, "1501": 0, "1510": 0, "1511": 0, "1512": 0, "1513": 0, "1514": 0, "1519": 0, "1520": 0, "1521": 0, "1522": 0, "1523": 0, "1532": 0, "1533": 0, "1573": 0, "1574": 0, "1578": 0, "1580": 0, "1581": 0, "1582": 0, "1583": 0, "1584": 0, "1586": 0, "1587": 0, "1589": 0, "1590": 0, "1593": 0, "1594": 0, "1596": 0, "1597": 0, "1598": 0, "1626": 0, "1628": 0, "1629": 0, "1633": 0, "1634": 0, "1635": 0, "1636": 0, "1637": 0, "1638": 0, "1639": 0, "1680": 0, "1681": 0, "1682": 0, "1684": 0, "1685": 0, "1687": 0, "1688": 0, "1689": 0, "1690": 0, "1696": 0, "1697": 0, "1698": 0, "1700": 0, "1701": 0, "1702": 0, "1703": 0, "1704": 0, "1705": 0, "1706": 0, "1712": 0, "1713": 0, "1715": 0, "1716": 0, "1719": 0, "1720": 0, "1722": 0, "1723": 0, "1724": 0, "1726": 0, "1727": 0, "1729": 0, "1730": 0, "1731": 0, "1732": 0, "1733": 0, "1734": 0, "1735": 0, "1736": 0, "1737": 0, "1738": 0, "1740": 0, "1741": 0, "1742": 0, "1743": 0, "1744": 0, "1745": 0, "1746": 0, "1747": 0, "1749": 0, "1750": 0, "1752": 0, "1753": 0, "1754": 0, "1756": 0, "1757": 0, "1758": 0, "1759": 0, "1760": 0, "1761": 0, "1762": 0, "1763": 0, "1764": 0, "1765": 0, "1767": 0, "1769": 0, "1773": 0, "1777": 0, "1780": 0, "1784": 12, "1785": 12, "1788": 12, "1786": 0, "1787": 0, "1792": 26, "1793": 26, "1805": 26, "1794": 0, "1795": 0, "1796": 0, "1797": 0, "1798": 0, "1799": 0, "1801": 0, "1800": 0, "1802": 0, "1803": 0, "1804": 0, "1809": 10, "1810": 10, "1820": 10, "1811": 0, "1812": 0, "1813": 0, "1814": 0, "1815": 0, "1817": 0, "1818": 0, "1819": 0, "1826": 0, "1827": 0, "1828": 0, "1845": 1, "1855": 1, "1860": 1, "1861": 1, "1866": 1, "1870": 1, "1878": 1, "1881": 1, "1882": 2, "1883": 1, "1920": 1, "1954": 1, "1967": 1, "1970": 1, "1974": 1, "1976": 1, "1992": 1, "1993": 1, "2003": 1, "2010": 1, "2011": 1, "2014": 1, "2015": 1, "1846": 3, "1847": 3, "1848": 0, "1849": 0, "1852": 3, "1853": 2, "1857": 0, "1862": 0, "1863": 0, "1864": 0, "1868": 0, "1871": 0, "1873": 0, "1875": 0, "1876": 0, "1879": 0, "1889": 0, "1890": 0, "1892": 0, "1893": 0, "1895": 0, "1896": 0, "1899": 0, "1901": 0, "1902": 0, "1904": 0, "1906": 0, "1907": 0, "1908": 0, "1909": 0, "1911": 0, "1912": 0, "1913": 0, "1914": 0, "1915": 0, "1916": 0, "1918": 0, "1944": 1, "1945": 1, "1947": 0, "1948": 0, "1949": 0, "1950": 0, "1951": 2, "1952": 1, "1956": 0, "1957": 0, "1960": 0, "1961": 0, "1962": 0, "1963": 0, "1964": 0, "1965": 0, "1968": 0, "1972": 1, "1980": 2, "1981": 2, "1982": 2, "1985": 0, "1986": 0, "1987": 0, "1990": 2, "1989": 2, "1997": 1, "1998": 1, "1999": 0, "2001": 1, "2005": 0, "2006": 0, "2007": 0, "2008": 0, "2012": 2, "2016": 0, "2025": 0, "2026": 0, "2027": 0, "2028": 0, "2029": 0, "2030": 0, "2031": 0, "2032": 0, "2033": 0, "2034": 0, "2035": 0, "2037": 0, "2038": 0, "2047": 0, "2048": 0, "2049": 0, "2062": 0, "2063": 0, "2064": 0, "2065": 0, "2069": 0, "2070": 0, "2072": 0, "2073": 0, "2074": 0, "2075": 0, "2078": 0, "2079": 0, "2080": 0, "2081": 0, "2114": 0, "2115": 0, "2116": 0, "2118": 0, "2128": 1, "2129": 1, "2130": 4, "2131": 3, "2132": 1, "2133": 1, "2134": 0, "2135": 0, "2136": 1, "2137": 0, "2139": 1, "2140": 0, "2141": 0, "2143": 2, "2144": 1, "2145": 0, "2146": 0, "2147": 0, "2149": 4, "2150": 6, "2151": 3, "2152": 0, "2153": 0}}
+ git checkout 1757dffac2fa493d7b9a074b84cf8c830a706688
Note: switching to '1757dffac2fa493d7b9a074b84cf8c830a706688'.

You are in 'detached HEAD' state. You can look around, make experimental
changes and commit them, and you can discard any commits you make in this
state without impacting any branches by switching back to a branch.

If you want to create a new branch to retain commits you create, you may
do so (now or later) by using -c with the switch command. Example:

  git switch -c <new-branch-name>

Or undo this operation with:

  git switch -

Turn off this advice by setting config variable advice.detachedHead to false

HEAD is now at 1757dffa More annotations in Dataset (#3112)
M	xarray/core/indexing.py
M	xarray/core/variable.py
+ git apply /root/pre_state.patch
error: unrecognized input
