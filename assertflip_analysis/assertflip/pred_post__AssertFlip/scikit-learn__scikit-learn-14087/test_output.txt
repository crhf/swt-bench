+ source /opt/miniconda3/bin/activate
++ _CONDA_ROOT=/opt/miniconda3
++ . /opt/miniconda3/etc/profile.d/conda.sh
+++ export CONDA_EXE=/opt/miniconda3/bin/conda
+++ CONDA_EXE=/opt/miniconda3/bin/conda
+++ export _CE_M=
+++ _CE_M=
+++ export _CE_CONDA=
+++ _CE_CONDA=
+++ export CONDA_PYTHON_EXE=/opt/miniconda3/bin/python
+++ CONDA_PYTHON_EXE=/opt/miniconda3/bin/python
+++ '[' -z '' ']'
+++ export CONDA_SHLVL=0
+++ CONDA_SHLVL=0
+++ '[' -n '' ']'
+++++ dirname /opt/miniconda3/bin/conda
++++ dirname /opt/miniconda3/bin
+++ PATH=/opt/miniconda3/condabin:/opt/miniconda3/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin
+++ export PATH
+++ '[' -z '' ']'
+++ PS1=
++ conda activate
++ local cmd=activate
++ case "$cmd" in
++ __conda_activate activate
++ '[' -n '' ']'
++ local ask_conda
+++ PS1=
+++ __conda_exe shell.posix activate
+++ /opt/miniconda3/bin/conda shell.posix activate
++ ask_conda='PS1='\''(base) '\''
export PATH='\''/opt/miniconda3/bin:/opt/miniconda3/condabin:/opt/miniconda3/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin'\''
export CONDA_PREFIX='\''/opt/miniconda3'\''
export CONDA_SHLVL='\''1'\''
export CONDA_DEFAULT_ENV='\''base'\''
export CONDA_PROMPT_MODIFIER='\''(base) '\''
export CONDA_EXE='\''/opt/miniconda3/bin/conda'\''
export _CE_M='\'''\''
export _CE_CONDA='\'''\''
export CONDA_PYTHON_EXE='\''/opt/miniconda3/bin/python'\'''
++ eval 'PS1='\''(base) '\''
export PATH='\''/opt/miniconda3/bin:/opt/miniconda3/condabin:/opt/miniconda3/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin'\''
export CONDA_PREFIX='\''/opt/miniconda3'\''
export CONDA_SHLVL='\''1'\''
export CONDA_DEFAULT_ENV='\''base'\''
export CONDA_PROMPT_MODIFIER='\''(base) '\''
export CONDA_EXE='\''/opt/miniconda3/bin/conda'\''
export _CE_M='\'''\''
export _CE_CONDA='\'''\''
export CONDA_PYTHON_EXE='\''/opt/miniconda3/bin/python'\'''
+++ PS1='(base) '
+++ export PATH=/opt/miniconda3/bin:/opt/miniconda3/condabin:/opt/miniconda3/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin
+++ PATH=/opt/miniconda3/bin:/opt/miniconda3/condabin:/opt/miniconda3/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin
+++ export CONDA_PREFIX=/opt/miniconda3
+++ CONDA_PREFIX=/opt/miniconda3
+++ export CONDA_SHLVL=1
+++ CONDA_SHLVL=1
+++ export CONDA_DEFAULT_ENV=base
+++ CONDA_DEFAULT_ENV=base
+++ export 'CONDA_PROMPT_MODIFIER=(base) '
+++ CONDA_PROMPT_MODIFIER='(base) '
+++ export CONDA_EXE=/opt/miniconda3/bin/conda
+++ CONDA_EXE=/opt/miniconda3/bin/conda
+++ export _CE_M=
+++ _CE_M=
+++ export _CE_CONDA=
+++ _CE_CONDA=
+++ export CONDA_PYTHON_EXE=/opt/miniconda3/bin/python
+++ CONDA_PYTHON_EXE=/opt/miniconda3/bin/python
++ __conda_hashr
++ '[' -n '' ']'
++ '[' -n '' ']'
++ hash -r
+ conda activate testbed
+ local cmd=activate
+ case "$cmd" in
+ __conda_activate activate testbed
+ '[' -n '' ']'
+ local ask_conda
++ PS1='(base) '
++ __conda_exe shell.posix activate testbed
++ /opt/miniconda3/bin/conda shell.posix activate testbed
+ ask_conda='PS1='\''(testbed) '\''
export PATH='\''/opt/miniconda3/envs/testbed/bin:/opt/miniconda3/condabin:/opt/miniconda3/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin'\''
export CONDA_PREFIX='\''/opt/miniconda3/envs/testbed'\''
export CONDA_SHLVL='\''2'\''
export CONDA_DEFAULT_ENV='\''testbed'\''
export CONDA_PROMPT_MODIFIER='\''(testbed) '\''
export CONDA_PREFIX_1='\''/opt/miniconda3'\''
export CONDA_EXE='\''/opt/miniconda3/bin/conda'\''
export _CE_M='\'''\''
export _CE_CONDA='\'''\''
export CONDA_PYTHON_EXE='\''/opt/miniconda3/bin/python'\'''
+ eval 'PS1='\''(testbed) '\''
export PATH='\''/opt/miniconda3/envs/testbed/bin:/opt/miniconda3/condabin:/opt/miniconda3/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin'\''
export CONDA_PREFIX='\''/opt/miniconda3/envs/testbed'\''
export CONDA_SHLVL='\''2'\''
export CONDA_DEFAULT_ENV='\''testbed'\''
export CONDA_PROMPT_MODIFIER='\''(testbed) '\''
export CONDA_PREFIX_1='\''/opt/miniconda3'\''
export CONDA_EXE='\''/opt/miniconda3/bin/conda'\''
export _CE_M='\'''\''
export _CE_CONDA='\'''\''
export CONDA_PYTHON_EXE='\''/opt/miniconda3/bin/python'\'''
++ PS1='(testbed) '
++ export PATH=/opt/miniconda3/envs/testbed/bin:/opt/miniconda3/condabin:/opt/miniconda3/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin
++ PATH=/opt/miniconda3/envs/testbed/bin:/opt/miniconda3/condabin:/opt/miniconda3/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin
++ export CONDA_PREFIX=/opt/miniconda3/envs/testbed
++ CONDA_PREFIX=/opt/miniconda3/envs/testbed
++ export CONDA_SHLVL=2
++ CONDA_SHLVL=2
++ export CONDA_DEFAULT_ENV=testbed
++ CONDA_DEFAULT_ENV=testbed
++ export 'CONDA_PROMPT_MODIFIER=(testbed) '
++ CONDA_PROMPT_MODIFIER='(testbed) '
++ export CONDA_PREFIX_1=/opt/miniconda3
++ CONDA_PREFIX_1=/opt/miniconda3
++ export CONDA_EXE=/opt/miniconda3/bin/conda
++ CONDA_EXE=/opt/miniconda3/bin/conda
++ export _CE_M=
++ _CE_M=
++ export _CE_CONDA=
++ _CE_CONDA=
++ export CONDA_PYTHON_EXE=/opt/miniconda3/bin/python
++ CONDA_PYTHON_EXE=/opt/miniconda3/bin/python
+ __conda_hashr
+ '[' -n '' ']'
+ '[' -n '' ']'
+ hash -r
+ cd /testbed
+ git diff HEAD a5743ed36fbd3fbc8e351bdab16561fbfca7dfa1
+ git config --global --add safe.directory /testbed
+ cd /testbed
+ git status
On branch main
nothing to commit, working tree clean
+ git show
commit a5743ed36fbd3fbc8e351bdab16561fbfca7dfa1
Author: Nicolas Hug <contact@nicolas-hug.com>
Date:   Thu Jun 13 14:52:56 2019 -0400

    TST add test for LAD-loss and quantile loss equivalence (old GBDT code) (#14086)

diff --git a/sklearn/ensemble/tests/test_gradient_boosting_loss_functions.py b/sklearn/ensemble/tests/test_gradient_boosting_loss_functions.py
index 32c65aabfa..a82dbab4e7 100644
--- a/sklearn/ensemble/tests/test_gradient_boosting_loss_functions.py
+++ b/sklearn/ensemble/tests/test_gradient_boosting_loss_functions.py
@@ -6,6 +6,7 @@ import numpy as np
 from numpy.testing import assert_almost_equal
 from numpy.testing import assert_allclose
 from numpy.testing import assert_equal
+import pytest
 
 from sklearn.utils import check_random_state
 from sklearn.utils.stats import _weighted_percentile
@@ -273,3 +274,24 @@ def test_init_raw_predictions_values():
         for k in range(n_classes):
             p = (y == k).mean()
         assert_almost_equal(raw_predictions[:, k], np.log(p))
+
+
+@pytest.mark.parametrize('seed', range(5))
+def test_lad_equals_quantile_50(seed):
+    # Make sure quantile loss with alpha = .5 is equivalent to LAD
+    lad = LeastAbsoluteError(n_classes=1)
+    ql = QuantileLossFunction(n_classes=1, alpha=0.5)
+
+    n_samples = 50
+    rng = np.random.RandomState(seed)
+    raw_predictions = rng.normal(size=(n_samples))
+    y_true = rng.normal(size=(n_samples))
+
+    lad_loss = lad(y_true, raw_predictions)
+    ql_loss = ql(y_true, raw_predictions)
+    assert_almost_equal(lad_loss, 2 * ql_loss)
+
+    weights = np.linspace(0, 1, n_samples) ** 2
+    lad_weighted_loss = lad(y_true, raw_predictions, sample_weight=weights)
+    ql_weighted_loss = ql(y_true, raw_predictions, sample_weight=weights)
+    assert_almost_equal(lad_weighted_loss, 2 * ql_weighted_loss)
+ git diff a5743ed36fbd3fbc8e351bdab16561fbfca7dfa1
+ source /opt/miniconda3/bin/activate
++ _CONDA_ROOT=/opt/miniconda3
++ . /opt/miniconda3/etc/profile.d/conda.sh
+++ export CONDA_EXE=/opt/miniconda3/bin/conda
+++ CONDA_EXE=/opt/miniconda3/bin/conda
+++ export _CE_M=
+++ _CE_M=
+++ export _CE_CONDA=
+++ _CE_CONDA=
+++ export CONDA_PYTHON_EXE=/opt/miniconda3/bin/python
+++ CONDA_PYTHON_EXE=/opt/miniconda3/bin/python
+++ '[' -z x ']'
++ conda activate
++ local cmd=activate
++ case "$cmd" in
++ __conda_activate activate
++ '[' -n '' ']'
++ local ask_conda
+++ PS1='(testbed) '
+++ __conda_exe shell.posix activate
+++ /opt/miniconda3/bin/conda shell.posix activate
++ ask_conda='PS1='\''(base) '\''
export PATH='\''/opt/miniconda3/bin:/opt/miniconda3/condabin:/opt/miniconda3/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin'\''
export CONDA_PREFIX='\''/opt/miniconda3'\''
export CONDA_SHLVL='\''3'\''
export CONDA_DEFAULT_ENV='\''base'\''
export CONDA_PROMPT_MODIFIER='\''(base) '\''
export CONDA_PREFIX_2='\''/opt/miniconda3/envs/testbed'\''
export CONDA_EXE='\''/opt/miniconda3/bin/conda'\''
export _CE_M='\'''\''
export _CE_CONDA='\'''\''
export CONDA_PYTHON_EXE='\''/opt/miniconda3/bin/python'\'''
++ eval 'PS1='\''(base) '\''
export PATH='\''/opt/miniconda3/bin:/opt/miniconda3/condabin:/opt/miniconda3/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin'\''
export CONDA_PREFIX='\''/opt/miniconda3'\''
export CONDA_SHLVL='\''3'\''
export CONDA_DEFAULT_ENV='\''base'\''
export CONDA_PROMPT_MODIFIER='\''(base) '\''
export CONDA_PREFIX_2='\''/opt/miniconda3/envs/testbed'\''
export CONDA_EXE='\''/opt/miniconda3/bin/conda'\''
export _CE_M='\'''\''
export _CE_CONDA='\'''\''
export CONDA_PYTHON_EXE='\''/opt/miniconda3/bin/python'\'''
+++ PS1='(base) '
+++ export PATH=/opt/miniconda3/bin:/opt/miniconda3/condabin:/opt/miniconda3/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin
+++ PATH=/opt/miniconda3/bin:/opt/miniconda3/condabin:/opt/miniconda3/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin
+++ export CONDA_PREFIX=/opt/miniconda3
+++ CONDA_PREFIX=/opt/miniconda3
+++ export CONDA_SHLVL=3
+++ CONDA_SHLVL=3
+++ export CONDA_DEFAULT_ENV=base
+++ CONDA_DEFAULT_ENV=base
+++ export 'CONDA_PROMPT_MODIFIER=(base) '
+++ CONDA_PROMPT_MODIFIER='(base) '
+++ export CONDA_PREFIX_2=/opt/miniconda3/envs/testbed
+++ CONDA_PREFIX_2=/opt/miniconda3/envs/testbed
+++ export CONDA_EXE=/opt/miniconda3/bin/conda
+++ CONDA_EXE=/opt/miniconda3/bin/conda
+++ export _CE_M=
+++ _CE_M=
+++ export _CE_CONDA=
+++ _CE_CONDA=
+++ export CONDA_PYTHON_EXE=/opt/miniconda3/bin/python
+++ CONDA_PYTHON_EXE=/opt/miniconda3/bin/python
++ __conda_hashr
++ '[' -n '' ']'
++ '[' -n '' ']'
++ hash -r
+ conda activate testbed
+ local cmd=activate
+ case "$cmd" in
+ __conda_activate activate testbed
+ '[' -n '' ']'
+ local ask_conda
++ PS1='(base) '
++ __conda_exe shell.posix activate testbed
++ /opt/miniconda3/bin/conda shell.posix activate testbed
+ ask_conda='PS1='\''(testbed) '\''
export PATH='\''/opt/miniconda3/envs/testbed/bin:/opt/miniconda3/condabin:/opt/miniconda3/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin'\''
export CONDA_PREFIX='\''/opt/miniconda3/envs/testbed'\''
export CONDA_SHLVL='\''4'\''
export CONDA_DEFAULT_ENV='\''testbed'\''
export CONDA_PROMPT_MODIFIER='\''(testbed) '\''
export CONDA_PREFIX_3='\''/opt/miniconda3'\''
export CONDA_EXE='\''/opt/miniconda3/bin/conda'\''
export _CE_M='\'''\''
export _CE_CONDA='\'''\''
export CONDA_PYTHON_EXE='\''/opt/miniconda3/bin/python'\'''
+ eval 'PS1='\''(testbed) '\''
export PATH='\''/opt/miniconda3/envs/testbed/bin:/opt/miniconda3/condabin:/opt/miniconda3/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin'\''
export CONDA_PREFIX='\''/opt/miniconda3/envs/testbed'\''
export CONDA_SHLVL='\''4'\''
export CONDA_DEFAULT_ENV='\''testbed'\''
export CONDA_PROMPT_MODIFIER='\''(testbed) '\''
export CONDA_PREFIX_3='\''/opt/miniconda3'\''
export CONDA_EXE='\''/opt/miniconda3/bin/conda'\''
export _CE_M='\'''\''
export _CE_CONDA='\'''\''
export CONDA_PYTHON_EXE='\''/opt/miniconda3/bin/python'\'''
++ PS1='(testbed) '
++ export PATH=/opt/miniconda3/envs/testbed/bin:/opt/miniconda3/condabin:/opt/miniconda3/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin
++ PATH=/opt/miniconda3/envs/testbed/bin:/opt/miniconda3/condabin:/opt/miniconda3/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin
++ export CONDA_PREFIX=/opt/miniconda3/envs/testbed
++ CONDA_PREFIX=/opt/miniconda3/envs/testbed
++ export CONDA_SHLVL=4
++ CONDA_SHLVL=4
++ export CONDA_DEFAULT_ENV=testbed
++ CONDA_DEFAULT_ENV=testbed
++ export 'CONDA_PROMPT_MODIFIER=(testbed) '
++ CONDA_PROMPT_MODIFIER='(testbed) '
++ export CONDA_PREFIX_3=/opt/miniconda3
++ CONDA_PREFIX_3=/opt/miniconda3
++ export CONDA_EXE=/opt/miniconda3/bin/conda
++ CONDA_EXE=/opt/miniconda3/bin/conda
++ export _CE_M=
++ _CE_M=
++ export _CE_CONDA=
++ _CE_CONDA=
++ export CONDA_PYTHON_EXE=/opt/miniconda3/bin/python
++ CONDA_PYTHON_EXE=/opt/miniconda3/bin/python
+ __conda_hashr
+ '[' -n '' ']'
+ '[' -n '' ']'
+ hash -r
+ python -m pip install -v --no-use-pep517 --no-build-isolation -e .
Using pip 21.2.2 from /opt/miniconda3/envs/testbed/lib/python3.6/site-packages/pip (python 3.6)
Obtaining file:///testbed
    Running command python setup.py egg_info
    running egg_info
    creating /tmp/pip-pip-egg-info-_qus94p_/scikit_learn.egg-info
    writing /tmp/pip-pip-egg-info-_qus94p_/scikit_learn.egg-info/PKG-INFO
    writing dependency_links to /tmp/pip-pip-egg-info-_qus94p_/scikit_learn.egg-info/dependency_links.txt
    writing requirements to /tmp/pip-pip-egg-info-_qus94p_/scikit_learn.egg-info/requires.txt
    writing top-level names to /tmp/pip-pip-egg-info-_qus94p_/scikit_learn.egg-info/top_level.txt
    writing manifest file '/tmp/pip-pip-egg-info-_qus94p_/scikit_learn.egg-info/SOURCES.txt'
    reading manifest file '/tmp/pip-pip-egg-info-_qus94p_/scikit_learn.egg-info/SOURCES.txt'
    reading manifest template 'MANIFEST.in'
    adding license file 'COPYING'
    writing manifest file '/tmp/pip-pip-egg-info-_qus94p_/scikit_learn.egg-info/SOURCES.txt'
    Partial import of sklearn during the build process.
Requirement already satisfied: numpy>=1.11.0 in /opt/miniconda3/envs/testbed/lib/python3.6/site-packages (from scikit-learn==0.22.dev0) (1.19.2)
Requirement already satisfied: scipy>=0.17.0 in /opt/miniconda3/envs/testbed/lib/python3.6/site-packages (from scikit-learn==0.22.dev0) (1.5.2)
Requirement already satisfied: joblib>=0.11 in /opt/miniconda3/envs/testbed/lib/python3.6/site-packages (from scikit-learn==0.22.dev0) (1.1.1)
Installing collected packages: scikit-learn
  Attempting uninstall: scikit-learn
    Found existing installation: scikit-learn 0.22.dev0
    Uninstalling scikit-learn-0.22.dev0:
      Removing file or directory /opt/miniconda3/envs/testbed/lib/python3.6/site-packages/scikit-learn.egg-link
      Removing pth entries from /opt/miniconda3/envs/testbed/lib/python3.6/site-packages/easy-install.pth:
      Removing entry: /testbed
      Successfully uninstalled scikit-learn-0.22.dev0
  Running setup.py develop for scikit-learn
    Running command /opt/miniconda3/envs/testbed/bin/python -c 'import io, os, sys, setuptools, tokenize; sys.argv[0] = '"'"'/testbed/setup.py'"'"'; __file__='"'"'/testbed/setup.py'"'"';f = getattr(tokenize, '"'"'open'"'"', open)(__file__) if os.path.exists(__file__) else io.StringIO('"'"'from setuptools import setup; setup()'"'"');code = f.read().replace('"'"'\r\n'"'"', '"'"'\n'"'"');f.close();exec(compile(code, __file__, '"'"'exec'"'"'))' develop --no-deps
    blas_opt_info:
    blas_mkl_info:
    customize UnixCCompiler
      libraries mkl_rt not found in ['/opt/miniconda3/envs/testbed/lib', '/usr/local/lib', '/usr/lib64', '/usr/lib', '/usr/lib/x86_64-linux-gnu']
      NOT AVAILABLE

    blis_info:
      libraries blis not found in ['/opt/miniconda3/envs/testbed/lib', '/usr/local/lib', '/usr/lib64', '/usr/lib', '/usr/lib/x86_64-linux-gnu']
      NOT AVAILABLE

    openblas_info:
    C compiler: gcc -pthread -B /opt/miniconda3/envs/testbed/compiler_compat -Wl,--sysroot=/ -Wsign-compare -DNDEBUG -g -fwrapv -O3 -Wall -Wstrict-prototypes -fPIC

    creating /tmp/tmpkrtj4zzn/tmp
    creating /tmp/tmpkrtj4zzn/tmp/tmpkrtj4zzn
    compile options: '-c'
    gcc: /tmp/tmpkrtj4zzn/source.c
    gcc -pthread -B /opt/miniconda3/envs/testbed/compiler_compat -Wl,--sysroot=/ /tmp/tmpkrtj4zzn/tmp/tmpkrtj4zzn/source.o -L/opt/miniconda3/envs/testbed/lib -lopenblas -o /tmp/tmpkrtj4zzn/a.out
      FOUND:
        libraries = ['openblas', 'openblas']
        library_dirs = ['/opt/miniconda3/envs/testbed/lib']
        language = c
        define_macros = [('HAVE_CBLAS', None)]

      FOUND:
        libraries = ['openblas', 'openblas']
        library_dirs = ['/opt/miniconda3/envs/testbed/lib']
        language = c
        define_macros = [('HAVE_CBLAS', None)]

    C compiler: gcc -pthread -B /opt/miniconda3/envs/testbed/compiler_compat -Wl,--sysroot=/ -Wsign-compare -DNDEBUG -g -fwrapv -O3 -Wall -Wstrict-prototypes -fPIC

    compile options: '-c'
    extra options: '-fopenmp'
    gcc: test_openmp.c
    gcc -pthread -B /opt/miniconda3/envs/testbed/compiler_compat -Wl,--sysroot=/ objects/test_openmp.o -o test_openmp -fopenmp
    running develop
    running build_scripts
    running egg_info
    running build_src
    build_src
    building library "libsvm-skl" sources
    building extension "sklearn.__check_build._check_build" sources
    building extension "sklearn.preprocessing._csr_polynomial_expansion" sources
    building extension "sklearn.cluster._dbscan_inner" sources
    building extension "sklearn.cluster._hierarchical" sources
    building extension "sklearn.cluster._k_means_elkan" sources
    building extension "sklearn.cluster._k_means" sources
    building extension "sklearn.datasets._svmlight_format" sources
    building extension "sklearn.decomposition._online_lda" sources
    building extension "sklearn.decomposition.cdnmf_fast" sources
    building extension "sklearn.ensemble._gradient_boosting" sources
    building extension "sklearn.ensemble._hist_gradient_boosting._gradient_boosting" sources
    building extension "sklearn.ensemble._hist_gradient_boosting.histogram" sources
    building extension "sklearn.ensemble._hist_gradient_boosting.splitting" sources
    building extension "sklearn.ensemble._hist_gradient_boosting._binning" sources
    building extension "sklearn.ensemble._hist_gradient_boosting._predictor" sources
    building extension "sklearn.ensemble._hist_gradient_boosting._loss" sources
    building extension "sklearn.ensemble._hist_gradient_boosting.types" sources
    building extension "sklearn.ensemble._hist_gradient_boosting.utils" sources
    building extension "sklearn.feature_extraction._hashing" sources
    building extension "sklearn.manifold._utils" sources
    building extension "sklearn.manifold._barnes_hut_tsne" sources
    building extension "sklearn.metrics.cluster.expected_mutual_info_fast" sources
    building extension "sklearn.metrics.pairwise_fast" sources
    building extension "sklearn.neighbors.ball_tree" sources
    building extension "sklearn.neighbors.kd_tree" sources
    building extension "sklearn.neighbors.dist_metrics" sources
    building extension "sklearn.neighbors.typedefs" sources
    building extension "sklearn.neighbors.quad_tree" sources
    building extension "sklearn.tree._tree" sources
    building extension "sklearn.tree._splitter" sources
    building extension "sklearn.tree._criterion" sources
    building extension "sklearn.tree._utils" sources
    building extension "sklearn.utils.sparsefuncs_fast" sources
    building extension "sklearn.utils._cython_blas" sources
    building extension "sklearn.utils.arrayfuncs" sources
    building extension "sklearn.utils.murmurhash" sources
    building extension "sklearn.utils.lgamma" sources
    building extension "sklearn.utils.graph_shortest_path" sources
    building extension "sklearn.utils.fast_dict" sources
    building extension "sklearn.utils.seq_dataset" sources
    building extension "sklearn.utils.weight_vector" sources
    building extension "sklearn.utils._random" sources
    building extension "sklearn.utils._logistic_sigmoid" sources
    building extension "sklearn.svm.libsvm" sources
    building extension "sklearn.svm.liblinear" sources
    building extension "sklearn.svm.libsvm_sparse" sources
    building extension "sklearn.linear_model.cd_fast" sources
    building extension "sklearn.linear_model.sgd_fast" sources
    building extension "sklearn.linear_model.sag_fast" sources
    building extension "sklearn._isotonic" sources
    building data_files sources
    build_src: building npy-pkg config files
    writing scikit_learn.egg-info/PKG-INFO
    writing dependency_links to scikit_learn.egg-info/dependency_links.txt
    writing requirements to scikit_learn.egg-info/requires.txt
    writing top-level names to scikit_learn.egg-info/top_level.txt
    reading manifest file 'scikit_learn.egg-info/SOURCES.txt'
    reading manifest template 'MANIFEST.in'
    adding license file 'COPYING'
    writing manifest file 'scikit_learn.egg-info/SOURCES.txt'
    running build_ext
    customize UnixCCompiler
    customize UnixCCompiler using build_clib
    customize UnixCCompiler
    customize UnixCCompiler using build_ext_subclass
    customize UnixCCompiler
    customize UnixCCompiler using build_ext_subclass
    Creating /opt/miniconda3/envs/testbed/lib/python3.6/site-packages/scikit-learn.egg-link (link to .)
    Adding scikit-learn 0.22.dev0 to easy-install.pth file

    Installed /testbed
    Partial import of sklearn during the build process.
Successfully installed scikit-learn-0.22.dev0
WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv
+ git apply -v -
Checking patch sklearn/linear_model/logistic.py...
Applied patch sklearn/linear_model/logistic.py cleanly.
+ git apply -v -
<stdin>:14: trailing whitespace.
    
<stdin>:20: trailing whitespace.
    
<stdin>:23: trailing whitespace.
    
Checking patch sklearn/tests/test_coverup_scikit-learn__scikit-learn-14087.py...
Applied patch sklearn/tests/test_coverup_scikit-learn__scikit-learn-14087.py cleanly.
warning: 3 lines add whitespace errors.
+ python3 /root/trace.py --timing --trace --count -C coverage.cover --include-pattern '/testbed/(sklearn/linear_model/logistic\.py)' -m pytest --no-header -rA -p no:cacheprovider sklearn/tests/test_coverup_scikit-learn__scikit-learn-14087.py
['--timing', '--trace', '--count', '-C', 'coverage.cover', '--include-pattern', '/testbed/(sklearn/linear_model/logistic\\.py)']
============================= test session starts ==============================
collected 1 item

sklearn/tests/test_coverup_scikit-learn__scikit-learn-14087.py .         [100%]

==================================== PASSES ====================================
_____________ test_logistic_regression_cv_refit_false_index_error ______________
----------------------------- Captured stdout call -----------------------------
0.96 /testbed/sklearn/linear_model/logistic.py(1918):         self.Cs = Cs
0.96 /testbed/sklearn/linear_model/logistic.py(1919):         self.fit_intercept = fit_intercept
0.96 /testbed/sklearn/linear_model/logistic.py(1920):         self.cv = cv
0.96 /testbed/sklearn/linear_model/logistic.py(1921):         self.dual = dual
0.96 /testbed/sklearn/linear_model/logistic.py(1922):         self.penalty = penalty
0.96 /testbed/sklearn/linear_model/logistic.py(1923):         self.scoring = scoring
0.96 /testbed/sklearn/linear_model/logistic.py(1924):         self.tol = tol
0.96 /testbed/sklearn/linear_model/logistic.py(1925):         self.max_iter = max_iter
0.96 /testbed/sklearn/linear_model/logistic.py(1926):         self.class_weight = class_weight
0.96 /testbed/sklearn/linear_model/logistic.py(1927):         self.n_jobs = n_jobs
0.96 /testbed/sklearn/linear_model/logistic.py(1928):         self.verbose = verbose
0.96 /testbed/sklearn/linear_model/logistic.py(1929):         self.solver = solver
0.96 /testbed/sklearn/linear_model/logistic.py(1930):         self.refit = refit
0.96 /testbed/sklearn/linear_model/logistic.py(1931):         self.intercept_scaling = intercept_scaling
0.96 /testbed/sklearn/linear_model/logistic.py(1932):         self.multi_class = multi_class
0.96 /testbed/sklearn/linear_model/logistic.py(1933):         self.random_state = random_state
0.96 /testbed/sklearn/linear_model/logistic.py(1934):         self.l1_ratios = l1_ratios
0.96 /testbed/sklearn/linear_model/logistic.py(1956):         solver = _check_solver(self.solver, self.penalty, self.dual)
0.96 /testbed/sklearn/linear_model/logistic.py(428):     all_solvers = ['liblinear', 'newton-cg', 'lbfgs', 'sag', 'saga']
0.96 /testbed/sklearn/linear_model/logistic.py(429):     if solver not in all_solvers:
0.96 /testbed/sklearn/linear_model/logistic.py(433):     all_penalties = ['l1', 'l2', 'elasticnet', 'none']
0.96 /testbed/sklearn/linear_model/logistic.py(434):     if penalty not in all_penalties:
0.96 /testbed/sklearn/linear_model/logistic.py(438):     if solver not in ['liblinear', 'saga'] and penalty not in ('l2', 'none'):
0.96 /testbed/sklearn/linear_model/logistic.py(441):     if solver != 'liblinear' and dual:
0.96 /testbed/sklearn/linear_model/logistic.py(445):     if penalty == 'elasticnet' and solver != 'saga':
0.96 /testbed/sklearn/linear_model/logistic.py(449):     if solver == 'liblinear' and penalty == 'none':
0.96 /testbed/sklearn/linear_model/logistic.py(454):     return solver
0.96 /testbed/sklearn/linear_model/logistic.py(1958):         if not isinstance(self.max_iter, numbers.Number) or self.max_iter < 0:
0.96 /testbed/sklearn/linear_model/logistic.py(1961):         if not isinstance(self.tol, numbers.Number) or self.tol < 0:
0.96 /testbed/sklearn/linear_model/logistic.py(1964):         if self.penalty == 'elasticnet':
0.96 /testbed/sklearn/linear_model/logistic.py(1973):             if self.l1_ratios is not None:
0.96 /testbed/sklearn/linear_model/logistic.py(1978):             l1_ratios_ = [None]
0.96 /testbed/sklearn/linear_model/logistic.py(1980):         if self.penalty == 'none':
0.96 /testbed/sklearn/linear_model/logistic.py(1986):         X, y = check_X_y(X, y, accept_sparse='csr', dtype=np.float64,
0.96 /testbed/sklearn/linear_model/logistic.py(1987):                          order="C",
0.96 /testbed/sklearn/linear_model/logistic.py(1988):                          accept_large_sparse=solver != 'liblinear')
0.96 /testbed/sklearn/linear_model/logistic.py(1989):         check_classification_targets(y)
0.96 /testbed/sklearn/linear_model/logistic.py(1991):         class_weight = self.class_weight
0.96 /testbed/sklearn/linear_model/logistic.py(1994):         label_encoder = LabelEncoder().fit(y)
0.96 /testbed/sklearn/linear_model/logistic.py(1995):         y = label_encoder.transform(y)
0.96 /testbed/sklearn/linear_model/logistic.py(1996):         if isinstance(class_weight, dict):
0.96 /testbed/sklearn/linear_model/logistic.py(2001):         classes = self.classes_ = label_encoder.classes_
0.96 /testbed/sklearn/linear_model/logistic.py(2002):         encoded_labels = label_encoder.transform(label_encoder.classes_)
0.96 /testbed/sklearn/linear_model/logistic.py(2004):         multi_class = _check_multi_class(self.multi_class, solver,
0.96 /testbed/sklearn/linear_model/logistic.py(2005):                                          len(classes))
0.96 /testbed/sklearn/linear_model/logistic.py(458):     if multi_class == 'auto':
0.96 /testbed/sklearn/linear_model/logistic.py(459):         if solver == 'liblinear':
0.96 /testbed/sklearn/linear_model/logistic.py(461):         elif n_classes > 2:
0.96 /testbed/sklearn/linear_model/logistic.py(464):             multi_class = 'ovr'
0.96 /testbed/sklearn/linear_model/logistic.py(465):     if multi_class not in ('multinomial', 'ovr'):
0.96 /testbed/sklearn/linear_model/logistic.py(468):     if multi_class == 'multinomial' and solver == 'liblinear':
0.96 /testbed/sklearn/linear_model/logistic.py(471):     return multi_class
0.96 /testbed/sklearn/linear_model/logistic.py(2007):         if solver in ['sag', 'saga']:
0.96 /testbed/sklearn/linear_model/logistic.py(2008):             max_squared_sum = row_norms(X, squared=True).max()
0.96 /testbed/sklearn/linear_model/logistic.py(2013):         cv = check_cv(self.cv, y, classifier=True)
0.96 /testbed/sklearn/linear_model/logistic.py(2014):         folds = list(cv.split(X, y))
0.96 /testbed/sklearn/linear_model/logistic.py(2017):         n_classes = len(encoded_labels)
0.96 /testbed/sklearn/linear_model/logistic.py(2019):         if n_classes < 2:
0.96 /testbed/sklearn/linear_model/logistic.py(2024):         if n_classes == 2:
0.96 /testbed/sklearn/linear_model/logistic.py(2027):             n_classes = 1
0.96 /testbed/sklearn/linear_model/logistic.py(2028):             encoded_labels = encoded_labels[1:]
0.96 /testbed/sklearn/linear_model/logistic.py(2029):             classes = classes[1:]
0.96 /testbed/sklearn/linear_model/logistic.py(2033):         if multi_class == 'multinomial':
0.96 /testbed/sklearn/linear_model/logistic.py(2036):             iter_encoded_labels = encoded_labels
0.96 /testbed/sklearn/linear_model/logistic.py(2037):             iter_classes = classes
0.96 /testbed/sklearn/linear_model/logistic.py(2040):         if class_weight == "balanced":
0.96 /testbed/sklearn/linear_model/logistic.py(2046):         path_func = delayed(_log_reg_scoring_path)
0.96 /testbed/sklearn/linear_model/logistic.py(2050):         if self.solver in ['sag', 'saga']:
0.96 /testbed/sklearn/linear_model/logistic.py(2051):             prefer = 'threads'
0.96 /testbed/sklearn/linear_model/logistic.py(2055):         fold_coefs_ = Parallel(n_jobs=self.n_jobs, verbose=self.verbose,
0.96 /testbed/sklearn/linear_model/logistic.py(2056):                                **_joblib_parallel_args(prefer=prefer))(
0.96 /testbed/sklearn/linear_model/logistic.py(2057):             path_func(X, y, train, test, pos_class=label, Cs=self.Cs,
0.96 /testbed/sklearn/linear_model/logistic.py(2069):             for label in iter_encoded_labels
0.96 /testbed/sklearn/linear_model/logistic.py(2057):             path_func(X, y, train, test, pos_class=label, Cs=self.Cs,
0.96 /testbed/sklearn/linear_model/logistic.py(2069):             for label in iter_encoded_labels
0.96 /testbed/sklearn/linear_model/logistic.py(2070):             for train, test in folds
0.96 /testbed/sklearn/linear_model/logistic.py(2071):             for l1_ratio in l1_ratios_)
0.96 /testbed/sklearn/linear_model/logistic.py(1132):     X_train = X[train]
0.96 /testbed/sklearn/linear_model/logistic.py(1133):     X_test = X[test]
0.96 /testbed/sklearn/linear_model/logistic.py(1134):     y_train = y[train]
0.96 /testbed/sklearn/linear_model/logistic.py(1135):     y_test = y[test]
0.96 /testbed/sklearn/linear_model/logistic.py(1137):     if sample_weight is not None:
0.96 /testbed/sklearn/linear_model/logistic.py(1143):     coefs, Cs, n_iter = _logistic_regression_path(
0.96 /testbed/sklearn/linear_model/logistic.py(1144):         X_train, y_train, Cs=Cs, l1_ratio=l1_ratio,
0.96 /testbed/sklearn/linear_model/logistic.py(1145):         fit_intercept=fit_intercept, solver=solver, max_iter=max_iter,
0.96 /testbed/sklearn/linear_model/logistic.py(1146):         class_weight=class_weight, pos_class=pos_class,
0.96 /testbed/sklearn/linear_model/logistic.py(1147):         multi_class=multi_class, tol=tol, verbose=verbose, dual=dual,
0.96 /testbed/sklearn/linear_model/logistic.py(1148):         penalty=penalty, intercept_scaling=intercept_scaling,
0.96 /testbed/sklearn/linear_model/logistic.py(1149):         random_state=random_state, check_input=False,
0.96 /testbed/sklearn/linear_model/logistic.py(1150):         max_squared_sum=max_squared_sum, sample_weight=sample_weight)
0.96 /testbed/sklearn/linear_model/logistic.py(803):     if isinstance(Cs, numbers.Integral):
0.96 /testbed/sklearn/linear_model/logistic.py(804):         Cs = np.logspace(-4, 4, Cs)
0.96 /testbed/sklearn/linear_model/logistic.py(806):     solver = _check_solver(solver, penalty, dual)
0.96 /testbed/sklearn/linear_model/logistic.py(428):     all_solvers = ['liblinear', 'newton-cg', 'lbfgs', 'sag', 'saga']
0.96 /testbed/sklearn/linear_model/logistic.py(429):     if solver not in all_solvers:
0.96 /testbed/sklearn/linear_model/logistic.py(433):     all_penalties = ['l1', 'l2', 'elasticnet', 'none']
0.96 /testbed/sklearn/linear_model/logistic.py(434):     if penalty not in all_penalties:
0.96 /testbed/sklearn/linear_model/logistic.py(438):     if solver not in ['liblinear', 'saga'] and penalty not in ('l2', 'none'):
0.96 /testbed/sklearn/linear_model/logistic.py(441):     if solver != 'liblinear' and dual:
0.96 /testbed/sklearn/linear_model/logistic.py(445):     if penalty == 'elasticnet' and solver != 'saga':
0.96 /testbed/sklearn/linear_model/logistic.py(449):     if solver == 'liblinear' and penalty == 'none':
0.96 /testbed/sklearn/linear_model/logistic.py(454):     return solver
0.96 /testbed/sklearn/linear_model/logistic.py(809):     if check_input:
0.96 /testbed/sklearn/linear_model/logistic.py(814):     _, n_features = X.shape
0.96 /testbed/sklearn/linear_model/logistic.py(816):     classes = np.unique(y)
0.96 /testbed/sklearn/linear_model/logistic.py(817):     random_state = check_random_state(random_state)
0.96 /testbed/sklearn/linear_model/logistic.py(819):     multi_class = _check_multi_class(multi_class, solver, len(classes))
0.96 /testbed/sklearn/linear_model/logistic.py(458):     if multi_class == 'auto':
0.96 /testbed/sklearn/linear_model/logistic.py(465):     if multi_class not in ('multinomial', 'ovr'):
0.96 /testbed/sklearn/linear_model/logistic.py(468):     if multi_class == 'multinomial' and solver == 'liblinear':
0.96 /testbed/sklearn/linear_model/logistic.py(471):     return multi_class
0.96 /testbed/sklearn/linear_model/logistic.py(820):     if pos_class is None and multi_class != 'multinomial':
0.96 /testbed/sklearn/linear_model/logistic.py(829):     if sample_weight is not None:
0.96 /testbed/sklearn/linear_model/logistic.py(833):         sample_weight = np.ones(X.shape[0], dtype=X.dtype)
0.96 /testbed/sklearn/linear_model/logistic.py(838):     le = LabelEncoder()
0.96 /testbed/sklearn/linear_model/logistic.py(839):     if isinstance(class_weight, dict) or multi_class == 'multinomial':
0.96 /testbed/sklearn/linear_model/logistic.py(845):     if multi_class == 'ovr':
0.96 /testbed/sklearn/linear_model/logistic.py(846):         w0 = np.zeros(n_features + int(fit_intercept), dtype=X.dtype)
0.96 /testbed/sklearn/linear_model/logistic.py(847):         mask_classes = np.array([-1, 1])
0.96 /testbed/sklearn/linear_model/logistic.py(848):         mask = (y == pos_class)
0.96 /testbed/sklearn/linear_model/logistic.py(849):         y_bin = np.ones(y.shape, dtype=X.dtype)
0.96 /testbed/sklearn/linear_model/logistic.py(850):         y_bin[~mask] = -1.
0.96 /testbed/sklearn/linear_model/logistic.py(853):         if class_weight == "balanced":
0.96 /testbed/sklearn/linear_model/logistic.py(872):     if coef is not None:
0.96 /testbed/sklearn/linear_model/logistic.py(901):     if multi_class == 'multinomial':
0.96 /testbed/sklearn/linear_model/logistic.py(914):         target = y_bin
0.96 /testbed/sklearn/linear_model/logistic.py(915):         if solver == 'lbfgs':
0.96 /testbed/sklearn/linear_model/logistic.py(917):         elif solver == 'newton-cg':
0.96 /testbed/sklearn/linear_model/logistic.py(921):         warm_start_sag = {'coef': np.expand_dims(w0, axis=1)}
0.96 /testbed/sklearn/linear_model/logistic.py(923):     coefs = list()
0.96 /testbed/sklearn/linear_model/logistic.py(924):     n_iter = np.zeros(len(Cs), dtype=np.int32)
0.96 /testbed/sklearn/linear_model/logistic.py(925):     for i, C in enumerate(Cs):
0.96 /testbed/sklearn/linear_model/logistic.py(926):         if solver == 'lbfgs':
0.96 /testbed/sklearn/linear_model/logistic.py(939):         elif solver == 'newton-cg':
0.96 /testbed/sklearn/linear_model/logistic.py(943):         elif solver == 'liblinear':
0.96 /testbed/sklearn/linear_model/logistic.py(953):         elif solver in ['sag', 'saga']:
0.96 /testbed/sklearn/linear_model/logistic.py(954):             if multi_class == 'multinomial':
0.96 /testbed/sklearn/linear_model/logistic.py(958):                 loss = 'log'
0.96 /testbed/sklearn/linear_model/logistic.py(960):             if penalty == 'l1':
0.96 /testbed/sklearn/linear_model/logistic.py(963):             elif penalty == 'l2':
0.96 /testbed/sklearn/linear_model/logistic.py(964):                 alpha = 1. / C
0.96 /testbed/sklearn/linear_model/logistic.py(965):                 beta = 0.
0.96 /testbed/sklearn/linear_model/logistic.py(970):             w0, n_iter_i, warm_start_sag = sag_solver(
0.96 /testbed/sklearn/linear_model/logistic.py(971):                 X, target, sample_weight, loss, alpha,
0.96 /testbed/sklearn/linear_model/logistic.py(972):                 beta, max_iter, tol,
0.96 /testbed/sklearn/linear_model/logistic.py(973):                 verbose, random_state, False, max_squared_sum, warm_start_sag,
0.96 /testbed/sklearn/linear_model/logistic.py(974):                 is_saga=(solver == 'saga'))
0.96 /testbed/sklearn/linear_model/logistic.py(980):         if multi_class == 'multinomial':
0.96 /testbed/sklearn/linear_model/logistic.py(987):             coefs.append(w0.copy())
0.96 /testbed/sklearn/linear_model/logistic.py(989):         n_iter[i] = n_iter_i
0.96 /testbed/sklearn/linear_model/logistic.py(925):     for i, C in enumerate(Cs):
0.96 /testbed/sklearn/linear_model/logistic.py(926):         if solver == 'lbfgs':
0.96 /testbed/sklearn/linear_model/logistic.py(939):         elif solver == 'newton-cg':
0.96 /testbed/sklearn/linear_model/logistic.py(943):         elif solver == 'liblinear':
0.96 /testbed/sklearn/linear_model/logistic.py(953):         elif solver in ['sag', 'saga']:
0.96 /testbed/sklearn/linear_model/logistic.py(954):             if multi_class == 'multinomial':
0.96 /testbed/sklearn/linear_model/logistic.py(958):                 loss = 'log'
0.96 /testbed/sklearn/linear_model/logistic.py(960):             if penalty == 'l1':
0.96 /testbed/sklearn/linear_model/logistic.py(963):             elif penalty == 'l2':
0.96 /testbed/sklearn/linear_model/logistic.py(964):                 alpha = 1. / C
0.96 /testbed/sklearn/linear_model/logistic.py(965):                 beta = 0.
0.96 /testbed/sklearn/linear_model/logistic.py(970):             w0, n_iter_i, warm_start_sag = sag_solver(
0.96 /testbed/sklearn/linear_model/logistic.py(971):                 X, target, sample_weight, loss, alpha,
0.96 /testbed/sklearn/linear_model/logistic.py(972):                 beta, max_iter, tol,
0.96 /testbed/sklearn/linear_model/logistic.py(973):                 verbose, random_state, False, max_squared_sum, warm_start_sag,
0.96 /testbed/sklearn/linear_model/logistic.py(974):                 is_saga=(solver == 'saga'))
0.96 /testbed/sklearn/linear_model/logistic.py(980):         if multi_class == 'multinomial':
0.96 /testbed/sklearn/linear_model/logistic.py(987):             coefs.append(w0.copy())
0.96 /testbed/sklearn/linear_model/logistic.py(989):         n_iter[i] = n_iter_i
0.96 /testbed/sklearn/linear_model/logistic.py(925):     for i, C in enumerate(Cs):
0.96 /testbed/sklearn/linear_model/logistic.py(926):         if solver == 'lbfgs':
0.96 /testbed/sklearn/linear_model/logistic.py(939):         elif solver == 'newton-cg':
0.96 /testbed/sklearn/linear_model/logistic.py(943):         elif solver == 'liblinear':
0.96 /testbed/sklearn/linear_model/logistic.py(953):         elif solver in ['sag', 'saga']:
0.96 /testbed/sklearn/linear_model/logistic.py(954):             if multi_class == 'multinomial':
0.96 /testbed/sklearn/linear_model/logistic.py(958):                 loss = 'log'
0.96 /testbed/sklearn/linear_model/logistic.py(960):             if penalty == 'l1':
0.96 /testbed/sklearn/linear_model/logistic.py(963):             elif penalty == 'l2':
0.96 /testbed/sklearn/linear_model/logistic.py(964):                 alpha = 1. / C
0.96 /testbed/sklearn/linear_model/logistic.py(965):                 beta = 0.
0.96 /testbed/sklearn/linear_model/logistic.py(970):             w0, n_iter_i, warm_start_sag = sag_solver(
0.96 /testbed/sklearn/linear_model/logistic.py(971):                 X, target, sample_weight, loss, alpha,
0.96 /testbed/sklearn/linear_model/logistic.py(972):                 beta, max_iter, tol,
0.97 /testbed/sklearn/linear_model/logistic.py(973):                 verbose, random_state, False, max_squared_sum, warm_start_sag,
0.97 /testbed/sklearn/linear_model/logistic.py(974):                 is_saga=(solver == 'saga'))
0.97 /testbed/sklearn/linear_model/logistic.py(980):         if multi_class == 'multinomial':
0.97 /testbed/sklearn/linear_model/logistic.py(987):             coefs.append(w0.copy())
0.97 /testbed/sklearn/linear_model/logistic.py(989):         n_iter[i] = n_iter_i
0.97 /testbed/sklearn/linear_model/logistic.py(925):     for i, C in enumerate(Cs):
0.97 /testbed/sklearn/linear_model/logistic.py(926):         if solver == 'lbfgs':
0.97 /testbed/sklearn/linear_model/logistic.py(939):         elif solver == 'newton-cg':
0.97 /testbed/sklearn/linear_model/logistic.py(943):         elif solver == 'liblinear':
0.97 /testbed/sklearn/linear_model/logistic.py(953):         elif solver in ['sag', 'saga']:
0.97 /testbed/sklearn/linear_model/logistic.py(954):             if multi_class == 'multinomial':
0.97 /testbed/sklearn/linear_model/logistic.py(958):                 loss = 'log'
0.97 /testbed/sklearn/linear_model/logistic.py(960):             if penalty == 'l1':
0.97 /testbed/sklearn/linear_model/logistic.py(963):             elif penalty == 'l2':
0.97 /testbed/sklearn/linear_model/logistic.py(964):                 alpha = 1. / C
0.97 /testbed/sklearn/linear_model/logistic.py(965):                 beta = 0.
0.97 /testbed/sklearn/linear_model/logistic.py(970):             w0, n_iter_i, warm_start_sag = sag_solver(
0.97 /testbed/sklearn/linear_model/logistic.py(971):                 X, target, sample_weight, loss, alpha,
0.97 /testbed/sklearn/linear_model/logistic.py(972):                 beta, max_iter, tol,
0.97 /testbed/sklearn/linear_model/logistic.py(973):                 verbose, random_state, False, max_squared_sum, warm_start_sag,
0.97 /testbed/sklearn/linear_model/logistic.py(974):                 is_saga=(solver == 'saga'))
0.97 /testbed/sklearn/linear_model/logistic.py(980):         if multi_class == 'multinomial':
0.97 /testbed/sklearn/linear_model/logistic.py(987):             coefs.append(w0.copy())
0.97 /testbed/sklearn/linear_model/logistic.py(989):         n_iter[i] = n_iter_i
0.97 /testbed/sklearn/linear_model/logistic.py(925):     for i, C in enumerate(Cs):
0.97 /testbed/sklearn/linear_model/logistic.py(926):         if solver == 'lbfgs':
0.97 /testbed/sklearn/linear_model/logistic.py(939):         elif solver == 'newton-cg':
0.97 /testbed/sklearn/linear_model/logistic.py(943):         elif solver == 'liblinear':
0.97 /testbed/sklearn/linear_model/logistic.py(953):         elif solver in ['sag', 'saga']:
0.97 /testbed/sklearn/linear_model/logistic.py(954):             if multi_class == 'multinomial':
0.97 /testbed/sklearn/linear_model/logistic.py(958):                 loss = 'log'
0.97 /testbed/sklearn/linear_model/logistic.py(960):             if penalty == 'l1':
0.97 /testbed/sklearn/linear_model/logistic.py(963):             elif penalty == 'l2':
0.97 /testbed/sklearn/linear_model/logistic.py(964):                 alpha = 1. / C
0.97 /testbed/sklearn/linear_model/logistic.py(965):                 beta = 0.
0.97 /testbed/sklearn/linear_model/logistic.py(970):             w0, n_iter_i, warm_start_sag = sag_solver(
0.97 /testbed/sklearn/linear_model/logistic.py(971):                 X, target, sample_weight, loss, alpha,
0.97 /testbed/sklearn/linear_model/logistic.py(972):                 beta, max_iter, tol,
0.97 /testbed/sklearn/linear_model/logistic.py(973):                 verbose, random_state, False, max_squared_sum, warm_start_sag,
0.97 /testbed/sklearn/linear_model/logistic.py(974):                 is_saga=(solver == 'saga'))
0.97 /testbed/sklearn/linear_model/logistic.py(980):         if multi_class == 'multinomial':
0.97 /testbed/sklearn/linear_model/logistic.py(987):             coefs.append(w0.copy())
0.97 /testbed/sklearn/linear_model/logistic.py(989):         n_iter[i] = n_iter_i
0.97 /testbed/sklearn/linear_model/logistic.py(925):     for i, C in enumerate(Cs):
0.97 /testbed/sklearn/linear_model/logistic.py(926):         if solver == 'lbfgs':
0.97 /testbed/sklearn/linear_model/logistic.py(939):         elif solver == 'newton-cg':
0.97 /testbed/sklearn/linear_model/logistic.py(943):         elif solver == 'liblinear':
0.97 /testbed/sklearn/linear_model/logistic.py(953):         elif solver in ['sag', 'saga']:
0.97 /testbed/sklearn/linear_model/logistic.py(954):             if multi_class == 'multinomial':
0.97 /testbed/sklearn/linear_model/logistic.py(958):                 loss = 'log'
0.97 /testbed/sklearn/linear_model/logistic.py(960):             if penalty == 'l1':
0.97 /testbed/sklearn/linear_model/logistic.py(963):             elif penalty == 'l2':
0.97 /testbed/sklearn/linear_model/logistic.py(964):                 alpha = 1. / C
0.97 /testbed/sklearn/linear_model/logistic.py(965):                 beta = 0.
0.97 /testbed/sklearn/linear_model/logistic.py(970):             w0, n_iter_i, warm_start_sag = sag_solver(
0.97 /testbed/sklearn/linear_model/logistic.py(971):                 X, target, sample_weight, loss, alpha,
0.97 /testbed/sklearn/linear_model/logistic.py(972):                 beta, max_iter, tol,
0.97 /testbed/sklearn/linear_model/logistic.py(973):                 verbose, random_state, False, max_squared_sum, warm_start_sag,
0.97 /testbed/sklearn/linear_model/logistic.py(974):                 is_saga=(solver == 'saga'))
0.97 /testbed/sklearn/linear_model/logistic.py(980):         if multi_class == 'multinomial':
0.97 /testbed/sklearn/linear_model/logistic.py(987):             coefs.append(w0.copy())
0.97 /testbed/sklearn/linear_model/logistic.py(989):         n_iter[i] = n_iter_i
0.97 /testbed/sklearn/linear_model/logistic.py(925):     for i, C in enumerate(Cs):
0.97 /testbed/sklearn/linear_model/logistic.py(926):         if solver == 'lbfgs':
0.97 /testbed/sklearn/linear_model/logistic.py(939):         elif solver == 'newton-cg':
0.97 /testbed/sklearn/linear_model/logistic.py(943):         elif solver == 'liblinear':
0.97 /testbed/sklearn/linear_model/logistic.py(953):         elif solver in ['sag', 'saga']:
0.97 /testbed/sklearn/linear_model/logistic.py(954):             if multi_class == 'multinomial':
0.97 /testbed/sklearn/linear_model/logistic.py(958):                 loss = 'log'
0.97 /testbed/sklearn/linear_model/logistic.py(960):             if penalty == 'l1':
0.97 /testbed/sklearn/linear_model/logistic.py(963):             elif penalty == 'l2':
0.97 /testbed/sklearn/linear_model/logistic.py(964):                 alpha = 1. / C
0.97 /testbed/sklearn/linear_model/logistic.py(965):                 beta = 0.
0.97 /testbed/sklearn/linear_model/logistic.py(970):             w0, n_iter_i, warm_start_sag = sag_solver(
0.97 /testbed/sklearn/linear_model/logistic.py(971):                 X, target, sample_weight, loss, alpha,
0.97 /testbed/sklearn/linear_model/logistic.py(972):                 beta, max_iter, tol,
0.97 /testbed/sklearn/linear_model/logistic.py(973):                 verbose, random_state, False, max_squared_sum, warm_start_sag,
0.97 /testbed/sklearn/linear_model/logistic.py(974):                 is_saga=(solver == 'saga'))
0.97 /testbed/sklearn/linear_model/logistic.py(980):         if multi_class == 'multinomial':
0.97 /testbed/sklearn/linear_model/logistic.py(987):             coefs.append(w0.copy())
0.97 /testbed/sklearn/linear_model/logistic.py(989):         n_iter[i] = n_iter_i
0.97 /testbed/sklearn/linear_model/logistic.py(925):     for i, C in enumerate(Cs):
0.97 /testbed/sklearn/linear_model/logistic.py(926):         if solver == 'lbfgs':
0.97 /testbed/sklearn/linear_model/logistic.py(939):         elif solver == 'newton-cg':
0.97 /testbed/sklearn/linear_model/logistic.py(943):         elif solver == 'liblinear':
0.97 /testbed/sklearn/linear_model/logistic.py(953):         elif solver in ['sag', 'saga']:
0.97 /testbed/sklearn/linear_model/logistic.py(954):             if multi_class == 'multinomial':
0.97 /testbed/sklearn/linear_model/logistic.py(958):                 loss = 'log'
0.97 /testbed/sklearn/linear_model/logistic.py(960):             if penalty == 'l1':
0.97 /testbed/sklearn/linear_model/logistic.py(963):             elif penalty == 'l2':
0.97 /testbed/sklearn/linear_model/logistic.py(964):                 alpha = 1. / C
0.97 /testbed/sklearn/linear_model/logistic.py(965):                 beta = 0.
0.97 /testbed/sklearn/linear_model/logistic.py(970):             w0, n_iter_i, warm_start_sag = sag_solver(
0.97 /testbed/sklearn/linear_model/logistic.py(971):                 X, target, sample_weight, loss, alpha,
0.97 /testbed/sklearn/linear_model/logistic.py(972):                 beta, max_iter, tol,
0.97 /testbed/sklearn/linear_model/logistic.py(973):                 verbose, random_state, False, max_squared_sum, warm_start_sag,
0.97 /testbed/sklearn/linear_model/logistic.py(974):                 is_saga=(solver == 'saga'))
0.97 /testbed/sklearn/linear_model/logistic.py(980):         if multi_class == 'multinomial':
0.97 /testbed/sklearn/linear_model/logistic.py(987):             coefs.append(w0.copy())
0.97 /testbed/sklearn/linear_model/logistic.py(989):         n_iter[i] = n_iter_i
0.97 /testbed/sklearn/linear_model/logistic.py(925):     for i, C in enumerate(Cs):
0.97 /testbed/sklearn/linear_model/logistic.py(926):         if solver == 'lbfgs':
0.97 /testbed/sklearn/linear_model/logistic.py(939):         elif solver == 'newton-cg':
0.97 /testbed/sklearn/linear_model/logistic.py(943):         elif solver == 'liblinear':
0.97 /testbed/sklearn/linear_model/logistic.py(953):         elif solver in ['sag', 'saga']:
0.97 /testbed/sklearn/linear_model/logistic.py(954):             if multi_class == 'multinomial':
0.97 /testbed/sklearn/linear_model/logistic.py(958):                 loss = 'log'
0.97 /testbed/sklearn/linear_model/logistic.py(960):             if penalty == 'l1':
0.97 /testbed/sklearn/linear_model/logistic.py(963):             elif penalty == 'l2':
0.97 /testbed/sklearn/linear_model/logistic.py(964):                 alpha = 1. / C
0.97 /testbed/sklearn/linear_model/logistic.py(965):                 beta = 0.
0.97 /testbed/sklearn/linear_model/logistic.py(970):             w0, n_iter_i, warm_start_sag = sag_solver(
0.97 /testbed/sklearn/linear_model/logistic.py(971):                 X, target, sample_weight, loss, alpha,
0.97 /testbed/sklearn/linear_model/logistic.py(972):                 beta, max_iter, tol,
0.97 /testbed/sklearn/linear_model/logistic.py(973):                 verbose, random_state, False, max_squared_sum, warm_start_sag,
0.97 /testbed/sklearn/linear_model/logistic.py(974):                 is_saga=(solver == 'saga'))
0.97 /testbed/sklearn/linear_model/logistic.py(980):         if multi_class == 'multinomial':
0.97 /testbed/sklearn/linear_model/logistic.py(987):             coefs.append(w0.copy())
0.97 /testbed/sklearn/linear_model/logistic.py(989):         n_iter[i] = n_iter_i
0.97 /testbed/sklearn/linear_model/logistic.py(925):     for i, C in enumerate(Cs):
0.97 /testbed/sklearn/linear_model/logistic.py(926):         if solver == 'lbfgs':
0.97 /testbed/sklearn/linear_model/logistic.py(939):         elif solver == 'newton-cg':
0.97 /testbed/sklearn/linear_model/logistic.py(943):         elif solver == 'liblinear':
0.97 /testbed/sklearn/linear_model/logistic.py(953):         elif solver in ['sag', 'saga']:
0.97 /testbed/sklearn/linear_model/logistic.py(954):             if multi_class == 'multinomial':
0.97 /testbed/sklearn/linear_model/logistic.py(958):                 loss = 'log'
0.97 /testbed/sklearn/linear_model/logistic.py(960):             if penalty == 'l1':
0.97 /testbed/sklearn/linear_model/logistic.py(963):             elif penalty == 'l2':
0.97 /testbed/sklearn/linear_model/logistic.py(964):                 alpha = 1. / C
0.97 /testbed/sklearn/linear_model/logistic.py(965):                 beta = 0.
0.97 /testbed/sklearn/linear_model/logistic.py(970):             w0, n_iter_i, warm_start_sag = sag_solver(
0.97 /testbed/sklearn/linear_model/logistic.py(971):                 X, target, sample_weight, loss, alpha,
0.97 /testbed/sklearn/linear_model/logistic.py(972):                 beta, max_iter, tol,
0.97 /testbed/sklearn/linear_model/logistic.py(973):                 verbose, random_state, False, max_squared_sum, warm_start_sag,
0.97 /testbed/sklearn/linear_model/logistic.py(974):                 is_saga=(solver == 'saga'))
0.97 /testbed/sklearn/linear_model/logistic.py(980):         if multi_class == 'multinomial':
0.97 /testbed/sklearn/linear_model/logistic.py(987):             coefs.append(w0.copy())
0.97 /testbed/sklearn/linear_model/logistic.py(989):         n_iter[i] = n_iter_i
0.97 /testbed/sklearn/linear_model/logistic.py(925):     for i, C in enumerate(Cs):
0.97 /testbed/sklearn/linear_model/logistic.py(991):     return np.array(coefs), np.array(Cs), n_iter
0.97 /testbed/sklearn/linear_model/logistic.py(1152):     log_reg = LogisticRegression(solver=solver, multi_class=multi_class)
0.97 /testbed/sklearn/linear_model/logistic.py(1437):         self.penalty = penalty
0.97 /testbed/sklearn/linear_model/logistic.py(1438):         self.dual = dual
0.97 /testbed/sklearn/linear_model/logistic.py(1439):         self.tol = tol
0.97 /testbed/sklearn/linear_model/logistic.py(1440):         self.C = C
0.97 /testbed/sklearn/linear_model/logistic.py(1441):         self.fit_intercept = fit_intercept
0.97 /testbed/sklearn/linear_model/logistic.py(1442):         self.intercept_scaling = intercept_scaling
0.97 /testbed/sklearn/linear_model/logistic.py(1443):         self.class_weight = class_weight
0.97 /testbed/sklearn/linear_model/logistic.py(1444):         self.random_state = random_state
0.97 /testbed/sklearn/linear_model/logistic.py(1445):         self.solver = solver
0.97 /testbed/sklearn/linear_model/logistic.py(1446):         self.max_iter = max_iter
0.97 /testbed/sklearn/linear_model/logistic.py(1447):         self.multi_class = multi_class
0.97 /testbed/sklearn/linear_model/logistic.py(1448):         self.verbose = verbose
0.97 /testbed/sklearn/linear_model/logistic.py(1449):         self.warm_start = warm_start
0.97 /testbed/sklearn/linear_model/logistic.py(1450):         self.n_jobs = n_jobs
0.97 /testbed/sklearn/linear_model/logistic.py(1451):         self.l1_ratio = l1_ratio
0.97 /testbed/sklearn/linear_model/logistic.py(1155):     if multi_class == 'ovr':
0.97 /testbed/sklearn/linear_model/logistic.py(1156):         log_reg.classes_ = np.array([-1, 1])
0.97 /testbed/sklearn/linear_model/logistic.py(1163):     if pos_class is not None:
0.97 /testbed/sklearn/linear_model/logistic.py(1164):         mask = (y_test == pos_class)
0.97 /testbed/sklearn/linear_model/logistic.py(1165):         y_test = np.ones(y_test.shape, dtype=np.float64)
0.97 /testbed/sklearn/linear_model/logistic.py(1166):         y_test[~mask] = -1.
0.97 /testbed/sklearn/linear_model/logistic.py(1168):     scores = list()
0.97 /testbed/sklearn/linear_model/logistic.py(1170):     if isinstance(scoring, str):
0.97 /testbed/sklearn/linear_model/logistic.py(1172):     for w in coefs:
0.97 /testbed/sklearn/linear_model/logistic.py(1173):         if multi_class == 'ovr':
0.97 /testbed/sklearn/linear_model/logistic.py(1174):             w = w[np.newaxis, :]
0.97 /testbed/sklearn/linear_model/logistic.py(1175):         if fit_intercept:
0.97 /testbed/sklearn/linear_model/logistic.py(1176):             log_reg.coef_ = w[:, :-1]
0.97 /testbed/sklearn/linear_model/logistic.py(1177):             log_reg.intercept_ = w[:, -1]
0.97 /testbed/sklearn/linear_model/logistic.py(1182):         if scoring is None:
0.97 /testbed/sklearn/linear_model/logistic.py(1183):             scores.append(log_reg.score(X_test, y_test))
0.97 /testbed/sklearn/linear_model/logistic.py(1172):     for w in coefs:
0.97 /testbed/sklearn/linear_model/logistic.py(1173):         if multi_class == 'ovr':
0.97 /testbed/sklearn/linear_model/logistic.py(1174):             w = w[np.newaxis, :]
0.97 /testbed/sklearn/linear_model/logistic.py(1175):         if fit_intercept:
0.97 /testbed/sklearn/linear_model/logistic.py(1176):             log_reg.coef_ = w[:, :-1]
0.97 /testbed/sklearn/linear_model/logistic.py(1177):             log_reg.intercept_ = w[:, -1]
0.97 /testbed/sklearn/linear_model/logistic.py(1182):         if scoring is None:
0.97 /testbed/sklearn/linear_model/logistic.py(1183):             scores.append(log_reg.score(X_test, y_test))
0.97 /testbed/sklearn/linear_model/logistic.py(1172):     for w in coefs:
0.97 /testbed/sklearn/linear_model/logistic.py(1173):         if multi_class == 'ovr':
0.97 /testbed/sklearn/linear_model/logistic.py(1174):             w = w[np.newaxis, :]
0.97 /testbed/sklearn/linear_model/logistic.py(1175):         if fit_intercept:
0.97 /testbed/sklearn/linear_model/logistic.py(1176):             log_reg.coef_ = w[:, :-1]
0.97 /testbed/sklearn/linear_model/logistic.py(1177):             log_reg.intercept_ = w[:, -1]
0.97 /testbed/sklearn/linear_model/logistic.py(1182):         if scoring is None:
0.97 /testbed/sklearn/linear_model/logistic.py(1183):             scores.append(log_reg.score(X_test, y_test))
0.97 /testbed/sklearn/linear_model/logistic.py(1172):     for w in coefs:
0.97 /testbed/sklearn/linear_model/logistic.py(1173):         if multi_class == 'ovr':
0.97 /testbed/sklearn/linear_model/logistic.py(1174):             w = w[np.newaxis, :]
0.97 /testbed/sklearn/linear_model/logistic.py(1175):         if fit_intercept:
0.97 /testbed/sklearn/linear_model/logistic.py(1176):             log_reg.coef_ = w[:, :-1]
0.97 /testbed/sklearn/linear_model/logistic.py(1177):             log_reg.intercept_ = w[:, -1]
0.97 /testbed/sklearn/linear_model/logistic.py(1182):         if scoring is None:
0.97 /testbed/sklearn/linear_model/logistic.py(1183):             scores.append(log_reg.score(X_test, y_test))
0.97 /testbed/sklearn/linear_model/logistic.py(1172):     for w in coefs:
0.97 /testbed/sklearn/linear_model/logistic.py(1173):         if multi_class == 'ovr':
0.97 /testbed/sklearn/linear_model/logistic.py(1174):             w = w[np.newaxis, :]
0.97 /testbed/sklearn/linear_model/logistic.py(1175):         if fit_intercept:
0.97 /testbed/sklearn/linear_model/logistic.py(1176):             log_reg.coef_ = w[:, :-1]
0.97 /testbed/sklearn/linear_model/logistic.py(1177):             log_reg.intercept_ = w[:, -1]
0.97 /testbed/sklearn/linear_model/logistic.py(1182):         if scoring is None:
0.97 /testbed/sklearn/linear_model/logistic.py(1183):             scores.append(log_reg.score(X_test, y_test))
0.97 /testbed/sklearn/linear_model/logistic.py(1172):     for w in coefs:
0.97 /testbed/sklearn/linear_model/logistic.py(1173):         if multi_class == 'ovr':
0.97 /testbed/sklearn/linear_model/logistic.py(1174):             w = w[np.newaxis, :]
0.97 /testbed/sklearn/linear_model/logistic.py(1175):         if fit_intercept:
0.97 /testbed/sklearn/linear_model/logistic.py(1176):             log_reg.coef_ = w[:, :-1]
0.97 /testbed/sklearn/linear_model/logistic.py(1177):             log_reg.intercept_ = w[:, -1]
0.97 /testbed/sklearn/linear_model/logistic.py(1182):         if scoring is None:
0.97 /testbed/sklearn/linear_model/logistic.py(1183):             scores.append(log_reg.score(X_test, y_test))
0.97 /testbed/sklearn/linear_model/logistic.py(1172):     for w in coefs:
0.97 /testbed/sklearn/linear_model/logistic.py(1173):         if multi_class == 'ovr':
0.97 /testbed/sklearn/linear_model/logistic.py(1174):             w = w[np.newaxis, :]
0.97 /testbed/sklearn/linear_model/logistic.py(1175):         if fit_intercept:
0.97 /testbed/sklearn/linear_model/logistic.py(1176):             log_reg.coef_ = w[:, :-1]
0.97 /testbed/sklearn/linear_model/logistic.py(1177):             log_reg.intercept_ = w[:, -1]
0.97 /testbed/sklearn/linear_model/logistic.py(1182):         if scoring is None:
0.97 /testbed/sklearn/linear_model/logistic.py(1183):             scores.append(log_reg.score(X_test, y_test))
0.97 /testbed/sklearn/linear_model/logistic.py(1172):     for w in coefs:
0.97 /testbed/sklearn/linear_model/logistic.py(1173):         if multi_class == 'ovr':
0.97 /testbed/sklearn/linear_model/logistic.py(1174):             w = w[np.newaxis, :]
0.97 /testbed/sklearn/linear_model/logistic.py(1175):         if fit_intercept:
0.97 /testbed/sklearn/linear_model/logistic.py(1176):             log_reg.coef_ = w[:, :-1]
0.97 /testbed/sklearn/linear_model/logistic.py(1177):             log_reg.intercept_ = w[:, -1]
0.97 /testbed/sklearn/linear_model/logistic.py(1182):         if scoring is None:
0.97 /testbed/sklearn/linear_model/logistic.py(1183):             scores.append(log_reg.score(X_test, y_test))
0.97 /testbed/sklearn/linear_model/logistic.py(1172):     for w in coefs:
0.97 /testbed/sklearn/linear_model/logistic.py(1173):         if multi_class == 'ovr':
0.97 /testbed/sklearn/linear_model/logistic.py(1174):             w = w[np.newaxis, :]
0.97 /testbed/sklearn/linear_model/logistic.py(1175):         if fit_intercept:
0.97 /testbed/sklearn/linear_model/logistic.py(1176):             log_reg.coef_ = w[:, :-1]
0.97 /testbed/sklearn/linear_model/logistic.py(1177):             log_reg.intercept_ = w[:, -1]
0.97 /testbed/sklearn/linear_model/logistic.py(1182):         if scoring is None:
0.97 /testbed/sklearn/linear_model/logistic.py(1183):             scores.append(log_reg.score(X_test, y_test))
0.97 /testbed/sklearn/linear_model/logistic.py(1172):     for w in coefs:
0.97 /testbed/sklearn/linear_model/logistic.py(1173):         if multi_class == 'ovr':
0.97 /testbed/sklearn/linear_model/logistic.py(1174):             w = w[np.newaxis, :]
0.97 /testbed/sklearn/linear_model/logistic.py(1175):         if fit_intercept:
0.97 /testbed/sklearn/linear_model/logistic.py(1176):             log_reg.coef_ = w[:, :-1]
0.97 /testbed/sklearn/linear_model/logistic.py(1177):             log_reg.intercept_ = w[:, -1]
0.97 /testbed/sklearn/linear_model/logistic.py(1182):         if scoring is None:
0.97 /testbed/sklearn/linear_model/logistic.py(1183):             scores.append(log_reg.score(X_test, y_test))
0.97 /testbed/sklearn/linear_model/logistic.py(1172):     for w in coefs:
0.97 /testbed/sklearn/linear_model/logistic.py(1187):     return coefs, Cs, np.array(scores), n_iter
0.97 /testbed/sklearn/linear_model/logistic.py(2071):             for l1_ratio in l1_ratios_)
0.97 /testbed/sklearn/linear_model/logistic.py(2070):             for train, test in folds
0.97 /testbed/sklearn/linear_model/logistic.py(2071):             for l1_ratio in l1_ratios_)
0.97 /testbed/sklearn/linear_model/logistic.py(1132):     X_train = X[train]
0.97 /testbed/sklearn/linear_model/logistic.py(1133):     X_test = X[test]
0.97 /testbed/sklearn/linear_model/logistic.py(1134):     y_train = y[train]
0.97 /testbed/sklearn/linear_model/logistic.py(1135):     y_test = y[test]
0.97 /testbed/sklearn/linear_model/logistic.py(1137):     if sample_weight is not None:
0.97 /testbed/sklearn/linear_model/logistic.py(1143):     coefs, Cs, n_iter = _logistic_regression_path(
0.97 /testbed/sklearn/linear_model/logistic.py(1144):         X_train, y_train, Cs=Cs, l1_ratio=l1_ratio,
0.97 /testbed/sklearn/linear_model/logistic.py(1145):         fit_intercept=fit_intercept, solver=solver, max_iter=max_iter,
0.97 /testbed/sklearn/linear_model/logistic.py(1146):         class_weight=class_weight, pos_class=pos_class,
0.97 /testbed/sklearn/linear_model/logistic.py(1147):         multi_class=multi_class, tol=tol, verbose=verbose, dual=dual,
0.98 /testbed/sklearn/linear_model/logistic.py(1148):         penalty=penalty, intercept_scaling=intercept_scaling,
0.98 /testbed/sklearn/linear_model/logistic.py(1149):         random_state=random_state, check_input=False,
0.98 /testbed/sklearn/linear_model/logistic.py(1150):         max_squared_sum=max_squared_sum, sample_weight=sample_weight)
0.98 /testbed/sklearn/linear_model/logistic.py(803):     if isinstance(Cs, numbers.Integral):
0.98 /testbed/sklearn/linear_model/logistic.py(804):         Cs = np.logspace(-4, 4, Cs)
0.98 /testbed/sklearn/linear_model/logistic.py(806):     solver = _check_solver(solver, penalty, dual)
0.98 /testbed/sklearn/linear_model/logistic.py(428):     all_solvers = ['liblinear', 'newton-cg', 'lbfgs', 'sag', 'saga']
0.98 /testbed/sklearn/linear_model/logistic.py(429):     if solver not in all_solvers:
0.98 /testbed/sklearn/linear_model/logistic.py(433):     all_penalties = ['l1', 'l2', 'elasticnet', 'none']
0.98 /testbed/sklearn/linear_model/logistic.py(434):     if penalty not in all_penalties:
0.98 /testbed/sklearn/linear_model/logistic.py(438):     if solver not in ['liblinear', 'saga'] and penalty not in ('l2', 'none'):
0.98 /testbed/sklearn/linear_model/logistic.py(441):     if solver != 'liblinear' and dual:
0.98 /testbed/sklearn/linear_model/logistic.py(445):     if penalty == 'elasticnet' and solver != 'saga':
0.98 /testbed/sklearn/linear_model/logistic.py(449):     if solver == 'liblinear' and penalty == 'none':
0.98 /testbed/sklearn/linear_model/logistic.py(454):     return solver
0.98 /testbed/sklearn/linear_model/logistic.py(809):     if check_input:
0.98 /testbed/sklearn/linear_model/logistic.py(814):     _, n_features = X.shape
0.98 /testbed/sklearn/linear_model/logistic.py(816):     classes = np.unique(y)
0.98 /testbed/sklearn/linear_model/logistic.py(817):     random_state = check_random_state(random_state)
0.98 /testbed/sklearn/linear_model/logistic.py(819):     multi_class = _check_multi_class(multi_class, solver, len(classes))
0.98 /testbed/sklearn/linear_model/logistic.py(458):     if multi_class == 'auto':
0.98 /testbed/sklearn/linear_model/logistic.py(465):     if multi_class not in ('multinomial', 'ovr'):
0.98 /testbed/sklearn/linear_model/logistic.py(468):     if multi_class == 'multinomial' and solver == 'liblinear':
0.98 /testbed/sklearn/linear_model/logistic.py(471):     return multi_class
0.98 /testbed/sklearn/linear_model/logistic.py(820):     if pos_class is None and multi_class != 'multinomial':
0.98 /testbed/sklearn/linear_model/logistic.py(829):     if sample_weight is not None:
0.98 /testbed/sklearn/linear_model/logistic.py(833):         sample_weight = np.ones(X.shape[0], dtype=X.dtype)
0.98 /testbed/sklearn/linear_model/logistic.py(838):     le = LabelEncoder()
0.98 /testbed/sklearn/linear_model/logistic.py(839):     if isinstance(class_weight, dict) or multi_class == 'multinomial':
0.98 /testbed/sklearn/linear_model/logistic.py(845):     if multi_class == 'ovr':
0.98 /testbed/sklearn/linear_model/logistic.py(846):         w0 = np.zeros(n_features + int(fit_intercept), dtype=X.dtype)
0.98 /testbed/sklearn/linear_model/logistic.py(847):         mask_classes = np.array([-1, 1])
0.98 /testbed/sklearn/linear_model/logistic.py(848):         mask = (y == pos_class)
0.98 /testbed/sklearn/linear_model/logistic.py(849):         y_bin = np.ones(y.shape, dtype=X.dtype)
0.98 /testbed/sklearn/linear_model/logistic.py(850):         y_bin[~mask] = -1.
0.98 /testbed/sklearn/linear_model/logistic.py(853):         if class_weight == "balanced":
0.98 /testbed/sklearn/linear_model/logistic.py(872):     if coef is not None:
0.98 /testbed/sklearn/linear_model/logistic.py(901):     if multi_class == 'multinomial':
0.98 /testbed/sklearn/linear_model/logistic.py(914):         target = y_bin
0.98 /testbed/sklearn/linear_model/logistic.py(915):         if solver == 'lbfgs':
0.98 /testbed/sklearn/linear_model/logistic.py(917):         elif solver == 'newton-cg':
0.98 /testbed/sklearn/linear_model/logistic.py(921):         warm_start_sag = {'coef': np.expand_dims(w0, axis=1)}
0.98 /testbed/sklearn/linear_model/logistic.py(923):     coefs = list()
0.98 /testbed/sklearn/linear_model/logistic.py(924):     n_iter = np.zeros(len(Cs), dtype=np.int32)
0.98 /testbed/sklearn/linear_model/logistic.py(925):     for i, C in enumerate(Cs):
0.98 /testbed/sklearn/linear_model/logistic.py(926):         if solver == 'lbfgs':
0.98 /testbed/sklearn/linear_model/logistic.py(939):         elif solver == 'newton-cg':
0.98 /testbed/sklearn/linear_model/logistic.py(943):         elif solver == 'liblinear':
0.98 /testbed/sklearn/linear_model/logistic.py(953):         elif solver in ['sag', 'saga']:
0.98 /testbed/sklearn/linear_model/logistic.py(954):             if multi_class == 'multinomial':
0.98 /testbed/sklearn/linear_model/logistic.py(958):                 loss = 'log'
0.98 /testbed/sklearn/linear_model/logistic.py(960):             if penalty == 'l1':
0.98 /testbed/sklearn/linear_model/logistic.py(963):             elif penalty == 'l2':
0.98 /testbed/sklearn/linear_model/logistic.py(964):                 alpha = 1. / C
0.98 /testbed/sklearn/linear_model/logistic.py(965):                 beta = 0.
0.98 /testbed/sklearn/linear_model/logistic.py(970):             w0, n_iter_i, warm_start_sag = sag_solver(
0.98 /testbed/sklearn/linear_model/logistic.py(971):                 X, target, sample_weight, loss, alpha,
0.98 /testbed/sklearn/linear_model/logistic.py(972):                 beta, max_iter, tol,
0.98 /testbed/sklearn/linear_model/logistic.py(973):                 verbose, random_state, False, max_squared_sum, warm_start_sag,
0.98 /testbed/sklearn/linear_model/logistic.py(974):                 is_saga=(solver == 'saga'))
0.98 /testbed/sklearn/linear_model/logistic.py(980):         if multi_class == 'multinomial':
0.98 /testbed/sklearn/linear_model/logistic.py(987):             coefs.append(w0.copy())
0.98 /testbed/sklearn/linear_model/logistic.py(989):         n_iter[i] = n_iter_i
0.98 /testbed/sklearn/linear_model/logistic.py(925):     for i, C in enumerate(Cs):
0.98 /testbed/sklearn/linear_model/logistic.py(926):         if solver == 'lbfgs':
0.98 /testbed/sklearn/linear_model/logistic.py(939):         elif solver == 'newton-cg':
0.98 /testbed/sklearn/linear_model/logistic.py(943):         elif solver == 'liblinear':
0.98 /testbed/sklearn/linear_model/logistic.py(953):         elif solver in ['sag', 'saga']:
0.98 /testbed/sklearn/linear_model/logistic.py(954):             if multi_class == 'multinomial':
0.98 /testbed/sklearn/linear_model/logistic.py(958):                 loss = 'log'
0.98 /testbed/sklearn/linear_model/logistic.py(960):             if penalty == 'l1':
0.98 /testbed/sklearn/linear_model/logistic.py(963):             elif penalty == 'l2':
0.98 /testbed/sklearn/linear_model/logistic.py(964):                 alpha = 1. / C
0.98 /testbed/sklearn/linear_model/logistic.py(965):                 beta = 0.
0.98 /testbed/sklearn/linear_model/logistic.py(970):             w0, n_iter_i, warm_start_sag = sag_solver(
0.98 /testbed/sklearn/linear_model/logistic.py(971):                 X, target, sample_weight, loss, alpha,
0.98 /testbed/sklearn/linear_model/logistic.py(972):                 beta, max_iter, tol,
0.98 /testbed/sklearn/linear_model/logistic.py(973):                 verbose, random_state, False, max_squared_sum, warm_start_sag,
0.98 /testbed/sklearn/linear_model/logistic.py(974):                 is_saga=(solver == 'saga'))
0.98 /testbed/sklearn/linear_model/logistic.py(980):         if multi_class == 'multinomial':
0.98 /testbed/sklearn/linear_model/logistic.py(987):             coefs.append(w0.copy())
0.98 /testbed/sklearn/linear_model/logistic.py(989):         n_iter[i] = n_iter_i
0.98 /testbed/sklearn/linear_model/logistic.py(925):     for i, C in enumerate(Cs):
0.98 /testbed/sklearn/linear_model/logistic.py(926):         if solver == 'lbfgs':
0.98 /testbed/sklearn/linear_model/logistic.py(939):         elif solver == 'newton-cg':
0.98 /testbed/sklearn/linear_model/logistic.py(943):         elif solver == 'liblinear':
0.98 /testbed/sklearn/linear_model/logistic.py(953):         elif solver in ['sag', 'saga']:
0.98 /testbed/sklearn/linear_model/logistic.py(954):             if multi_class == 'multinomial':
0.98 /testbed/sklearn/linear_model/logistic.py(958):                 loss = 'log'
0.98 /testbed/sklearn/linear_model/logistic.py(960):             if penalty == 'l1':
0.98 /testbed/sklearn/linear_model/logistic.py(963):             elif penalty == 'l2':
0.98 /testbed/sklearn/linear_model/logistic.py(964):                 alpha = 1. / C
0.98 /testbed/sklearn/linear_model/logistic.py(965):                 beta = 0.
0.98 /testbed/sklearn/linear_model/logistic.py(970):             w0, n_iter_i, warm_start_sag = sag_solver(
0.98 /testbed/sklearn/linear_model/logistic.py(971):                 X, target, sample_weight, loss, alpha,
0.98 /testbed/sklearn/linear_model/logistic.py(972):                 beta, max_iter, tol,
0.98 /testbed/sklearn/linear_model/logistic.py(973):                 verbose, random_state, False, max_squared_sum, warm_start_sag,
0.98 /testbed/sklearn/linear_model/logistic.py(974):                 is_saga=(solver == 'saga'))
0.98 /testbed/sklearn/linear_model/logistic.py(980):         if multi_class == 'multinomial':
0.98 /testbed/sklearn/linear_model/logistic.py(987):             coefs.append(w0.copy())
0.98 /testbed/sklearn/linear_model/logistic.py(989):         n_iter[i] = n_iter_i
0.98 /testbed/sklearn/linear_model/logistic.py(925):     for i, C in enumerate(Cs):
0.98 /testbed/sklearn/linear_model/logistic.py(926):         if solver == 'lbfgs':
0.98 /testbed/sklearn/linear_model/logistic.py(939):         elif solver == 'newton-cg':
0.98 /testbed/sklearn/linear_model/logistic.py(943):         elif solver == 'liblinear':
0.98 /testbed/sklearn/linear_model/logistic.py(953):         elif solver in ['sag', 'saga']:
0.98 /testbed/sklearn/linear_model/logistic.py(954):             if multi_class == 'multinomial':
0.98 /testbed/sklearn/linear_model/logistic.py(958):                 loss = 'log'
0.98 /testbed/sklearn/linear_model/logistic.py(960):             if penalty == 'l1':
0.98 /testbed/sklearn/linear_model/logistic.py(963):             elif penalty == 'l2':
0.98 /testbed/sklearn/linear_model/logistic.py(964):                 alpha = 1. / C
0.98 /testbed/sklearn/linear_model/logistic.py(965):                 beta = 0.
0.98 /testbed/sklearn/linear_model/logistic.py(970):             w0, n_iter_i, warm_start_sag = sag_solver(
0.98 /testbed/sklearn/linear_model/logistic.py(971):                 X, target, sample_weight, loss, alpha,
0.98 /testbed/sklearn/linear_model/logistic.py(972):                 beta, max_iter, tol,
0.98 /testbed/sklearn/linear_model/logistic.py(973):                 verbose, random_state, False, max_squared_sum, warm_start_sag,
0.98 /testbed/sklearn/linear_model/logistic.py(974):                 is_saga=(solver == 'saga'))
0.98 /testbed/sklearn/linear_model/logistic.py(980):         if multi_class == 'multinomial':
0.98 /testbed/sklearn/linear_model/logistic.py(987):             coefs.append(w0.copy())
0.98 /testbed/sklearn/linear_model/logistic.py(989):         n_iter[i] = n_iter_i
0.98 /testbed/sklearn/linear_model/logistic.py(925):     for i, C in enumerate(Cs):
0.98 /testbed/sklearn/linear_model/logistic.py(926):         if solver == 'lbfgs':
0.98 /testbed/sklearn/linear_model/logistic.py(939):         elif solver == 'newton-cg':
0.98 /testbed/sklearn/linear_model/logistic.py(943):         elif solver == 'liblinear':
0.98 /testbed/sklearn/linear_model/logistic.py(953):         elif solver in ['sag', 'saga']:
0.98 /testbed/sklearn/linear_model/logistic.py(954):             if multi_class == 'multinomial':
0.98 /testbed/sklearn/linear_model/logistic.py(958):                 loss = 'log'
0.98 /testbed/sklearn/linear_model/logistic.py(960):             if penalty == 'l1':
0.98 /testbed/sklearn/linear_model/logistic.py(963):             elif penalty == 'l2':
0.98 /testbed/sklearn/linear_model/logistic.py(964):                 alpha = 1. / C
0.98 /testbed/sklearn/linear_model/logistic.py(965):                 beta = 0.
0.98 /testbed/sklearn/linear_model/logistic.py(970):             w0, n_iter_i, warm_start_sag = sag_solver(
0.98 /testbed/sklearn/linear_model/logistic.py(971):                 X, target, sample_weight, loss, alpha,
0.98 /testbed/sklearn/linear_model/logistic.py(972):                 beta, max_iter, tol,
0.98 /testbed/sklearn/linear_model/logistic.py(973):                 verbose, random_state, False, max_squared_sum, warm_start_sag,
0.98 /testbed/sklearn/linear_model/logistic.py(974):                 is_saga=(solver == 'saga'))
0.98 /testbed/sklearn/linear_model/logistic.py(980):         if multi_class == 'multinomial':
0.98 /testbed/sklearn/linear_model/logistic.py(987):             coefs.append(w0.copy())
0.98 /testbed/sklearn/linear_model/logistic.py(989):         n_iter[i] = n_iter_i
0.98 /testbed/sklearn/linear_model/logistic.py(925):     for i, C in enumerate(Cs):
0.98 /testbed/sklearn/linear_model/logistic.py(926):         if solver == 'lbfgs':
0.98 /testbed/sklearn/linear_model/logistic.py(939):         elif solver == 'newton-cg':
0.98 /testbed/sklearn/linear_model/logistic.py(943):         elif solver == 'liblinear':
0.98 /testbed/sklearn/linear_model/logistic.py(953):         elif solver in ['sag', 'saga']:
0.98 /testbed/sklearn/linear_model/logistic.py(954):             if multi_class == 'multinomial':
0.98 /testbed/sklearn/linear_model/logistic.py(958):                 loss = 'log'
0.98 /testbed/sklearn/linear_model/logistic.py(960):             if penalty == 'l1':
0.98 /testbed/sklearn/linear_model/logistic.py(963):             elif penalty == 'l2':
0.98 /testbed/sklearn/linear_model/logistic.py(964):                 alpha = 1. / C
0.98 /testbed/sklearn/linear_model/logistic.py(965):                 beta = 0.
0.98 /testbed/sklearn/linear_model/logistic.py(970):             w0, n_iter_i, warm_start_sag = sag_solver(
0.98 /testbed/sklearn/linear_model/logistic.py(971):                 X, target, sample_weight, loss, alpha,
0.98 /testbed/sklearn/linear_model/logistic.py(972):                 beta, max_iter, tol,
0.98 /testbed/sklearn/linear_model/logistic.py(973):                 verbose, random_state, False, max_squared_sum, warm_start_sag,
0.98 /testbed/sklearn/linear_model/logistic.py(974):                 is_saga=(solver == 'saga'))
0.98 /testbed/sklearn/linear_model/logistic.py(980):         if multi_class == 'multinomial':
0.98 /testbed/sklearn/linear_model/logistic.py(987):             coefs.append(w0.copy())
0.98 /testbed/sklearn/linear_model/logistic.py(989):         n_iter[i] = n_iter_i
0.98 /testbed/sklearn/linear_model/logistic.py(925):     for i, C in enumerate(Cs):
0.98 /testbed/sklearn/linear_model/logistic.py(926):         if solver == 'lbfgs':
0.98 /testbed/sklearn/linear_model/logistic.py(939):         elif solver == 'newton-cg':
0.98 /testbed/sklearn/linear_model/logistic.py(943):         elif solver == 'liblinear':
0.98 /testbed/sklearn/linear_model/logistic.py(953):         elif solver in ['sag', 'saga']:
0.98 /testbed/sklearn/linear_model/logistic.py(954):             if multi_class == 'multinomial':
0.98 /testbed/sklearn/linear_model/logistic.py(958):                 loss = 'log'
0.98 /testbed/sklearn/linear_model/logistic.py(960):             if penalty == 'l1':
0.98 /testbed/sklearn/linear_model/logistic.py(963):             elif penalty == 'l2':
0.98 /testbed/sklearn/linear_model/logistic.py(964):                 alpha = 1. / C
0.98 /testbed/sklearn/linear_model/logistic.py(965):                 beta = 0.
0.98 /testbed/sklearn/linear_model/logistic.py(970):             w0, n_iter_i, warm_start_sag = sag_solver(
0.98 /testbed/sklearn/linear_model/logistic.py(971):                 X, target, sample_weight, loss, alpha,
0.98 /testbed/sklearn/linear_model/logistic.py(972):                 beta, max_iter, tol,
0.98 /testbed/sklearn/linear_model/logistic.py(973):                 verbose, random_state, False, max_squared_sum, warm_start_sag,
0.98 /testbed/sklearn/linear_model/logistic.py(974):                 is_saga=(solver == 'saga'))
0.98 /testbed/sklearn/linear_model/logistic.py(980):         if multi_class == 'multinomial':
0.98 /testbed/sklearn/linear_model/logistic.py(987):             coefs.append(w0.copy())
0.98 /testbed/sklearn/linear_model/logistic.py(989):         n_iter[i] = n_iter_i
0.98 /testbed/sklearn/linear_model/logistic.py(925):     for i, C in enumerate(Cs):
0.98 /testbed/sklearn/linear_model/logistic.py(926):         if solver == 'lbfgs':
0.98 /testbed/sklearn/linear_model/logistic.py(939):         elif solver == 'newton-cg':
0.98 /testbed/sklearn/linear_model/logistic.py(943):         elif solver == 'liblinear':
0.98 /testbed/sklearn/linear_model/logistic.py(953):         elif solver in ['sag', 'saga']:
0.98 /testbed/sklearn/linear_model/logistic.py(954):             if multi_class == 'multinomial':
0.98 /testbed/sklearn/linear_model/logistic.py(958):                 loss = 'log'
0.98 /testbed/sklearn/linear_model/logistic.py(960):             if penalty == 'l1':
0.98 /testbed/sklearn/linear_model/logistic.py(963):             elif penalty == 'l2':
0.98 /testbed/sklearn/linear_model/logistic.py(964):                 alpha = 1. / C
0.98 /testbed/sklearn/linear_model/logistic.py(965):                 beta = 0.
0.98 /testbed/sklearn/linear_model/logistic.py(970):             w0, n_iter_i, warm_start_sag = sag_solver(
0.98 /testbed/sklearn/linear_model/logistic.py(971):                 X, target, sample_weight, loss, alpha,
0.98 /testbed/sklearn/linear_model/logistic.py(972):                 beta, max_iter, tol,
0.98 /testbed/sklearn/linear_model/logistic.py(973):                 verbose, random_state, False, max_squared_sum, warm_start_sag,
0.98 /testbed/sklearn/linear_model/logistic.py(974):                 is_saga=(solver == 'saga'))
0.98 /testbed/sklearn/linear_model/logistic.py(980):         if multi_class == 'multinomial':
0.98 /testbed/sklearn/linear_model/logistic.py(987):             coefs.append(w0.copy())
0.98 /testbed/sklearn/linear_model/logistic.py(989):         n_iter[i] = n_iter_i
0.98 /testbed/sklearn/linear_model/logistic.py(925):     for i, C in enumerate(Cs):
0.98 /testbed/sklearn/linear_model/logistic.py(926):         if solver == 'lbfgs':
0.98 /testbed/sklearn/linear_model/logistic.py(939):         elif solver == 'newton-cg':
0.98 /testbed/sklearn/linear_model/logistic.py(943):         elif solver == 'liblinear':
0.98 /testbed/sklearn/linear_model/logistic.py(953):         elif solver in ['sag', 'saga']:
0.98 /testbed/sklearn/linear_model/logistic.py(954):             if multi_class == 'multinomial':
0.98 /testbed/sklearn/linear_model/logistic.py(958):                 loss = 'log'
0.98 /testbed/sklearn/linear_model/logistic.py(960):             if penalty == 'l1':
0.98 /testbed/sklearn/linear_model/logistic.py(963):             elif penalty == 'l2':
0.98 /testbed/sklearn/linear_model/logistic.py(964):                 alpha = 1. / C
0.98 /testbed/sklearn/linear_model/logistic.py(965):                 beta = 0.
0.98 /testbed/sklearn/linear_model/logistic.py(970):             w0, n_iter_i, warm_start_sag = sag_solver(
0.98 /testbed/sklearn/linear_model/logistic.py(971):                 X, target, sample_weight, loss, alpha,
0.98 /testbed/sklearn/linear_model/logistic.py(972):                 beta, max_iter, tol,
0.98 /testbed/sklearn/linear_model/logistic.py(973):                 verbose, random_state, False, max_squared_sum, warm_start_sag,
0.98 /testbed/sklearn/linear_model/logistic.py(974):                 is_saga=(solver == 'saga'))
0.98 /testbed/sklearn/linear_model/logistic.py(980):         if multi_class == 'multinomial':
0.98 /testbed/sklearn/linear_model/logistic.py(987):             coefs.append(w0.copy())
0.98 /testbed/sklearn/linear_model/logistic.py(989):         n_iter[i] = n_iter_i
0.98 /testbed/sklearn/linear_model/logistic.py(925):     for i, C in enumerate(Cs):
0.98 /testbed/sklearn/linear_model/logistic.py(926):         if solver == 'lbfgs':
0.98 /testbed/sklearn/linear_model/logistic.py(939):         elif solver == 'newton-cg':
0.98 /testbed/sklearn/linear_model/logistic.py(943):         elif solver == 'liblinear':
0.98 /testbed/sklearn/linear_model/logistic.py(953):         elif solver in ['sag', 'saga']:
0.98 /testbed/sklearn/linear_model/logistic.py(954):             if multi_class == 'multinomial':
0.98 /testbed/sklearn/linear_model/logistic.py(958):                 loss = 'log'
0.98 /testbed/sklearn/linear_model/logistic.py(960):             if penalty == 'l1':
0.98 /testbed/sklearn/linear_model/logistic.py(963):             elif penalty == 'l2':
0.98 /testbed/sklearn/linear_model/logistic.py(964):                 alpha = 1. / C
0.98 /testbed/sklearn/linear_model/logistic.py(965):                 beta = 0.
0.98 /testbed/sklearn/linear_model/logistic.py(970):             w0, n_iter_i, warm_start_sag = sag_solver(
0.98 /testbed/sklearn/linear_model/logistic.py(971):                 X, target, sample_weight, loss, alpha,
0.98 /testbed/sklearn/linear_model/logistic.py(972):                 beta, max_iter, tol,
0.98 /testbed/sklearn/linear_model/logistic.py(973):                 verbose, random_state, False, max_squared_sum, warm_start_sag,
0.98 /testbed/sklearn/linear_model/logistic.py(974):                 is_saga=(solver == 'saga'))
0.98 /testbed/sklearn/linear_model/logistic.py(980):         if multi_class == 'multinomial':
0.98 /testbed/sklearn/linear_model/logistic.py(987):             coefs.append(w0.copy())
0.98 /testbed/sklearn/linear_model/logistic.py(989):         n_iter[i] = n_iter_i
0.98 /testbed/sklearn/linear_model/logistic.py(925):     for i, C in enumerate(Cs):
0.98 /testbed/sklearn/linear_model/logistic.py(991):     return np.array(coefs), np.array(Cs), n_iter
0.98 /testbed/sklearn/linear_model/logistic.py(1152):     log_reg = LogisticRegression(solver=solver, multi_class=multi_class)
0.98 /testbed/sklearn/linear_model/logistic.py(1437):         self.penalty = penalty
0.98 /testbed/sklearn/linear_model/logistic.py(1438):         self.dual = dual
0.98 /testbed/sklearn/linear_model/logistic.py(1439):         self.tol = tol
0.98 /testbed/sklearn/linear_model/logistic.py(1440):         self.C = C
0.98 /testbed/sklearn/linear_model/logistic.py(1441):         self.fit_intercept = fit_intercept
0.98 /testbed/sklearn/linear_model/logistic.py(1442):         self.intercept_scaling = intercept_scaling
0.98 /testbed/sklearn/linear_model/logistic.py(1443):         self.class_weight = class_weight
0.98 /testbed/sklearn/linear_model/logistic.py(1444):         self.random_state = random_state
0.98 /testbed/sklearn/linear_model/logistic.py(1445):         self.solver = solver
0.98 /testbed/sklearn/linear_model/logistic.py(1446):         self.max_iter = max_iter
0.98 /testbed/sklearn/linear_model/logistic.py(1447):         self.multi_class = multi_class
0.98 /testbed/sklearn/linear_model/logistic.py(1448):         self.verbose = verbose
0.98 /testbed/sklearn/linear_model/logistic.py(1449):         self.warm_start = warm_start
0.98 /testbed/sklearn/linear_model/logistic.py(1450):         self.n_jobs = n_jobs
0.98 /testbed/sklearn/linear_model/logistic.py(1451):         self.l1_ratio = l1_ratio
0.98 /testbed/sklearn/linear_model/logistic.py(1155):     if multi_class == 'ovr':
0.98 /testbed/sklearn/linear_model/logistic.py(1156):         log_reg.classes_ = np.array([-1, 1])
0.98 /testbed/sklearn/linear_model/logistic.py(1163):     if pos_class is not None:
0.98 /testbed/sklearn/linear_model/logistic.py(1164):         mask = (y_test == pos_class)
0.98 /testbed/sklearn/linear_model/logistic.py(1165):         y_test = np.ones(y_test.shape, dtype=np.float64)
0.98 /testbed/sklearn/linear_model/logistic.py(1166):         y_test[~mask] = -1.
0.98 /testbed/sklearn/linear_model/logistic.py(1168):     scores = list()
0.98 /testbed/sklearn/linear_model/logistic.py(1170):     if isinstance(scoring, str):
0.98 /testbed/sklearn/linear_model/logistic.py(1172):     for w in coefs:
0.98 /testbed/sklearn/linear_model/logistic.py(1173):         if multi_class == 'ovr':
0.98 /testbed/sklearn/linear_model/logistic.py(1174):             w = w[np.newaxis, :]
0.98 /testbed/sklearn/linear_model/logistic.py(1175):         if fit_intercept:
0.98 /testbed/sklearn/linear_model/logistic.py(1176):             log_reg.coef_ = w[:, :-1]
0.98 /testbed/sklearn/linear_model/logistic.py(1177):             log_reg.intercept_ = w[:, -1]
0.98 /testbed/sklearn/linear_model/logistic.py(1182):         if scoring is None:
0.98 /testbed/sklearn/linear_model/logistic.py(1183):             scores.append(log_reg.score(X_test, y_test))
0.98 /testbed/sklearn/linear_model/logistic.py(1172):     for w in coefs:
0.98 /testbed/sklearn/linear_model/logistic.py(1173):         if multi_class == 'ovr':
0.98 /testbed/sklearn/linear_model/logistic.py(1174):             w = w[np.newaxis, :]
0.98 /testbed/sklearn/linear_model/logistic.py(1175):         if fit_intercept:
0.98 /testbed/sklearn/linear_model/logistic.py(1176):             log_reg.coef_ = w[:, :-1]
0.98 /testbed/sklearn/linear_model/logistic.py(1177):             log_reg.intercept_ = w[:, -1]
0.98 /testbed/sklearn/linear_model/logistic.py(1182):         if scoring is None:
0.98 /testbed/sklearn/linear_model/logistic.py(1183):             scores.append(log_reg.score(X_test, y_test))
0.98 /testbed/sklearn/linear_model/logistic.py(1172):     for w in coefs:
0.98 /testbed/sklearn/linear_model/logistic.py(1173):         if multi_class == 'ovr':
0.98 /testbed/sklearn/linear_model/logistic.py(1174):             w = w[np.newaxis, :]
0.98 /testbed/sklearn/linear_model/logistic.py(1175):         if fit_intercept:
0.98 /testbed/sklearn/linear_model/logistic.py(1176):             log_reg.coef_ = w[:, :-1]
0.98 /testbed/sklearn/linear_model/logistic.py(1177):             log_reg.intercept_ = w[:, -1]
0.98 /testbed/sklearn/linear_model/logistic.py(1182):         if scoring is None:
0.98 /testbed/sklearn/linear_model/logistic.py(1183):             scores.append(log_reg.score(X_test, y_test))
0.98 /testbed/sklearn/linear_model/logistic.py(1172):     for w in coefs:
0.98 /testbed/sklearn/linear_model/logistic.py(1173):         if multi_class == 'ovr':
0.98 /testbed/sklearn/linear_model/logistic.py(1174):             w = w[np.newaxis, :]
0.98 /testbed/sklearn/linear_model/logistic.py(1175):         if fit_intercept:
0.98 /testbed/sklearn/linear_model/logistic.py(1176):             log_reg.coef_ = w[:, :-1]
0.98 /testbed/sklearn/linear_model/logistic.py(1177):             log_reg.intercept_ = w[:, -1]
0.98 /testbed/sklearn/linear_model/logistic.py(1182):         if scoring is None:
0.98 /testbed/sklearn/linear_model/logistic.py(1183):             scores.append(log_reg.score(X_test, y_test))
0.98 /testbed/sklearn/linear_model/logistic.py(1172):     for w in coefs:
0.98 /testbed/sklearn/linear_model/logistic.py(1173):         if multi_class == 'ovr':
0.99 /testbed/sklearn/linear_model/logistic.py(1174):             w = w[np.newaxis, :]
0.99 /testbed/sklearn/linear_model/logistic.py(1175):         if fit_intercept:
0.99 /testbed/sklearn/linear_model/logistic.py(1176):             log_reg.coef_ = w[:, :-1]
0.99 /testbed/sklearn/linear_model/logistic.py(1177):             log_reg.intercept_ = w[:, -1]
0.99 /testbed/sklearn/linear_model/logistic.py(1182):         if scoring is None:
0.99 /testbed/sklearn/linear_model/logistic.py(1183):             scores.append(log_reg.score(X_test, y_test))
0.99 /testbed/sklearn/linear_model/logistic.py(1172):     for w in coefs:
0.99 /testbed/sklearn/linear_model/logistic.py(1173):         if multi_class == 'ovr':
0.99 /testbed/sklearn/linear_model/logistic.py(1174):             w = w[np.newaxis, :]
0.99 /testbed/sklearn/linear_model/logistic.py(1175):         if fit_intercept:
0.99 /testbed/sklearn/linear_model/logistic.py(1176):             log_reg.coef_ = w[:, :-1]
0.99 /testbed/sklearn/linear_model/logistic.py(1177):             log_reg.intercept_ = w[:, -1]
0.99 /testbed/sklearn/linear_model/logistic.py(1182):         if scoring is None:
0.99 /testbed/sklearn/linear_model/logistic.py(1183):             scores.append(log_reg.score(X_test, y_test))
0.99 /testbed/sklearn/linear_model/logistic.py(1172):     for w in coefs:
0.99 /testbed/sklearn/linear_model/logistic.py(1173):         if multi_class == 'ovr':
0.99 /testbed/sklearn/linear_model/logistic.py(1174):             w = w[np.newaxis, :]
0.99 /testbed/sklearn/linear_model/logistic.py(1175):         if fit_intercept:
0.99 /testbed/sklearn/linear_model/logistic.py(1176):             log_reg.coef_ = w[:, :-1]
0.99 /testbed/sklearn/linear_model/logistic.py(1177):             log_reg.intercept_ = w[:, -1]
0.99 /testbed/sklearn/linear_model/logistic.py(1182):         if scoring is None:
0.99 /testbed/sklearn/linear_model/logistic.py(1183):             scores.append(log_reg.score(X_test, y_test))
0.99 /testbed/sklearn/linear_model/logistic.py(1172):     for w in coefs:
0.99 /testbed/sklearn/linear_model/logistic.py(1173):         if multi_class == 'ovr':
0.99 /testbed/sklearn/linear_model/logistic.py(1174):             w = w[np.newaxis, :]
0.99 /testbed/sklearn/linear_model/logistic.py(1175):         if fit_intercept:
0.99 /testbed/sklearn/linear_model/logistic.py(1176):             log_reg.coef_ = w[:, :-1]
0.99 /testbed/sklearn/linear_model/logistic.py(1177):             log_reg.intercept_ = w[:, -1]
0.99 /testbed/sklearn/linear_model/logistic.py(1182):         if scoring is None:
0.99 /testbed/sklearn/linear_model/logistic.py(1183):             scores.append(log_reg.score(X_test, y_test))
0.99 /testbed/sklearn/linear_model/logistic.py(1172):     for w in coefs:
0.99 /testbed/sklearn/linear_model/logistic.py(1173):         if multi_class == 'ovr':
0.99 /testbed/sklearn/linear_model/logistic.py(1174):             w = w[np.newaxis, :]
0.99 /testbed/sklearn/linear_model/logistic.py(1175):         if fit_intercept:
0.99 /testbed/sklearn/linear_model/logistic.py(1176):             log_reg.coef_ = w[:, :-1]
0.99 /testbed/sklearn/linear_model/logistic.py(1177):             log_reg.intercept_ = w[:, -1]
0.99 /testbed/sklearn/linear_model/logistic.py(1182):         if scoring is None:
0.99 /testbed/sklearn/linear_model/logistic.py(1183):             scores.append(log_reg.score(X_test, y_test))
0.99 /testbed/sklearn/linear_model/logistic.py(1172):     for w in coefs:
0.99 /testbed/sklearn/linear_model/logistic.py(1173):         if multi_class == 'ovr':
0.99 /testbed/sklearn/linear_model/logistic.py(1174):             w = w[np.newaxis, :]
0.99 /testbed/sklearn/linear_model/logistic.py(1175):         if fit_intercept:
0.99 /testbed/sklearn/linear_model/logistic.py(1176):             log_reg.coef_ = w[:, :-1]
0.99 /testbed/sklearn/linear_model/logistic.py(1177):             log_reg.intercept_ = w[:, -1]
0.99 /testbed/sklearn/linear_model/logistic.py(1182):         if scoring is None:
0.99 /testbed/sklearn/linear_model/logistic.py(1183):             scores.append(log_reg.score(X_test, y_test))
0.99 /testbed/sklearn/linear_model/logistic.py(1172):     for w in coefs:
0.99 /testbed/sklearn/linear_model/logistic.py(1187):     return coefs, Cs, np.array(scores), n_iter
0.99 /testbed/sklearn/linear_model/logistic.py(2071):             for l1_ratio in l1_ratios_)
0.99 /testbed/sklearn/linear_model/logistic.py(2070):             for train, test in folds
0.99 /testbed/sklearn/linear_model/logistic.py(2071):             for l1_ratio in l1_ratios_)
0.99 /testbed/sklearn/linear_model/logistic.py(1132):     X_train = X[train]
0.99 /testbed/sklearn/linear_model/logistic.py(1133):     X_test = X[test]
0.99 /testbed/sklearn/linear_model/logistic.py(1134):     y_train = y[train]
0.99 /testbed/sklearn/linear_model/logistic.py(1135):     y_test = y[test]
0.99 /testbed/sklearn/linear_model/logistic.py(1137):     if sample_weight is not None:
0.99 /testbed/sklearn/linear_model/logistic.py(1143):     coefs, Cs, n_iter = _logistic_regression_path(
0.99 /testbed/sklearn/linear_model/logistic.py(1144):         X_train, y_train, Cs=Cs, l1_ratio=l1_ratio,
0.99 /testbed/sklearn/linear_model/logistic.py(1145):         fit_intercept=fit_intercept, solver=solver, max_iter=max_iter,
0.99 /testbed/sklearn/linear_model/logistic.py(1146):         class_weight=class_weight, pos_class=pos_class,
0.99 /testbed/sklearn/linear_model/logistic.py(1147):         multi_class=multi_class, tol=tol, verbose=verbose, dual=dual,
0.99 /testbed/sklearn/linear_model/logistic.py(1148):         penalty=penalty, intercept_scaling=intercept_scaling,
0.99 /testbed/sklearn/linear_model/logistic.py(1149):         random_state=random_state, check_input=False,
0.99 /testbed/sklearn/linear_model/logistic.py(1150):         max_squared_sum=max_squared_sum, sample_weight=sample_weight)
0.99 /testbed/sklearn/linear_model/logistic.py(803):     if isinstance(Cs, numbers.Integral):
0.99 /testbed/sklearn/linear_model/logistic.py(804):         Cs = np.logspace(-4, 4, Cs)
0.99 /testbed/sklearn/linear_model/logistic.py(806):     solver = _check_solver(solver, penalty, dual)
0.99 /testbed/sklearn/linear_model/logistic.py(428):     all_solvers = ['liblinear', 'newton-cg', 'lbfgs', 'sag', 'saga']
0.99 /testbed/sklearn/linear_model/logistic.py(429):     if solver not in all_solvers:
0.99 /testbed/sklearn/linear_model/logistic.py(433):     all_penalties = ['l1', 'l2', 'elasticnet', 'none']
0.99 /testbed/sklearn/linear_model/logistic.py(434):     if penalty not in all_penalties:
0.99 /testbed/sklearn/linear_model/logistic.py(438):     if solver not in ['liblinear', 'saga'] and penalty not in ('l2', 'none'):
0.99 /testbed/sklearn/linear_model/logistic.py(441):     if solver != 'liblinear' and dual:
0.99 /testbed/sklearn/linear_model/logistic.py(445):     if penalty == 'elasticnet' and solver != 'saga':
0.99 /testbed/sklearn/linear_model/logistic.py(449):     if solver == 'liblinear' and penalty == 'none':
0.99 /testbed/sklearn/linear_model/logistic.py(454):     return solver
0.99 /testbed/sklearn/linear_model/logistic.py(809):     if check_input:
0.99 /testbed/sklearn/linear_model/logistic.py(814):     _, n_features = X.shape
0.99 /testbed/sklearn/linear_model/logistic.py(816):     classes = np.unique(y)
0.99 /testbed/sklearn/linear_model/logistic.py(817):     random_state = check_random_state(random_state)
0.99 /testbed/sklearn/linear_model/logistic.py(819):     multi_class = _check_multi_class(multi_class, solver, len(classes))
0.99 /testbed/sklearn/linear_model/logistic.py(458):     if multi_class == 'auto':
0.99 /testbed/sklearn/linear_model/logistic.py(465):     if multi_class not in ('multinomial', 'ovr'):
0.99 /testbed/sklearn/linear_model/logistic.py(468):     if multi_class == 'multinomial' and solver == 'liblinear':
0.99 /testbed/sklearn/linear_model/logistic.py(471):     return multi_class
0.99 /testbed/sklearn/linear_model/logistic.py(820):     if pos_class is None and multi_class != 'multinomial':
0.99 /testbed/sklearn/linear_model/logistic.py(829):     if sample_weight is not None:
0.99 /testbed/sklearn/linear_model/logistic.py(833):         sample_weight = np.ones(X.shape[0], dtype=X.dtype)
0.99 /testbed/sklearn/linear_model/logistic.py(838):     le = LabelEncoder()
0.99 /testbed/sklearn/linear_model/logistic.py(839):     if isinstance(class_weight, dict) or multi_class == 'multinomial':
0.99 /testbed/sklearn/linear_model/logistic.py(845):     if multi_class == 'ovr':
0.99 /testbed/sklearn/linear_model/logistic.py(846):         w0 = np.zeros(n_features + int(fit_intercept), dtype=X.dtype)
0.99 /testbed/sklearn/linear_model/logistic.py(847):         mask_classes = np.array([-1, 1])
0.99 /testbed/sklearn/linear_model/logistic.py(848):         mask = (y == pos_class)
0.99 /testbed/sklearn/linear_model/logistic.py(849):         y_bin = np.ones(y.shape, dtype=X.dtype)
0.99 /testbed/sklearn/linear_model/logistic.py(850):         y_bin[~mask] = -1.
0.99 /testbed/sklearn/linear_model/logistic.py(853):         if class_weight == "balanced":
0.99 /testbed/sklearn/linear_model/logistic.py(872):     if coef is not None:
0.99 /testbed/sklearn/linear_model/logistic.py(901):     if multi_class == 'multinomial':
0.99 /testbed/sklearn/linear_model/logistic.py(914):         target = y_bin
0.99 /testbed/sklearn/linear_model/logistic.py(915):         if solver == 'lbfgs':
0.99 /testbed/sklearn/linear_model/logistic.py(917):         elif solver == 'newton-cg':
0.99 /testbed/sklearn/linear_model/logistic.py(921):         warm_start_sag = {'coef': np.expand_dims(w0, axis=1)}
0.99 /testbed/sklearn/linear_model/logistic.py(923):     coefs = list()
0.99 /testbed/sklearn/linear_model/logistic.py(924):     n_iter = np.zeros(len(Cs), dtype=np.int32)
0.99 /testbed/sklearn/linear_model/logistic.py(925):     for i, C in enumerate(Cs):
0.99 /testbed/sklearn/linear_model/logistic.py(926):         if solver == 'lbfgs':
0.99 /testbed/sklearn/linear_model/logistic.py(939):         elif solver == 'newton-cg':
0.99 /testbed/sklearn/linear_model/logistic.py(943):         elif solver == 'liblinear':
0.99 /testbed/sklearn/linear_model/logistic.py(953):         elif solver in ['sag', 'saga']:
0.99 /testbed/sklearn/linear_model/logistic.py(954):             if multi_class == 'multinomial':
0.99 /testbed/sklearn/linear_model/logistic.py(958):                 loss = 'log'
0.99 /testbed/sklearn/linear_model/logistic.py(960):             if penalty == 'l1':
0.99 /testbed/sklearn/linear_model/logistic.py(963):             elif penalty == 'l2':
0.99 /testbed/sklearn/linear_model/logistic.py(964):                 alpha = 1. / C
0.99 /testbed/sklearn/linear_model/logistic.py(965):                 beta = 0.
0.99 /testbed/sklearn/linear_model/logistic.py(970):             w0, n_iter_i, warm_start_sag = sag_solver(
0.99 /testbed/sklearn/linear_model/logistic.py(971):                 X, target, sample_weight, loss, alpha,
0.99 /testbed/sklearn/linear_model/logistic.py(972):                 beta, max_iter, tol,
0.99 /testbed/sklearn/linear_model/logistic.py(973):                 verbose, random_state, False, max_squared_sum, warm_start_sag,
0.99 /testbed/sklearn/linear_model/logistic.py(974):                 is_saga=(solver == 'saga'))
0.99 /testbed/sklearn/linear_model/logistic.py(980):         if multi_class == 'multinomial':
0.99 /testbed/sklearn/linear_model/logistic.py(987):             coefs.append(w0.copy())
0.99 /testbed/sklearn/linear_model/logistic.py(989):         n_iter[i] = n_iter_i
0.99 /testbed/sklearn/linear_model/logistic.py(925):     for i, C in enumerate(Cs):
0.99 /testbed/sklearn/linear_model/logistic.py(926):         if solver == 'lbfgs':
0.99 /testbed/sklearn/linear_model/logistic.py(939):         elif solver == 'newton-cg':
0.99 /testbed/sklearn/linear_model/logistic.py(943):         elif solver == 'liblinear':
0.99 /testbed/sklearn/linear_model/logistic.py(953):         elif solver in ['sag', 'saga']:
0.99 /testbed/sklearn/linear_model/logistic.py(954):             if multi_class == 'multinomial':
0.99 /testbed/sklearn/linear_model/logistic.py(958):                 loss = 'log'
0.99 /testbed/sklearn/linear_model/logistic.py(960):             if penalty == 'l1':
0.99 /testbed/sklearn/linear_model/logistic.py(963):             elif penalty == 'l2':
0.99 /testbed/sklearn/linear_model/logistic.py(964):                 alpha = 1. / C
0.99 /testbed/sklearn/linear_model/logistic.py(965):                 beta = 0.
0.99 /testbed/sklearn/linear_model/logistic.py(970):             w0, n_iter_i, warm_start_sag = sag_solver(
0.99 /testbed/sklearn/linear_model/logistic.py(971):                 X, target, sample_weight, loss, alpha,
0.99 /testbed/sklearn/linear_model/logistic.py(972):                 beta, max_iter, tol,
0.99 /testbed/sklearn/linear_model/logistic.py(973):                 verbose, random_state, False, max_squared_sum, warm_start_sag,
0.99 /testbed/sklearn/linear_model/logistic.py(974):                 is_saga=(solver == 'saga'))
0.99 /testbed/sklearn/linear_model/logistic.py(980):         if multi_class == 'multinomial':
0.99 /testbed/sklearn/linear_model/logistic.py(987):             coefs.append(w0.copy())
0.99 /testbed/sklearn/linear_model/logistic.py(989):         n_iter[i] = n_iter_i
0.99 /testbed/sklearn/linear_model/logistic.py(925):     for i, C in enumerate(Cs):
0.99 /testbed/sklearn/linear_model/logistic.py(926):         if solver == 'lbfgs':
0.99 /testbed/sklearn/linear_model/logistic.py(939):         elif solver == 'newton-cg':
0.99 /testbed/sklearn/linear_model/logistic.py(943):         elif solver == 'liblinear':
0.99 /testbed/sklearn/linear_model/logistic.py(953):         elif solver in ['sag', 'saga']:
0.99 /testbed/sklearn/linear_model/logistic.py(954):             if multi_class == 'multinomial':
0.99 /testbed/sklearn/linear_model/logistic.py(958):                 loss = 'log'
0.99 /testbed/sklearn/linear_model/logistic.py(960):             if penalty == 'l1':
0.99 /testbed/sklearn/linear_model/logistic.py(963):             elif penalty == 'l2':
0.99 /testbed/sklearn/linear_model/logistic.py(964):                 alpha = 1. / C
0.99 /testbed/sklearn/linear_model/logistic.py(965):                 beta = 0.
0.99 /testbed/sklearn/linear_model/logistic.py(970):             w0, n_iter_i, warm_start_sag = sag_solver(
0.99 /testbed/sklearn/linear_model/logistic.py(971):                 X, target, sample_weight, loss, alpha,
0.99 /testbed/sklearn/linear_model/logistic.py(972):                 beta, max_iter, tol,
0.99 /testbed/sklearn/linear_model/logistic.py(973):                 verbose, random_state, False, max_squared_sum, warm_start_sag,
0.99 /testbed/sklearn/linear_model/logistic.py(974):                 is_saga=(solver == 'saga'))
0.99 /testbed/sklearn/linear_model/logistic.py(980):         if multi_class == 'multinomial':
0.99 /testbed/sklearn/linear_model/logistic.py(987):             coefs.append(w0.copy())
0.99 /testbed/sklearn/linear_model/logistic.py(989):         n_iter[i] = n_iter_i
0.99 /testbed/sklearn/linear_model/logistic.py(925):     for i, C in enumerate(Cs):
0.99 /testbed/sklearn/linear_model/logistic.py(926):         if solver == 'lbfgs':
0.99 /testbed/sklearn/linear_model/logistic.py(939):         elif solver == 'newton-cg':
0.99 /testbed/sklearn/linear_model/logistic.py(943):         elif solver == 'liblinear':
0.99 /testbed/sklearn/linear_model/logistic.py(953):         elif solver in ['sag', 'saga']:
0.99 /testbed/sklearn/linear_model/logistic.py(954):             if multi_class == 'multinomial':
0.99 /testbed/sklearn/linear_model/logistic.py(958):                 loss = 'log'
0.99 /testbed/sklearn/linear_model/logistic.py(960):             if penalty == 'l1':
0.99 /testbed/sklearn/linear_model/logistic.py(963):             elif penalty == 'l2':
0.99 /testbed/sklearn/linear_model/logistic.py(964):                 alpha = 1. / C
0.99 /testbed/sklearn/linear_model/logistic.py(965):                 beta = 0.
0.99 /testbed/sklearn/linear_model/logistic.py(970):             w0, n_iter_i, warm_start_sag = sag_solver(
0.99 /testbed/sklearn/linear_model/logistic.py(971):                 X, target, sample_weight, loss, alpha,
0.99 /testbed/sklearn/linear_model/logistic.py(972):                 beta, max_iter, tol,
0.99 /testbed/sklearn/linear_model/logistic.py(973):                 verbose, random_state, False, max_squared_sum, warm_start_sag,
0.99 /testbed/sklearn/linear_model/logistic.py(974):                 is_saga=(solver == 'saga'))
0.99 /testbed/sklearn/linear_model/logistic.py(980):         if multi_class == 'multinomial':
0.99 /testbed/sklearn/linear_model/logistic.py(987):             coefs.append(w0.copy())
0.99 /testbed/sklearn/linear_model/logistic.py(989):         n_iter[i] = n_iter_i
0.99 /testbed/sklearn/linear_model/logistic.py(925):     for i, C in enumerate(Cs):
0.99 /testbed/sklearn/linear_model/logistic.py(926):         if solver == 'lbfgs':
0.99 /testbed/sklearn/linear_model/logistic.py(939):         elif solver == 'newton-cg':
0.99 /testbed/sklearn/linear_model/logistic.py(943):         elif solver == 'liblinear':
0.99 /testbed/sklearn/linear_model/logistic.py(953):         elif solver in ['sag', 'saga']:
0.99 /testbed/sklearn/linear_model/logistic.py(954):             if multi_class == 'multinomial':
0.99 /testbed/sklearn/linear_model/logistic.py(958):                 loss = 'log'
0.99 /testbed/sklearn/linear_model/logistic.py(960):             if penalty == 'l1':
0.99 /testbed/sklearn/linear_model/logistic.py(963):             elif penalty == 'l2':
0.99 /testbed/sklearn/linear_model/logistic.py(964):                 alpha = 1. / C
0.99 /testbed/sklearn/linear_model/logistic.py(965):                 beta = 0.
0.99 /testbed/sklearn/linear_model/logistic.py(970):             w0, n_iter_i, warm_start_sag = sag_solver(
0.99 /testbed/sklearn/linear_model/logistic.py(971):                 X, target, sample_weight, loss, alpha,
0.99 /testbed/sklearn/linear_model/logistic.py(972):                 beta, max_iter, tol,
0.99 /testbed/sklearn/linear_model/logistic.py(973):                 verbose, random_state, False, max_squared_sum, warm_start_sag,
0.99 /testbed/sklearn/linear_model/logistic.py(974):                 is_saga=(solver == 'saga'))
0.99 /testbed/sklearn/linear_model/logistic.py(980):         if multi_class == 'multinomial':
0.99 /testbed/sklearn/linear_model/logistic.py(987):             coefs.append(w0.copy())
0.99 /testbed/sklearn/linear_model/logistic.py(989):         n_iter[i] = n_iter_i
0.99 /testbed/sklearn/linear_model/logistic.py(925):     for i, C in enumerate(Cs):
0.99 /testbed/sklearn/linear_model/logistic.py(926):         if solver == 'lbfgs':
0.99 /testbed/sklearn/linear_model/logistic.py(939):         elif solver == 'newton-cg':
0.99 /testbed/sklearn/linear_model/logistic.py(943):         elif solver == 'liblinear':
0.99 /testbed/sklearn/linear_model/logistic.py(953):         elif solver in ['sag', 'saga']:
0.99 /testbed/sklearn/linear_model/logistic.py(954):             if multi_class == 'multinomial':
0.99 /testbed/sklearn/linear_model/logistic.py(958):                 loss = 'log'
0.99 /testbed/sklearn/linear_model/logistic.py(960):             if penalty == 'l1':
0.99 /testbed/sklearn/linear_model/logistic.py(963):             elif penalty == 'l2':
0.99 /testbed/sklearn/linear_model/logistic.py(964):                 alpha = 1. / C
0.99 /testbed/sklearn/linear_model/logistic.py(965):                 beta = 0.
0.99 /testbed/sklearn/linear_model/logistic.py(970):             w0, n_iter_i, warm_start_sag = sag_solver(
0.99 /testbed/sklearn/linear_model/logistic.py(971):                 X, target, sample_weight, loss, alpha,
0.99 /testbed/sklearn/linear_model/logistic.py(972):                 beta, max_iter, tol,
0.99 /testbed/sklearn/linear_model/logistic.py(973):                 verbose, random_state, False, max_squared_sum, warm_start_sag,
0.99 /testbed/sklearn/linear_model/logistic.py(974):                 is_saga=(solver == 'saga'))
0.99 /testbed/sklearn/linear_model/logistic.py(980):         if multi_class == 'multinomial':
0.99 /testbed/sklearn/linear_model/logistic.py(987):             coefs.append(w0.copy())
0.99 /testbed/sklearn/linear_model/logistic.py(989):         n_iter[i] = n_iter_i
0.99 /testbed/sklearn/linear_model/logistic.py(925):     for i, C in enumerate(Cs):
0.99 /testbed/sklearn/linear_model/logistic.py(926):         if solver == 'lbfgs':
0.99 /testbed/sklearn/linear_model/logistic.py(939):         elif solver == 'newton-cg':
0.99 /testbed/sklearn/linear_model/logistic.py(943):         elif solver == 'liblinear':
0.99 /testbed/sklearn/linear_model/logistic.py(953):         elif solver in ['sag', 'saga']:
0.99 /testbed/sklearn/linear_model/logistic.py(954):             if multi_class == 'multinomial':
0.99 /testbed/sklearn/linear_model/logistic.py(958):                 loss = 'log'
0.99 /testbed/sklearn/linear_model/logistic.py(960):             if penalty == 'l1':
0.99 /testbed/sklearn/linear_model/logistic.py(963):             elif penalty == 'l2':
0.99 /testbed/sklearn/linear_model/logistic.py(964):                 alpha = 1. / C
0.99 /testbed/sklearn/linear_model/logistic.py(965):                 beta = 0.
0.99 /testbed/sklearn/linear_model/logistic.py(970):             w0, n_iter_i, warm_start_sag = sag_solver(
0.99 /testbed/sklearn/linear_model/logistic.py(971):                 X, target, sample_weight, loss, alpha,
0.99 /testbed/sklearn/linear_model/logistic.py(972):                 beta, max_iter, tol,
0.99 /testbed/sklearn/linear_model/logistic.py(973):                 verbose, random_state, False, max_squared_sum, warm_start_sag,
0.99 /testbed/sklearn/linear_model/logistic.py(974):                 is_saga=(solver == 'saga'))
0.99 /testbed/sklearn/linear_model/logistic.py(980):         if multi_class == 'multinomial':
0.99 /testbed/sklearn/linear_model/logistic.py(987):             coefs.append(w0.copy())
0.99 /testbed/sklearn/linear_model/logistic.py(989):         n_iter[i] = n_iter_i
0.99 /testbed/sklearn/linear_model/logistic.py(925):     for i, C in enumerate(Cs):
0.99 /testbed/sklearn/linear_model/logistic.py(926):         if solver == 'lbfgs':
0.99 /testbed/sklearn/linear_model/logistic.py(939):         elif solver == 'newton-cg':
0.99 /testbed/sklearn/linear_model/logistic.py(943):         elif solver == 'liblinear':
0.99 /testbed/sklearn/linear_model/logistic.py(953):         elif solver in ['sag', 'saga']:
0.99 /testbed/sklearn/linear_model/logistic.py(954):             if multi_class == 'multinomial':
0.99 /testbed/sklearn/linear_model/logistic.py(958):                 loss = 'log'
0.99 /testbed/sklearn/linear_model/logistic.py(960):             if penalty == 'l1':
0.99 /testbed/sklearn/linear_model/logistic.py(963):             elif penalty == 'l2':
0.99 /testbed/sklearn/linear_model/logistic.py(964):                 alpha = 1. / C
0.99 /testbed/sklearn/linear_model/logistic.py(965):                 beta = 0.
0.99 /testbed/sklearn/linear_model/logistic.py(970):             w0, n_iter_i, warm_start_sag = sag_solver(
0.99 /testbed/sklearn/linear_model/logistic.py(971):                 X, target, sample_weight, loss, alpha,
0.99 /testbed/sklearn/linear_model/logistic.py(972):                 beta, max_iter, tol,
0.99 /testbed/sklearn/linear_model/logistic.py(973):                 verbose, random_state, False, max_squared_sum, warm_start_sag,
0.99 /testbed/sklearn/linear_model/logistic.py(974):                 is_saga=(solver == 'saga'))
0.99 /testbed/sklearn/linear_model/logistic.py(980):         if multi_class == 'multinomial':
0.99 /testbed/sklearn/linear_model/logistic.py(987):             coefs.append(w0.copy())
0.99 /testbed/sklearn/linear_model/logistic.py(989):         n_iter[i] = n_iter_i
0.99 /testbed/sklearn/linear_model/logistic.py(925):     for i, C in enumerate(Cs):
0.99 /testbed/sklearn/linear_model/logistic.py(926):         if solver == 'lbfgs':
0.99 /testbed/sklearn/linear_model/logistic.py(939):         elif solver == 'newton-cg':
0.99 /testbed/sklearn/linear_model/logistic.py(943):         elif solver == 'liblinear':
0.99 /testbed/sklearn/linear_model/logistic.py(953):         elif solver in ['sag', 'saga']:
0.99 /testbed/sklearn/linear_model/logistic.py(954):             if multi_class == 'multinomial':
0.99 /testbed/sklearn/linear_model/logistic.py(958):                 loss = 'log'
0.99 /testbed/sklearn/linear_model/logistic.py(960):             if penalty == 'l1':
0.99 /testbed/sklearn/linear_model/logistic.py(963):             elif penalty == 'l2':
0.99 /testbed/sklearn/linear_model/logistic.py(964):                 alpha = 1. / C
0.99 /testbed/sklearn/linear_model/logistic.py(965):                 beta = 0.
0.99 /testbed/sklearn/linear_model/logistic.py(970):             w0, n_iter_i, warm_start_sag = sag_solver(
0.99 /testbed/sklearn/linear_model/logistic.py(971):                 X, target, sample_weight, loss, alpha,
0.99 /testbed/sklearn/linear_model/logistic.py(972):                 beta, max_iter, tol,
0.99 /testbed/sklearn/linear_model/logistic.py(973):                 verbose, random_state, False, max_squared_sum, warm_start_sag,
0.99 /testbed/sklearn/linear_model/logistic.py(974):                 is_saga=(solver == 'saga'))
0.99 /testbed/sklearn/linear_model/logistic.py(980):         if multi_class == 'multinomial':
0.99 /testbed/sklearn/linear_model/logistic.py(987):             coefs.append(w0.copy())
0.99 /testbed/sklearn/linear_model/logistic.py(989):         n_iter[i] = n_iter_i
0.99 /testbed/sklearn/linear_model/logistic.py(925):     for i, C in enumerate(Cs):
0.99 /testbed/sklearn/linear_model/logistic.py(926):         if solver == 'lbfgs':
0.99 /testbed/sklearn/linear_model/logistic.py(939):         elif solver == 'newton-cg':
0.99 /testbed/sklearn/linear_model/logistic.py(943):         elif solver == 'liblinear':
0.99 /testbed/sklearn/linear_model/logistic.py(953):         elif solver in ['sag', 'saga']:
0.99 /testbed/sklearn/linear_model/logistic.py(954):             if multi_class == 'multinomial':
0.99 /testbed/sklearn/linear_model/logistic.py(958):                 loss = 'log'
0.99 /testbed/sklearn/linear_model/logistic.py(960):             if penalty == 'l1':
0.99 /testbed/sklearn/linear_model/logistic.py(963):             elif penalty == 'l2':
0.99 /testbed/sklearn/linear_model/logistic.py(964):                 alpha = 1. / C
0.99 /testbed/sklearn/linear_model/logistic.py(965):                 beta = 0.
0.99 /testbed/sklearn/linear_model/logistic.py(970):             w0, n_iter_i, warm_start_sag = sag_solver(
0.99 /testbed/sklearn/linear_model/logistic.py(971):                 X, target, sample_weight, loss, alpha,
0.99 /testbed/sklearn/linear_model/logistic.py(972):                 beta, max_iter, tol,
0.99 /testbed/sklearn/linear_model/logistic.py(973):                 verbose, random_state, False, max_squared_sum, warm_start_sag,
0.99 /testbed/sklearn/linear_model/logistic.py(974):                 is_saga=(solver == 'saga'))
1.00 /testbed/sklearn/linear_model/logistic.py(980):         if multi_class == 'multinomial':
1.00 /testbed/sklearn/linear_model/logistic.py(987):             coefs.append(w0.copy())
1.00 /testbed/sklearn/linear_model/logistic.py(989):         n_iter[i] = n_iter_i
1.00 /testbed/sklearn/linear_model/logistic.py(925):     for i, C in enumerate(Cs):
1.00 /testbed/sklearn/linear_model/logistic.py(991):     return np.array(coefs), np.array(Cs), n_iter
1.00 /testbed/sklearn/linear_model/logistic.py(1152):     log_reg = LogisticRegression(solver=solver, multi_class=multi_class)
1.00 /testbed/sklearn/linear_model/logistic.py(1437):         self.penalty = penalty
1.00 /testbed/sklearn/linear_model/logistic.py(1438):         self.dual = dual
1.00 /testbed/sklearn/linear_model/logistic.py(1439):         self.tol = tol
1.00 /testbed/sklearn/linear_model/logistic.py(1440):         self.C = C
1.00 /testbed/sklearn/linear_model/logistic.py(1441):         self.fit_intercept = fit_intercept
1.00 /testbed/sklearn/linear_model/logistic.py(1442):         self.intercept_scaling = intercept_scaling
1.00 /testbed/sklearn/linear_model/logistic.py(1443):         self.class_weight = class_weight
1.00 /testbed/sklearn/linear_model/logistic.py(1444):         self.random_state = random_state
1.00 /testbed/sklearn/linear_model/logistic.py(1445):         self.solver = solver
1.00 /testbed/sklearn/linear_model/logistic.py(1446):         self.max_iter = max_iter
1.00 /testbed/sklearn/linear_model/logistic.py(1447):         self.multi_class = multi_class
1.00 /testbed/sklearn/linear_model/logistic.py(1448):         self.verbose = verbose
1.00 /testbed/sklearn/linear_model/logistic.py(1449):         self.warm_start = warm_start
1.00 /testbed/sklearn/linear_model/logistic.py(1450):         self.n_jobs = n_jobs
1.00 /testbed/sklearn/linear_model/logistic.py(1451):         self.l1_ratio = l1_ratio
1.00 /testbed/sklearn/linear_model/logistic.py(1155):     if multi_class == 'ovr':
1.00 /testbed/sklearn/linear_model/logistic.py(1156):         log_reg.classes_ = np.array([-1, 1])
1.00 /testbed/sklearn/linear_model/logistic.py(1163):     if pos_class is not None:
1.00 /testbed/sklearn/linear_model/logistic.py(1164):         mask = (y_test == pos_class)
1.00 /testbed/sklearn/linear_model/logistic.py(1165):         y_test = np.ones(y_test.shape, dtype=np.float64)
1.00 /testbed/sklearn/linear_model/logistic.py(1166):         y_test[~mask] = -1.
1.00 /testbed/sklearn/linear_model/logistic.py(1168):     scores = list()
1.00 /testbed/sklearn/linear_model/logistic.py(1170):     if isinstance(scoring, str):
1.00 /testbed/sklearn/linear_model/logistic.py(1172):     for w in coefs:
1.00 /testbed/sklearn/linear_model/logistic.py(1173):         if multi_class == 'ovr':
1.00 /testbed/sklearn/linear_model/logistic.py(1174):             w = w[np.newaxis, :]
1.00 /testbed/sklearn/linear_model/logistic.py(1175):         if fit_intercept:
1.00 /testbed/sklearn/linear_model/logistic.py(1176):             log_reg.coef_ = w[:, :-1]
1.00 /testbed/sklearn/linear_model/logistic.py(1177):             log_reg.intercept_ = w[:, -1]
1.00 /testbed/sklearn/linear_model/logistic.py(1182):         if scoring is None:
1.00 /testbed/sklearn/linear_model/logistic.py(1183):             scores.append(log_reg.score(X_test, y_test))
1.00 /testbed/sklearn/linear_model/logistic.py(1172):     for w in coefs:
1.00 /testbed/sklearn/linear_model/logistic.py(1173):         if multi_class == 'ovr':
1.00 /testbed/sklearn/linear_model/logistic.py(1174):             w = w[np.newaxis, :]
1.00 /testbed/sklearn/linear_model/logistic.py(1175):         if fit_intercept:
1.00 /testbed/sklearn/linear_model/logistic.py(1176):             log_reg.coef_ = w[:, :-1]
1.00 /testbed/sklearn/linear_model/logistic.py(1177):             log_reg.intercept_ = w[:, -1]
1.00 /testbed/sklearn/linear_model/logistic.py(1182):         if scoring is None:
1.00 /testbed/sklearn/linear_model/logistic.py(1183):             scores.append(log_reg.score(X_test, y_test))
1.00 /testbed/sklearn/linear_model/logistic.py(1172):     for w in coefs:
1.00 /testbed/sklearn/linear_model/logistic.py(1173):         if multi_class == 'ovr':
1.00 /testbed/sklearn/linear_model/logistic.py(1174):             w = w[np.newaxis, :]
1.00 /testbed/sklearn/linear_model/logistic.py(1175):         if fit_intercept:
1.00 /testbed/sklearn/linear_model/logistic.py(1176):             log_reg.coef_ = w[:, :-1]
1.00 /testbed/sklearn/linear_model/logistic.py(1177):             log_reg.intercept_ = w[:, -1]
1.00 /testbed/sklearn/linear_model/logistic.py(1182):         if scoring is None:
1.00 /testbed/sklearn/linear_model/logistic.py(1183):             scores.append(log_reg.score(X_test, y_test))
1.00 /testbed/sklearn/linear_model/logistic.py(1172):     for w in coefs:
1.00 /testbed/sklearn/linear_model/logistic.py(1173):         if multi_class == 'ovr':
1.00 /testbed/sklearn/linear_model/logistic.py(1174):             w = w[np.newaxis, :]
1.00 /testbed/sklearn/linear_model/logistic.py(1175):         if fit_intercept:
1.00 /testbed/sklearn/linear_model/logistic.py(1176):             log_reg.coef_ = w[:, :-1]
1.00 /testbed/sklearn/linear_model/logistic.py(1177):             log_reg.intercept_ = w[:, -1]
1.00 /testbed/sklearn/linear_model/logistic.py(1182):         if scoring is None:
1.00 /testbed/sklearn/linear_model/logistic.py(1183):             scores.append(log_reg.score(X_test, y_test))
1.00 /testbed/sklearn/linear_model/logistic.py(1172):     for w in coefs:
1.00 /testbed/sklearn/linear_model/logistic.py(1173):         if multi_class == 'ovr':
1.00 /testbed/sklearn/linear_model/logistic.py(1174):             w = w[np.newaxis, :]
1.00 /testbed/sklearn/linear_model/logistic.py(1175):         if fit_intercept:
1.00 /testbed/sklearn/linear_model/logistic.py(1176):             log_reg.coef_ = w[:, :-1]
1.00 /testbed/sklearn/linear_model/logistic.py(1177):             log_reg.intercept_ = w[:, -1]
1.00 /testbed/sklearn/linear_model/logistic.py(1182):         if scoring is None:
1.00 /testbed/sklearn/linear_model/logistic.py(1183):             scores.append(log_reg.score(X_test, y_test))
1.00 /testbed/sklearn/linear_model/logistic.py(1172):     for w in coefs:
1.00 /testbed/sklearn/linear_model/logistic.py(1173):         if multi_class == 'ovr':
1.00 /testbed/sklearn/linear_model/logistic.py(1174):             w = w[np.newaxis, :]
1.00 /testbed/sklearn/linear_model/logistic.py(1175):         if fit_intercept:
1.00 /testbed/sklearn/linear_model/logistic.py(1176):             log_reg.coef_ = w[:, :-1]
1.00 /testbed/sklearn/linear_model/logistic.py(1177):             log_reg.intercept_ = w[:, -1]
1.00 /testbed/sklearn/linear_model/logistic.py(1182):         if scoring is None:
1.00 /testbed/sklearn/linear_model/logistic.py(1183):             scores.append(log_reg.score(X_test, y_test))
1.00 /testbed/sklearn/linear_model/logistic.py(1172):     for w in coefs:
1.00 /testbed/sklearn/linear_model/logistic.py(1173):         if multi_class == 'ovr':
1.00 /testbed/sklearn/linear_model/logistic.py(1174):             w = w[np.newaxis, :]
1.00 /testbed/sklearn/linear_model/logistic.py(1175):         if fit_intercept:
1.00 /testbed/sklearn/linear_model/logistic.py(1176):             log_reg.coef_ = w[:, :-1]
1.00 /testbed/sklearn/linear_model/logistic.py(1177):             log_reg.intercept_ = w[:, -1]
1.00 /testbed/sklearn/linear_model/logistic.py(1182):         if scoring is None:
1.00 /testbed/sklearn/linear_model/logistic.py(1183):             scores.append(log_reg.score(X_test, y_test))
1.00 /testbed/sklearn/linear_model/logistic.py(1172):     for w in coefs:
1.00 /testbed/sklearn/linear_model/logistic.py(1173):         if multi_class == 'ovr':
1.00 /testbed/sklearn/linear_model/logistic.py(1174):             w = w[np.newaxis, :]
1.00 /testbed/sklearn/linear_model/logistic.py(1175):         if fit_intercept:
1.00 /testbed/sklearn/linear_model/logistic.py(1176):             log_reg.coef_ = w[:, :-1]
1.00 /testbed/sklearn/linear_model/logistic.py(1177):             log_reg.intercept_ = w[:, -1]
1.00 /testbed/sklearn/linear_model/logistic.py(1182):         if scoring is None:
1.00 /testbed/sklearn/linear_model/logistic.py(1183):             scores.append(log_reg.score(X_test, y_test))
1.00 /testbed/sklearn/linear_model/logistic.py(1172):     for w in coefs:
1.00 /testbed/sklearn/linear_model/logistic.py(1173):         if multi_class == 'ovr':
1.00 /testbed/sklearn/linear_model/logistic.py(1174):             w = w[np.newaxis, :]
1.00 /testbed/sklearn/linear_model/logistic.py(1175):         if fit_intercept:
1.00 /testbed/sklearn/linear_model/logistic.py(1176):             log_reg.coef_ = w[:, :-1]
1.00 /testbed/sklearn/linear_model/logistic.py(1177):             log_reg.intercept_ = w[:, -1]
1.00 /testbed/sklearn/linear_model/logistic.py(1182):         if scoring is None:
1.00 /testbed/sklearn/linear_model/logistic.py(1183):             scores.append(log_reg.score(X_test, y_test))
1.00 /testbed/sklearn/linear_model/logistic.py(1172):     for w in coefs:
1.00 /testbed/sklearn/linear_model/logistic.py(1173):         if multi_class == 'ovr':
1.00 /testbed/sklearn/linear_model/logistic.py(1174):             w = w[np.newaxis, :]
1.00 /testbed/sklearn/linear_model/logistic.py(1175):         if fit_intercept:
1.00 /testbed/sklearn/linear_model/logistic.py(1176):             log_reg.coef_ = w[:, :-1]
1.00 /testbed/sklearn/linear_model/logistic.py(1177):             log_reg.intercept_ = w[:, -1]
1.00 /testbed/sklearn/linear_model/logistic.py(1182):         if scoring is None:
1.00 /testbed/sklearn/linear_model/logistic.py(1183):             scores.append(log_reg.score(X_test, y_test))
1.00 /testbed/sklearn/linear_model/logistic.py(1172):     for w in coefs:
1.00 /testbed/sklearn/linear_model/logistic.py(1187):     return coefs, Cs, np.array(scores), n_iter
1.00 /testbed/sklearn/linear_model/logistic.py(2071):             for l1_ratio in l1_ratios_)
1.00 /testbed/sklearn/linear_model/logistic.py(2070):             for train, test in folds
1.00 /testbed/sklearn/linear_model/logistic.py(2071):             for l1_ratio in l1_ratios_)
1.00 /testbed/sklearn/linear_model/logistic.py(1132):     X_train = X[train]
1.00 /testbed/sklearn/linear_model/logistic.py(1133):     X_test = X[test]
1.00 /testbed/sklearn/linear_model/logistic.py(1134):     y_train = y[train]
1.00 /testbed/sklearn/linear_model/logistic.py(1135):     y_test = y[test]
1.00 /testbed/sklearn/linear_model/logistic.py(1137):     if sample_weight is not None:
1.00 /testbed/sklearn/linear_model/logistic.py(1143):     coefs, Cs, n_iter = _logistic_regression_path(
1.00 /testbed/sklearn/linear_model/logistic.py(1144):         X_train, y_train, Cs=Cs, l1_ratio=l1_ratio,
1.00 /testbed/sklearn/linear_model/logistic.py(1145):         fit_intercept=fit_intercept, solver=solver, max_iter=max_iter,
1.00 /testbed/sklearn/linear_model/logistic.py(1146):         class_weight=class_weight, pos_class=pos_class,
1.00 /testbed/sklearn/linear_model/logistic.py(1147):         multi_class=multi_class, tol=tol, verbose=verbose, dual=dual,
1.00 /testbed/sklearn/linear_model/logistic.py(1148):         penalty=penalty, intercept_scaling=intercept_scaling,
1.00 /testbed/sklearn/linear_model/logistic.py(1149):         random_state=random_state, check_input=False,
1.00 /testbed/sklearn/linear_model/logistic.py(1150):         max_squared_sum=max_squared_sum, sample_weight=sample_weight)
1.00 /testbed/sklearn/linear_model/logistic.py(803):     if isinstance(Cs, numbers.Integral):
1.00 /testbed/sklearn/linear_model/logistic.py(804):         Cs = np.logspace(-4, 4, Cs)
1.00 /testbed/sklearn/linear_model/logistic.py(806):     solver = _check_solver(solver, penalty, dual)
1.00 /testbed/sklearn/linear_model/logistic.py(428):     all_solvers = ['liblinear', 'newton-cg', 'lbfgs', 'sag', 'saga']
1.00 /testbed/sklearn/linear_model/logistic.py(429):     if solver not in all_solvers:
1.00 /testbed/sklearn/linear_model/logistic.py(433):     all_penalties = ['l1', 'l2', 'elasticnet', 'none']
1.00 /testbed/sklearn/linear_model/logistic.py(434):     if penalty not in all_penalties:
1.00 /testbed/sklearn/linear_model/logistic.py(438):     if solver not in ['liblinear', 'saga'] and penalty not in ('l2', 'none'):
1.00 /testbed/sklearn/linear_model/logistic.py(441):     if solver != 'liblinear' and dual:
1.00 /testbed/sklearn/linear_model/logistic.py(445):     if penalty == 'elasticnet' and solver != 'saga':
1.00 /testbed/sklearn/linear_model/logistic.py(449):     if solver == 'liblinear' and penalty == 'none':
1.00 /testbed/sklearn/linear_model/logistic.py(454):     return solver
1.00 /testbed/sklearn/linear_model/logistic.py(809):     if check_input:
1.00 /testbed/sklearn/linear_model/logistic.py(814):     _, n_features = X.shape
1.00 /testbed/sklearn/linear_model/logistic.py(816):     classes = np.unique(y)
1.00 /testbed/sklearn/linear_model/logistic.py(817):     random_state = check_random_state(random_state)
1.00 /testbed/sklearn/linear_model/logistic.py(819):     multi_class = _check_multi_class(multi_class, solver, len(classes))
1.00 /testbed/sklearn/linear_model/logistic.py(458):     if multi_class == 'auto':
1.00 /testbed/sklearn/linear_model/logistic.py(465):     if multi_class not in ('multinomial', 'ovr'):
1.00 /testbed/sklearn/linear_model/logistic.py(468):     if multi_class == 'multinomial' and solver == 'liblinear':
1.00 /testbed/sklearn/linear_model/logistic.py(471):     return multi_class
1.00 /testbed/sklearn/linear_model/logistic.py(820):     if pos_class is None and multi_class != 'multinomial':
1.00 /testbed/sklearn/linear_model/logistic.py(829):     if sample_weight is not None:
1.00 /testbed/sklearn/linear_model/logistic.py(833):         sample_weight = np.ones(X.shape[0], dtype=X.dtype)
1.00 /testbed/sklearn/linear_model/logistic.py(838):     le = LabelEncoder()
1.00 /testbed/sklearn/linear_model/logistic.py(839):     if isinstance(class_weight, dict) or multi_class == 'multinomial':
1.00 /testbed/sklearn/linear_model/logistic.py(845):     if multi_class == 'ovr':
1.00 /testbed/sklearn/linear_model/logistic.py(846):         w0 = np.zeros(n_features + int(fit_intercept), dtype=X.dtype)
1.00 /testbed/sklearn/linear_model/logistic.py(847):         mask_classes = np.array([-1, 1])
1.00 /testbed/sklearn/linear_model/logistic.py(848):         mask = (y == pos_class)
1.00 /testbed/sklearn/linear_model/logistic.py(849):         y_bin = np.ones(y.shape, dtype=X.dtype)
1.00 /testbed/sklearn/linear_model/logistic.py(850):         y_bin[~mask] = -1.
1.00 /testbed/sklearn/linear_model/logistic.py(853):         if class_weight == "balanced":
1.00 /testbed/sklearn/linear_model/logistic.py(872):     if coef is not None:
1.00 /testbed/sklearn/linear_model/logistic.py(901):     if multi_class == 'multinomial':
1.00 /testbed/sklearn/linear_model/logistic.py(914):         target = y_bin
1.00 /testbed/sklearn/linear_model/logistic.py(915):         if solver == 'lbfgs':
1.00 /testbed/sklearn/linear_model/logistic.py(917):         elif solver == 'newton-cg':
1.00 /testbed/sklearn/linear_model/logistic.py(921):         warm_start_sag = {'coef': np.expand_dims(w0, axis=1)}
1.00 /testbed/sklearn/linear_model/logistic.py(923):     coefs = list()
1.00 /testbed/sklearn/linear_model/logistic.py(924):     n_iter = np.zeros(len(Cs), dtype=np.int32)
1.00 /testbed/sklearn/linear_model/logistic.py(925):     for i, C in enumerate(Cs):
1.00 /testbed/sklearn/linear_model/logistic.py(926):         if solver == 'lbfgs':
1.00 /testbed/sklearn/linear_model/logistic.py(939):         elif solver == 'newton-cg':
1.00 /testbed/sklearn/linear_model/logistic.py(943):         elif solver == 'liblinear':
1.00 /testbed/sklearn/linear_model/logistic.py(953):         elif solver in ['sag', 'saga']:
1.00 /testbed/sklearn/linear_model/logistic.py(954):             if multi_class == 'multinomial':
1.00 /testbed/sklearn/linear_model/logistic.py(958):                 loss = 'log'
1.00 /testbed/sklearn/linear_model/logistic.py(960):             if penalty == 'l1':
1.00 /testbed/sklearn/linear_model/logistic.py(963):             elif penalty == 'l2':
1.00 /testbed/sklearn/linear_model/logistic.py(964):                 alpha = 1. / C
1.00 /testbed/sklearn/linear_model/logistic.py(965):                 beta = 0.
1.00 /testbed/sklearn/linear_model/logistic.py(970):             w0, n_iter_i, warm_start_sag = sag_solver(
1.00 /testbed/sklearn/linear_model/logistic.py(971):                 X, target, sample_weight, loss, alpha,
1.00 /testbed/sklearn/linear_model/logistic.py(972):                 beta, max_iter, tol,
1.00 /testbed/sklearn/linear_model/logistic.py(973):                 verbose, random_state, False, max_squared_sum, warm_start_sag,
1.00 /testbed/sklearn/linear_model/logistic.py(974):                 is_saga=(solver == 'saga'))
1.00 /testbed/sklearn/linear_model/logistic.py(980):         if multi_class == 'multinomial':
1.00 /testbed/sklearn/linear_model/logistic.py(987):             coefs.append(w0.copy())
1.00 /testbed/sklearn/linear_model/logistic.py(989):         n_iter[i] = n_iter_i
1.00 /testbed/sklearn/linear_model/logistic.py(925):     for i, C in enumerate(Cs):
1.00 /testbed/sklearn/linear_model/logistic.py(926):         if solver == 'lbfgs':
1.00 /testbed/sklearn/linear_model/logistic.py(939):         elif solver == 'newton-cg':
1.00 /testbed/sklearn/linear_model/logistic.py(943):         elif solver == 'liblinear':
1.00 /testbed/sklearn/linear_model/logistic.py(953):         elif solver in ['sag', 'saga']:
1.00 /testbed/sklearn/linear_model/logistic.py(954):             if multi_class == 'multinomial':
1.00 /testbed/sklearn/linear_model/logistic.py(958):                 loss = 'log'
1.00 /testbed/sklearn/linear_model/logistic.py(960):             if penalty == 'l1':
1.00 /testbed/sklearn/linear_model/logistic.py(963):             elif penalty == 'l2':
1.00 /testbed/sklearn/linear_model/logistic.py(964):                 alpha = 1. / C
1.00 /testbed/sklearn/linear_model/logistic.py(965):                 beta = 0.
1.00 /testbed/sklearn/linear_model/logistic.py(970):             w0, n_iter_i, warm_start_sag = sag_solver(
1.00 /testbed/sklearn/linear_model/logistic.py(971):                 X, target, sample_weight, loss, alpha,
1.00 /testbed/sklearn/linear_model/logistic.py(972):                 beta, max_iter, tol,
1.00 /testbed/sklearn/linear_model/logistic.py(973):                 verbose, random_state, False, max_squared_sum, warm_start_sag,
1.00 /testbed/sklearn/linear_model/logistic.py(974):                 is_saga=(solver == 'saga'))
1.00 /testbed/sklearn/linear_model/logistic.py(980):         if multi_class == 'multinomial':
1.00 /testbed/sklearn/linear_model/logistic.py(987):             coefs.append(w0.copy())
1.00 /testbed/sklearn/linear_model/logistic.py(989):         n_iter[i] = n_iter_i
1.00 /testbed/sklearn/linear_model/logistic.py(925):     for i, C in enumerate(Cs):
1.00 /testbed/sklearn/linear_model/logistic.py(926):         if solver == 'lbfgs':
1.00 /testbed/sklearn/linear_model/logistic.py(939):         elif solver == 'newton-cg':
1.00 /testbed/sklearn/linear_model/logistic.py(943):         elif solver == 'liblinear':
1.00 /testbed/sklearn/linear_model/logistic.py(953):         elif solver in ['sag', 'saga']:
1.00 /testbed/sklearn/linear_model/logistic.py(954):             if multi_class == 'multinomial':
1.00 /testbed/sklearn/linear_model/logistic.py(958):                 loss = 'log'
1.00 /testbed/sklearn/linear_model/logistic.py(960):             if penalty == 'l1':
1.00 /testbed/sklearn/linear_model/logistic.py(963):             elif penalty == 'l2':
1.00 /testbed/sklearn/linear_model/logistic.py(964):                 alpha = 1. / C
1.00 /testbed/sklearn/linear_model/logistic.py(965):                 beta = 0.
1.00 /testbed/sklearn/linear_model/logistic.py(970):             w0, n_iter_i, warm_start_sag = sag_solver(
1.00 /testbed/sklearn/linear_model/logistic.py(971):                 X, target, sample_weight, loss, alpha,
1.00 /testbed/sklearn/linear_model/logistic.py(972):                 beta, max_iter, tol,
1.00 /testbed/sklearn/linear_model/logistic.py(973):                 verbose, random_state, False, max_squared_sum, warm_start_sag,
1.00 /testbed/sklearn/linear_model/logistic.py(974):                 is_saga=(solver == 'saga'))
1.00 /testbed/sklearn/linear_model/logistic.py(980):         if multi_class == 'multinomial':
1.00 /testbed/sklearn/linear_model/logistic.py(987):             coefs.append(w0.copy())
1.00 /testbed/sklearn/linear_model/logistic.py(989):         n_iter[i] = n_iter_i
1.00 /testbed/sklearn/linear_model/logistic.py(925):     for i, C in enumerate(Cs):
1.00 /testbed/sklearn/linear_model/logistic.py(926):         if solver == 'lbfgs':
1.00 /testbed/sklearn/linear_model/logistic.py(939):         elif solver == 'newton-cg':
1.00 /testbed/sklearn/linear_model/logistic.py(943):         elif solver == 'liblinear':
1.00 /testbed/sklearn/linear_model/logistic.py(953):         elif solver in ['sag', 'saga']:
1.00 /testbed/sklearn/linear_model/logistic.py(954):             if multi_class == 'multinomial':
1.00 /testbed/sklearn/linear_model/logistic.py(958):                 loss = 'log'
1.00 /testbed/sklearn/linear_model/logistic.py(960):             if penalty == 'l1':
1.00 /testbed/sklearn/linear_model/logistic.py(963):             elif penalty == 'l2':
1.00 /testbed/sklearn/linear_model/logistic.py(964):                 alpha = 1. / C
1.00 /testbed/sklearn/linear_model/logistic.py(965):                 beta = 0.
1.00 /testbed/sklearn/linear_model/logistic.py(970):             w0, n_iter_i, warm_start_sag = sag_solver(
1.00 /testbed/sklearn/linear_model/logistic.py(971):                 X, target, sample_weight, loss, alpha,
1.00 /testbed/sklearn/linear_model/logistic.py(972):                 beta, max_iter, tol,
1.00 /testbed/sklearn/linear_model/logistic.py(973):                 verbose, random_state, False, max_squared_sum, warm_start_sag,
1.00 /testbed/sklearn/linear_model/logistic.py(974):                 is_saga=(solver == 'saga'))
1.00 /testbed/sklearn/linear_model/logistic.py(980):         if multi_class == 'multinomial':
1.00 /testbed/sklearn/linear_model/logistic.py(987):             coefs.append(w0.copy())
1.00 /testbed/sklearn/linear_model/logistic.py(989):         n_iter[i] = n_iter_i
1.00 /testbed/sklearn/linear_model/logistic.py(925):     for i, C in enumerate(Cs):
1.00 /testbed/sklearn/linear_model/logistic.py(926):         if solver == 'lbfgs':
1.00 /testbed/sklearn/linear_model/logistic.py(939):         elif solver == 'newton-cg':
1.00 /testbed/sklearn/linear_model/logistic.py(943):         elif solver == 'liblinear':
1.00 /testbed/sklearn/linear_model/logistic.py(953):         elif solver in ['sag', 'saga']:
1.00 /testbed/sklearn/linear_model/logistic.py(954):             if multi_class == 'multinomial':
1.00 /testbed/sklearn/linear_model/logistic.py(958):                 loss = 'log'
1.00 /testbed/sklearn/linear_model/logistic.py(960):             if penalty == 'l1':
1.00 /testbed/sklearn/linear_model/logistic.py(963):             elif penalty == 'l2':
1.00 /testbed/sklearn/linear_model/logistic.py(964):                 alpha = 1. / C
1.00 /testbed/sklearn/linear_model/logistic.py(965):                 beta = 0.
1.00 /testbed/sklearn/linear_model/logistic.py(970):             w0, n_iter_i, warm_start_sag = sag_solver(
1.00 /testbed/sklearn/linear_model/logistic.py(971):                 X, target, sample_weight, loss, alpha,
1.00 /testbed/sklearn/linear_model/logistic.py(972):                 beta, max_iter, tol,
1.00 /testbed/sklearn/linear_model/logistic.py(973):                 verbose, random_state, False, max_squared_sum, warm_start_sag,
1.00 /testbed/sklearn/linear_model/logistic.py(974):                 is_saga=(solver == 'saga'))
1.00 /testbed/sklearn/linear_model/logistic.py(980):         if multi_class == 'multinomial':
1.00 /testbed/sklearn/linear_model/logistic.py(987):             coefs.append(w0.copy())
1.00 /testbed/sklearn/linear_model/logistic.py(989):         n_iter[i] = n_iter_i
1.00 /testbed/sklearn/linear_model/logistic.py(925):     for i, C in enumerate(Cs):
1.00 /testbed/sklearn/linear_model/logistic.py(926):         if solver == 'lbfgs':
1.00 /testbed/sklearn/linear_model/logistic.py(939):         elif solver == 'newton-cg':
1.00 /testbed/sklearn/linear_model/logistic.py(943):         elif solver == 'liblinear':
1.00 /testbed/sklearn/linear_model/logistic.py(953):         elif solver in ['sag', 'saga']:
1.00 /testbed/sklearn/linear_model/logistic.py(954):             if multi_class == 'multinomial':
1.00 /testbed/sklearn/linear_model/logistic.py(958):                 loss = 'log'
1.00 /testbed/sklearn/linear_model/logistic.py(960):             if penalty == 'l1':
1.00 /testbed/sklearn/linear_model/logistic.py(963):             elif penalty == 'l2':
1.00 /testbed/sklearn/linear_model/logistic.py(964):                 alpha = 1. / C
1.00 /testbed/sklearn/linear_model/logistic.py(965):                 beta = 0.
1.00 /testbed/sklearn/linear_model/logistic.py(970):             w0, n_iter_i, warm_start_sag = sag_solver(
1.00 /testbed/sklearn/linear_model/logistic.py(971):                 X, target, sample_weight, loss, alpha,
1.00 /testbed/sklearn/linear_model/logistic.py(972):                 beta, max_iter, tol,
1.00 /testbed/sklearn/linear_model/logistic.py(973):                 verbose, random_state, False, max_squared_sum, warm_start_sag,
1.00 /testbed/sklearn/linear_model/logistic.py(974):                 is_saga=(solver == 'saga'))
1.00 /testbed/sklearn/linear_model/logistic.py(980):         if multi_class == 'multinomial':
1.00 /testbed/sklearn/linear_model/logistic.py(987):             coefs.append(w0.copy())
1.00 /testbed/sklearn/linear_model/logistic.py(989):         n_iter[i] = n_iter_i
1.00 /testbed/sklearn/linear_model/logistic.py(925):     for i, C in enumerate(Cs):
1.00 /testbed/sklearn/linear_model/logistic.py(926):         if solver == 'lbfgs':
1.00 /testbed/sklearn/linear_model/logistic.py(939):         elif solver == 'newton-cg':
1.00 /testbed/sklearn/linear_model/logistic.py(943):         elif solver == 'liblinear':
1.00 /testbed/sklearn/linear_model/logistic.py(953):         elif solver in ['sag', 'saga']:
1.00 /testbed/sklearn/linear_model/logistic.py(954):             if multi_class == 'multinomial':
1.00 /testbed/sklearn/linear_model/logistic.py(958):                 loss = 'log'
1.00 /testbed/sklearn/linear_model/logistic.py(960):             if penalty == 'l1':
1.00 /testbed/sklearn/linear_model/logistic.py(963):             elif penalty == 'l2':
1.00 /testbed/sklearn/linear_model/logistic.py(964):                 alpha = 1. / C
1.00 /testbed/sklearn/linear_model/logistic.py(965):                 beta = 0.
1.00 /testbed/sklearn/linear_model/logistic.py(970):             w0, n_iter_i, warm_start_sag = sag_solver(
1.00 /testbed/sklearn/linear_model/logistic.py(971):                 X, target, sample_weight, loss, alpha,
1.00 /testbed/sklearn/linear_model/logistic.py(972):                 beta, max_iter, tol,
1.00 /testbed/sklearn/linear_model/logistic.py(973):                 verbose, random_state, False, max_squared_sum, warm_start_sag,
1.00 /testbed/sklearn/linear_model/logistic.py(974):                 is_saga=(solver == 'saga'))
1.01 /testbed/sklearn/linear_model/logistic.py(980):         if multi_class == 'multinomial':
1.01 /testbed/sklearn/linear_model/logistic.py(987):             coefs.append(w0.copy())
1.01 /testbed/sklearn/linear_model/logistic.py(989):         n_iter[i] = n_iter_i
1.01 /testbed/sklearn/linear_model/logistic.py(925):     for i, C in enumerate(Cs):
1.01 /testbed/sklearn/linear_model/logistic.py(926):         if solver == 'lbfgs':
1.01 /testbed/sklearn/linear_model/logistic.py(939):         elif solver == 'newton-cg':
1.01 /testbed/sklearn/linear_model/logistic.py(943):         elif solver == 'liblinear':
1.01 /testbed/sklearn/linear_model/logistic.py(953):         elif solver in ['sag', 'saga']:
1.01 /testbed/sklearn/linear_model/logistic.py(954):             if multi_class == 'multinomial':
1.01 /testbed/sklearn/linear_model/logistic.py(958):                 loss = 'log'
1.01 /testbed/sklearn/linear_model/logistic.py(960):             if penalty == 'l1':
1.01 /testbed/sklearn/linear_model/logistic.py(963):             elif penalty == 'l2':
1.01 /testbed/sklearn/linear_model/logistic.py(964):                 alpha = 1. / C
1.01 /testbed/sklearn/linear_model/logistic.py(965):                 beta = 0.
1.01 /testbed/sklearn/linear_model/logistic.py(970):             w0, n_iter_i, warm_start_sag = sag_solver(
1.01 /testbed/sklearn/linear_model/logistic.py(971):                 X, target, sample_weight, loss, alpha,
1.01 /testbed/sklearn/linear_model/logistic.py(972):                 beta, max_iter, tol,
1.01 /testbed/sklearn/linear_model/logistic.py(973):                 verbose, random_state, False, max_squared_sum, warm_start_sag,
1.01 /testbed/sklearn/linear_model/logistic.py(974):                 is_saga=(solver == 'saga'))
1.01 /testbed/sklearn/linear_model/logistic.py(980):         if multi_class == 'multinomial':
1.01 /testbed/sklearn/linear_model/logistic.py(987):             coefs.append(w0.copy())
1.01 /testbed/sklearn/linear_model/logistic.py(989):         n_iter[i] = n_iter_i
1.01 /testbed/sklearn/linear_model/logistic.py(925):     for i, C in enumerate(Cs):
1.01 /testbed/sklearn/linear_model/logistic.py(926):         if solver == 'lbfgs':
1.01 /testbed/sklearn/linear_model/logistic.py(939):         elif solver == 'newton-cg':
1.01 /testbed/sklearn/linear_model/logistic.py(943):         elif solver == 'liblinear':
1.01 /testbed/sklearn/linear_model/logistic.py(953):         elif solver in ['sag', 'saga']:
1.01 /testbed/sklearn/linear_model/logistic.py(954):             if multi_class == 'multinomial':
1.01 /testbed/sklearn/linear_model/logistic.py(958):                 loss = 'log'
1.01 /testbed/sklearn/linear_model/logistic.py(960):             if penalty == 'l1':
1.01 /testbed/sklearn/linear_model/logistic.py(963):             elif penalty == 'l2':
1.01 /testbed/sklearn/linear_model/logistic.py(964):                 alpha = 1. / C
1.01 /testbed/sklearn/linear_model/logistic.py(965):                 beta = 0.
1.01 /testbed/sklearn/linear_model/logistic.py(970):             w0, n_iter_i, warm_start_sag = sag_solver(
1.01 /testbed/sklearn/linear_model/logistic.py(971):                 X, target, sample_weight, loss, alpha,
1.01 /testbed/sklearn/linear_model/logistic.py(972):                 beta, max_iter, tol,
1.01 /testbed/sklearn/linear_model/logistic.py(973):                 verbose, random_state, False, max_squared_sum, warm_start_sag,
1.01 /testbed/sklearn/linear_model/logistic.py(974):                 is_saga=(solver == 'saga'))
1.01 /testbed/sklearn/linear_model/logistic.py(980):         if multi_class == 'multinomial':
1.01 /testbed/sklearn/linear_model/logistic.py(987):             coefs.append(w0.copy())
1.01 /testbed/sklearn/linear_model/logistic.py(989):         n_iter[i] = n_iter_i
1.01 /testbed/sklearn/linear_model/logistic.py(925):     for i, C in enumerate(Cs):
1.01 /testbed/sklearn/linear_model/logistic.py(926):         if solver == 'lbfgs':
1.01 /testbed/sklearn/linear_model/logistic.py(939):         elif solver == 'newton-cg':
1.01 /testbed/sklearn/linear_model/logistic.py(943):         elif solver == 'liblinear':
1.01 /testbed/sklearn/linear_model/logistic.py(953):         elif solver in ['sag', 'saga']:
1.01 /testbed/sklearn/linear_model/logistic.py(954):             if multi_class == 'multinomial':
1.01 /testbed/sklearn/linear_model/logistic.py(958):                 loss = 'log'
1.01 /testbed/sklearn/linear_model/logistic.py(960):             if penalty == 'l1':
1.01 /testbed/sklearn/linear_model/logistic.py(963):             elif penalty == 'l2':
1.01 /testbed/sklearn/linear_model/logistic.py(964):                 alpha = 1. / C
1.01 /testbed/sklearn/linear_model/logistic.py(965):                 beta = 0.
1.01 /testbed/sklearn/linear_model/logistic.py(970):             w0, n_iter_i, warm_start_sag = sag_solver(
1.01 /testbed/sklearn/linear_model/logistic.py(971):                 X, target, sample_weight, loss, alpha,
1.01 /testbed/sklearn/linear_model/logistic.py(972):                 beta, max_iter, tol,
1.01 /testbed/sklearn/linear_model/logistic.py(973):                 verbose, random_state, False, max_squared_sum, warm_start_sag,
1.01 /testbed/sklearn/linear_model/logistic.py(974):                 is_saga=(solver == 'saga'))
1.01 /testbed/sklearn/linear_model/logistic.py(980):         if multi_class == 'multinomial':
1.01 /testbed/sklearn/linear_model/logistic.py(987):             coefs.append(w0.copy())
1.01 /testbed/sklearn/linear_model/logistic.py(989):         n_iter[i] = n_iter_i
1.01 /testbed/sklearn/linear_model/logistic.py(925):     for i, C in enumerate(Cs):
1.01 /testbed/sklearn/linear_model/logistic.py(991):     return np.array(coefs), np.array(Cs), n_iter
1.01 /testbed/sklearn/linear_model/logistic.py(1152):     log_reg = LogisticRegression(solver=solver, multi_class=multi_class)
1.01 /testbed/sklearn/linear_model/logistic.py(1437):         self.penalty = penalty
1.01 /testbed/sklearn/linear_model/logistic.py(1438):         self.dual = dual
1.01 /testbed/sklearn/linear_model/logistic.py(1439):         self.tol = tol
1.01 /testbed/sklearn/linear_model/logistic.py(1440):         self.C = C
1.01 /testbed/sklearn/linear_model/logistic.py(1441):         self.fit_intercept = fit_intercept
1.01 /testbed/sklearn/linear_model/logistic.py(1442):         self.intercept_scaling = intercept_scaling
1.01 /testbed/sklearn/linear_model/logistic.py(1443):         self.class_weight = class_weight
1.01 /testbed/sklearn/linear_model/logistic.py(1444):         self.random_state = random_state
1.01 /testbed/sklearn/linear_model/logistic.py(1445):         self.solver = solver
1.01 /testbed/sklearn/linear_model/logistic.py(1446):         self.max_iter = max_iter
1.01 /testbed/sklearn/linear_model/logistic.py(1447):         self.multi_class = multi_class
1.01 /testbed/sklearn/linear_model/logistic.py(1448):         self.verbose = verbose
1.01 /testbed/sklearn/linear_model/logistic.py(1449):         self.warm_start = warm_start
1.01 /testbed/sklearn/linear_model/logistic.py(1450):         self.n_jobs = n_jobs
1.01 /testbed/sklearn/linear_model/logistic.py(1451):         self.l1_ratio = l1_ratio
1.01 /testbed/sklearn/linear_model/logistic.py(1155):     if multi_class == 'ovr':
1.01 /testbed/sklearn/linear_model/logistic.py(1156):         log_reg.classes_ = np.array([-1, 1])
1.01 /testbed/sklearn/linear_model/logistic.py(1163):     if pos_class is not None:
1.01 /testbed/sklearn/linear_model/logistic.py(1164):         mask = (y_test == pos_class)
1.01 /testbed/sklearn/linear_model/logistic.py(1165):         y_test = np.ones(y_test.shape, dtype=np.float64)
1.01 /testbed/sklearn/linear_model/logistic.py(1166):         y_test[~mask] = -1.
1.01 /testbed/sklearn/linear_model/logistic.py(1168):     scores = list()
1.01 /testbed/sklearn/linear_model/logistic.py(1170):     if isinstance(scoring, str):
1.01 /testbed/sklearn/linear_model/logistic.py(1172):     for w in coefs:
1.01 /testbed/sklearn/linear_model/logistic.py(1173):         if multi_class == 'ovr':
1.01 /testbed/sklearn/linear_model/logistic.py(1174):             w = w[np.newaxis, :]
1.01 /testbed/sklearn/linear_model/logistic.py(1175):         if fit_intercept:
1.01 /testbed/sklearn/linear_model/logistic.py(1176):             log_reg.coef_ = w[:, :-1]
1.01 /testbed/sklearn/linear_model/logistic.py(1177):             log_reg.intercept_ = w[:, -1]
1.01 /testbed/sklearn/linear_model/logistic.py(1182):         if scoring is None:
1.01 /testbed/sklearn/linear_model/logistic.py(1183):             scores.append(log_reg.score(X_test, y_test))
1.01 /testbed/sklearn/linear_model/logistic.py(1172):     for w in coefs:
1.01 /testbed/sklearn/linear_model/logistic.py(1173):         if multi_class == 'ovr':
1.01 /testbed/sklearn/linear_model/logistic.py(1174):             w = w[np.newaxis, :]
1.01 /testbed/sklearn/linear_model/logistic.py(1175):         if fit_intercept:
1.01 /testbed/sklearn/linear_model/logistic.py(1176):             log_reg.coef_ = w[:, :-1]
1.01 /testbed/sklearn/linear_model/logistic.py(1177):             log_reg.intercept_ = w[:, -1]
1.01 /testbed/sklearn/linear_model/logistic.py(1182):         if scoring is None:
1.01 /testbed/sklearn/linear_model/logistic.py(1183):             scores.append(log_reg.score(X_test, y_test))
1.01 /testbed/sklearn/linear_model/logistic.py(1172):     for w in coefs:
1.01 /testbed/sklearn/linear_model/logistic.py(1173):         if multi_class == 'ovr':
1.01 /testbed/sklearn/linear_model/logistic.py(1174):             w = w[np.newaxis, :]
1.01 /testbed/sklearn/linear_model/logistic.py(1175):         if fit_intercept:
1.01 /testbed/sklearn/linear_model/logistic.py(1176):             log_reg.coef_ = w[:, :-1]
1.01 /testbed/sklearn/linear_model/logistic.py(1177):             log_reg.intercept_ = w[:, -1]
1.01 /testbed/sklearn/linear_model/logistic.py(1182):         if scoring is None:
1.01 /testbed/sklearn/linear_model/logistic.py(1183):             scores.append(log_reg.score(X_test, y_test))
1.01 /testbed/sklearn/linear_model/logistic.py(1172):     for w in coefs:
1.01 /testbed/sklearn/linear_model/logistic.py(1173):         if multi_class == 'ovr':
1.01 /testbed/sklearn/linear_model/logistic.py(1174):             w = w[np.newaxis, :]
1.01 /testbed/sklearn/linear_model/logistic.py(1175):         if fit_intercept:
1.01 /testbed/sklearn/linear_model/logistic.py(1176):             log_reg.coef_ = w[:, :-1]
1.01 /testbed/sklearn/linear_model/logistic.py(1177):             log_reg.intercept_ = w[:, -1]
1.01 /testbed/sklearn/linear_model/logistic.py(1182):         if scoring is None:
1.01 /testbed/sklearn/linear_model/logistic.py(1183):             scores.append(log_reg.score(X_test, y_test))
1.01 /testbed/sklearn/linear_model/logistic.py(1172):     for w in coefs:
1.01 /testbed/sklearn/linear_model/logistic.py(1173):         if multi_class == 'ovr':
1.01 /testbed/sklearn/linear_model/logistic.py(1174):             w = w[np.newaxis, :]
1.01 /testbed/sklearn/linear_model/logistic.py(1175):         if fit_intercept:
1.01 /testbed/sklearn/linear_model/logistic.py(1176):             log_reg.coef_ = w[:, :-1]
1.01 /testbed/sklearn/linear_model/logistic.py(1177):             log_reg.intercept_ = w[:, -1]
1.01 /testbed/sklearn/linear_model/logistic.py(1182):         if scoring is None:
1.01 /testbed/sklearn/linear_model/logistic.py(1183):             scores.append(log_reg.score(X_test, y_test))
1.01 /testbed/sklearn/linear_model/logistic.py(1172):     for w in coefs:
1.01 /testbed/sklearn/linear_model/logistic.py(1173):         if multi_class == 'ovr':
1.01 /testbed/sklearn/linear_model/logistic.py(1174):             w = w[np.newaxis, :]
1.01 /testbed/sklearn/linear_model/logistic.py(1175):         if fit_intercept:
1.01 /testbed/sklearn/linear_model/logistic.py(1176):             log_reg.coef_ = w[:, :-1]
1.01 /testbed/sklearn/linear_model/logistic.py(1177):             log_reg.intercept_ = w[:, -1]
1.01 /testbed/sklearn/linear_model/logistic.py(1182):         if scoring is None:
1.01 /testbed/sklearn/linear_model/logistic.py(1183):             scores.append(log_reg.score(X_test, y_test))
1.01 /testbed/sklearn/linear_model/logistic.py(1172):     for w in coefs:
1.01 /testbed/sklearn/linear_model/logistic.py(1173):         if multi_class == 'ovr':
1.01 /testbed/sklearn/linear_model/logistic.py(1174):             w = w[np.newaxis, :]
1.01 /testbed/sklearn/linear_model/logistic.py(1175):         if fit_intercept:
1.01 /testbed/sklearn/linear_model/logistic.py(1176):             log_reg.coef_ = w[:, :-1]
1.01 /testbed/sklearn/linear_model/logistic.py(1177):             log_reg.intercept_ = w[:, -1]
1.01 /testbed/sklearn/linear_model/logistic.py(1182):         if scoring is None:
1.01 /testbed/sklearn/linear_model/logistic.py(1183):             scores.append(log_reg.score(X_test, y_test))
1.01 /testbed/sklearn/linear_model/logistic.py(1172):     for w in coefs:
1.01 /testbed/sklearn/linear_model/logistic.py(1173):         if multi_class == 'ovr':
1.01 /testbed/sklearn/linear_model/logistic.py(1174):             w = w[np.newaxis, :]
1.01 /testbed/sklearn/linear_model/logistic.py(1175):         if fit_intercept:
1.01 /testbed/sklearn/linear_model/logistic.py(1176):             log_reg.coef_ = w[:, :-1]
1.01 /testbed/sklearn/linear_model/logistic.py(1177):             log_reg.intercept_ = w[:, -1]
1.01 /testbed/sklearn/linear_model/logistic.py(1182):         if scoring is None:
1.01 /testbed/sklearn/linear_model/logistic.py(1183):             scores.append(log_reg.score(X_test, y_test))
1.01 /testbed/sklearn/linear_model/logistic.py(1172):     for w in coefs:
1.01 /testbed/sklearn/linear_model/logistic.py(1173):         if multi_class == 'ovr':
1.01 /testbed/sklearn/linear_model/logistic.py(1174):             w = w[np.newaxis, :]
1.01 /testbed/sklearn/linear_model/logistic.py(1175):         if fit_intercept:
1.01 /testbed/sklearn/linear_model/logistic.py(1176):             log_reg.coef_ = w[:, :-1]
1.01 /testbed/sklearn/linear_model/logistic.py(1177):             log_reg.intercept_ = w[:, -1]
1.01 /testbed/sklearn/linear_model/logistic.py(1182):         if scoring is None:
1.01 /testbed/sklearn/linear_model/logistic.py(1183):             scores.append(log_reg.score(X_test, y_test))
1.01 /testbed/sklearn/linear_model/logistic.py(1172):     for w in coefs:
1.01 /testbed/sklearn/linear_model/logistic.py(1173):         if multi_class == 'ovr':
1.01 /testbed/sklearn/linear_model/logistic.py(1174):             w = w[np.newaxis, :]
1.01 /testbed/sklearn/linear_model/logistic.py(1175):         if fit_intercept:
1.01 /testbed/sklearn/linear_model/logistic.py(1176):             log_reg.coef_ = w[:, :-1]
1.01 /testbed/sklearn/linear_model/logistic.py(1177):             log_reg.intercept_ = w[:, -1]
1.01 /testbed/sklearn/linear_model/logistic.py(1182):         if scoring is None:
1.01 /testbed/sklearn/linear_model/logistic.py(1183):             scores.append(log_reg.score(X_test, y_test))
1.01 /testbed/sklearn/linear_model/logistic.py(1172):     for w in coefs:
1.01 /testbed/sklearn/linear_model/logistic.py(1187):     return coefs, Cs, np.array(scores), n_iter
1.01 /testbed/sklearn/linear_model/logistic.py(2071):             for l1_ratio in l1_ratios_)
1.01 /testbed/sklearn/linear_model/logistic.py(2070):             for train, test in folds
1.01 /testbed/sklearn/linear_model/logistic.py(2071):             for l1_ratio in l1_ratios_)
1.01 /testbed/sklearn/linear_model/logistic.py(1132):     X_train = X[train]
1.01 /testbed/sklearn/linear_model/logistic.py(1133):     X_test = X[test]
1.01 /testbed/sklearn/linear_model/logistic.py(1134):     y_train = y[train]
1.01 /testbed/sklearn/linear_model/logistic.py(1135):     y_test = y[test]
1.01 /testbed/sklearn/linear_model/logistic.py(1137):     if sample_weight is not None:
1.01 /testbed/sklearn/linear_model/logistic.py(1143):     coefs, Cs, n_iter = _logistic_regression_path(
1.01 /testbed/sklearn/linear_model/logistic.py(1144):         X_train, y_train, Cs=Cs, l1_ratio=l1_ratio,
1.01 /testbed/sklearn/linear_model/logistic.py(1145):         fit_intercept=fit_intercept, solver=solver, max_iter=max_iter,
1.01 /testbed/sklearn/linear_model/logistic.py(1146):         class_weight=class_weight, pos_class=pos_class,
1.01 /testbed/sklearn/linear_model/logistic.py(1147):         multi_class=multi_class, tol=tol, verbose=verbose, dual=dual,
1.01 /testbed/sklearn/linear_model/logistic.py(1148):         penalty=penalty, intercept_scaling=intercept_scaling,
1.01 /testbed/sklearn/linear_model/logistic.py(1149):         random_state=random_state, check_input=False,
1.01 /testbed/sklearn/linear_model/logistic.py(1150):         max_squared_sum=max_squared_sum, sample_weight=sample_weight)
1.01 /testbed/sklearn/linear_model/logistic.py(803):     if isinstance(Cs, numbers.Integral):
1.01 /testbed/sklearn/linear_model/logistic.py(804):         Cs = np.logspace(-4, 4, Cs)
1.01 /testbed/sklearn/linear_model/logistic.py(806):     solver = _check_solver(solver, penalty, dual)
1.01 /testbed/sklearn/linear_model/logistic.py(428):     all_solvers = ['liblinear', 'newton-cg', 'lbfgs', 'sag', 'saga']
1.01 /testbed/sklearn/linear_model/logistic.py(429):     if solver not in all_solvers:
1.01 /testbed/sklearn/linear_model/logistic.py(433):     all_penalties = ['l1', 'l2', 'elasticnet', 'none']
1.01 /testbed/sklearn/linear_model/logistic.py(434):     if penalty not in all_penalties:
1.01 /testbed/sklearn/linear_model/logistic.py(438):     if solver not in ['liblinear', 'saga'] and penalty not in ('l2', 'none'):
1.01 /testbed/sklearn/linear_model/logistic.py(441):     if solver != 'liblinear' and dual:
1.01 /testbed/sklearn/linear_model/logistic.py(445):     if penalty == 'elasticnet' and solver != 'saga':
1.01 /testbed/sklearn/linear_model/logistic.py(449):     if solver == 'liblinear' and penalty == 'none':
1.01 /testbed/sklearn/linear_model/logistic.py(454):     return solver
1.01 /testbed/sklearn/linear_model/logistic.py(809):     if check_input:
1.01 /testbed/sklearn/linear_model/logistic.py(814):     _, n_features = X.shape
1.01 /testbed/sklearn/linear_model/logistic.py(816):     classes = np.unique(y)
1.01 /testbed/sklearn/linear_model/logistic.py(817):     random_state = check_random_state(random_state)
1.01 /testbed/sklearn/linear_model/logistic.py(819):     multi_class = _check_multi_class(multi_class, solver, len(classes))
1.01 /testbed/sklearn/linear_model/logistic.py(458):     if multi_class == 'auto':
1.01 /testbed/sklearn/linear_model/logistic.py(465):     if multi_class not in ('multinomial', 'ovr'):
1.01 /testbed/sklearn/linear_model/logistic.py(468):     if multi_class == 'multinomial' and solver == 'liblinear':
1.01 /testbed/sklearn/linear_model/logistic.py(471):     return multi_class
1.01 /testbed/sklearn/linear_model/logistic.py(820):     if pos_class is None and multi_class != 'multinomial':
1.01 /testbed/sklearn/linear_model/logistic.py(829):     if sample_weight is not None:
1.01 /testbed/sklearn/linear_model/logistic.py(833):         sample_weight = np.ones(X.shape[0], dtype=X.dtype)
1.01 /testbed/sklearn/linear_model/logistic.py(838):     le = LabelEncoder()
1.01 /testbed/sklearn/linear_model/logistic.py(839):     if isinstance(class_weight, dict) or multi_class == 'multinomial':
1.01 /testbed/sklearn/linear_model/logistic.py(845):     if multi_class == 'ovr':
1.01 /testbed/sklearn/linear_model/logistic.py(846):         w0 = np.zeros(n_features + int(fit_intercept), dtype=X.dtype)
1.01 /testbed/sklearn/linear_model/logistic.py(847):         mask_classes = np.array([-1, 1])
1.01 /testbed/sklearn/linear_model/logistic.py(848):         mask = (y == pos_class)
1.01 /testbed/sklearn/linear_model/logistic.py(849):         y_bin = np.ones(y.shape, dtype=X.dtype)
1.01 /testbed/sklearn/linear_model/logistic.py(850):         y_bin[~mask] = -1.
1.01 /testbed/sklearn/linear_model/logistic.py(853):         if class_weight == "balanced":
1.01 /testbed/sklearn/linear_model/logistic.py(872):     if coef is not None:
1.01 /testbed/sklearn/linear_model/logistic.py(901):     if multi_class == 'multinomial':
1.01 /testbed/sklearn/linear_model/logistic.py(914):         target = y_bin
1.01 /testbed/sklearn/linear_model/logistic.py(915):         if solver == 'lbfgs':
1.01 /testbed/sklearn/linear_model/logistic.py(917):         elif solver == 'newton-cg':
1.01 /testbed/sklearn/linear_model/logistic.py(921):         warm_start_sag = {'coef': np.expand_dims(w0, axis=1)}
1.01 /testbed/sklearn/linear_model/logistic.py(923):     coefs = list()
1.01 /testbed/sklearn/linear_model/logistic.py(924):     n_iter = np.zeros(len(Cs), dtype=np.int32)
1.01 /testbed/sklearn/linear_model/logistic.py(925):     for i, C in enumerate(Cs):
1.01 /testbed/sklearn/linear_model/logistic.py(926):         if solver == 'lbfgs':
1.01 /testbed/sklearn/linear_model/logistic.py(939):         elif solver == 'newton-cg':
1.01 /testbed/sklearn/linear_model/logistic.py(943):         elif solver == 'liblinear':
1.01 /testbed/sklearn/linear_model/logistic.py(953):         elif solver in ['sag', 'saga']:
1.01 /testbed/sklearn/linear_model/logistic.py(954):             if multi_class == 'multinomial':
1.01 /testbed/sklearn/linear_model/logistic.py(958):                 loss = 'log'
1.01 /testbed/sklearn/linear_model/logistic.py(960):             if penalty == 'l1':
1.01 /testbed/sklearn/linear_model/logistic.py(963):             elif penalty == 'l2':
1.01 /testbed/sklearn/linear_model/logistic.py(964):                 alpha = 1. / C
1.01 /testbed/sklearn/linear_model/logistic.py(965):                 beta = 0.
1.01 /testbed/sklearn/linear_model/logistic.py(970):             w0, n_iter_i, warm_start_sag = sag_solver(
1.01 /testbed/sklearn/linear_model/logistic.py(971):                 X, target, sample_weight, loss, alpha,
1.01 /testbed/sklearn/linear_model/logistic.py(972):                 beta, max_iter, tol,
1.01 /testbed/sklearn/linear_model/logistic.py(973):                 verbose, random_state, False, max_squared_sum, warm_start_sag,
1.01 /testbed/sklearn/linear_model/logistic.py(974):                 is_saga=(solver == 'saga'))
1.01 /testbed/sklearn/linear_model/logistic.py(980):         if multi_class == 'multinomial':
1.01 /testbed/sklearn/linear_model/logistic.py(987):             coefs.append(w0.copy())
1.01 /testbed/sklearn/linear_model/logistic.py(989):         n_iter[i] = n_iter_i
1.01 /testbed/sklearn/linear_model/logistic.py(925):     for i, C in enumerate(Cs):
1.01 /testbed/sklearn/linear_model/logistic.py(926):         if solver == 'lbfgs':
1.01 /testbed/sklearn/linear_model/logistic.py(939):         elif solver == 'newton-cg':
1.01 /testbed/sklearn/linear_model/logistic.py(943):         elif solver == 'liblinear':
1.01 /testbed/sklearn/linear_model/logistic.py(953):         elif solver in ['sag', 'saga']:
1.01 /testbed/sklearn/linear_model/logistic.py(954):             if multi_class == 'multinomial':
1.01 /testbed/sklearn/linear_model/logistic.py(958):                 loss = 'log'
1.01 /testbed/sklearn/linear_model/logistic.py(960):             if penalty == 'l1':
1.01 /testbed/sklearn/linear_model/logistic.py(963):             elif penalty == 'l2':
1.01 /testbed/sklearn/linear_model/logistic.py(964):                 alpha = 1. / C
1.01 /testbed/sklearn/linear_model/logistic.py(965):                 beta = 0.
1.01 /testbed/sklearn/linear_model/logistic.py(970):             w0, n_iter_i, warm_start_sag = sag_solver(
1.01 /testbed/sklearn/linear_model/logistic.py(971):                 X, target, sample_weight, loss, alpha,
1.01 /testbed/sklearn/linear_model/logistic.py(972):                 beta, max_iter, tol,
1.01 /testbed/sklearn/linear_model/logistic.py(973):                 verbose, random_state, False, max_squared_sum, warm_start_sag,
1.01 /testbed/sklearn/linear_model/logistic.py(974):                 is_saga=(solver == 'saga'))
1.01 /testbed/sklearn/linear_model/logistic.py(980):         if multi_class == 'multinomial':
1.01 /testbed/sklearn/linear_model/logistic.py(987):             coefs.append(w0.copy())
1.01 /testbed/sklearn/linear_model/logistic.py(989):         n_iter[i] = n_iter_i
1.01 /testbed/sklearn/linear_model/logistic.py(925):     for i, C in enumerate(Cs):
1.01 /testbed/sklearn/linear_model/logistic.py(926):         if solver == 'lbfgs':
1.01 /testbed/sklearn/linear_model/logistic.py(939):         elif solver == 'newton-cg':
1.01 /testbed/sklearn/linear_model/logistic.py(943):         elif solver == 'liblinear':
1.01 /testbed/sklearn/linear_model/logistic.py(953):         elif solver in ['sag', 'saga']:
1.01 /testbed/sklearn/linear_model/logistic.py(954):             if multi_class == 'multinomial':
1.01 /testbed/sklearn/linear_model/logistic.py(958):                 loss = 'log'
1.01 /testbed/sklearn/linear_model/logistic.py(960):             if penalty == 'l1':
1.01 /testbed/sklearn/linear_model/logistic.py(963):             elif penalty == 'l2':
1.01 /testbed/sklearn/linear_model/logistic.py(964):                 alpha = 1. / C
1.01 /testbed/sklearn/linear_model/logistic.py(965):                 beta = 0.
1.01 /testbed/sklearn/linear_model/logistic.py(970):             w0, n_iter_i, warm_start_sag = sag_solver(
1.01 /testbed/sklearn/linear_model/logistic.py(971):                 X, target, sample_weight, loss, alpha,
1.01 /testbed/sklearn/linear_model/logistic.py(972):                 beta, max_iter, tol,
1.01 /testbed/sklearn/linear_model/logistic.py(973):                 verbose, random_state, False, max_squared_sum, warm_start_sag,
1.01 /testbed/sklearn/linear_model/logistic.py(974):                 is_saga=(solver == 'saga'))
1.01 /testbed/sklearn/linear_model/logistic.py(980):         if multi_class == 'multinomial':
1.01 /testbed/sklearn/linear_model/logistic.py(987):             coefs.append(w0.copy())
1.01 /testbed/sklearn/linear_model/logistic.py(989):         n_iter[i] = n_iter_i
1.01 /testbed/sklearn/linear_model/logistic.py(925):     for i, C in enumerate(Cs):
1.01 /testbed/sklearn/linear_model/logistic.py(926):         if solver == 'lbfgs':
1.01 /testbed/sklearn/linear_model/logistic.py(939):         elif solver == 'newton-cg':
1.01 /testbed/sklearn/linear_model/logistic.py(943):         elif solver == 'liblinear':
1.01 /testbed/sklearn/linear_model/logistic.py(953):         elif solver in ['sag', 'saga']:
1.01 /testbed/sklearn/linear_model/logistic.py(954):             if multi_class == 'multinomial':
1.01 /testbed/sklearn/linear_model/logistic.py(958):                 loss = 'log'
1.01 /testbed/sklearn/linear_model/logistic.py(960):             if penalty == 'l1':
1.01 /testbed/sklearn/linear_model/logistic.py(963):             elif penalty == 'l2':
1.01 /testbed/sklearn/linear_model/logistic.py(964):                 alpha = 1. / C
1.01 /testbed/sklearn/linear_model/logistic.py(965):                 beta = 0.
1.01 /testbed/sklearn/linear_model/logistic.py(970):             w0, n_iter_i, warm_start_sag = sag_solver(
1.01 /testbed/sklearn/linear_model/logistic.py(971):                 X, target, sample_weight, loss, alpha,
1.01 /testbed/sklearn/linear_model/logistic.py(972):                 beta, max_iter, tol,
1.01 /testbed/sklearn/linear_model/logistic.py(973):                 verbose, random_state, False, max_squared_sum, warm_start_sag,
1.01 /testbed/sklearn/linear_model/logistic.py(974):                 is_saga=(solver == 'saga'))
1.01 /testbed/sklearn/linear_model/logistic.py(980):         if multi_class == 'multinomial':
1.01 /testbed/sklearn/linear_model/logistic.py(987):             coefs.append(w0.copy())
1.01 /testbed/sklearn/linear_model/logistic.py(989):         n_iter[i] = n_iter_i
1.01 /testbed/sklearn/linear_model/logistic.py(925):     for i, C in enumerate(Cs):
1.01 /testbed/sklearn/linear_model/logistic.py(926):         if solver == 'lbfgs':
1.01 /testbed/sklearn/linear_model/logistic.py(939):         elif solver == 'newton-cg':
1.01 /testbed/sklearn/linear_model/logistic.py(943):         elif solver == 'liblinear':
1.01 /testbed/sklearn/linear_model/logistic.py(953):         elif solver in ['sag', 'saga']:
1.01 /testbed/sklearn/linear_model/logistic.py(954):             if multi_class == 'multinomial':
1.01 /testbed/sklearn/linear_model/logistic.py(958):                 loss = 'log'
1.01 /testbed/sklearn/linear_model/logistic.py(960):             if penalty == 'l1':
1.01 /testbed/sklearn/linear_model/logistic.py(963):             elif penalty == 'l2':
1.01 /testbed/sklearn/linear_model/logistic.py(964):                 alpha = 1. / C
1.01 /testbed/sklearn/linear_model/logistic.py(965):                 beta = 0.
1.01 /testbed/sklearn/linear_model/logistic.py(970):             w0, n_iter_i, warm_start_sag = sag_solver(
1.01 /testbed/sklearn/linear_model/logistic.py(971):                 X, target, sample_weight, loss, alpha,
1.02 /testbed/sklearn/linear_model/logistic.py(972):                 beta, max_iter, tol,
1.02 /testbed/sklearn/linear_model/logistic.py(973):                 verbose, random_state, False, max_squared_sum, warm_start_sag,
1.02 /testbed/sklearn/linear_model/logistic.py(974):                 is_saga=(solver == 'saga'))
1.02 /testbed/sklearn/linear_model/logistic.py(980):         if multi_class == 'multinomial':
1.02 /testbed/sklearn/linear_model/logistic.py(987):             coefs.append(w0.copy())
1.02 /testbed/sklearn/linear_model/logistic.py(989):         n_iter[i] = n_iter_i
1.02 /testbed/sklearn/linear_model/logistic.py(925):     for i, C in enumerate(Cs):
1.02 /testbed/sklearn/linear_model/logistic.py(926):         if solver == 'lbfgs':
1.02 /testbed/sklearn/linear_model/logistic.py(939):         elif solver == 'newton-cg':
1.02 /testbed/sklearn/linear_model/logistic.py(943):         elif solver == 'liblinear':
1.02 /testbed/sklearn/linear_model/logistic.py(953):         elif solver in ['sag', 'saga']:
1.02 /testbed/sklearn/linear_model/logistic.py(954):             if multi_class == 'multinomial':
1.02 /testbed/sklearn/linear_model/logistic.py(958):                 loss = 'log'
1.02 /testbed/sklearn/linear_model/logistic.py(960):             if penalty == 'l1':
1.02 /testbed/sklearn/linear_model/logistic.py(963):             elif penalty == 'l2':
1.02 /testbed/sklearn/linear_model/logistic.py(964):                 alpha = 1. / C
1.02 /testbed/sklearn/linear_model/logistic.py(965):                 beta = 0.
1.02 /testbed/sklearn/linear_model/logistic.py(970):             w0, n_iter_i, warm_start_sag = sag_solver(
1.02 /testbed/sklearn/linear_model/logistic.py(971):                 X, target, sample_weight, loss, alpha,
1.02 /testbed/sklearn/linear_model/logistic.py(972):                 beta, max_iter, tol,
1.02 /testbed/sklearn/linear_model/logistic.py(973):                 verbose, random_state, False, max_squared_sum, warm_start_sag,
1.02 /testbed/sklearn/linear_model/logistic.py(974):                 is_saga=(solver == 'saga'))
1.02 /testbed/sklearn/linear_model/logistic.py(980):         if multi_class == 'multinomial':
1.02 /testbed/sklearn/linear_model/logistic.py(987):             coefs.append(w0.copy())
1.02 /testbed/sklearn/linear_model/logistic.py(989):         n_iter[i] = n_iter_i
1.02 /testbed/sklearn/linear_model/logistic.py(925):     for i, C in enumerate(Cs):
1.02 /testbed/sklearn/linear_model/logistic.py(926):         if solver == 'lbfgs':
1.02 /testbed/sklearn/linear_model/logistic.py(939):         elif solver == 'newton-cg':
1.02 /testbed/sklearn/linear_model/logistic.py(943):         elif solver == 'liblinear':
1.02 /testbed/sklearn/linear_model/logistic.py(953):         elif solver in ['sag', 'saga']:
1.02 /testbed/sklearn/linear_model/logistic.py(954):             if multi_class == 'multinomial':
1.02 /testbed/sklearn/linear_model/logistic.py(958):                 loss = 'log'
1.02 /testbed/sklearn/linear_model/logistic.py(960):             if penalty == 'l1':
1.02 /testbed/sklearn/linear_model/logistic.py(963):             elif penalty == 'l2':
1.02 /testbed/sklearn/linear_model/logistic.py(964):                 alpha = 1. / C
1.02 /testbed/sklearn/linear_model/logistic.py(965):                 beta = 0.
1.02 /testbed/sklearn/linear_model/logistic.py(970):             w0, n_iter_i, warm_start_sag = sag_solver(
1.02 /testbed/sklearn/linear_model/logistic.py(971):                 X, target, sample_weight, loss, alpha,
1.02 /testbed/sklearn/linear_model/logistic.py(972):                 beta, max_iter, tol,
1.02 /testbed/sklearn/linear_model/logistic.py(973):                 verbose, random_state, False, max_squared_sum, warm_start_sag,
1.02 /testbed/sklearn/linear_model/logistic.py(974):                 is_saga=(solver == 'saga'))
1.02 /testbed/sklearn/linear_model/logistic.py(980):         if multi_class == 'multinomial':
1.02 /testbed/sklearn/linear_model/logistic.py(987):             coefs.append(w0.copy())
1.02 /testbed/sklearn/linear_model/logistic.py(989):         n_iter[i] = n_iter_i
1.02 /testbed/sklearn/linear_model/logistic.py(925):     for i, C in enumerate(Cs):
1.02 /testbed/sklearn/linear_model/logistic.py(926):         if solver == 'lbfgs':
1.02 /testbed/sklearn/linear_model/logistic.py(939):         elif solver == 'newton-cg':
1.02 /testbed/sklearn/linear_model/logistic.py(943):         elif solver == 'liblinear':
1.02 /testbed/sklearn/linear_model/logistic.py(953):         elif solver in ['sag', 'saga']:
1.02 /testbed/sklearn/linear_model/logistic.py(954):             if multi_class == 'multinomial':
1.02 /testbed/sklearn/linear_model/logistic.py(958):                 loss = 'log'
1.02 /testbed/sklearn/linear_model/logistic.py(960):             if penalty == 'l1':
1.02 /testbed/sklearn/linear_model/logistic.py(963):             elif penalty == 'l2':
1.02 /testbed/sklearn/linear_model/logistic.py(964):                 alpha = 1. / C
1.02 /testbed/sklearn/linear_model/logistic.py(965):                 beta = 0.
1.02 /testbed/sklearn/linear_model/logistic.py(970):             w0, n_iter_i, warm_start_sag = sag_solver(
1.02 /testbed/sklearn/linear_model/logistic.py(971):                 X, target, sample_weight, loss, alpha,
1.02 /testbed/sklearn/linear_model/logistic.py(972):                 beta, max_iter, tol,
1.02 /testbed/sklearn/linear_model/logistic.py(973):                 verbose, random_state, False, max_squared_sum, warm_start_sag,
1.02 /testbed/sklearn/linear_model/logistic.py(974):                 is_saga=(solver == 'saga'))
1.02 /testbed/sklearn/linear_model/logistic.py(980):         if multi_class == 'multinomial':
1.02 /testbed/sklearn/linear_model/logistic.py(987):             coefs.append(w0.copy())
1.02 /testbed/sklearn/linear_model/logistic.py(989):         n_iter[i] = n_iter_i
1.02 /testbed/sklearn/linear_model/logistic.py(925):     for i, C in enumerate(Cs):
1.02 /testbed/sklearn/linear_model/logistic.py(926):         if solver == 'lbfgs':
1.02 /testbed/sklearn/linear_model/logistic.py(939):         elif solver == 'newton-cg':
1.02 /testbed/sklearn/linear_model/logistic.py(943):         elif solver == 'liblinear':
1.02 /testbed/sklearn/linear_model/logistic.py(953):         elif solver in ['sag', 'saga']:
1.02 /testbed/sklearn/linear_model/logistic.py(954):             if multi_class == 'multinomial':
1.02 /testbed/sklearn/linear_model/logistic.py(958):                 loss = 'log'
1.02 /testbed/sklearn/linear_model/logistic.py(960):             if penalty == 'l1':
1.02 /testbed/sklearn/linear_model/logistic.py(963):             elif penalty == 'l2':
1.02 /testbed/sklearn/linear_model/logistic.py(964):                 alpha = 1. / C
1.02 /testbed/sklearn/linear_model/logistic.py(965):                 beta = 0.
1.02 /testbed/sklearn/linear_model/logistic.py(970):             w0, n_iter_i, warm_start_sag = sag_solver(
1.02 /testbed/sklearn/linear_model/logistic.py(971):                 X, target, sample_weight, loss, alpha,
1.02 /testbed/sklearn/linear_model/logistic.py(972):                 beta, max_iter, tol,
1.02 /testbed/sklearn/linear_model/logistic.py(973):                 verbose, random_state, False, max_squared_sum, warm_start_sag,
1.02 /testbed/sklearn/linear_model/logistic.py(974):                 is_saga=(solver == 'saga'))
1.02 /testbed/sklearn/linear_model/logistic.py(980):         if multi_class == 'multinomial':
1.02 /testbed/sklearn/linear_model/logistic.py(987):             coefs.append(w0.copy())
1.02 /testbed/sklearn/linear_model/logistic.py(989):         n_iter[i] = n_iter_i
1.02 /testbed/sklearn/linear_model/logistic.py(925):     for i, C in enumerate(Cs):
1.02 /testbed/sklearn/linear_model/logistic.py(926):         if solver == 'lbfgs':
1.02 /testbed/sklearn/linear_model/logistic.py(939):         elif solver == 'newton-cg':
1.02 /testbed/sklearn/linear_model/logistic.py(943):         elif solver == 'liblinear':
1.02 /testbed/sklearn/linear_model/logistic.py(953):         elif solver in ['sag', 'saga']:
1.02 /testbed/sklearn/linear_model/logistic.py(954):             if multi_class == 'multinomial':
1.02 /testbed/sklearn/linear_model/logistic.py(958):                 loss = 'log'
1.02 /testbed/sklearn/linear_model/logistic.py(960):             if penalty == 'l1':
1.02 /testbed/sklearn/linear_model/logistic.py(963):             elif penalty == 'l2':
1.02 /testbed/sklearn/linear_model/logistic.py(964):                 alpha = 1. / C
1.02 /testbed/sklearn/linear_model/logistic.py(965):                 beta = 0.
1.02 /testbed/sklearn/linear_model/logistic.py(970):             w0, n_iter_i, warm_start_sag = sag_solver(
1.02 /testbed/sklearn/linear_model/logistic.py(971):                 X, target, sample_weight, loss, alpha,
1.02 /testbed/sklearn/linear_model/logistic.py(972):                 beta, max_iter, tol,
1.02 /testbed/sklearn/linear_model/logistic.py(973):                 verbose, random_state, False, max_squared_sum, warm_start_sag,
1.02 /testbed/sklearn/linear_model/logistic.py(974):                 is_saga=(solver == 'saga'))
1.02 /testbed/sklearn/linear_model/logistic.py(980):         if multi_class == 'multinomial':
1.02 /testbed/sklearn/linear_model/logistic.py(987):             coefs.append(w0.copy())
1.02 /testbed/sklearn/linear_model/logistic.py(989):         n_iter[i] = n_iter_i
1.02 /testbed/sklearn/linear_model/logistic.py(925):     for i, C in enumerate(Cs):
1.02 /testbed/sklearn/linear_model/logistic.py(991):     return np.array(coefs), np.array(Cs), n_iter
1.02 /testbed/sklearn/linear_model/logistic.py(1152):     log_reg = LogisticRegression(solver=solver, multi_class=multi_class)
1.02 /testbed/sklearn/linear_model/logistic.py(1437):         self.penalty = penalty
1.02 /testbed/sklearn/linear_model/logistic.py(1438):         self.dual = dual
1.02 /testbed/sklearn/linear_model/logistic.py(1439):         self.tol = tol
1.02 /testbed/sklearn/linear_model/logistic.py(1440):         self.C = C
1.02 /testbed/sklearn/linear_model/logistic.py(1441):         self.fit_intercept = fit_intercept
1.02 /testbed/sklearn/linear_model/logistic.py(1442):         self.intercept_scaling = intercept_scaling
1.02 /testbed/sklearn/linear_model/logistic.py(1443):         self.class_weight = class_weight
1.02 /testbed/sklearn/linear_model/logistic.py(1444):         self.random_state = random_state
1.02 /testbed/sklearn/linear_model/logistic.py(1445):         self.solver = solver
1.02 /testbed/sklearn/linear_model/logistic.py(1446):         self.max_iter = max_iter
1.02 /testbed/sklearn/linear_model/logistic.py(1447):         self.multi_class = multi_class
1.02 /testbed/sklearn/linear_model/logistic.py(1448):         self.verbose = verbose
1.02 /testbed/sklearn/linear_model/logistic.py(1449):         self.warm_start = warm_start
1.02 /testbed/sklearn/linear_model/logistic.py(1450):         self.n_jobs = n_jobs
1.02 /testbed/sklearn/linear_model/logistic.py(1451):         self.l1_ratio = l1_ratio
1.02 /testbed/sklearn/linear_model/logistic.py(1155):     if multi_class == 'ovr':
1.02 /testbed/sklearn/linear_model/logistic.py(1156):         log_reg.classes_ = np.array([-1, 1])
1.02 /testbed/sklearn/linear_model/logistic.py(1163):     if pos_class is not None:
1.02 /testbed/sklearn/linear_model/logistic.py(1164):         mask = (y_test == pos_class)
1.02 /testbed/sklearn/linear_model/logistic.py(1165):         y_test = np.ones(y_test.shape, dtype=np.float64)
1.02 /testbed/sklearn/linear_model/logistic.py(1166):         y_test[~mask] = -1.
1.02 /testbed/sklearn/linear_model/logistic.py(1168):     scores = list()
1.02 /testbed/sklearn/linear_model/logistic.py(1170):     if isinstance(scoring, str):
1.02 /testbed/sklearn/linear_model/logistic.py(1172):     for w in coefs:
1.02 /testbed/sklearn/linear_model/logistic.py(1173):         if multi_class == 'ovr':
1.02 /testbed/sklearn/linear_model/logistic.py(1174):             w = w[np.newaxis, :]
1.02 /testbed/sklearn/linear_model/logistic.py(1175):         if fit_intercept:
1.02 /testbed/sklearn/linear_model/logistic.py(1176):             log_reg.coef_ = w[:, :-1]
1.02 /testbed/sklearn/linear_model/logistic.py(1177):             log_reg.intercept_ = w[:, -1]
1.02 /testbed/sklearn/linear_model/logistic.py(1182):         if scoring is None:
1.02 /testbed/sklearn/linear_model/logistic.py(1183):             scores.append(log_reg.score(X_test, y_test))
1.02 /testbed/sklearn/linear_model/logistic.py(1172):     for w in coefs:
1.02 /testbed/sklearn/linear_model/logistic.py(1173):         if multi_class == 'ovr':
1.02 /testbed/sklearn/linear_model/logistic.py(1174):             w = w[np.newaxis, :]
1.02 /testbed/sklearn/linear_model/logistic.py(1175):         if fit_intercept:
1.02 /testbed/sklearn/linear_model/logistic.py(1176):             log_reg.coef_ = w[:, :-1]
1.02 /testbed/sklearn/linear_model/logistic.py(1177):             log_reg.intercept_ = w[:, -1]
1.02 /testbed/sklearn/linear_model/logistic.py(1182):         if scoring is None:
1.02 /testbed/sklearn/linear_model/logistic.py(1183):             scores.append(log_reg.score(X_test, y_test))
1.02 /testbed/sklearn/linear_model/logistic.py(1172):     for w in coefs:
1.02 /testbed/sklearn/linear_model/logistic.py(1173):         if multi_class == 'ovr':
1.02 /testbed/sklearn/linear_model/logistic.py(1174):             w = w[np.newaxis, :]
1.02 /testbed/sklearn/linear_model/logistic.py(1175):         if fit_intercept:
1.02 /testbed/sklearn/linear_model/logistic.py(1176):             log_reg.coef_ = w[:, :-1]
1.02 /testbed/sklearn/linear_model/logistic.py(1177):             log_reg.intercept_ = w[:, -1]
1.02 /testbed/sklearn/linear_model/logistic.py(1182):         if scoring is None:
1.02 /testbed/sklearn/linear_model/logistic.py(1183):             scores.append(log_reg.score(X_test, y_test))
1.02 /testbed/sklearn/linear_model/logistic.py(1172):     for w in coefs:
1.02 /testbed/sklearn/linear_model/logistic.py(1173):         if multi_class == 'ovr':
1.02 /testbed/sklearn/linear_model/logistic.py(1174):             w = w[np.newaxis, :]
1.02 /testbed/sklearn/linear_model/logistic.py(1175):         if fit_intercept:
1.02 /testbed/sklearn/linear_model/logistic.py(1176):             log_reg.coef_ = w[:, :-1]
1.02 /testbed/sklearn/linear_model/logistic.py(1177):             log_reg.intercept_ = w[:, -1]
1.02 /testbed/sklearn/linear_model/logistic.py(1182):         if scoring is None:
1.02 /testbed/sklearn/linear_model/logistic.py(1183):             scores.append(log_reg.score(X_test, y_test))
1.02 /testbed/sklearn/linear_model/logistic.py(1172):     for w in coefs:
1.02 /testbed/sklearn/linear_model/logistic.py(1173):         if multi_class == 'ovr':
1.02 /testbed/sklearn/linear_model/logistic.py(1174):             w = w[np.newaxis, :]
1.02 /testbed/sklearn/linear_model/logistic.py(1175):         if fit_intercept:
1.02 /testbed/sklearn/linear_model/logistic.py(1176):             log_reg.coef_ = w[:, :-1]
1.02 /testbed/sklearn/linear_model/logistic.py(1177):             log_reg.intercept_ = w[:, -1]
1.02 /testbed/sklearn/linear_model/logistic.py(1182):         if scoring is None:
1.02 /testbed/sklearn/linear_model/logistic.py(1183):             scores.append(log_reg.score(X_test, y_test))
1.02 /testbed/sklearn/linear_model/logistic.py(1172):     for w in coefs:
1.02 /testbed/sklearn/linear_model/logistic.py(1173):         if multi_class == 'ovr':
1.02 /testbed/sklearn/linear_model/logistic.py(1174):             w = w[np.newaxis, :]
1.02 /testbed/sklearn/linear_model/logistic.py(1175):         if fit_intercept:
1.02 /testbed/sklearn/linear_model/logistic.py(1176):             log_reg.coef_ = w[:, :-1]
1.02 /testbed/sklearn/linear_model/logistic.py(1177):             log_reg.intercept_ = w[:, -1]
1.02 /testbed/sklearn/linear_model/logistic.py(1182):         if scoring is None:
1.02 /testbed/sklearn/linear_model/logistic.py(1183):             scores.append(log_reg.score(X_test, y_test))
1.02 /testbed/sklearn/linear_model/logistic.py(1172):     for w in coefs:
1.02 /testbed/sklearn/linear_model/logistic.py(1173):         if multi_class == 'ovr':
1.02 /testbed/sklearn/linear_model/logistic.py(1174):             w = w[np.newaxis, :]
1.02 /testbed/sklearn/linear_model/logistic.py(1175):         if fit_intercept:
1.02 /testbed/sklearn/linear_model/logistic.py(1176):             log_reg.coef_ = w[:, :-1]
1.02 /testbed/sklearn/linear_model/logistic.py(1177):             log_reg.intercept_ = w[:, -1]
1.02 /testbed/sklearn/linear_model/logistic.py(1182):         if scoring is None:
1.02 /testbed/sklearn/linear_model/logistic.py(1183):             scores.append(log_reg.score(X_test, y_test))
1.02 /testbed/sklearn/linear_model/logistic.py(1172):     for w in coefs:
1.02 /testbed/sklearn/linear_model/logistic.py(1173):         if multi_class == 'ovr':
1.02 /testbed/sklearn/linear_model/logistic.py(1174):             w = w[np.newaxis, :]
1.02 /testbed/sklearn/linear_model/logistic.py(1175):         if fit_intercept:
1.02 /testbed/sklearn/linear_model/logistic.py(1176):             log_reg.coef_ = w[:, :-1]
1.02 /testbed/sklearn/linear_model/logistic.py(1177):             log_reg.intercept_ = w[:, -1]
1.02 /testbed/sklearn/linear_model/logistic.py(1182):         if scoring is None:
1.02 /testbed/sklearn/linear_model/logistic.py(1183):             scores.append(log_reg.score(X_test, y_test))
1.02 /testbed/sklearn/linear_model/logistic.py(1172):     for w in coefs:
1.02 /testbed/sklearn/linear_model/logistic.py(1173):         if multi_class == 'ovr':
1.02 /testbed/sklearn/linear_model/logistic.py(1174):             w = w[np.newaxis, :]
1.02 /testbed/sklearn/linear_model/logistic.py(1175):         if fit_intercept:
1.02 /testbed/sklearn/linear_model/logistic.py(1176):             log_reg.coef_ = w[:, :-1]
1.02 /testbed/sklearn/linear_model/logistic.py(1177):             log_reg.intercept_ = w[:, -1]
1.02 /testbed/sklearn/linear_model/logistic.py(1182):         if scoring is None:
1.02 /testbed/sklearn/linear_model/logistic.py(1183):             scores.append(log_reg.score(X_test, y_test))
1.02 /testbed/sklearn/linear_model/logistic.py(1172):     for w in coefs:
1.02 /testbed/sklearn/linear_model/logistic.py(1173):         if multi_class == 'ovr':
1.02 /testbed/sklearn/linear_model/logistic.py(1174):             w = w[np.newaxis, :]
1.02 /testbed/sklearn/linear_model/logistic.py(1175):         if fit_intercept:
1.02 /testbed/sklearn/linear_model/logistic.py(1176):             log_reg.coef_ = w[:, :-1]
1.02 /testbed/sklearn/linear_model/logistic.py(1177):             log_reg.intercept_ = w[:, -1]
1.02 /testbed/sklearn/linear_model/logistic.py(1182):         if scoring is None:
1.02 /testbed/sklearn/linear_model/logistic.py(1183):             scores.append(log_reg.score(X_test, y_test))
1.02 /testbed/sklearn/linear_model/logistic.py(1172):     for w in coefs:
1.02 /testbed/sklearn/linear_model/logistic.py(1187):     return coefs, Cs, np.array(scores), n_iter
1.02 /testbed/sklearn/linear_model/logistic.py(2071):             for l1_ratio in l1_ratios_)
1.02 /testbed/sklearn/linear_model/logistic.py(2070):             for train, test in folds
1.02 /testbed/sklearn/linear_model/logistic.py(2057):             path_func(X, y, train, test, pos_class=label, Cs=self.Cs,
1.02 /testbed/sklearn/linear_model/logistic.py(2084):         coefs_paths, Cs, scores, n_iter_ = zip(*fold_coefs_)
1.02 /testbed/sklearn/linear_model/logistic.py(2085):         self.Cs_ = Cs[0]
1.02 /testbed/sklearn/linear_model/logistic.py(2086):         if multi_class == 'multinomial':
1.02 /testbed/sklearn/linear_model/logistic.py(2102):             coefs_paths = np.reshape(
1.02 /testbed/sklearn/linear_model/logistic.py(2103):                 coefs_paths,
1.02 /testbed/sklearn/linear_model/logistic.py(2104):                 (n_classes, len(folds), len(self.Cs_) * len(l1_ratios_),
1.02 /testbed/sklearn/linear_model/logistic.py(2105):                  -1)
1.02 /testbed/sklearn/linear_model/logistic.py(2107):             self.n_iter_ = np.reshape(
1.02 /testbed/sklearn/linear_model/logistic.py(2108):                 n_iter_,
1.02 /testbed/sklearn/linear_model/logistic.py(2109):                 (n_classes, len(folds), len(self.Cs_) * len(l1_ratios_))
1.02 /testbed/sklearn/linear_model/logistic.py(2111):         scores = np.reshape(scores, (n_classes, len(folds), -1))
1.02 /testbed/sklearn/linear_model/logistic.py(2112):         self.scores_ = dict(zip(classes, scores))
1.02 /testbed/sklearn/linear_model/logistic.py(2113):         self.coefs_paths_ = dict(zip(classes, coefs_paths))
1.02 /testbed/sklearn/linear_model/logistic.py(2115):         self.C_ = list()
1.02 /testbed/sklearn/linear_model/logistic.py(2116):         self.l1_ratio_ = list()
1.02 /testbed/sklearn/linear_model/logistic.py(2117):         self.coef_ = np.empty((n_classes, X.shape[1]))
1.02 /testbed/sklearn/linear_model/logistic.py(2118):         self.intercept_ = np.zeros(n_classes)
1.02 /testbed/sklearn/linear_model/logistic.py(2119):         for index, (cls, encoded_label) in enumerate(
1.02 /testbed/sklearn/linear_model/logistic.py(2120):                 zip(iter_classes, iter_encoded_labels)):
1.02 /testbed/sklearn/linear_model/logistic.py(2122):             if multi_class == 'ovr':
1.02 /testbed/sklearn/linear_model/logistic.py(2123):                 scores = self.scores_[cls]
1.02 /testbed/sklearn/linear_model/logistic.py(2124):                 coefs_paths = self.coefs_paths_[cls]
1.02 /testbed/sklearn/linear_model/logistic.py(2131):             if self.refit:
1.02 /testbed/sklearn/linear_model/logistic.py(2172):                 best_indices = np.argmax(scores, axis=1)
1.02 /testbed/sklearn/linear_model/logistic.py(2173):                 if multi_class == 'ovr':
1.02 /testbed/sklearn/linear_model/logistic.py(2174):                     w = np.mean([coefs_paths[i, best_indices[i], :]
1.02 /testbed/sklearn/linear_model/logistic.py(2175):                                  for i in range(len(folds))], axis=0)
1.02 /testbed/sklearn/linear_model/logistic.py(2174):                     w = np.mean([coefs_paths[i, best_indices[i], :]
1.02 /testbed/sklearn/linear_model/logistic.py(2175):                                  for i in range(len(folds))], axis=0)
1.02 /testbed/sklearn/linear_model/logistic.py(2174):                     w = np.mean([coefs_paths[i, best_indices[i], :]
1.02 /testbed/sklearn/linear_model/logistic.py(2175):                                  for i in range(len(folds))], axis=0)
1.02 /testbed/sklearn/linear_model/logistic.py(2174):                     w = np.mean([coefs_paths[i, best_indices[i], :]
1.02 /testbed/sklearn/linear_model/logistic.py(2175):                                  for i in range(len(folds))], axis=0)
1.02 /testbed/sklearn/linear_model/logistic.py(2174):                     w = np.mean([coefs_paths[i, best_indices[i], :]
1.02 /testbed/sklearn/linear_model/logistic.py(2175):                                  for i in range(len(folds))], axis=0)
1.02 /testbed/sklearn/linear_model/logistic.py(2174):                     w = np.mean([coefs_paths[i, best_indices[i], :]
1.02 /testbed/sklearn/linear_model/logistic.py(2175):                                  for i in range(len(folds))], axis=0)
1.02 /testbed/sklearn/linear_model/logistic.py(2174):                     w = np.mean([coefs_paths[i, best_indices[i], :]
1.02 /testbed/sklearn/linear_model/logistic.py(2180):                 best_indices_C = best_indices % len(self.Cs_)
1.02 /testbed/sklearn/linear_model/logistic.py(2181):                 self.C_.append(np.mean(self.Cs_[best_indices_C]))
1.02 /testbed/sklearn/linear_model/logistic.py(2183):                 if self.penalty == 'elasticnet':
1.02 /testbed/sklearn/linear_model/logistic.py(2187):                     self.l1_ratio_.append(None)
1.02 /testbed/sklearn/linear_model/logistic.py(2189):             if multi_class == 'multinomial':
1.02 /testbed/sklearn/linear_model/logistic.py(2196):                 self.coef_[index] = w[: X.shape[1]]
1.02 /testbed/sklearn/linear_model/logistic.py(2197):                 if self.fit_intercept:
1.02 /testbed/sklearn/linear_model/logistic.py(2198):                     self.intercept_[index] = w[-1]
1.02 /testbed/sklearn/linear_model/logistic.py(2120):                 zip(iter_classes, iter_encoded_labels)):
1.02 /testbed/sklearn/linear_model/logistic.py(2200):         self.C_ = np.asarray(self.C_)
1.02 /testbed/sklearn/linear_model/logistic.py(2201):         self.l1_ratio_ = np.asarray(self.l1_ratio_)
1.02 /testbed/sklearn/linear_model/logistic.py(2202):         self.l1_ratios_ = np.asarray(l1_ratios_)
1.02 /testbed/sklearn/linear_model/logistic.py(2205):         if self.l1_ratios is not None:
1.02 /testbed/sklearn/linear_model/logistic.py(2215):         return self
=========================== short test summary info ============================
PASSED sklearn/tests/test_coverup_scikit-learn__scikit-learn-14087.py::test_logistic_regression_cv_refit_false_index_error
========================= 1 passed, 1 warning in 0.36s =========================
+ cat coverage.cover
{"/testbed/sklearn/linear_model/logistic.py": {"13": 1, "14": 1, "16": 1, "17": 1, "18": 1, "20": 1, "21": 1, "22": 1, "23": 1, "24": 1, "25": 1, "26": 1, "28": 1, "29": 1, "30": 1, "31": 1, "32": 1, "33": 1, "34": 1, "35": 1, "36": 1, "37": 1, "38": 1, "39": 1, "43": 1, "81": 1, "132": 1, "168": 1, "245": 1, "301": 1, "354": 1, "427": 1, "457": 1, "474": 1, "483": 1, "653": 1, "1002": 1, "1190": 2, "1191": 1, "1670": 2, "1671": 1, "71": 0, "72": 0, "73": 0, "74": 0, "76": 0, "77": 0, "78": 0, "110": 0, "111": 0, "113": 0, "115": 0, "116": 0, "119": 0, "121": 0, "122": 0, "124": 0, "127": 0, "128": 0, "129": 0, "158": 0, "160": 0, "161": 0, "164": 0, "165": 0, "198": 0, "199": 0, "200": 0, "202": 0, "204": 0, "205": 0, "207": 0, "208": 0, "210": 0, "213": 0, "214": 0, "217": 0, "218": 0, "219": 0, "220": 0, "223": 0, "225": 0, "228": 0, "230": 0, "242": 0, "231": 0, "232": 0, "233": 0, "236": 0, "237": 0, "238": 0, "239": 0, "240": 0, "282": 0, "283": 0, "284": 0, "285": 0, "286": 0, "287": 0, "288": 0, "289": 0, "291": 0, "292": 0, "293": 0, "294": 0, "295": 0, "296": 0, "297": 0, "298": 0, "339": 0, "340": 0, "341": 0, "342": 0, "343": 0, "344": 0, "345": 0, "346": 0, "347": 0, "348": 0, "349": 0, "350": 0, "351": 0, "392": 0, "393": 0, "394": 0, "398": 0, "399": 0, "403": 0, "424": 0, "404": 0, "405": 0, "406": 0, "407": 0, "409": 0, "412": 0, "413": 0, "414": 0, "415": 0, "416": 0, "417": 0, "418": 0, "419": 0, "420": 0, "421": 0, "422": 0, "428": 6, "429": 6, "430": 0, "431": 0, "433": 6, "434": 6, "435": 0, "436": 0, "438": 6, "439": 0, "440": 0, "441": 6, "442": 0, "443": 0, "445": 6, "446": 0, "447": 0, "449": 6, "450": 0, "451": 0, "454": 6, "458": 6, "459": 1, "460": 0, "461": 1, "462": 0, "464": 1, "465": 6, "466": 0, "467": 0, "468": 6, "469": 0, "470": 0, "471": 6, "638": 0, "639": 0, "640": 0, "641": 0, "642": 0, "643": 0, "803": 5, "804": 5, "806": 5, "809": 5, "810": 0, "811": 0, "812": 0, "813": 0, "814": 5, "816": 5, "817": 5, "819": 5, "820": 5, "821": 0, "822": 0, "824": 0, "829": 5, "830": 0, "831": 0, "833": 5, "838": 5, "839": 5, "840": 0, "841": 0, "845": 5, "846": 5, "847": 5, "848": 5, "849": 5, "850": 5, "853": 5, "854": 0, "855": 0, "856": 0, "859": 0, "860": 0, "861": 0, "862": 0, "863": 0, "866": 0, "867": 0, "869": 0, "870": 0, "872": 5, "874": 0, "875": 0, "876": 0, "877": 0, "878": 0, "879": 0, "883": 0, "884": 0, "885": 0, "887": 0, "888": 0, "889": 0, "890": 0, "892": 0, "893": 0, "895": 0, "896": 0, "897": 0, "899": 0, "901": 5, "903": 0, "904": 0, "905": 0, "906": 0, "907": 0, "908": 0, "909": 0, "910": 0, "911": 0, "912": 0, "914": 5, "915": 5, "916": 0, "917": 5, "918": 0, "919": 0, "920": 0, "921": 5, "923": 5, "924": 5, "925": 55, "926": 50, "927": 0, "928": 0, "929": 0, "930": 0, "931": 0, "932": 0, "933": 0, "934": 0, "935": 0, "938": 0, "939": 50, "940": 0, "941": 0, "942": 0, "943": 50, "944": 0, "945": 0, "946": 0, "947": 0, "948": 0, "949": 0, "951": 0, "953": 50, "954": 50, "955": 0, "956": 0, "958": 50, "960": 50, "961": 0, "962": 0, "963": 50, "964": 50, "965": 50, "967": 0, "968": 0, "970": 50, "971": 50, "972": 50, "973": 50, "974": 50, "977": 0, "978": 0, "980": 50, "981": 0, "982": 0, "983": 0, "984": 0, "985": 0, "987": 50, "989": 50, "991": 5, "1132": 5, "1133": 5, "1134": 5, "1135": 5, "1137": 5, "1138": 0, "1139": 0, "1141": 0, "1143": 5, "1144": 5, "1145": 5, "1146": 5, "1147": 5, "1148": 5, "1149": 5, "1150": 5, "1152": 5, "1155": 5, "1156": 5, "1157": 0, "1158": 0, "1160": 0, "1161": 0, "1163": 5, "1164": 5, "1165": 5, "1166": 5, "1168": 5, "1170": 5, "1171": 0, "1172": 55, "1173": 50, "1174": 50, "1175": 50, "1176": 50, "1177": 50, "1179": 0, "1180": 0, "1182": 50, "1183": 50, "1185": 0, "1187": 5, "1435": 1, "1453": 1, "1611": 1, "1651": 1, "1437": 5, "1438": 5, "1439": 5, "1440": 5, "1441": 5, "1442": 5, "1443": 5, "1444": 5, "1445": 5, "1446": 5, "1447": 5, "1448": 5, "1449": 5, "1450": 5, "1451": 5, "1480": 0, "1482": 0, "1483": 0, "1484": 0, "1485": 0, "1486": 0, "1487": 0, "1488": 0, "1489": 0, "1490": 0, "1491": 0, "1493": 0, "1494": 0, "1495": 0, "1496": 0, "1497": 0, "1501": 0, "1502": 0, "1504": 0, "1505": 0, "1506": 0, "1507": 0, "1508": 0, "1509": 0, "1510": 0, "1511": 0, "1513": 0, "1514": 0, "1516": 0, "1518": 0, "1519": 0, "1520": 0, "1521": 0, "1522": 0, "1524": 0, "1525": 0, "1527": 0, "1528": 0, "1529": 0, "1531": 0, "1532": 0, "1533": 0, "1534": 0, "1535": 0, "1536": 0, "1537": 0, "1538": 0, "1540": 0, "1541": 0, "1543": 0, "1545": 0, "1546": 0, "1547": 0, "1548": 0, "1550": 0, "1552": 0, "1553": 0, "1554": 0, "1556": 0, "1557": 0, "1559": 0, "1560": 0, "1561": 0, "1562": 0, "1563": 0, "1565": 0, "1566": 0, "1569": 0, "1570": 0, "1571": 0, "1572": 0, "1573": 0, "1575": 0, "1579": 0, "1580": 0, "1582": 0, "1583": 0, "1584": 0, "1585": 0, "1593": 0, "1595": 0, "1596": 0, "1598": 0, "1599": 0, "1601": 0, "1602": 0, "1603": 0, "1605": 0, "1606": 0, "1607": 0, "1609": 0, "1634": 0, "1636": 0, "1637": 0, "1638": 0, "1639": 0, "1640": 0, "1642": 0, "1643": 0, "1646": 0, "1648": 0, "1649": 0, "1667": 0, "1917": 1, "1936": 1, "2217": 1, "1918": 1, "1919": 1, "1920": 1, "1921": 1, "1922": 1, "1923": 1, "1924": 1, "1925": 1, "1926": 1, "1927": 1, "1928": 1, "1929": 1, "1930": 1, "1931": 1, "1932": 1, "1933": 1, "1934": 1, "1956": 1, "1958": 1, "1959": 0, "1960": 0, "1961": 1, "1962": 0, "1963": 0, "1964": 1, "1965": 0, "1966": 0, "1967": 0, "1968": 0, "1970": 0, "1971": 0, "1973": 1, "1974": 0, "1976": 0, "1978": 1, "1980": 1, "1981": 0, "1982": 0, "1986": 1, "1987": 1, "1988": 1, "1989": 1, "1991": 1, "1994": 1, "1995": 1, "1996": 1, "1997": 0, "1998": 0, "2001": 1, "2002": 1, "2004": 1, "2005": 1, "2007": 1, "2008": 1, "2010": 0, "2013": 1, "2014": 1, "2017": 1, "2019": 1, "2020": 0, "2022": 0, "2024": 1, "2027": 1, "2028": 1, "2029": 1, "2033": 1, "2034": 0, "2036": 1, "2037": 1, "2040": 1, "2041": 0, "2042": 0, "2043": 0, "2044": 0, "2046": 1, "2050": 1, "2051": 1, "2053": 0, "2055": 1, "2056": 1, "2057": 3, "2069": 2, "2084": 1, "2085": 1, "2086": 1, "2087": 0, "2088": 0, "2089": 0, "2093": 0, "2094": 0, "2095": 0, "2096": 0, "2097": 0, "2100": 0, "2102": 1, "2103": 1, "2104": 1, "2105": 1, "2107": 1, "2108": 1, "2109": 1, "2111": 1, "2112": 1, "2113": 1, "2115": 1, "2116": 1, "2117": 1, "2118": 1, "2119": 1, "2120": 2, "2122": 1, "2123": 1, "2124": 1, "2127": 0, "2131": 1, "2137": 0, "2139": 0, "2140": 0, "2141": 0, "2143": 0, "2144": 0, "2145": 0, "2147": 0, "2148": 0, "2149": 0, "2151": 0, "2155": 0, "2156": 0, "2157": 0, "2158": 0, "2159": 0, "2160": 0, "2161": 0, "2162": 0, "2163": 0, "2164": 0, "2165": 0, "2166": 0, "2167": 0, "2172": 1, "2173": 1, "2174": 7, "2175": 6, "2177": 0, "2178": 0, "2180": 1, "2181": 1, "2183": 1, "2184": 0, "2185": 0, "2187": 1, "2189": 1, "2190": 0, "2191": 0, "2192": 0, "2193": 0, "2194": 0, "2196": 1, "2197": 1, "2198": 1, "2200": 1, "2201": 1, "2202": 1, "2205": 1, "2206": 0, "2207": 0, "2208": 0, "2209": 0, "2210": 0, "2211": 0, "2212": 0, "2213": 0, "2215": 1, "2070": 6, "2071": 10, "2239": 0, "2240": 0, "2244": 0, "2245": 0, "2246": 0, "2247": 0, "2249": 0}}
+ git checkout a5743ed36fbd3fbc8e351bdab16561fbfca7dfa1
Note: switching to 'a5743ed36fbd3fbc8e351bdab16561fbfca7dfa1'.

You are in 'detached HEAD' state. You can look around, make experimental
changes and commit them, and you can discard any commits you make in this
state without impacting any branches by switching back to a branch.

If you want to create a new branch to retain commits you create, you may
do so (now or later) by using -c with the switch command. Example:

  git switch -c <new-branch-name>

Or undo this operation with:

  git switch -

Turn off this advice by setting config variable advice.detachedHead to false

HEAD is now at a5743ed36f TST add test for LAD-loss and quantile loss equivalence (old GBDT code) (#14086)
M	sklearn/linear_model/logistic.py
+ git apply /root/pre_state.patch
error: unrecognized input
