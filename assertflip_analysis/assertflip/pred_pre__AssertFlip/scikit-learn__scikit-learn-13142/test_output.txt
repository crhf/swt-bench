+ source /opt/miniconda3/bin/activate
++ _CONDA_ROOT=/opt/miniconda3
++ . /opt/miniconda3/etc/profile.d/conda.sh
+++ export CONDA_EXE=/opt/miniconda3/bin/conda
+++ CONDA_EXE=/opt/miniconda3/bin/conda
+++ export _CE_M=
+++ _CE_M=
+++ export _CE_CONDA=
+++ _CE_CONDA=
+++ export CONDA_PYTHON_EXE=/opt/miniconda3/bin/python
+++ CONDA_PYTHON_EXE=/opt/miniconda3/bin/python
+++ '[' -z '' ']'
+++ export CONDA_SHLVL=0
+++ CONDA_SHLVL=0
+++ '[' -n '' ']'
+++++ dirname /opt/miniconda3/bin/conda
++++ dirname /opt/miniconda3/bin
+++ PATH=/opt/miniconda3/condabin:/opt/miniconda3/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin
+++ export PATH
+++ '[' -z '' ']'
+++ PS1=
++ conda activate
++ local cmd=activate
++ case "$cmd" in
++ __conda_activate activate
++ '[' -n '' ']'
++ local ask_conda
+++ PS1=
+++ __conda_exe shell.posix activate
+++ /opt/miniconda3/bin/conda shell.posix activate
++ ask_conda='PS1='\''(base) '\''
export PATH='\''/opt/miniconda3/bin:/opt/miniconda3/condabin:/opt/miniconda3/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin'\''
export CONDA_PREFIX='\''/opt/miniconda3'\''
export CONDA_SHLVL='\''1'\''
export CONDA_DEFAULT_ENV='\''base'\''
export CONDA_PROMPT_MODIFIER='\''(base) '\''
export CONDA_EXE='\''/opt/miniconda3/bin/conda'\''
export _CE_M='\'''\''
export _CE_CONDA='\'''\''
export CONDA_PYTHON_EXE='\''/opt/miniconda3/bin/python'\'''
++ eval 'PS1='\''(base) '\''
export PATH='\''/opt/miniconda3/bin:/opt/miniconda3/condabin:/opt/miniconda3/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin'\''
export CONDA_PREFIX='\''/opt/miniconda3'\''
export CONDA_SHLVL='\''1'\''
export CONDA_DEFAULT_ENV='\''base'\''
export CONDA_PROMPT_MODIFIER='\''(base) '\''
export CONDA_EXE='\''/opt/miniconda3/bin/conda'\''
export _CE_M='\'''\''
export _CE_CONDA='\'''\''
export CONDA_PYTHON_EXE='\''/opt/miniconda3/bin/python'\'''
+++ PS1='(base) '
+++ export PATH=/opt/miniconda3/bin:/opt/miniconda3/condabin:/opt/miniconda3/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin
+++ PATH=/opt/miniconda3/bin:/opt/miniconda3/condabin:/opt/miniconda3/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin
+++ export CONDA_PREFIX=/opt/miniconda3
+++ CONDA_PREFIX=/opt/miniconda3
+++ export CONDA_SHLVL=1
+++ CONDA_SHLVL=1
+++ export CONDA_DEFAULT_ENV=base
+++ CONDA_DEFAULT_ENV=base
+++ export 'CONDA_PROMPT_MODIFIER=(base) '
+++ CONDA_PROMPT_MODIFIER='(base) '
+++ export CONDA_EXE=/opt/miniconda3/bin/conda
+++ CONDA_EXE=/opt/miniconda3/bin/conda
+++ export _CE_M=
+++ _CE_M=
+++ export _CE_CONDA=
+++ _CE_CONDA=
+++ export CONDA_PYTHON_EXE=/opt/miniconda3/bin/python
+++ CONDA_PYTHON_EXE=/opt/miniconda3/bin/python
++ __conda_hashr
++ '[' -n '' ']'
++ '[' -n '' ']'
++ hash -r
+ conda activate testbed
+ local cmd=activate
+ case "$cmd" in
+ __conda_activate activate testbed
+ '[' -n '' ']'
+ local ask_conda
++ PS1='(base) '
++ __conda_exe shell.posix activate testbed
++ /opt/miniconda3/bin/conda shell.posix activate testbed
+ ask_conda='PS1='\''(testbed) '\''
export PATH='\''/opt/miniconda3/envs/testbed/bin:/opt/miniconda3/condabin:/opt/miniconda3/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin'\''
export CONDA_PREFIX='\''/opt/miniconda3/envs/testbed'\''
export CONDA_SHLVL='\''2'\''
export CONDA_DEFAULT_ENV='\''testbed'\''
export CONDA_PROMPT_MODIFIER='\''(testbed) '\''
export CONDA_PREFIX_1='\''/opt/miniconda3'\''
export CONDA_EXE='\''/opt/miniconda3/bin/conda'\''
export _CE_M='\'''\''
export _CE_CONDA='\'''\''
export CONDA_PYTHON_EXE='\''/opt/miniconda3/bin/python'\'''
+ eval 'PS1='\''(testbed) '\''
export PATH='\''/opt/miniconda3/envs/testbed/bin:/opt/miniconda3/condabin:/opt/miniconda3/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin'\''
export CONDA_PREFIX='\''/opt/miniconda3/envs/testbed'\''
export CONDA_SHLVL='\''2'\''
export CONDA_DEFAULT_ENV='\''testbed'\''
export CONDA_PROMPT_MODIFIER='\''(testbed) '\''
export CONDA_PREFIX_1='\''/opt/miniconda3'\''
export CONDA_EXE='\''/opt/miniconda3/bin/conda'\''
export _CE_M='\'''\''
export _CE_CONDA='\'''\''
export CONDA_PYTHON_EXE='\''/opt/miniconda3/bin/python'\'''
++ PS1='(testbed) '
++ export PATH=/opt/miniconda3/envs/testbed/bin:/opt/miniconda3/condabin:/opt/miniconda3/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin
++ PATH=/opt/miniconda3/envs/testbed/bin:/opt/miniconda3/condabin:/opt/miniconda3/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin
++ export CONDA_PREFIX=/opt/miniconda3/envs/testbed
++ CONDA_PREFIX=/opt/miniconda3/envs/testbed
++ export CONDA_SHLVL=2
++ CONDA_SHLVL=2
++ export CONDA_DEFAULT_ENV=testbed
++ CONDA_DEFAULT_ENV=testbed
++ export 'CONDA_PROMPT_MODIFIER=(testbed) '
++ CONDA_PROMPT_MODIFIER='(testbed) '
++ export CONDA_PREFIX_1=/opt/miniconda3
++ CONDA_PREFIX_1=/opt/miniconda3
++ export CONDA_EXE=/opt/miniconda3/bin/conda
++ CONDA_EXE=/opt/miniconda3/bin/conda
++ export _CE_M=
++ _CE_M=
++ export _CE_CONDA=
++ _CE_CONDA=
++ export CONDA_PYTHON_EXE=/opt/miniconda3/bin/python
++ CONDA_PYTHON_EXE=/opt/miniconda3/bin/python
+ __conda_hashr
+ '[' -n '' ']'
+ '[' -n '' ']'
+ hash -r
+ cd /testbed
+ git diff HEAD 1c8668b0a021832386470ddf740d834e02c66f69
+ git config --global --add safe.directory /testbed
+ cd /testbed
+ git status
On branch main
nothing to commit, working tree clean
+ git show
commit 1c8668b0a021832386470ddf740d834e02c66f69
Author: Joel Nothman <joel.nothman@gmail.com>
Date:   Thu Feb 14 13:27:46 2019 +1100

    DOC what's new: Fix class name

diff --git a/doc/whats_new/v0.21.rst b/doc/whats_new/v0.21.rst
index fc54574145..7355f75b83 100644
--- a/doc/whats_new/v0.21.rst
+++ b/doc/whats_new/v0.21.rst
@@ -202,7 +202,7 @@ Support for Python 3.4 and below has been officially dropped.
 ..............................
 
 - |Feature| Classes :class:`~model_selection.GridSearchCV` and
-  :class:`~model_selection.RandomSearchCV` now allow for refit=callable
+  :class:`~model_selection.RandomizedSearchCV` now allow for refit=callable
   to add flexibility in identifying the best
   estimator. An example for this interface has been added.
   :issue:`11354` by :user:`Wenhao Zhang <wenhaoz@ucla.edu>`,
+ git diff 1c8668b0a021832386470ddf740d834e02c66f69
+ source /opt/miniconda3/bin/activate
++ _CONDA_ROOT=/opt/miniconda3
++ . /opt/miniconda3/etc/profile.d/conda.sh
+++ export CONDA_EXE=/opt/miniconda3/bin/conda
+++ CONDA_EXE=/opt/miniconda3/bin/conda
+++ export _CE_M=
+++ _CE_M=
+++ export _CE_CONDA=
+++ _CE_CONDA=
+++ export CONDA_PYTHON_EXE=/opt/miniconda3/bin/python
+++ CONDA_PYTHON_EXE=/opt/miniconda3/bin/python
+++ '[' -z x ']'
++ conda activate
++ local cmd=activate
++ case "$cmd" in
++ __conda_activate activate
++ '[' -n '' ']'
++ local ask_conda
+++ PS1='(testbed) '
+++ __conda_exe shell.posix activate
+++ /opt/miniconda3/bin/conda shell.posix activate
++ ask_conda='PS1='\''(base) '\''
export PATH='\''/opt/miniconda3/bin:/opt/miniconda3/condabin:/opt/miniconda3/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin'\''
export CONDA_PREFIX='\''/opt/miniconda3'\''
export CONDA_SHLVL='\''3'\''
export CONDA_DEFAULT_ENV='\''base'\''
export CONDA_PROMPT_MODIFIER='\''(base) '\''
export CONDA_PREFIX_2='\''/opt/miniconda3/envs/testbed'\''
export CONDA_EXE='\''/opt/miniconda3/bin/conda'\''
export _CE_M='\'''\''
export _CE_CONDA='\'''\''
export CONDA_PYTHON_EXE='\''/opt/miniconda3/bin/python'\'''
++ eval 'PS1='\''(base) '\''
export PATH='\''/opt/miniconda3/bin:/opt/miniconda3/condabin:/opt/miniconda3/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin'\''
export CONDA_PREFIX='\''/opt/miniconda3'\''
export CONDA_SHLVL='\''3'\''
export CONDA_DEFAULT_ENV='\''base'\''
export CONDA_PROMPT_MODIFIER='\''(base) '\''
export CONDA_PREFIX_2='\''/opt/miniconda3/envs/testbed'\''
export CONDA_EXE='\''/opt/miniconda3/bin/conda'\''
export _CE_M='\'''\''
export _CE_CONDA='\'''\''
export CONDA_PYTHON_EXE='\''/opt/miniconda3/bin/python'\'''
+++ PS1='(base) '
+++ export PATH=/opt/miniconda3/bin:/opt/miniconda3/condabin:/opt/miniconda3/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin
+++ PATH=/opt/miniconda3/bin:/opt/miniconda3/condabin:/opt/miniconda3/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin
+++ export CONDA_PREFIX=/opt/miniconda3
+++ CONDA_PREFIX=/opt/miniconda3
+++ export CONDA_SHLVL=3
+++ CONDA_SHLVL=3
+++ export CONDA_DEFAULT_ENV=base
+++ CONDA_DEFAULT_ENV=base
+++ export 'CONDA_PROMPT_MODIFIER=(base) '
+++ CONDA_PROMPT_MODIFIER='(base) '
+++ export CONDA_PREFIX_2=/opt/miniconda3/envs/testbed
+++ CONDA_PREFIX_2=/opt/miniconda3/envs/testbed
+++ export CONDA_EXE=/opt/miniconda3/bin/conda
+++ CONDA_EXE=/opt/miniconda3/bin/conda
+++ export _CE_M=
+++ _CE_M=
+++ export _CE_CONDA=
+++ _CE_CONDA=
+++ export CONDA_PYTHON_EXE=/opt/miniconda3/bin/python
+++ CONDA_PYTHON_EXE=/opt/miniconda3/bin/python
++ __conda_hashr
++ '[' -n '' ']'
++ '[' -n '' ']'
++ hash -r
+ conda activate testbed
+ local cmd=activate
+ case "$cmd" in
+ __conda_activate activate testbed
+ '[' -n '' ']'
+ local ask_conda
++ PS1='(base) '
++ __conda_exe shell.posix activate testbed
++ /opt/miniconda3/bin/conda shell.posix activate testbed
+ ask_conda='PS1='\''(testbed) '\''
export PATH='\''/opt/miniconda3/envs/testbed/bin:/opt/miniconda3/condabin:/opt/miniconda3/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin'\''
export CONDA_PREFIX='\''/opt/miniconda3/envs/testbed'\''
export CONDA_SHLVL='\''4'\''
export CONDA_DEFAULT_ENV='\''testbed'\''
export CONDA_PROMPT_MODIFIER='\''(testbed) '\''
export CONDA_PREFIX_3='\''/opt/miniconda3'\''
export CONDA_EXE='\''/opt/miniconda3/bin/conda'\''
export _CE_M='\'''\''
export _CE_CONDA='\'''\''
export CONDA_PYTHON_EXE='\''/opt/miniconda3/bin/python'\'''
+ eval 'PS1='\''(testbed) '\''
export PATH='\''/opt/miniconda3/envs/testbed/bin:/opt/miniconda3/condabin:/opt/miniconda3/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin'\''
export CONDA_PREFIX='\''/opt/miniconda3/envs/testbed'\''
export CONDA_SHLVL='\''4'\''
export CONDA_DEFAULT_ENV='\''testbed'\''
export CONDA_PROMPT_MODIFIER='\''(testbed) '\''
export CONDA_PREFIX_3='\''/opt/miniconda3'\''
export CONDA_EXE='\''/opt/miniconda3/bin/conda'\''
export _CE_M='\'''\''
export _CE_CONDA='\'''\''
export CONDA_PYTHON_EXE='\''/opt/miniconda3/bin/python'\'''
++ PS1='(testbed) '
++ export PATH=/opt/miniconda3/envs/testbed/bin:/opt/miniconda3/condabin:/opt/miniconda3/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin
++ PATH=/opt/miniconda3/envs/testbed/bin:/opt/miniconda3/condabin:/opt/miniconda3/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin
++ export CONDA_PREFIX=/opt/miniconda3/envs/testbed
++ CONDA_PREFIX=/opt/miniconda3/envs/testbed
++ export CONDA_SHLVL=4
++ CONDA_SHLVL=4
++ export CONDA_DEFAULT_ENV=testbed
++ CONDA_DEFAULT_ENV=testbed
++ export 'CONDA_PROMPT_MODIFIER=(testbed) '
++ CONDA_PROMPT_MODIFIER='(testbed) '
++ export CONDA_PREFIX_3=/opt/miniconda3
++ CONDA_PREFIX_3=/opt/miniconda3
++ export CONDA_EXE=/opt/miniconda3/bin/conda
++ CONDA_EXE=/opt/miniconda3/bin/conda
++ export _CE_M=
++ _CE_M=
++ export _CE_CONDA=
++ _CE_CONDA=
++ export CONDA_PYTHON_EXE=/opt/miniconda3/bin/python
++ CONDA_PYTHON_EXE=/opt/miniconda3/bin/python
+ __conda_hashr
+ '[' -n '' ']'
+ '[' -n '' ']'
+ hash -r
+ python -m pip install -v --no-use-pep517 --no-build-isolation -e .
Using pip 21.2.2 from /opt/miniconda3/envs/testbed/lib/python3.6/site-packages/pip (python 3.6)
Obtaining file:///testbed
    Running command python setup.py egg_info
    running egg_info
    creating /tmp/pip-pip-egg-info-018jrgmk/scikit_learn.egg-info
    writing /tmp/pip-pip-egg-info-018jrgmk/scikit_learn.egg-info/PKG-INFO
    writing dependency_links to /tmp/pip-pip-egg-info-018jrgmk/scikit_learn.egg-info/dependency_links.txt
    writing requirements to /tmp/pip-pip-egg-info-018jrgmk/scikit_learn.egg-info/requires.txt
    writing top-level names to /tmp/pip-pip-egg-info-018jrgmk/scikit_learn.egg-info/top_level.txt
    writing manifest file '/tmp/pip-pip-egg-info-018jrgmk/scikit_learn.egg-info/SOURCES.txt'
    reading manifest file '/tmp/pip-pip-egg-info-018jrgmk/scikit_learn.egg-info/SOURCES.txt'
    reading manifest template 'MANIFEST.in'
    adding license file 'COPYING'
    writing manifest file '/tmp/pip-pip-egg-info-018jrgmk/scikit_learn.egg-info/SOURCES.txt'
    Partial import of sklearn during the build process.
Requirement already satisfied: numpy>=1.11.0 in /opt/miniconda3/envs/testbed/lib/python3.6/site-packages (from scikit-learn==0.21.dev0) (1.19.2)
Requirement already satisfied: scipy>=0.17.0 in /opt/miniconda3/envs/testbed/lib/python3.6/site-packages (from scikit-learn==0.21.dev0) (1.5.2)
Installing collected packages: scikit-learn
  Attempting uninstall: scikit-learn
    Found existing installation: scikit-learn 0.21.dev0
    Uninstalling scikit-learn-0.21.dev0:
      Removing file or directory /opt/miniconda3/envs/testbed/lib/python3.6/site-packages/scikit-learn.egg-link
      Removing pth entries from /opt/miniconda3/envs/testbed/lib/python3.6/site-packages/easy-install.pth:
      Removing entry: /testbed
      Successfully uninstalled scikit-learn-0.21.dev0
  Running setup.py develop for scikit-learn
    Running command /opt/miniconda3/envs/testbed/bin/python -c 'import io, os, sys, setuptools, tokenize; sys.argv[0] = '"'"'/testbed/setup.py'"'"'; __file__='"'"'/testbed/setup.py'"'"';f = getattr(tokenize, '"'"'open'"'"', open)(__file__) if os.path.exists(__file__) else io.StringIO('"'"'from setuptools import setup; setup()'"'"');code = f.read().replace('"'"'\r\n'"'"', '"'"'\n'"'"');f.close();exec(compile(code, __file__, '"'"'exec'"'"'))' develop --no-deps
    blas_opt_info:
    blas_mkl_info:
    customize UnixCCompiler
      libraries mkl_rt not found in ['/opt/miniconda3/envs/testbed/lib', '/usr/local/lib', '/usr/lib64', '/usr/lib', '/usr/lib/x86_64-linux-gnu']
      NOT AVAILABLE

    blis_info:
      libraries blis not found in ['/opt/miniconda3/envs/testbed/lib', '/usr/local/lib', '/usr/lib64', '/usr/lib', '/usr/lib/x86_64-linux-gnu']
      NOT AVAILABLE

    openblas_info:
    C compiler: gcc -pthread -B /opt/miniconda3/envs/testbed/compiler_compat -Wl,--sysroot=/ -Wsign-compare -DNDEBUG -g -fwrapv -O3 -Wall -Wstrict-prototypes -fPIC

    creating /tmp/tmpkfkdznxe/tmp
    creating /tmp/tmpkfkdznxe/tmp/tmpkfkdznxe
    compile options: '-c'
    gcc: /tmp/tmpkfkdznxe/source.c
    gcc -pthread -B /opt/miniconda3/envs/testbed/compiler_compat -Wl,--sysroot=/ /tmp/tmpkfkdznxe/tmp/tmpkfkdznxe/source.o -L/opt/miniconda3/envs/testbed/lib -lopenblas -o /tmp/tmpkfkdznxe/a.out
      FOUND:
        libraries = ['openblas', 'openblas']
        library_dirs = ['/opt/miniconda3/envs/testbed/lib']
        language = c
        define_macros = [('HAVE_CBLAS', None)]

      FOUND:
        libraries = ['openblas', 'openblas']
        library_dirs = ['/opt/miniconda3/envs/testbed/lib']
        language = c
        define_macros = [('HAVE_CBLAS', None)]

    running develop
    running build_scripts
    running egg_info
    running build_src
    build_src
    building library "libsvm-skl" sources
    building extension "sklearn.__check_build._check_build" sources
    building extension "sklearn.preprocessing._csr_polynomial_expansion" sources
    building extension "sklearn.cluster._dbscan_inner" sources
    building extension "sklearn.cluster._hierarchical" sources
    building extension "sklearn.cluster._k_means_elkan" sources
    building extension "sklearn.cluster._k_means" sources
    building extension "sklearn.datasets._svmlight_format" sources
    building extension "sklearn.decomposition._online_lda" sources
    building extension "sklearn.decomposition.cdnmf_fast" sources
    building extension "sklearn.ensemble._gradient_boosting" sources
    building extension "sklearn.feature_extraction._hashing" sources
    building extension "sklearn.manifold._utils" sources
    building extension "sklearn.manifold._barnes_hut_tsne" sources
    building extension "sklearn.metrics.cluster.expected_mutual_info_fast" sources
    building extension "sklearn.metrics.pairwise_fast" sources
    building extension "sklearn.neighbors.ball_tree" sources
    building extension "sklearn.neighbors.kd_tree" sources
    building extension "sklearn.neighbors.dist_metrics" sources
    building extension "sklearn.neighbors.typedefs" sources
    building extension "sklearn.neighbors.quad_tree" sources
    building extension "sklearn.tree._tree" sources
    building extension "sklearn.tree._splitter" sources
    building extension "sklearn.tree._criterion" sources
    building extension "sklearn.tree._utils" sources
    building extension "sklearn.svm.libsvm" sources
    building extension "sklearn.svm.liblinear" sources
    building extension "sklearn.svm.libsvm_sparse" sources
    building extension "sklearn._isotonic" sources
    building extension "sklearn.linear_model.cd_fast" sources
    building extension "sklearn.linear_model.sgd_fast" sources
    building extension "sklearn.linear_model.sag_fast" sources
    building extension "sklearn.utils.sparsefuncs_fast" sources
    building extension "sklearn.utils._cython_blas" sources
    building extension "sklearn.utils.arrayfuncs" sources
    building extension "sklearn.utils.murmurhash" sources
    building extension "sklearn.utils.lgamma" sources
    building extension "sklearn.utils.graph_shortest_path" sources
    building extension "sklearn.utils.fast_dict" sources
    building extension "sklearn.utils.seq_dataset" sources
    building extension "sklearn.utils.weight_vector" sources
    building extension "sklearn.utils._random" sources
    building extension "sklearn.utils._logistic_sigmoid" sources
    building data_files sources
    build_src: building npy-pkg config files
    writing scikit_learn.egg-info/PKG-INFO
    writing dependency_links to scikit_learn.egg-info/dependency_links.txt
    writing requirements to scikit_learn.egg-info/requires.txt
    writing top-level names to scikit_learn.egg-info/top_level.txt
    reading manifest file 'scikit_learn.egg-info/SOURCES.txt'
    reading manifest template 'MANIFEST.in'
    adding license file 'COPYING'
    writing manifest file 'scikit_learn.egg-info/SOURCES.txt'
    running build_ext
    customize UnixCCompiler
    customize UnixCCompiler using build_clib
    customize UnixCCompiler
    customize UnixCCompiler using build_ext_subclass
    resetting extension 'sklearn.svm.liblinear' language from 'c' to 'c++'.
    customize UnixCCompiler
    customize UnixCCompiler using build_ext_subclass
    Creating /opt/miniconda3/envs/testbed/lib/python3.6/site-packages/scikit-learn.egg-link (link to .)
    Adding scikit-learn 0.21.dev0 to easy-install.pth file

    Installed /testbed
    Partial import of sklearn during the build process.
WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv
Successfully installed scikit-learn-0.21.dev0
+ git apply -v -
Checking patch sklearn/tests/test_coverup_scikit-learn__scikit-learn-13142.py...
Applied patch sklearn/tests/test_coverup_scikit-learn__scikit-learn-13142.py cleanly.
+ python3 /root/trace.py --timing --trace --count -C coverage.cover --include-pattern '/testbed/(sklearn/mixture/base\.py)' -m pytest --no-header -rA -p no:cacheprovider sklearn/tests/test_coverup_scikit-learn__scikit-learn-13142.py
['--timing', '--trace', '--count', '-C', 'coverage.cover', '--include-pattern', '/testbed/(sklearn/mixture/base\\.py)']
============================= test session starts ==============================
collected 1 item

sklearn/tests/test_coverup_scikit-learn__scikit-learn-13142.py F         [100%]

=================================== FAILURES ===================================
________________ test_gaussian_mixture_fit_predict_discrepancy _________________

    def test_gaussian_mixture_fit_predict_discrepancy():
        # Generate random data with a fixed seed for reproducibility
        rng = np.random.RandomState(42)
        X = rng.randn(1000, 5)
    
        # Initialize GaussianMixture with n_components=5 and n_init=5
        gm = GaussianMixture(n_components=5, n_init=5, random_state=42)
    
        # Fit and predict using fit_predict
        c1 = gm.fit_predict(X)
    
        # Predict using predict
        c2 = gm.predict(X)
    
        # Assert that the results are the same, which is the expected correct behavior
>       assert np.array_equal(c1, c2), "fit_predict and predict should agree when n_init > 1"
E       AssertionError: fit_predict and predict should agree when n_init > 1
E       assert False
E        +  where False = <function array_equal at 0x7f24df2ae048>(array([4, 1, 3, 0, 3, 0, 1, 3, 3, 0, 4, 4, 3, 4, 1, 3, 1, 1, 3, 2, 2, 1,\n       2, 4, 0, 3, 2, 3, 3, 4, 3, 1, 0, 1, 2,...4, 4, 4,\n       0, 0, 2, 1, 3, 2, 1, 0, 1, 0, 2, 4, 2, 3, 1, 4, 1, 0, 0, 4, 0, 3,\n       1, 0, 4, 0, 3, 2, 2, 4, 2, 1]), array([1, 4, 1, 1, 4, 2, 4, 3, 1, 1, 0, 0, 3, 2, 1, 1, 1, 4, 4, 1, 3, 4,\n       0, 0, 2, 3, 0, 3, 0, 3, 0, 4, 2, 1, 0,...1, 3, 3,\n       2, 2, 4, 4, 3, 0, 4, 2, 0, 2, 3, 0, 3, 3, 1, 0, 2, 3, 2, 1, 2, 3,\n       1, 1, 3, 2, 4, 0, 4, 2, 0, 1]))
E        +    where <function array_equal at 0x7f24df2ae048> = np.array_equal

sklearn/tests/test_coverup_scikit-learn__scikit-learn-13142.py:20: AssertionError
----------------------------- Captured stdout call -----------------------------
1.04 /testbed/sklearn/mixture/base.py(74):         self.n_components = n_components
1.04 /testbed/sklearn/mixture/base.py(75):         self.tol = tol
1.04 /testbed/sklearn/mixture/base.py(76):         self.reg_covar = reg_covar
1.04 /testbed/sklearn/mixture/base.py(77):         self.max_iter = max_iter
1.04 /testbed/sklearn/mixture/base.py(78):         self.n_init = n_init
1.04 /testbed/sklearn/mixture/base.py(79):         self.init_params = init_params
1.04 /testbed/sklearn/mixture/base.py(80):         self.random_state = random_state
1.04 /testbed/sklearn/mixture/base.py(81):         self.warm_start = warm_start
1.04 /testbed/sklearn/mixture/base.py(82):         self.verbose = verbose
1.04 /testbed/sklearn/mixture/base.py(83):         self.verbose_interval = verbose_interval
1.04 /testbed/sklearn/mixture/base.py(217):         X = _check_X(X, self.n_components, ensure_min_samples=2)
1.04 /testbed/sklearn/mixture/base.py(51):     X = check_array(X, dtype=[np.float64, np.float32],
1.04 /testbed/sklearn/mixture/base.py(52):                     ensure_min_samples=ensure_min_samples)
1.04 /testbed/sklearn/mixture/base.py(53):     if n_components is not None and X.shape[0] < n_components:
1.04 /testbed/sklearn/mixture/base.py(57):     if n_features is not None and X.shape[1] != n_features:
1.04 /testbed/sklearn/mixture/base.py(61):     return X
1.04 /testbed/sklearn/mixture/base.py(218):         self._check_initial_parameters(X)
1.04 /testbed/sklearn/mixture/base.py(92):         if self.n_components < 1:
1.04 /testbed/sklearn/mixture/base.py(97):         if self.tol < 0.:
1.04 /testbed/sklearn/mixture/base.py(102):         if self.n_init < 1:
1.04 /testbed/sklearn/mixture/base.py(107):         if self.max_iter < 1:
1.04 /testbed/sklearn/mixture/base.py(112):         if self.reg_covar < 0.:
1.04 /testbed/sklearn/mixture/base.py(119):         self._check_parameters(X)
1.04 /testbed/sklearn/mixture/base.py(221):         do_init = not(self.warm_start and hasattr(self, 'converged_'))
1.04 /testbed/sklearn/mixture/base.py(222):         n_init = self.n_init if do_init else 1
1.04 /testbed/sklearn/mixture/base.py(224):         max_lower_bound = -np.infty
1.04 /testbed/sklearn/mixture/base.py(225):         self.converged_ = False
1.04 /testbed/sklearn/mixture/base.py(227):         random_state = check_random_state(self.random_state)
1.04 /testbed/sklearn/mixture/base.py(229):         n_samples, _ = X.shape
1.04 /testbed/sklearn/mixture/base.py(230):         for init in range(n_init):
1.04 /testbed/sklearn/mixture/base.py(231):             self._print_verbose_msg_init_beg(init)
1.04 /testbed/sklearn/mixture/base.py(512):         if self.verbose == 1:
1.04 /testbed/sklearn/mixture/base.py(514):         elif self.verbose >= 2:
1.04 /testbed/sklearn/mixture/base.py(233):             if do_init:
1.04 /testbed/sklearn/mixture/base.py(234):                 self._initialize_parameters(X, random_state)
1.04 /testbed/sklearn/mixture/base.py(141):         n_samples, _ = X.shape
1.04 /testbed/sklearn/mixture/base.py(143):         if self.init_params == 'kmeans':
1.04 /testbed/sklearn/mixture/base.py(144):             resp = np.zeros((n_samples, self.n_components))
1.04 /testbed/sklearn/mixture/base.py(145):             label = cluster.KMeans(n_clusters=self.n_components, n_init=1,
1.04 /testbed/sklearn/mixture/base.py(146):                                    random_state=random_state).fit(X).labels_
1.05 /testbed/sklearn/mixture/base.py(147):             resp[np.arange(n_samples), label] = 1
1.05 /testbed/sklearn/mixture/base.py(155):         self._initialize(X, resp)
1.06 /testbed/sklearn/mixture/base.py(236):             lower_bound = (-np.infty if do_init else self.lower_bound_)
1.06 /testbed/sklearn/mixture/base.py(238):             for n_iter in range(1, self.max_iter + 1):
1.06 /testbed/sklearn/mixture/base.py(239):                 prev_lower_bound = lower_bound
1.06 /testbed/sklearn/mixture/base.py(241):                 log_prob_norm, log_resp = self._e_step(X)
1.06 /testbed/sklearn/mixture/base.py(294):         log_prob_norm, log_resp = self._estimate_log_prob_resp(X)
1.06 /testbed/sklearn/mixture/base.py(503):         weighted_log_prob = self._estimate_weighted_log_prob(X)
1.06 /testbed/sklearn/mixture/base.py(456):         return self._estimate_log_prob(X) + self._estimate_log_weights()
1.06 /testbed/sklearn/mixture/base.py(504):         log_prob_norm = logsumexp(weighted_log_prob, axis=1)
1.06 /testbed/sklearn/mixture/base.py(505):         with np.errstate(under='ignore'):
1.06 /testbed/sklearn/mixture/base.py(507):             log_resp = weighted_log_prob - log_prob_norm[:, np.newaxis]
1.06 /testbed/sklearn/mixture/base.py(508):         return log_prob_norm, log_resp
1.06 /testbed/sklearn/mixture/base.py(295):         return np.mean(log_prob_norm), log_resp
1.06 /testbed/sklearn/mixture/base.py(242):                 self._m_step(X, log_resp)
1.06 /testbed/sklearn/mixture/base.py(243):                 lower_bound = self._compute_lower_bound(
1.06 /testbed/sklearn/mixture/base.py(244):                     log_resp, log_prob_norm)
1.06 /testbed/sklearn/mixture/base.py(246):                 change = lower_bound - prev_lower_bound
1.06 /testbed/sklearn/mixture/base.py(247):                 self._print_verbose_msg_iter_end(n_iter, change)
1.06 /testbed/sklearn/mixture/base.py(521):         if n_iter % self.verbose_interval == 0:
1.06 /testbed/sklearn/mixture/base.py(249):                 if abs(change) < self.tol:
1.06 /testbed/sklearn/mixture/base.py(238):             for n_iter in range(1, self.max_iter + 1):
1.06 /testbed/sklearn/mixture/base.py(239):                 prev_lower_bound = lower_bound
1.06 /testbed/sklearn/mixture/base.py(241):                 log_prob_norm, log_resp = self._e_step(X)
1.06 /testbed/sklearn/mixture/base.py(294):         log_prob_norm, log_resp = self._estimate_log_prob_resp(X)
1.06 /testbed/sklearn/mixture/base.py(503):         weighted_log_prob = self._estimate_weighted_log_prob(X)
1.06 /testbed/sklearn/mixture/base.py(456):         return self._estimate_log_prob(X) + self._estimate_log_weights()
1.06 /testbed/sklearn/mixture/base.py(504):         log_prob_norm = logsumexp(weighted_log_prob, axis=1)
1.06 /testbed/sklearn/mixture/base.py(505):         with np.errstate(under='ignore'):
1.06 /testbed/sklearn/mixture/base.py(507):             log_resp = weighted_log_prob - log_prob_norm[:, np.newaxis]
1.06 /testbed/sklearn/mixture/base.py(508):         return log_prob_norm, log_resp
1.06 /testbed/sklearn/mixture/base.py(295):         return np.mean(log_prob_norm), log_resp
1.06 /testbed/sklearn/mixture/base.py(242):                 self._m_step(X, log_resp)
1.06 /testbed/sklearn/mixture/base.py(243):                 lower_bound = self._compute_lower_bound(
1.06 /testbed/sklearn/mixture/base.py(244):                     log_resp, log_prob_norm)
1.06 /testbed/sklearn/mixture/base.py(246):                 change = lower_bound - prev_lower_bound
1.06 /testbed/sklearn/mixture/base.py(247):                 self._print_verbose_msg_iter_end(n_iter, change)
1.06 /testbed/sklearn/mixture/base.py(521):         if n_iter % self.verbose_interval == 0:
1.06 /testbed/sklearn/mixture/base.py(249):                 if abs(change) < self.tol:
1.06 /testbed/sklearn/mixture/base.py(238):             for n_iter in range(1, self.max_iter + 1):
1.06 /testbed/sklearn/mixture/base.py(239):                 prev_lower_bound = lower_bound
1.06 /testbed/sklearn/mixture/base.py(241):                 log_prob_norm, log_resp = self._e_step(X)
1.06 /testbed/sklearn/mixture/base.py(294):         log_prob_norm, log_resp = self._estimate_log_prob_resp(X)
1.06 /testbed/sklearn/mixture/base.py(503):         weighted_log_prob = self._estimate_weighted_log_prob(X)
1.06 /testbed/sklearn/mixture/base.py(456):         return self._estimate_log_prob(X) + self._estimate_log_weights()
1.06 /testbed/sklearn/mixture/base.py(504):         log_prob_norm = logsumexp(weighted_log_prob, axis=1)
1.06 /testbed/sklearn/mixture/base.py(505):         with np.errstate(under='ignore'):
1.06 /testbed/sklearn/mixture/base.py(507):             log_resp = weighted_log_prob - log_prob_norm[:, np.newaxis]
1.06 /testbed/sklearn/mixture/base.py(508):         return log_prob_norm, log_resp
1.06 /testbed/sklearn/mixture/base.py(295):         return np.mean(log_prob_norm), log_resp
1.06 /testbed/sklearn/mixture/base.py(242):                 self._m_step(X, log_resp)
1.06 /testbed/sklearn/mixture/base.py(243):                 lower_bound = self._compute_lower_bound(
1.06 /testbed/sklearn/mixture/base.py(244):                     log_resp, log_prob_norm)
1.06 /testbed/sklearn/mixture/base.py(246):                 change = lower_bound - prev_lower_bound
1.06 /testbed/sklearn/mixture/base.py(247):                 self._print_verbose_msg_iter_end(n_iter, change)
1.06 /testbed/sklearn/mixture/base.py(521):         if n_iter % self.verbose_interval == 0:
1.06 /testbed/sklearn/mixture/base.py(249):                 if abs(change) < self.tol:
1.06 /testbed/sklearn/mixture/base.py(238):             for n_iter in range(1, self.max_iter + 1):
1.06 /testbed/sklearn/mixture/base.py(239):                 prev_lower_bound = lower_bound
1.06 /testbed/sklearn/mixture/base.py(241):                 log_prob_norm, log_resp = self._e_step(X)
1.06 /testbed/sklearn/mixture/base.py(294):         log_prob_norm, log_resp = self._estimate_log_prob_resp(X)
1.06 /testbed/sklearn/mixture/base.py(503):         weighted_log_prob = self._estimate_weighted_log_prob(X)
1.06 /testbed/sklearn/mixture/base.py(456):         return self._estimate_log_prob(X) + self._estimate_log_weights()
1.06 /testbed/sklearn/mixture/base.py(504):         log_prob_norm = logsumexp(weighted_log_prob, axis=1)
1.06 /testbed/sklearn/mixture/base.py(505):         with np.errstate(under='ignore'):
1.06 /testbed/sklearn/mixture/base.py(507):             log_resp = weighted_log_prob - log_prob_norm[:, np.newaxis]
1.06 /testbed/sklearn/mixture/base.py(508):         return log_prob_norm, log_resp
1.06 /testbed/sklearn/mixture/base.py(295):         return np.mean(log_prob_norm), log_resp
1.06 /testbed/sklearn/mixture/base.py(242):                 self._m_step(X, log_resp)
1.06 /testbed/sklearn/mixture/base.py(243):                 lower_bound = self._compute_lower_bound(
1.06 /testbed/sklearn/mixture/base.py(244):                     log_resp, log_prob_norm)
1.07 /testbed/sklearn/mixture/base.py(246):                 change = lower_bound - prev_lower_bound
1.07 /testbed/sklearn/mixture/base.py(247):                 self._print_verbose_msg_iter_end(n_iter, change)
1.07 /testbed/sklearn/mixture/base.py(521):         if n_iter % self.verbose_interval == 0:
1.07 /testbed/sklearn/mixture/base.py(249):                 if abs(change) < self.tol:
1.07 /testbed/sklearn/mixture/base.py(238):             for n_iter in range(1, self.max_iter + 1):
1.07 /testbed/sklearn/mixture/base.py(239):                 prev_lower_bound = lower_bound
1.07 /testbed/sklearn/mixture/base.py(241):                 log_prob_norm, log_resp = self._e_step(X)
1.07 /testbed/sklearn/mixture/base.py(294):         log_prob_norm, log_resp = self._estimate_log_prob_resp(X)
1.07 /testbed/sklearn/mixture/base.py(503):         weighted_log_prob = self._estimate_weighted_log_prob(X)
1.07 /testbed/sklearn/mixture/base.py(456):         return self._estimate_log_prob(X) + self._estimate_log_weights()
1.07 /testbed/sklearn/mixture/base.py(504):         log_prob_norm = logsumexp(weighted_log_prob, axis=1)
1.07 /testbed/sklearn/mixture/base.py(505):         with np.errstate(under='ignore'):
1.07 /testbed/sklearn/mixture/base.py(507):             log_resp = weighted_log_prob - log_prob_norm[:, np.newaxis]
1.07 /testbed/sklearn/mixture/base.py(508):         return log_prob_norm, log_resp
1.07 /testbed/sklearn/mixture/base.py(295):         return np.mean(log_prob_norm), log_resp
1.07 /testbed/sklearn/mixture/base.py(242):                 self._m_step(X, log_resp)
1.07 /testbed/sklearn/mixture/base.py(243):                 lower_bound = self._compute_lower_bound(
1.07 /testbed/sklearn/mixture/base.py(244):                     log_resp, log_prob_norm)
1.07 /testbed/sklearn/mixture/base.py(246):                 change = lower_bound - prev_lower_bound
1.07 /testbed/sklearn/mixture/base.py(247):                 self._print_verbose_msg_iter_end(n_iter, change)
1.07 /testbed/sklearn/mixture/base.py(521):         if n_iter % self.verbose_interval == 0:
1.07 /testbed/sklearn/mixture/base.py(249):                 if abs(change) < self.tol:
1.07 /testbed/sklearn/mixture/base.py(238):             for n_iter in range(1, self.max_iter + 1):
1.07 /testbed/sklearn/mixture/base.py(239):                 prev_lower_bound = lower_bound
1.07 /testbed/sklearn/mixture/base.py(241):                 log_prob_norm, log_resp = self._e_step(X)
1.07 /testbed/sklearn/mixture/base.py(294):         log_prob_norm, log_resp = self._estimate_log_prob_resp(X)
1.07 /testbed/sklearn/mixture/base.py(503):         weighted_log_prob = self._estimate_weighted_log_prob(X)
1.07 /testbed/sklearn/mixture/base.py(456):         return self._estimate_log_prob(X) + self._estimate_log_weights()
1.07 /testbed/sklearn/mixture/base.py(504):         log_prob_norm = logsumexp(weighted_log_prob, axis=1)
1.07 /testbed/sklearn/mixture/base.py(505):         with np.errstate(under='ignore'):
1.07 /testbed/sklearn/mixture/base.py(507):             log_resp = weighted_log_prob - log_prob_norm[:, np.newaxis]
1.07 /testbed/sklearn/mixture/base.py(508):         return log_prob_norm, log_resp
1.07 /testbed/sklearn/mixture/base.py(295):         return np.mean(log_prob_norm), log_resp
1.07 /testbed/sklearn/mixture/base.py(242):                 self._m_step(X, log_resp)
1.07 /testbed/sklearn/mixture/base.py(243):                 lower_bound = self._compute_lower_bound(
1.07 /testbed/sklearn/mixture/base.py(244):                     log_resp, log_prob_norm)
1.07 /testbed/sklearn/mixture/base.py(246):                 change = lower_bound - prev_lower_bound
1.07 /testbed/sklearn/mixture/base.py(247):                 self._print_verbose_msg_iter_end(n_iter, change)
1.07 /testbed/sklearn/mixture/base.py(521):         if n_iter % self.verbose_interval == 0:
1.07 /testbed/sklearn/mixture/base.py(249):                 if abs(change) < self.tol:
1.07 /testbed/sklearn/mixture/base.py(238):             for n_iter in range(1, self.max_iter + 1):
1.07 /testbed/sklearn/mixture/base.py(239):                 prev_lower_bound = lower_bound
1.07 /testbed/sklearn/mixture/base.py(241):                 log_prob_norm, log_resp = self._e_step(X)
1.07 /testbed/sklearn/mixture/base.py(294):         log_prob_norm, log_resp = self._estimate_log_prob_resp(X)
1.07 /testbed/sklearn/mixture/base.py(503):         weighted_log_prob = self._estimate_weighted_log_prob(X)
1.07 /testbed/sklearn/mixture/base.py(456):         return self._estimate_log_prob(X) + self._estimate_log_weights()
1.07 /testbed/sklearn/mixture/base.py(504):         log_prob_norm = logsumexp(weighted_log_prob, axis=1)
1.07 /testbed/sklearn/mixture/base.py(505):         with np.errstate(under='ignore'):
1.07 /testbed/sklearn/mixture/base.py(507):             log_resp = weighted_log_prob - log_prob_norm[:, np.newaxis]
1.07 /testbed/sklearn/mixture/base.py(508):         return log_prob_norm, log_resp
1.07 /testbed/sklearn/mixture/base.py(295):         return np.mean(log_prob_norm), log_resp
1.07 /testbed/sklearn/mixture/base.py(242):                 self._m_step(X, log_resp)
1.07 /testbed/sklearn/mixture/base.py(243):                 lower_bound = self._compute_lower_bound(
1.07 /testbed/sklearn/mixture/base.py(244):                     log_resp, log_prob_norm)
1.07 /testbed/sklearn/mixture/base.py(246):                 change = lower_bound - prev_lower_bound
1.07 /testbed/sklearn/mixture/base.py(247):                 self._print_verbose_msg_iter_end(n_iter, change)
1.07 /testbed/sklearn/mixture/base.py(521):         if n_iter % self.verbose_interval == 0:
1.07 /testbed/sklearn/mixture/base.py(249):                 if abs(change) < self.tol:
1.07 /testbed/sklearn/mixture/base.py(238):             for n_iter in range(1, self.max_iter + 1):
1.07 /testbed/sklearn/mixture/base.py(239):                 prev_lower_bound = lower_bound
1.07 /testbed/sklearn/mixture/base.py(241):                 log_prob_norm, log_resp = self._e_step(X)
1.07 /testbed/sklearn/mixture/base.py(294):         log_prob_norm, log_resp = self._estimate_log_prob_resp(X)
1.07 /testbed/sklearn/mixture/base.py(503):         weighted_log_prob = self._estimate_weighted_log_prob(X)
1.07 /testbed/sklearn/mixture/base.py(456):         return self._estimate_log_prob(X) + self._estimate_log_weights()
1.07 /testbed/sklearn/mixture/base.py(504):         log_prob_norm = logsumexp(weighted_log_prob, axis=1)
1.07 /testbed/sklearn/mixture/base.py(505):         with np.errstate(under='ignore'):
1.07 /testbed/sklearn/mixture/base.py(507):             log_resp = weighted_log_prob - log_prob_norm[:, np.newaxis]
1.07 /testbed/sklearn/mixture/base.py(508):         return log_prob_norm, log_resp
1.07 /testbed/sklearn/mixture/base.py(295):         return np.mean(log_prob_norm), log_resp
1.07 /testbed/sklearn/mixture/base.py(242):                 self._m_step(X, log_resp)
1.07 /testbed/sklearn/mixture/base.py(243):                 lower_bound = self._compute_lower_bound(
1.07 /testbed/sklearn/mixture/base.py(244):                     log_resp, log_prob_norm)
1.07 /testbed/sklearn/mixture/base.py(246):                 change = lower_bound - prev_lower_bound
1.07 /testbed/sklearn/mixture/base.py(247):                 self._print_verbose_msg_iter_end(n_iter, change)
1.07 /testbed/sklearn/mixture/base.py(521):         if n_iter % self.verbose_interval == 0:
1.07 /testbed/sklearn/mixture/base.py(249):                 if abs(change) < self.tol:
1.07 /testbed/sklearn/mixture/base.py(238):             for n_iter in range(1, self.max_iter + 1):
1.07 /testbed/sklearn/mixture/base.py(239):                 prev_lower_bound = lower_bound
1.07 /testbed/sklearn/mixture/base.py(241):                 log_prob_norm, log_resp = self._e_step(X)
1.07 /testbed/sklearn/mixture/base.py(294):         log_prob_norm, log_resp = self._estimate_log_prob_resp(X)
1.07 /testbed/sklearn/mixture/base.py(503):         weighted_log_prob = self._estimate_weighted_log_prob(X)
1.07 /testbed/sklearn/mixture/base.py(456):         return self._estimate_log_prob(X) + self._estimate_log_weights()
1.07 /testbed/sklearn/mixture/base.py(504):         log_prob_norm = logsumexp(weighted_log_prob, axis=1)
1.07 /testbed/sklearn/mixture/base.py(505):         with np.errstate(under='ignore'):
1.07 /testbed/sklearn/mixture/base.py(507):             log_resp = weighted_log_prob - log_prob_norm[:, np.newaxis]
1.07 /testbed/sklearn/mixture/base.py(508):         return log_prob_norm, log_resp
1.07 /testbed/sklearn/mixture/base.py(295):         return np.mean(log_prob_norm), log_resp
1.07 /testbed/sklearn/mixture/base.py(242):                 self._m_step(X, log_resp)
1.07 /testbed/sklearn/mixture/base.py(243):                 lower_bound = self._compute_lower_bound(
1.07 /testbed/sklearn/mixture/base.py(244):                     log_resp, log_prob_norm)
1.07 /testbed/sklearn/mixture/base.py(246):                 change = lower_bound - prev_lower_bound
1.07 /testbed/sklearn/mixture/base.py(247):                 self._print_verbose_msg_iter_end(n_iter, change)
1.07 /testbed/sklearn/mixture/base.py(521):         if n_iter % self.verbose_interval == 0:
1.07 /testbed/sklearn/mixture/base.py(249):                 if abs(change) < self.tol:
1.07 /testbed/sklearn/mixture/base.py(238):             for n_iter in range(1, self.max_iter + 1):
1.07 /testbed/sklearn/mixture/base.py(239):                 prev_lower_bound = lower_bound
1.07 /testbed/sklearn/mixture/base.py(241):                 log_prob_norm, log_resp = self._e_step(X)
1.07 /testbed/sklearn/mixture/base.py(294):         log_prob_norm, log_resp = self._estimate_log_prob_resp(X)
1.07 /testbed/sklearn/mixture/base.py(503):         weighted_log_prob = self._estimate_weighted_log_prob(X)
1.07 /testbed/sklearn/mixture/base.py(456):         return self._estimate_log_prob(X) + self._estimate_log_weights()
1.07 /testbed/sklearn/mixture/base.py(504):         log_prob_norm = logsumexp(weighted_log_prob, axis=1)
1.07 /testbed/sklearn/mixture/base.py(505):         with np.errstate(under='ignore'):
1.07 /testbed/sklearn/mixture/base.py(507):             log_resp = weighted_log_prob - log_prob_norm[:, np.newaxis]
1.07 /testbed/sklearn/mixture/base.py(508):         return log_prob_norm, log_resp
1.07 /testbed/sklearn/mixture/base.py(295):         return np.mean(log_prob_norm), log_resp
1.07 /testbed/sklearn/mixture/base.py(242):                 self._m_step(X, log_resp)
1.07 /testbed/sklearn/mixture/base.py(243):                 lower_bound = self._compute_lower_bound(
1.07 /testbed/sklearn/mixture/base.py(244):                     log_resp, log_prob_norm)
1.07 /testbed/sklearn/mixture/base.py(246):                 change = lower_bound - prev_lower_bound
1.07 /testbed/sklearn/mixture/base.py(247):                 self._print_verbose_msg_iter_end(n_iter, change)
1.07 /testbed/sklearn/mixture/base.py(521):         if n_iter % self.verbose_interval == 0:
1.07 /testbed/sklearn/mixture/base.py(522):             if self.verbose == 1:
1.07 /testbed/sklearn/mixture/base.py(524):             elif self.verbose >= 2:
1.07 /testbed/sklearn/mixture/base.py(249):                 if abs(change) < self.tol:
1.07 /testbed/sklearn/mixture/base.py(238):             for n_iter in range(1, self.max_iter + 1):
1.07 /testbed/sklearn/mixture/base.py(239):                 prev_lower_bound = lower_bound
1.07 /testbed/sklearn/mixture/base.py(241):                 log_prob_norm, log_resp = self._e_step(X)
1.07 /testbed/sklearn/mixture/base.py(294):         log_prob_norm, log_resp = self._estimate_log_prob_resp(X)
1.07 /testbed/sklearn/mixture/base.py(503):         weighted_log_prob = self._estimate_weighted_log_prob(X)
1.07 /testbed/sklearn/mixture/base.py(456):         return self._estimate_log_prob(X) + self._estimate_log_weights()
1.07 /testbed/sklearn/mixture/base.py(504):         log_prob_norm = logsumexp(weighted_log_prob, axis=1)
1.07 /testbed/sklearn/mixture/base.py(505):         with np.errstate(under='ignore'):
1.07 /testbed/sklearn/mixture/base.py(507):             log_resp = weighted_log_prob - log_prob_norm[:, np.newaxis]
1.07 /testbed/sklearn/mixture/base.py(508):         return log_prob_norm, log_resp
1.07 /testbed/sklearn/mixture/base.py(295):         return np.mean(log_prob_norm), log_resp
1.07 /testbed/sklearn/mixture/base.py(242):                 self._m_step(X, log_resp)
1.08 /testbed/sklearn/mixture/base.py(243):                 lower_bound = self._compute_lower_bound(
1.08 /testbed/sklearn/mixture/base.py(244):                     log_resp, log_prob_norm)
1.08 /testbed/sklearn/mixture/base.py(246):                 change = lower_bound - prev_lower_bound
1.08 /testbed/sklearn/mixture/base.py(247):                 self._print_verbose_msg_iter_end(n_iter, change)
1.08 /testbed/sklearn/mixture/base.py(521):         if n_iter % self.verbose_interval == 0:
1.08 /testbed/sklearn/mixture/base.py(249):                 if abs(change) < self.tol:
1.08 /testbed/sklearn/mixture/base.py(238):             for n_iter in range(1, self.max_iter + 1):
1.08 /testbed/sklearn/mixture/base.py(239):                 prev_lower_bound = lower_bound
1.08 /testbed/sklearn/mixture/base.py(241):                 log_prob_norm, log_resp = self._e_step(X)
1.08 /testbed/sklearn/mixture/base.py(294):         log_prob_norm, log_resp = self._estimate_log_prob_resp(X)
1.08 /testbed/sklearn/mixture/base.py(503):         weighted_log_prob = self._estimate_weighted_log_prob(X)
1.08 /testbed/sklearn/mixture/base.py(456):         return self._estimate_log_prob(X) + self._estimate_log_weights()
1.08 /testbed/sklearn/mixture/base.py(504):         log_prob_norm = logsumexp(weighted_log_prob, axis=1)
1.08 /testbed/sklearn/mixture/base.py(505):         with np.errstate(under='ignore'):
1.08 /testbed/sklearn/mixture/base.py(507):             log_resp = weighted_log_prob - log_prob_norm[:, np.newaxis]
1.08 /testbed/sklearn/mixture/base.py(508):         return log_prob_norm, log_resp
1.08 /testbed/sklearn/mixture/base.py(295):         return np.mean(log_prob_norm), log_resp
1.08 /testbed/sklearn/mixture/base.py(242):                 self._m_step(X, log_resp)
1.08 /testbed/sklearn/mixture/base.py(243):                 lower_bound = self._compute_lower_bound(
1.08 /testbed/sklearn/mixture/base.py(244):                     log_resp, log_prob_norm)
1.08 /testbed/sklearn/mixture/base.py(246):                 change = lower_bound - prev_lower_bound
1.08 /testbed/sklearn/mixture/base.py(247):                 self._print_verbose_msg_iter_end(n_iter, change)
1.08 /testbed/sklearn/mixture/base.py(521):         if n_iter % self.verbose_interval == 0:
1.08 /testbed/sklearn/mixture/base.py(249):                 if abs(change) < self.tol:
1.08 /testbed/sklearn/mixture/base.py(238):             for n_iter in range(1, self.max_iter + 1):
1.08 /testbed/sklearn/mixture/base.py(239):                 prev_lower_bound = lower_bound
1.08 /testbed/sklearn/mixture/base.py(241):                 log_prob_norm, log_resp = self._e_step(X)
1.08 /testbed/sklearn/mixture/base.py(294):         log_prob_norm, log_resp = self._estimate_log_prob_resp(X)
1.08 /testbed/sklearn/mixture/base.py(503):         weighted_log_prob = self._estimate_weighted_log_prob(X)
1.08 /testbed/sklearn/mixture/base.py(456):         return self._estimate_log_prob(X) + self._estimate_log_weights()
1.08 /testbed/sklearn/mixture/base.py(504):         log_prob_norm = logsumexp(weighted_log_prob, axis=1)
1.08 /testbed/sklearn/mixture/base.py(505):         with np.errstate(under='ignore'):
1.08 /testbed/sklearn/mixture/base.py(507):             log_resp = weighted_log_prob - log_prob_norm[:, np.newaxis]
1.08 /testbed/sklearn/mixture/base.py(508):         return log_prob_norm, log_resp
1.08 /testbed/sklearn/mixture/base.py(295):         return np.mean(log_prob_norm), log_resp
1.08 /testbed/sklearn/mixture/base.py(242):                 self._m_step(X, log_resp)
1.08 /testbed/sklearn/mixture/base.py(243):                 lower_bound = self._compute_lower_bound(
1.08 /testbed/sklearn/mixture/base.py(244):                     log_resp, log_prob_norm)
1.08 /testbed/sklearn/mixture/base.py(246):                 change = lower_bound - prev_lower_bound
1.08 /testbed/sklearn/mixture/base.py(247):                 self._print_verbose_msg_iter_end(n_iter, change)
1.08 /testbed/sklearn/mixture/base.py(521):         if n_iter % self.verbose_interval == 0:
1.08 /testbed/sklearn/mixture/base.py(249):                 if abs(change) < self.tol:
1.08 /testbed/sklearn/mixture/base.py(238):             for n_iter in range(1, self.max_iter + 1):
1.08 /testbed/sklearn/mixture/base.py(239):                 prev_lower_bound = lower_bound
1.08 /testbed/sklearn/mixture/base.py(241):                 log_prob_norm, log_resp = self._e_step(X)
1.08 /testbed/sklearn/mixture/base.py(294):         log_prob_norm, log_resp = self._estimate_log_prob_resp(X)
1.08 /testbed/sklearn/mixture/base.py(503):         weighted_log_prob = self._estimate_weighted_log_prob(X)
1.08 /testbed/sklearn/mixture/base.py(456):         return self._estimate_log_prob(X) + self._estimate_log_weights()
1.08 /testbed/sklearn/mixture/base.py(504):         log_prob_norm = logsumexp(weighted_log_prob, axis=1)
1.08 /testbed/sklearn/mixture/base.py(505):         with np.errstate(under='ignore'):
1.08 /testbed/sklearn/mixture/base.py(507):             log_resp = weighted_log_prob - log_prob_norm[:, np.newaxis]
1.08 /testbed/sklearn/mixture/base.py(508):         return log_prob_norm, log_resp
1.08 /testbed/sklearn/mixture/base.py(295):         return np.mean(log_prob_norm), log_resp
1.08 /testbed/sklearn/mixture/base.py(242):                 self._m_step(X, log_resp)
1.08 /testbed/sklearn/mixture/base.py(243):                 lower_bound = self._compute_lower_bound(
1.08 /testbed/sklearn/mixture/base.py(244):                     log_resp, log_prob_norm)
1.08 /testbed/sklearn/mixture/base.py(246):                 change = lower_bound - prev_lower_bound
1.08 /testbed/sklearn/mixture/base.py(247):                 self._print_verbose_msg_iter_end(n_iter, change)
1.08 /testbed/sklearn/mixture/base.py(521):         if n_iter % self.verbose_interval == 0:
1.08 /testbed/sklearn/mixture/base.py(249):                 if abs(change) < self.tol:
1.08 /testbed/sklearn/mixture/base.py(238):             for n_iter in range(1, self.max_iter + 1):
1.08 /testbed/sklearn/mixture/base.py(239):                 prev_lower_bound = lower_bound
1.08 /testbed/sklearn/mixture/base.py(241):                 log_prob_norm, log_resp = self._e_step(X)
1.08 /testbed/sklearn/mixture/base.py(294):         log_prob_norm, log_resp = self._estimate_log_prob_resp(X)
1.08 /testbed/sklearn/mixture/base.py(503):         weighted_log_prob = self._estimate_weighted_log_prob(X)
1.08 /testbed/sklearn/mixture/base.py(456):         return self._estimate_log_prob(X) + self._estimate_log_weights()
1.08 /testbed/sklearn/mixture/base.py(504):         log_prob_norm = logsumexp(weighted_log_prob, axis=1)
1.08 /testbed/sklearn/mixture/base.py(505):         with np.errstate(under='ignore'):
1.08 /testbed/sklearn/mixture/base.py(507):             log_resp = weighted_log_prob - log_prob_norm[:, np.newaxis]
1.08 /testbed/sklearn/mixture/base.py(508):         return log_prob_norm, log_resp
1.08 /testbed/sklearn/mixture/base.py(295):         return np.mean(log_prob_norm), log_resp
1.08 /testbed/sklearn/mixture/base.py(242):                 self._m_step(X, log_resp)
1.08 /testbed/sklearn/mixture/base.py(243):                 lower_bound = self._compute_lower_bound(
1.08 /testbed/sklearn/mixture/base.py(244):                     log_resp, log_prob_norm)
1.08 /testbed/sklearn/mixture/base.py(246):                 change = lower_bound - prev_lower_bound
1.08 /testbed/sklearn/mixture/base.py(247):                 self._print_verbose_msg_iter_end(n_iter, change)
1.08 /testbed/sklearn/mixture/base.py(521):         if n_iter % self.verbose_interval == 0:
1.08 /testbed/sklearn/mixture/base.py(249):                 if abs(change) < self.tol:
1.08 /testbed/sklearn/mixture/base.py(238):             for n_iter in range(1, self.max_iter + 1):
1.08 /testbed/sklearn/mixture/base.py(239):                 prev_lower_bound = lower_bound
1.08 /testbed/sklearn/mixture/base.py(241):                 log_prob_norm, log_resp = self._e_step(X)
1.08 /testbed/sklearn/mixture/base.py(294):         log_prob_norm, log_resp = self._estimate_log_prob_resp(X)
1.08 /testbed/sklearn/mixture/base.py(503):         weighted_log_prob = self._estimate_weighted_log_prob(X)
1.08 /testbed/sklearn/mixture/base.py(456):         return self._estimate_log_prob(X) + self._estimate_log_weights()
1.08 /testbed/sklearn/mixture/base.py(504):         log_prob_norm = logsumexp(weighted_log_prob, axis=1)
1.08 /testbed/sklearn/mixture/base.py(505):         with np.errstate(under='ignore'):
1.08 /testbed/sklearn/mixture/base.py(507):             log_resp = weighted_log_prob - log_prob_norm[:, np.newaxis]
1.08 /testbed/sklearn/mixture/base.py(508):         return log_prob_norm, log_resp
1.08 /testbed/sklearn/mixture/base.py(295):         return np.mean(log_prob_norm), log_resp
1.08 /testbed/sklearn/mixture/base.py(242):                 self._m_step(X, log_resp)
1.08 /testbed/sklearn/mixture/base.py(243):                 lower_bound = self._compute_lower_bound(
1.08 /testbed/sklearn/mixture/base.py(244):                     log_resp, log_prob_norm)
1.08 /testbed/sklearn/mixture/base.py(246):                 change = lower_bound - prev_lower_bound
1.08 /testbed/sklearn/mixture/base.py(247):                 self._print_verbose_msg_iter_end(n_iter, change)
1.08 /testbed/sklearn/mixture/base.py(521):         if n_iter % self.verbose_interval == 0:
1.08 /testbed/sklearn/mixture/base.py(249):                 if abs(change) < self.tol:
1.08 /testbed/sklearn/mixture/base.py(238):             for n_iter in range(1, self.max_iter + 1):
1.08 /testbed/sklearn/mixture/base.py(239):                 prev_lower_bound = lower_bound
1.08 /testbed/sklearn/mixture/base.py(241):                 log_prob_norm, log_resp = self._e_step(X)
1.08 /testbed/sklearn/mixture/base.py(294):         log_prob_norm, log_resp = self._estimate_log_prob_resp(X)
1.08 /testbed/sklearn/mixture/base.py(503):         weighted_log_prob = self._estimate_weighted_log_prob(X)
1.08 /testbed/sklearn/mixture/base.py(456):         return self._estimate_log_prob(X) + self._estimate_log_weights()
1.08 /testbed/sklearn/mixture/base.py(504):         log_prob_norm = logsumexp(weighted_log_prob, axis=1)
1.08 /testbed/sklearn/mixture/base.py(505):         with np.errstate(under='ignore'):
1.08 /testbed/sklearn/mixture/base.py(507):             log_resp = weighted_log_prob - log_prob_norm[:, np.newaxis]
1.08 /testbed/sklearn/mixture/base.py(508):         return log_prob_norm, log_resp
1.08 /testbed/sklearn/mixture/base.py(295):         return np.mean(log_prob_norm), log_resp
1.08 /testbed/sklearn/mixture/base.py(242):                 self._m_step(X, log_resp)
1.08 /testbed/sklearn/mixture/base.py(243):                 lower_bound = self._compute_lower_bound(
1.08 /testbed/sklearn/mixture/base.py(244):                     log_resp, log_prob_norm)
1.08 /testbed/sklearn/mixture/base.py(246):                 change = lower_bound - prev_lower_bound
1.08 /testbed/sklearn/mixture/base.py(247):                 self._print_verbose_msg_iter_end(n_iter, change)
1.08 /testbed/sklearn/mixture/base.py(521):         if n_iter % self.verbose_interval == 0:
1.08 /testbed/sklearn/mixture/base.py(249):                 if abs(change) < self.tol:
1.08 /testbed/sklearn/mixture/base.py(250):                     self.converged_ = True
1.08 /testbed/sklearn/mixture/base.py(251):                     break
1.08 /testbed/sklearn/mixture/base.py(253):             self._print_verbose_msg_init_end(lower_bound)
1.08 /testbed/sklearn/mixture/base.py(532):         if self.verbose == 1:
1.08 /testbed/sklearn/mixture/base.py(534):         elif self.verbose >= 2:
1.08 /testbed/sklearn/mixture/base.py(255):             if lower_bound > max_lower_bound:
1.08 /testbed/sklearn/mixture/base.py(256):                 max_lower_bound = lower_bound
1.08 /testbed/sklearn/mixture/base.py(257):                 best_params = self._get_parameters()
1.08 /testbed/sklearn/mixture/base.py(258):                 best_n_iter = n_iter
1.08 /testbed/sklearn/mixture/base.py(230):         for init in range(n_init):
1.08 /testbed/sklearn/mixture/base.py(231):             self._print_verbose_msg_init_beg(init)
1.08 /testbed/sklearn/mixture/base.py(512):         if self.verbose == 1:
1.08 /testbed/sklearn/mixture/base.py(514):         elif self.verbose >= 2:
1.08 /testbed/sklearn/mixture/base.py(233):             if do_init:
1.08 /testbed/sklearn/mixture/base.py(234):                 self._initialize_parameters(X, random_state)
1.08 /testbed/sklearn/mixture/base.py(141):         n_samples, _ = X.shape
1.08 /testbed/sklearn/mixture/base.py(143):         if self.init_params == 'kmeans':
1.08 /testbed/sklearn/mixture/base.py(144):             resp = np.zeros((n_samples, self.n_components))
1.08 /testbed/sklearn/mixture/base.py(145):             label = cluster.KMeans(n_clusters=self.n_components, n_init=1,
1.08 /testbed/sklearn/mixture/base.py(146):                                    random_state=random_state).fit(X).labels_
1.09 /testbed/sklearn/mixture/base.py(147):             resp[np.arange(n_samples), label] = 1
1.09 /testbed/sklearn/mixture/base.py(155):         self._initialize(X, resp)
1.09 /testbed/sklearn/mixture/base.py(236):             lower_bound = (-np.infty if do_init else self.lower_bound_)
1.09 /testbed/sklearn/mixture/base.py(238):             for n_iter in range(1, self.max_iter + 1):
1.09 /testbed/sklearn/mixture/base.py(239):                 prev_lower_bound = lower_bound
1.09 /testbed/sklearn/mixture/base.py(241):                 log_prob_norm, log_resp = self._e_step(X)
1.09 /testbed/sklearn/mixture/base.py(294):         log_prob_norm, log_resp = self._estimate_log_prob_resp(X)
1.09 /testbed/sklearn/mixture/base.py(503):         weighted_log_prob = self._estimate_weighted_log_prob(X)
1.09 /testbed/sklearn/mixture/base.py(456):         return self._estimate_log_prob(X) + self._estimate_log_weights()
1.09 /testbed/sklearn/mixture/base.py(504):         log_prob_norm = logsumexp(weighted_log_prob, axis=1)
1.09 /testbed/sklearn/mixture/base.py(505):         with np.errstate(under='ignore'):
1.10 /testbed/sklearn/mixture/base.py(507):             log_resp = weighted_log_prob - log_prob_norm[:, np.newaxis]
1.10 /testbed/sklearn/mixture/base.py(508):         return log_prob_norm, log_resp
1.10 /testbed/sklearn/mixture/base.py(295):         return np.mean(log_prob_norm), log_resp
1.10 /testbed/sklearn/mixture/base.py(242):                 self._m_step(X, log_resp)
1.10 /testbed/sklearn/mixture/base.py(243):                 lower_bound = self._compute_lower_bound(
1.10 /testbed/sklearn/mixture/base.py(244):                     log_resp, log_prob_norm)
1.10 /testbed/sklearn/mixture/base.py(246):                 change = lower_bound - prev_lower_bound
1.10 /testbed/sklearn/mixture/base.py(247):                 self._print_verbose_msg_iter_end(n_iter, change)
1.10 /testbed/sklearn/mixture/base.py(521):         if n_iter % self.verbose_interval == 0:
1.10 /testbed/sklearn/mixture/base.py(249):                 if abs(change) < self.tol:
1.10 /testbed/sklearn/mixture/base.py(238):             for n_iter in range(1, self.max_iter + 1):
1.10 /testbed/sklearn/mixture/base.py(239):                 prev_lower_bound = lower_bound
1.10 /testbed/sklearn/mixture/base.py(241):                 log_prob_norm, log_resp = self._e_step(X)
1.10 /testbed/sklearn/mixture/base.py(294):         log_prob_norm, log_resp = self._estimate_log_prob_resp(X)
1.10 /testbed/sklearn/mixture/base.py(503):         weighted_log_prob = self._estimate_weighted_log_prob(X)
1.10 /testbed/sklearn/mixture/base.py(456):         return self._estimate_log_prob(X) + self._estimate_log_weights()
1.10 /testbed/sklearn/mixture/base.py(504):         log_prob_norm = logsumexp(weighted_log_prob, axis=1)
1.10 /testbed/sklearn/mixture/base.py(505):         with np.errstate(under='ignore'):
1.10 /testbed/sklearn/mixture/base.py(507):             log_resp = weighted_log_prob - log_prob_norm[:, np.newaxis]
1.10 /testbed/sklearn/mixture/base.py(508):         return log_prob_norm, log_resp
1.10 /testbed/sklearn/mixture/base.py(295):         return np.mean(log_prob_norm), log_resp
1.10 /testbed/sklearn/mixture/base.py(242):                 self._m_step(X, log_resp)
1.10 /testbed/sklearn/mixture/base.py(243):                 lower_bound = self._compute_lower_bound(
1.10 /testbed/sklearn/mixture/base.py(244):                     log_resp, log_prob_norm)
1.10 /testbed/sklearn/mixture/base.py(246):                 change = lower_bound - prev_lower_bound
1.10 /testbed/sklearn/mixture/base.py(247):                 self._print_verbose_msg_iter_end(n_iter, change)
1.10 /testbed/sklearn/mixture/base.py(521):         if n_iter % self.verbose_interval == 0:
1.10 /testbed/sklearn/mixture/base.py(249):                 if abs(change) < self.tol:
1.10 /testbed/sklearn/mixture/base.py(238):             for n_iter in range(1, self.max_iter + 1):
1.10 /testbed/sklearn/mixture/base.py(239):                 prev_lower_bound = lower_bound
1.10 /testbed/sklearn/mixture/base.py(241):                 log_prob_norm, log_resp = self._e_step(X)
1.10 /testbed/sklearn/mixture/base.py(294):         log_prob_norm, log_resp = self._estimate_log_prob_resp(X)
1.10 /testbed/sklearn/mixture/base.py(503):         weighted_log_prob = self._estimate_weighted_log_prob(X)
1.10 /testbed/sklearn/mixture/base.py(456):         return self._estimate_log_prob(X) + self._estimate_log_weights()
1.10 /testbed/sklearn/mixture/base.py(504):         log_prob_norm = logsumexp(weighted_log_prob, axis=1)
1.10 /testbed/sklearn/mixture/base.py(505):         with np.errstate(under='ignore'):
1.10 /testbed/sklearn/mixture/base.py(507):             log_resp = weighted_log_prob - log_prob_norm[:, np.newaxis]
1.10 /testbed/sklearn/mixture/base.py(508):         return log_prob_norm, log_resp
1.10 /testbed/sklearn/mixture/base.py(295):         return np.mean(log_prob_norm), log_resp
1.10 /testbed/sklearn/mixture/base.py(242):                 self._m_step(X, log_resp)
1.10 /testbed/sklearn/mixture/base.py(243):                 lower_bound = self._compute_lower_bound(
1.10 /testbed/sklearn/mixture/base.py(244):                     log_resp, log_prob_norm)
1.10 /testbed/sklearn/mixture/base.py(246):                 change = lower_bound - prev_lower_bound
1.10 /testbed/sklearn/mixture/base.py(247):                 self._print_verbose_msg_iter_end(n_iter, change)
1.10 /testbed/sklearn/mixture/base.py(521):         if n_iter % self.verbose_interval == 0:
1.10 /testbed/sklearn/mixture/base.py(249):                 if abs(change) < self.tol:
1.10 /testbed/sklearn/mixture/base.py(238):             for n_iter in range(1, self.max_iter + 1):
1.10 /testbed/sklearn/mixture/base.py(239):                 prev_lower_bound = lower_bound
1.10 /testbed/sklearn/mixture/base.py(241):                 log_prob_norm, log_resp = self._e_step(X)
1.10 /testbed/sklearn/mixture/base.py(294):         log_prob_norm, log_resp = self._estimate_log_prob_resp(X)
1.10 /testbed/sklearn/mixture/base.py(503):         weighted_log_prob = self._estimate_weighted_log_prob(X)
1.10 /testbed/sklearn/mixture/base.py(456):         return self._estimate_log_prob(X) + self._estimate_log_weights()
1.10 /testbed/sklearn/mixture/base.py(504):         log_prob_norm = logsumexp(weighted_log_prob, axis=1)
1.10 /testbed/sklearn/mixture/base.py(505):         with np.errstate(under='ignore'):
1.10 /testbed/sklearn/mixture/base.py(507):             log_resp = weighted_log_prob - log_prob_norm[:, np.newaxis]
1.10 /testbed/sklearn/mixture/base.py(508):         return log_prob_norm, log_resp
1.10 /testbed/sklearn/mixture/base.py(295):         return np.mean(log_prob_norm), log_resp
1.10 /testbed/sklearn/mixture/base.py(242):                 self._m_step(X, log_resp)
1.10 /testbed/sklearn/mixture/base.py(243):                 lower_bound = self._compute_lower_bound(
1.10 /testbed/sklearn/mixture/base.py(244):                     log_resp, log_prob_norm)
1.10 /testbed/sklearn/mixture/base.py(246):                 change = lower_bound - prev_lower_bound
1.10 /testbed/sklearn/mixture/base.py(247):                 self._print_verbose_msg_iter_end(n_iter, change)
1.10 /testbed/sklearn/mixture/base.py(521):         if n_iter % self.verbose_interval == 0:
1.10 /testbed/sklearn/mixture/base.py(249):                 if abs(change) < self.tol:
1.10 /testbed/sklearn/mixture/base.py(238):             for n_iter in range(1, self.max_iter + 1):
1.10 /testbed/sklearn/mixture/base.py(239):                 prev_lower_bound = lower_bound
1.10 /testbed/sklearn/mixture/base.py(241):                 log_prob_norm, log_resp = self._e_step(X)
1.10 /testbed/sklearn/mixture/base.py(294):         log_prob_norm, log_resp = self._estimate_log_prob_resp(X)
1.10 /testbed/sklearn/mixture/base.py(503):         weighted_log_prob = self._estimate_weighted_log_prob(X)
1.10 /testbed/sklearn/mixture/base.py(456):         return self._estimate_log_prob(X) + self._estimate_log_weights()
1.10 /testbed/sklearn/mixture/base.py(504):         log_prob_norm = logsumexp(weighted_log_prob, axis=1)
1.10 /testbed/sklearn/mixture/base.py(505):         with np.errstate(under='ignore'):
1.10 /testbed/sklearn/mixture/base.py(507):             log_resp = weighted_log_prob - log_prob_norm[:, np.newaxis]
1.10 /testbed/sklearn/mixture/base.py(508):         return log_prob_norm, log_resp
1.10 /testbed/sklearn/mixture/base.py(295):         return np.mean(log_prob_norm), log_resp
1.10 /testbed/sklearn/mixture/base.py(242):                 self._m_step(X, log_resp)
1.10 /testbed/sklearn/mixture/base.py(243):                 lower_bound = self._compute_lower_bound(
1.10 /testbed/sklearn/mixture/base.py(244):                     log_resp, log_prob_norm)
1.10 /testbed/sklearn/mixture/base.py(246):                 change = lower_bound - prev_lower_bound
1.10 /testbed/sklearn/mixture/base.py(247):                 self._print_verbose_msg_iter_end(n_iter, change)
1.10 /testbed/sklearn/mixture/base.py(521):         if n_iter % self.verbose_interval == 0:
1.10 /testbed/sklearn/mixture/base.py(249):                 if abs(change) < self.tol:
1.10 /testbed/sklearn/mixture/base.py(238):             for n_iter in range(1, self.max_iter + 1):
1.10 /testbed/sklearn/mixture/base.py(239):                 prev_lower_bound = lower_bound
1.10 /testbed/sklearn/mixture/base.py(241):                 log_prob_norm, log_resp = self._e_step(X)
1.10 /testbed/sklearn/mixture/base.py(294):         log_prob_norm, log_resp = self._estimate_log_prob_resp(X)
1.10 /testbed/sklearn/mixture/base.py(503):         weighted_log_prob = self._estimate_weighted_log_prob(X)
1.10 /testbed/sklearn/mixture/base.py(456):         return self._estimate_log_prob(X) + self._estimate_log_weights()
1.10 /testbed/sklearn/mixture/base.py(504):         log_prob_norm = logsumexp(weighted_log_prob, axis=1)
1.10 /testbed/sklearn/mixture/base.py(505):         with np.errstate(under='ignore'):
1.10 /testbed/sklearn/mixture/base.py(507):             log_resp = weighted_log_prob - log_prob_norm[:, np.newaxis]
1.10 /testbed/sklearn/mixture/base.py(508):         return log_prob_norm, log_resp
1.10 /testbed/sklearn/mixture/base.py(295):         return np.mean(log_prob_norm), log_resp
1.10 /testbed/sklearn/mixture/base.py(242):                 self._m_step(X, log_resp)
1.10 /testbed/sklearn/mixture/base.py(243):                 lower_bound = self._compute_lower_bound(
1.10 /testbed/sklearn/mixture/base.py(244):                     log_resp, log_prob_norm)
1.10 /testbed/sklearn/mixture/base.py(246):                 change = lower_bound - prev_lower_bound
1.10 /testbed/sklearn/mixture/base.py(247):                 self._print_verbose_msg_iter_end(n_iter, change)
1.10 /testbed/sklearn/mixture/base.py(521):         if n_iter % self.verbose_interval == 0:
1.10 /testbed/sklearn/mixture/base.py(249):                 if abs(change) < self.tol:
1.10 /testbed/sklearn/mixture/base.py(238):             for n_iter in range(1, self.max_iter + 1):
1.10 /testbed/sklearn/mixture/base.py(239):                 prev_lower_bound = lower_bound
1.10 /testbed/sklearn/mixture/base.py(241):                 log_prob_norm, log_resp = self._e_step(X)
1.10 /testbed/sklearn/mixture/base.py(294):         log_prob_norm, log_resp = self._estimate_log_prob_resp(X)
1.10 /testbed/sklearn/mixture/base.py(503):         weighted_log_prob = self._estimate_weighted_log_prob(X)
1.10 /testbed/sklearn/mixture/base.py(456):         return self._estimate_log_prob(X) + self._estimate_log_weights()
1.10 /testbed/sklearn/mixture/base.py(504):         log_prob_norm = logsumexp(weighted_log_prob, axis=1)
1.10 /testbed/sklearn/mixture/base.py(505):         with np.errstate(under='ignore'):
1.10 /testbed/sklearn/mixture/base.py(507):             log_resp = weighted_log_prob - log_prob_norm[:, np.newaxis]
1.10 /testbed/sklearn/mixture/base.py(508):         return log_prob_norm, log_resp
1.10 /testbed/sklearn/mixture/base.py(295):         return np.mean(log_prob_norm), log_resp
1.10 /testbed/sklearn/mixture/base.py(242):                 self._m_step(X, log_resp)
1.10 /testbed/sklearn/mixture/base.py(243):                 lower_bound = self._compute_lower_bound(
1.10 /testbed/sklearn/mixture/base.py(244):                     log_resp, log_prob_norm)
1.10 /testbed/sklearn/mixture/base.py(246):                 change = lower_bound - prev_lower_bound
1.10 /testbed/sklearn/mixture/base.py(247):                 self._print_verbose_msg_iter_end(n_iter, change)
1.10 /testbed/sklearn/mixture/base.py(521):         if n_iter % self.verbose_interval == 0:
1.10 /testbed/sklearn/mixture/base.py(249):                 if abs(change) < self.tol:
1.10 /testbed/sklearn/mixture/base.py(238):             for n_iter in range(1, self.max_iter + 1):
1.10 /testbed/sklearn/mixture/base.py(239):                 prev_lower_bound = lower_bound
1.10 /testbed/sklearn/mixture/base.py(241):                 log_prob_norm, log_resp = self._e_step(X)
1.10 /testbed/sklearn/mixture/base.py(294):         log_prob_norm, log_resp = self._estimate_log_prob_resp(X)
1.10 /testbed/sklearn/mixture/base.py(503):         weighted_log_prob = self._estimate_weighted_log_prob(X)
1.10 /testbed/sklearn/mixture/base.py(456):         return self._estimate_log_prob(X) + self._estimate_log_weights()
1.11 /testbed/sklearn/mixture/base.py(504):         log_prob_norm = logsumexp(weighted_log_prob, axis=1)
1.11 /testbed/sklearn/mixture/base.py(505):         with np.errstate(under='ignore'):
1.11 /testbed/sklearn/mixture/base.py(507):             log_resp = weighted_log_prob - log_prob_norm[:, np.newaxis]
1.11 /testbed/sklearn/mixture/base.py(508):         return log_prob_norm, log_resp
1.11 /testbed/sklearn/mixture/base.py(295):         return np.mean(log_prob_norm), log_resp
1.11 /testbed/sklearn/mixture/base.py(242):                 self._m_step(X, log_resp)
1.11 /testbed/sklearn/mixture/base.py(243):                 lower_bound = self._compute_lower_bound(
1.11 /testbed/sklearn/mixture/base.py(244):                     log_resp, log_prob_norm)
1.11 /testbed/sklearn/mixture/base.py(246):                 change = lower_bound - prev_lower_bound
1.11 /testbed/sklearn/mixture/base.py(247):                 self._print_verbose_msg_iter_end(n_iter, change)
1.11 /testbed/sklearn/mixture/base.py(521):         if n_iter % self.verbose_interval == 0:
1.11 /testbed/sklearn/mixture/base.py(249):                 if abs(change) < self.tol:
1.11 /testbed/sklearn/mixture/base.py(238):             for n_iter in range(1, self.max_iter + 1):
1.11 /testbed/sklearn/mixture/base.py(239):                 prev_lower_bound = lower_bound
1.11 /testbed/sklearn/mixture/base.py(241):                 log_prob_norm, log_resp = self._e_step(X)
1.11 /testbed/sklearn/mixture/base.py(294):         log_prob_norm, log_resp = self._estimate_log_prob_resp(X)
1.11 /testbed/sklearn/mixture/base.py(503):         weighted_log_prob = self._estimate_weighted_log_prob(X)
1.11 /testbed/sklearn/mixture/base.py(456):         return self._estimate_log_prob(X) + self._estimate_log_weights()
1.11 /testbed/sklearn/mixture/base.py(504):         log_prob_norm = logsumexp(weighted_log_prob, axis=1)
1.11 /testbed/sklearn/mixture/base.py(505):         with np.errstate(under='ignore'):
1.11 /testbed/sklearn/mixture/base.py(507):             log_resp = weighted_log_prob - log_prob_norm[:, np.newaxis]
1.11 /testbed/sklearn/mixture/base.py(508):         return log_prob_norm, log_resp
1.11 /testbed/sklearn/mixture/base.py(295):         return np.mean(log_prob_norm), log_resp
1.11 /testbed/sklearn/mixture/base.py(242):                 self._m_step(X, log_resp)
1.11 /testbed/sklearn/mixture/base.py(243):                 lower_bound = self._compute_lower_bound(
1.11 /testbed/sklearn/mixture/base.py(244):                     log_resp, log_prob_norm)
1.11 /testbed/sklearn/mixture/base.py(246):                 change = lower_bound - prev_lower_bound
1.11 /testbed/sklearn/mixture/base.py(247):                 self._print_verbose_msg_iter_end(n_iter, change)
1.11 /testbed/sklearn/mixture/base.py(521):         if n_iter % self.verbose_interval == 0:
1.11 /testbed/sklearn/mixture/base.py(249):                 if abs(change) < self.tol:
1.11 /testbed/sklearn/mixture/base.py(238):             for n_iter in range(1, self.max_iter + 1):
1.11 /testbed/sklearn/mixture/base.py(239):                 prev_lower_bound = lower_bound
1.11 /testbed/sklearn/mixture/base.py(241):                 log_prob_norm, log_resp = self._e_step(X)
1.11 /testbed/sklearn/mixture/base.py(294):         log_prob_norm, log_resp = self._estimate_log_prob_resp(X)
1.11 /testbed/sklearn/mixture/base.py(503):         weighted_log_prob = self._estimate_weighted_log_prob(X)
1.11 /testbed/sklearn/mixture/base.py(456):         return self._estimate_log_prob(X) + self._estimate_log_weights()
1.11 /testbed/sklearn/mixture/base.py(504):         log_prob_norm = logsumexp(weighted_log_prob, axis=1)
1.11 /testbed/sklearn/mixture/base.py(505):         with np.errstate(under='ignore'):
1.11 /testbed/sklearn/mixture/base.py(507):             log_resp = weighted_log_prob - log_prob_norm[:, np.newaxis]
1.11 /testbed/sklearn/mixture/base.py(508):         return log_prob_norm, log_resp
1.11 /testbed/sklearn/mixture/base.py(295):         return np.mean(log_prob_norm), log_resp
1.11 /testbed/sklearn/mixture/base.py(242):                 self._m_step(X, log_resp)
1.11 /testbed/sklearn/mixture/base.py(243):                 lower_bound = self._compute_lower_bound(
1.11 /testbed/sklearn/mixture/base.py(244):                     log_resp, log_prob_norm)
1.11 /testbed/sklearn/mixture/base.py(246):                 change = lower_bound - prev_lower_bound
1.11 /testbed/sklearn/mixture/base.py(247):                 self._print_verbose_msg_iter_end(n_iter, change)
1.11 /testbed/sklearn/mixture/base.py(521):         if n_iter % self.verbose_interval == 0:
1.11 /testbed/sklearn/mixture/base.py(522):             if self.verbose == 1:
1.11 /testbed/sklearn/mixture/base.py(524):             elif self.verbose >= 2:
1.11 /testbed/sklearn/mixture/base.py(249):                 if abs(change) < self.tol:
1.11 /testbed/sklearn/mixture/base.py(238):             for n_iter in range(1, self.max_iter + 1):
1.11 /testbed/sklearn/mixture/base.py(239):                 prev_lower_bound = lower_bound
1.11 /testbed/sklearn/mixture/base.py(241):                 log_prob_norm, log_resp = self._e_step(X)
1.11 /testbed/sklearn/mixture/base.py(294):         log_prob_norm, log_resp = self._estimate_log_prob_resp(X)
1.11 /testbed/sklearn/mixture/base.py(503):         weighted_log_prob = self._estimate_weighted_log_prob(X)
1.11 /testbed/sklearn/mixture/base.py(456):         return self._estimate_log_prob(X) + self._estimate_log_weights()
1.11 /testbed/sklearn/mixture/base.py(504):         log_prob_norm = logsumexp(weighted_log_prob, axis=1)
1.11 /testbed/sklearn/mixture/base.py(505):         with np.errstate(under='ignore'):
1.11 /testbed/sklearn/mixture/base.py(507):             log_resp = weighted_log_prob - log_prob_norm[:, np.newaxis]
1.11 /testbed/sklearn/mixture/base.py(508):         return log_prob_norm, log_resp
1.11 /testbed/sklearn/mixture/base.py(295):         return np.mean(log_prob_norm), log_resp
1.11 /testbed/sklearn/mixture/base.py(242):                 self._m_step(X, log_resp)
1.11 /testbed/sklearn/mixture/base.py(243):                 lower_bound = self._compute_lower_bound(
1.11 /testbed/sklearn/mixture/base.py(244):                     log_resp, log_prob_norm)
1.11 /testbed/sklearn/mixture/base.py(246):                 change = lower_bound - prev_lower_bound
1.11 /testbed/sklearn/mixture/base.py(247):                 self._print_verbose_msg_iter_end(n_iter, change)
1.11 /testbed/sklearn/mixture/base.py(521):         if n_iter % self.verbose_interval == 0:
1.11 /testbed/sklearn/mixture/base.py(249):                 if abs(change) < self.tol:
1.11 /testbed/sklearn/mixture/base.py(250):                     self.converged_ = True
1.11 /testbed/sklearn/mixture/base.py(251):                     break
1.11 /testbed/sklearn/mixture/base.py(253):             self._print_verbose_msg_init_end(lower_bound)
1.11 /testbed/sklearn/mixture/base.py(532):         if self.verbose == 1:
1.11 /testbed/sklearn/mixture/base.py(534):         elif self.verbose >= 2:
1.11 /testbed/sklearn/mixture/base.py(255):             if lower_bound > max_lower_bound:
1.11 /testbed/sklearn/mixture/base.py(230):         for init in range(n_init):
1.11 /testbed/sklearn/mixture/base.py(231):             self._print_verbose_msg_init_beg(init)
1.11 /testbed/sklearn/mixture/base.py(512):         if self.verbose == 1:
1.11 /testbed/sklearn/mixture/base.py(514):         elif self.verbose >= 2:
1.11 /testbed/sklearn/mixture/base.py(233):             if do_init:
1.11 /testbed/sklearn/mixture/base.py(234):                 self._initialize_parameters(X, random_state)
1.11 /testbed/sklearn/mixture/base.py(141):         n_samples, _ = X.shape
1.11 /testbed/sklearn/mixture/base.py(143):         if self.init_params == 'kmeans':
1.11 /testbed/sklearn/mixture/base.py(144):             resp = np.zeros((n_samples, self.n_components))
1.11 /testbed/sklearn/mixture/base.py(145):             label = cluster.KMeans(n_clusters=self.n_components, n_init=1,
1.11 /testbed/sklearn/mixture/base.py(146):                                    random_state=random_state).fit(X).labels_
1.12 /testbed/sklearn/mixture/base.py(147):             resp[np.arange(n_samples), label] = 1
1.12 /testbed/sklearn/mixture/base.py(155):         self._initialize(X, resp)
1.12 /testbed/sklearn/mixture/base.py(236):             lower_bound = (-np.infty if do_init else self.lower_bound_)
1.12 /testbed/sklearn/mixture/base.py(238):             for n_iter in range(1, self.max_iter + 1):
1.12 /testbed/sklearn/mixture/base.py(239):                 prev_lower_bound = lower_bound
1.12 /testbed/sklearn/mixture/base.py(241):                 log_prob_norm, log_resp = self._e_step(X)
1.12 /testbed/sklearn/mixture/base.py(294):         log_prob_norm, log_resp = self._estimate_log_prob_resp(X)
1.12 /testbed/sklearn/mixture/base.py(503):         weighted_log_prob = self._estimate_weighted_log_prob(X)
1.12 /testbed/sklearn/mixture/base.py(456):         return self._estimate_log_prob(X) + self._estimate_log_weights()
1.12 /testbed/sklearn/mixture/base.py(504):         log_prob_norm = logsumexp(weighted_log_prob, axis=1)
1.12 /testbed/sklearn/mixture/base.py(505):         with np.errstate(under='ignore'):
1.12 /testbed/sklearn/mixture/base.py(507):             log_resp = weighted_log_prob - log_prob_norm[:, np.newaxis]
1.12 /testbed/sklearn/mixture/base.py(508):         return log_prob_norm, log_resp
1.12 /testbed/sklearn/mixture/base.py(295):         return np.mean(log_prob_norm), log_resp
1.12 /testbed/sklearn/mixture/base.py(242):                 self._m_step(X, log_resp)
1.12 /testbed/sklearn/mixture/base.py(243):                 lower_bound = self._compute_lower_bound(
1.12 /testbed/sklearn/mixture/base.py(244):                     log_resp, log_prob_norm)
1.12 /testbed/sklearn/mixture/base.py(246):                 change = lower_bound - prev_lower_bound
1.12 /testbed/sklearn/mixture/base.py(247):                 self._print_verbose_msg_iter_end(n_iter, change)
1.12 /testbed/sklearn/mixture/base.py(521):         if n_iter % self.verbose_interval == 0:
1.12 /testbed/sklearn/mixture/base.py(249):                 if abs(change) < self.tol:
1.12 /testbed/sklearn/mixture/base.py(238):             for n_iter in range(1, self.max_iter + 1):
1.12 /testbed/sklearn/mixture/base.py(239):                 prev_lower_bound = lower_bound
1.12 /testbed/sklearn/mixture/base.py(241):                 log_prob_norm, log_resp = self._e_step(X)
1.12 /testbed/sklearn/mixture/base.py(294):         log_prob_norm, log_resp = self._estimate_log_prob_resp(X)
1.12 /testbed/sklearn/mixture/base.py(503):         weighted_log_prob = self._estimate_weighted_log_prob(X)
1.12 /testbed/sklearn/mixture/base.py(456):         return self._estimate_log_prob(X) + self._estimate_log_weights()
1.12 /testbed/sklearn/mixture/base.py(504):         log_prob_norm = logsumexp(weighted_log_prob, axis=1)
1.12 /testbed/sklearn/mixture/base.py(505):         with np.errstate(under='ignore'):
1.12 /testbed/sklearn/mixture/base.py(507):             log_resp = weighted_log_prob - log_prob_norm[:, np.newaxis]
1.12 /testbed/sklearn/mixture/base.py(508):         return log_prob_norm, log_resp
1.12 /testbed/sklearn/mixture/base.py(295):         return np.mean(log_prob_norm), log_resp
1.12 /testbed/sklearn/mixture/base.py(242):                 self._m_step(X, log_resp)
1.13 /testbed/sklearn/mixture/base.py(243):                 lower_bound = self._compute_lower_bound(
1.13 /testbed/sklearn/mixture/base.py(244):                     log_resp, log_prob_norm)
1.13 /testbed/sklearn/mixture/base.py(246):                 change = lower_bound - prev_lower_bound
1.13 /testbed/sklearn/mixture/base.py(247):                 self._print_verbose_msg_iter_end(n_iter, change)
1.13 /testbed/sklearn/mixture/base.py(521):         if n_iter % self.verbose_interval == 0:
1.13 /testbed/sklearn/mixture/base.py(249):                 if abs(change) < self.tol:
1.13 /testbed/sklearn/mixture/base.py(238):             for n_iter in range(1, self.max_iter + 1):
1.13 /testbed/sklearn/mixture/base.py(239):                 prev_lower_bound = lower_bound
1.13 /testbed/sklearn/mixture/base.py(241):                 log_prob_norm, log_resp = self._e_step(X)
1.13 /testbed/sklearn/mixture/base.py(294):         log_prob_norm, log_resp = self._estimate_log_prob_resp(X)
1.13 /testbed/sklearn/mixture/base.py(503):         weighted_log_prob = self._estimate_weighted_log_prob(X)
1.13 /testbed/sklearn/mixture/base.py(456):         return self._estimate_log_prob(X) + self._estimate_log_weights()
1.13 /testbed/sklearn/mixture/base.py(504):         log_prob_norm = logsumexp(weighted_log_prob, axis=1)
1.13 /testbed/sklearn/mixture/base.py(505):         with np.errstate(under='ignore'):
1.13 /testbed/sklearn/mixture/base.py(507):             log_resp = weighted_log_prob - log_prob_norm[:, np.newaxis]
1.13 /testbed/sklearn/mixture/base.py(508):         return log_prob_norm, log_resp
1.13 /testbed/sklearn/mixture/base.py(295):         return np.mean(log_prob_norm), log_resp
1.13 /testbed/sklearn/mixture/base.py(242):                 self._m_step(X, log_resp)
1.13 /testbed/sklearn/mixture/base.py(243):                 lower_bound = self._compute_lower_bound(
1.13 /testbed/sklearn/mixture/base.py(244):                     log_resp, log_prob_norm)
1.13 /testbed/sklearn/mixture/base.py(246):                 change = lower_bound - prev_lower_bound
1.13 /testbed/sklearn/mixture/base.py(247):                 self._print_verbose_msg_iter_end(n_iter, change)
1.13 /testbed/sklearn/mixture/base.py(521):         if n_iter % self.verbose_interval == 0:
1.13 /testbed/sklearn/mixture/base.py(249):                 if abs(change) < self.tol:
1.13 /testbed/sklearn/mixture/base.py(238):             for n_iter in range(1, self.max_iter + 1):
1.13 /testbed/sklearn/mixture/base.py(239):                 prev_lower_bound = lower_bound
1.13 /testbed/sklearn/mixture/base.py(241):                 log_prob_norm, log_resp = self._e_step(X)
1.13 /testbed/sklearn/mixture/base.py(294):         log_prob_norm, log_resp = self._estimate_log_prob_resp(X)
1.13 /testbed/sklearn/mixture/base.py(503):         weighted_log_prob = self._estimate_weighted_log_prob(X)
1.13 /testbed/sklearn/mixture/base.py(456):         return self._estimate_log_prob(X) + self._estimate_log_weights()
1.13 /testbed/sklearn/mixture/base.py(504):         log_prob_norm = logsumexp(weighted_log_prob, axis=1)
1.13 /testbed/sklearn/mixture/base.py(505):         with np.errstate(under='ignore'):
1.13 /testbed/sklearn/mixture/base.py(507):             log_resp = weighted_log_prob - log_prob_norm[:, np.newaxis]
1.13 /testbed/sklearn/mixture/base.py(508):         return log_prob_norm, log_resp
1.13 /testbed/sklearn/mixture/base.py(295):         return np.mean(log_prob_norm), log_resp
1.13 /testbed/sklearn/mixture/base.py(242):                 self._m_step(X, log_resp)
1.13 /testbed/sklearn/mixture/base.py(243):                 lower_bound = self._compute_lower_bound(
1.13 /testbed/sklearn/mixture/base.py(244):                     log_resp, log_prob_norm)
1.13 /testbed/sklearn/mixture/base.py(246):                 change = lower_bound - prev_lower_bound
1.13 /testbed/sklearn/mixture/base.py(247):                 self._print_verbose_msg_iter_end(n_iter, change)
1.13 /testbed/sklearn/mixture/base.py(521):         if n_iter % self.verbose_interval == 0:
1.13 /testbed/sklearn/mixture/base.py(249):                 if abs(change) < self.tol:
1.13 /testbed/sklearn/mixture/base.py(238):             for n_iter in range(1, self.max_iter + 1):
1.13 /testbed/sklearn/mixture/base.py(239):                 prev_lower_bound = lower_bound
1.13 /testbed/sklearn/mixture/base.py(241):                 log_prob_norm, log_resp = self._e_step(X)
1.13 /testbed/sklearn/mixture/base.py(294):         log_prob_norm, log_resp = self._estimate_log_prob_resp(X)
1.13 /testbed/sklearn/mixture/base.py(503):         weighted_log_prob = self._estimate_weighted_log_prob(X)
1.13 /testbed/sklearn/mixture/base.py(456):         return self._estimate_log_prob(X) + self._estimate_log_weights()
1.13 /testbed/sklearn/mixture/base.py(504):         log_prob_norm = logsumexp(weighted_log_prob, axis=1)
1.13 /testbed/sklearn/mixture/base.py(505):         with np.errstate(under='ignore'):
1.13 /testbed/sklearn/mixture/base.py(507):             log_resp = weighted_log_prob - log_prob_norm[:, np.newaxis]
1.13 /testbed/sklearn/mixture/base.py(508):         return log_prob_norm, log_resp
1.13 /testbed/sklearn/mixture/base.py(295):         return np.mean(log_prob_norm), log_resp
1.13 /testbed/sklearn/mixture/base.py(242):                 self._m_step(X, log_resp)
1.13 /testbed/sklearn/mixture/base.py(243):                 lower_bound = self._compute_lower_bound(
1.13 /testbed/sklearn/mixture/base.py(244):                     log_resp, log_prob_norm)
1.13 /testbed/sklearn/mixture/base.py(246):                 change = lower_bound - prev_lower_bound
1.13 /testbed/sklearn/mixture/base.py(247):                 self._print_verbose_msg_iter_end(n_iter, change)
1.13 /testbed/sklearn/mixture/base.py(521):         if n_iter % self.verbose_interval == 0:
1.13 /testbed/sklearn/mixture/base.py(249):                 if abs(change) < self.tol:
1.13 /testbed/sklearn/mixture/base.py(238):             for n_iter in range(1, self.max_iter + 1):
1.13 /testbed/sklearn/mixture/base.py(239):                 prev_lower_bound = lower_bound
1.13 /testbed/sklearn/mixture/base.py(241):                 log_prob_norm, log_resp = self._e_step(X)
1.13 /testbed/sklearn/mixture/base.py(294):         log_prob_norm, log_resp = self._estimate_log_prob_resp(X)
1.13 /testbed/sklearn/mixture/base.py(503):         weighted_log_prob = self._estimate_weighted_log_prob(X)
1.13 /testbed/sklearn/mixture/base.py(456):         return self._estimate_log_prob(X) + self._estimate_log_weights()
1.13 /testbed/sklearn/mixture/base.py(504):         log_prob_norm = logsumexp(weighted_log_prob, axis=1)
1.13 /testbed/sklearn/mixture/base.py(505):         with np.errstate(under='ignore'):
1.13 /testbed/sklearn/mixture/base.py(507):             log_resp = weighted_log_prob - log_prob_norm[:, np.newaxis]
1.13 /testbed/sklearn/mixture/base.py(508):         return log_prob_norm, log_resp
1.13 /testbed/sklearn/mixture/base.py(295):         return np.mean(log_prob_norm), log_resp
1.13 /testbed/sklearn/mixture/base.py(242):                 self._m_step(X, log_resp)
1.13 /testbed/sklearn/mixture/base.py(243):                 lower_bound = self._compute_lower_bound(
1.13 /testbed/sklearn/mixture/base.py(244):                     log_resp, log_prob_norm)
1.13 /testbed/sklearn/mixture/base.py(246):                 change = lower_bound - prev_lower_bound
1.13 /testbed/sklearn/mixture/base.py(247):                 self._print_verbose_msg_iter_end(n_iter, change)
1.13 /testbed/sklearn/mixture/base.py(521):         if n_iter % self.verbose_interval == 0:
1.13 /testbed/sklearn/mixture/base.py(249):                 if abs(change) < self.tol:
1.13 /testbed/sklearn/mixture/base.py(238):             for n_iter in range(1, self.max_iter + 1):
1.13 /testbed/sklearn/mixture/base.py(239):                 prev_lower_bound = lower_bound
1.13 /testbed/sklearn/mixture/base.py(241):                 log_prob_norm, log_resp = self._e_step(X)
1.13 /testbed/sklearn/mixture/base.py(294):         log_prob_norm, log_resp = self._estimate_log_prob_resp(X)
1.13 /testbed/sklearn/mixture/base.py(503):         weighted_log_prob = self._estimate_weighted_log_prob(X)
1.13 /testbed/sklearn/mixture/base.py(456):         return self._estimate_log_prob(X) + self._estimate_log_weights()
1.13 /testbed/sklearn/mixture/base.py(504):         log_prob_norm = logsumexp(weighted_log_prob, axis=1)
1.13 /testbed/sklearn/mixture/base.py(505):         with np.errstate(under='ignore'):
1.13 /testbed/sklearn/mixture/base.py(507):             log_resp = weighted_log_prob - log_prob_norm[:, np.newaxis]
1.13 /testbed/sklearn/mixture/base.py(508):         return log_prob_norm, log_resp
1.13 /testbed/sklearn/mixture/base.py(295):         return np.mean(log_prob_norm), log_resp
1.13 /testbed/sklearn/mixture/base.py(242):                 self._m_step(X, log_resp)
1.13 /testbed/sklearn/mixture/base.py(243):                 lower_bound = self._compute_lower_bound(
1.13 /testbed/sklearn/mixture/base.py(244):                     log_resp, log_prob_norm)
1.13 /testbed/sklearn/mixture/base.py(246):                 change = lower_bound - prev_lower_bound
1.13 /testbed/sklearn/mixture/base.py(247):                 self._print_verbose_msg_iter_end(n_iter, change)
1.13 /testbed/sklearn/mixture/base.py(521):         if n_iter % self.verbose_interval == 0:
1.13 /testbed/sklearn/mixture/base.py(249):                 if abs(change) < self.tol:
1.13 /testbed/sklearn/mixture/base.py(238):             for n_iter in range(1, self.max_iter + 1):
1.13 /testbed/sklearn/mixture/base.py(239):                 prev_lower_bound = lower_bound
1.13 /testbed/sklearn/mixture/base.py(241):                 log_prob_norm, log_resp = self._e_step(X)
1.13 /testbed/sklearn/mixture/base.py(294):         log_prob_norm, log_resp = self._estimate_log_prob_resp(X)
1.13 /testbed/sklearn/mixture/base.py(503):         weighted_log_prob = self._estimate_weighted_log_prob(X)
1.13 /testbed/sklearn/mixture/base.py(456):         return self._estimate_log_prob(X) + self._estimate_log_weights()
1.13 /testbed/sklearn/mixture/base.py(504):         log_prob_norm = logsumexp(weighted_log_prob, axis=1)
1.13 /testbed/sklearn/mixture/base.py(505):         with np.errstate(under='ignore'):
1.13 /testbed/sklearn/mixture/base.py(507):             log_resp = weighted_log_prob - log_prob_norm[:, np.newaxis]
1.13 /testbed/sklearn/mixture/base.py(508):         return log_prob_norm, log_resp
1.13 /testbed/sklearn/mixture/base.py(295):         return np.mean(log_prob_norm), log_resp
1.13 /testbed/sklearn/mixture/base.py(242):                 self._m_step(X, log_resp)
1.13 /testbed/sklearn/mixture/base.py(243):                 lower_bound = self._compute_lower_bound(
1.13 /testbed/sklearn/mixture/base.py(244):                     log_resp, log_prob_norm)
1.13 /testbed/sklearn/mixture/base.py(246):                 change = lower_bound - prev_lower_bound
1.13 /testbed/sklearn/mixture/base.py(247):                 self._print_verbose_msg_iter_end(n_iter, change)
1.13 /testbed/sklearn/mixture/base.py(521):         if n_iter % self.verbose_interval == 0:
1.13 /testbed/sklearn/mixture/base.py(249):                 if abs(change) < self.tol:
1.13 /testbed/sklearn/mixture/base.py(238):             for n_iter in range(1, self.max_iter + 1):
1.13 /testbed/sklearn/mixture/base.py(239):                 prev_lower_bound = lower_bound
1.13 /testbed/sklearn/mixture/base.py(241):                 log_prob_norm, log_resp = self._e_step(X)
1.13 /testbed/sklearn/mixture/base.py(294):         log_prob_norm, log_resp = self._estimate_log_prob_resp(X)
1.13 /testbed/sklearn/mixture/base.py(503):         weighted_log_prob = self._estimate_weighted_log_prob(X)
1.13 /testbed/sklearn/mixture/base.py(456):         return self._estimate_log_prob(X) + self._estimate_log_weights()
1.13 /testbed/sklearn/mixture/base.py(504):         log_prob_norm = logsumexp(weighted_log_prob, axis=1)
1.14 /testbed/sklearn/mixture/base.py(505):         with np.errstate(under='ignore'):
1.14 /testbed/sklearn/mixture/base.py(507):             log_resp = weighted_log_prob - log_prob_norm[:, np.newaxis]
1.14 /testbed/sklearn/mixture/base.py(508):         return log_prob_norm, log_resp
1.14 /testbed/sklearn/mixture/base.py(295):         return np.mean(log_prob_norm), log_resp
1.14 /testbed/sklearn/mixture/base.py(242):                 self._m_step(X, log_resp)
1.14 /testbed/sklearn/mixture/base.py(243):                 lower_bound = self._compute_lower_bound(
1.14 /testbed/sklearn/mixture/base.py(244):                     log_resp, log_prob_norm)
1.14 /testbed/sklearn/mixture/base.py(246):                 change = lower_bound - prev_lower_bound
1.14 /testbed/sklearn/mixture/base.py(247):                 self._print_verbose_msg_iter_end(n_iter, change)
1.14 /testbed/sklearn/mixture/base.py(521):         if n_iter % self.verbose_interval == 0:
1.14 /testbed/sklearn/mixture/base.py(249):                 if abs(change) < self.tol:
1.14 /testbed/sklearn/mixture/base.py(238):             for n_iter in range(1, self.max_iter + 1):
1.14 /testbed/sklearn/mixture/base.py(239):                 prev_lower_bound = lower_bound
1.14 /testbed/sklearn/mixture/base.py(241):                 log_prob_norm, log_resp = self._e_step(X)
1.14 /testbed/sklearn/mixture/base.py(294):         log_prob_norm, log_resp = self._estimate_log_prob_resp(X)
1.14 /testbed/sklearn/mixture/base.py(503):         weighted_log_prob = self._estimate_weighted_log_prob(X)
1.14 /testbed/sklearn/mixture/base.py(456):         return self._estimate_log_prob(X) + self._estimate_log_weights()
1.14 /testbed/sklearn/mixture/base.py(504):         log_prob_norm = logsumexp(weighted_log_prob, axis=1)
1.14 /testbed/sklearn/mixture/base.py(505):         with np.errstate(under='ignore'):
1.14 /testbed/sklearn/mixture/base.py(507):             log_resp = weighted_log_prob - log_prob_norm[:, np.newaxis]
1.14 /testbed/sklearn/mixture/base.py(508):         return log_prob_norm, log_resp
1.14 /testbed/sklearn/mixture/base.py(295):         return np.mean(log_prob_norm), log_resp
1.14 /testbed/sklearn/mixture/base.py(242):                 self._m_step(X, log_resp)
1.14 /testbed/sklearn/mixture/base.py(243):                 lower_bound = self._compute_lower_bound(
1.14 /testbed/sklearn/mixture/base.py(244):                     log_resp, log_prob_norm)
1.14 /testbed/sklearn/mixture/base.py(246):                 change = lower_bound - prev_lower_bound
1.14 /testbed/sklearn/mixture/base.py(247):                 self._print_verbose_msg_iter_end(n_iter, change)
1.14 /testbed/sklearn/mixture/base.py(521):         if n_iter % self.verbose_interval == 0:
1.14 /testbed/sklearn/mixture/base.py(522):             if self.verbose == 1:
1.14 /testbed/sklearn/mixture/base.py(524):             elif self.verbose >= 2:
1.14 /testbed/sklearn/mixture/base.py(249):                 if abs(change) < self.tol:
1.14 /testbed/sklearn/mixture/base.py(238):             for n_iter in range(1, self.max_iter + 1):
1.14 /testbed/sklearn/mixture/base.py(239):                 prev_lower_bound = lower_bound
1.14 /testbed/sklearn/mixture/base.py(241):                 log_prob_norm, log_resp = self._e_step(X)
1.14 /testbed/sklearn/mixture/base.py(294):         log_prob_norm, log_resp = self._estimate_log_prob_resp(X)
1.14 /testbed/sklearn/mixture/base.py(503):         weighted_log_prob = self._estimate_weighted_log_prob(X)
1.14 /testbed/sklearn/mixture/base.py(456):         return self._estimate_log_prob(X) + self._estimate_log_weights()
1.14 /testbed/sklearn/mixture/base.py(504):         log_prob_norm = logsumexp(weighted_log_prob, axis=1)
1.14 /testbed/sklearn/mixture/base.py(505):         with np.errstate(under='ignore'):
1.14 /testbed/sklearn/mixture/base.py(507):             log_resp = weighted_log_prob - log_prob_norm[:, np.newaxis]
1.14 /testbed/sklearn/mixture/base.py(508):         return log_prob_norm, log_resp
1.14 /testbed/sklearn/mixture/base.py(295):         return np.mean(log_prob_norm), log_resp
1.14 /testbed/sklearn/mixture/base.py(242):                 self._m_step(X, log_resp)
1.14 /testbed/sklearn/mixture/base.py(243):                 lower_bound = self._compute_lower_bound(
1.14 /testbed/sklearn/mixture/base.py(244):                     log_resp, log_prob_norm)
1.14 /testbed/sklearn/mixture/base.py(246):                 change = lower_bound - prev_lower_bound
1.14 /testbed/sklearn/mixture/base.py(247):                 self._print_verbose_msg_iter_end(n_iter, change)
1.14 /testbed/sklearn/mixture/base.py(521):         if n_iter % self.verbose_interval == 0:
1.14 /testbed/sklearn/mixture/base.py(249):                 if abs(change) < self.tol:
1.14 /testbed/sklearn/mixture/base.py(238):             for n_iter in range(1, self.max_iter + 1):
1.14 /testbed/sklearn/mixture/base.py(239):                 prev_lower_bound = lower_bound
1.14 /testbed/sklearn/mixture/base.py(241):                 log_prob_norm, log_resp = self._e_step(X)
1.14 /testbed/sklearn/mixture/base.py(294):         log_prob_norm, log_resp = self._estimate_log_prob_resp(X)
1.14 /testbed/sklearn/mixture/base.py(503):         weighted_log_prob = self._estimate_weighted_log_prob(X)
1.14 /testbed/sklearn/mixture/base.py(456):         return self._estimate_log_prob(X) + self._estimate_log_weights()
1.14 /testbed/sklearn/mixture/base.py(504):         log_prob_norm = logsumexp(weighted_log_prob, axis=1)
1.14 /testbed/sklearn/mixture/base.py(505):         with np.errstate(under='ignore'):
1.14 /testbed/sklearn/mixture/base.py(507):             log_resp = weighted_log_prob - log_prob_norm[:, np.newaxis]
1.14 /testbed/sklearn/mixture/base.py(508):         return log_prob_norm, log_resp
1.14 /testbed/sklearn/mixture/base.py(295):         return np.mean(log_prob_norm), log_resp
1.14 /testbed/sklearn/mixture/base.py(242):                 self._m_step(X, log_resp)
1.14 /testbed/sklearn/mixture/base.py(243):                 lower_bound = self._compute_lower_bound(
1.14 /testbed/sklearn/mixture/base.py(244):                     log_resp, log_prob_norm)
1.14 /testbed/sklearn/mixture/base.py(246):                 change = lower_bound - prev_lower_bound
1.14 /testbed/sklearn/mixture/base.py(247):                 self._print_verbose_msg_iter_end(n_iter, change)
1.14 /testbed/sklearn/mixture/base.py(521):         if n_iter % self.verbose_interval == 0:
1.14 /testbed/sklearn/mixture/base.py(249):                 if abs(change) < self.tol:
1.14 /testbed/sklearn/mixture/base.py(238):             for n_iter in range(1, self.max_iter + 1):
1.14 /testbed/sklearn/mixture/base.py(239):                 prev_lower_bound = lower_bound
1.14 /testbed/sklearn/mixture/base.py(241):                 log_prob_norm, log_resp = self._e_step(X)
1.14 /testbed/sklearn/mixture/base.py(294):         log_prob_norm, log_resp = self._estimate_log_prob_resp(X)
1.14 /testbed/sklearn/mixture/base.py(503):         weighted_log_prob = self._estimate_weighted_log_prob(X)
1.14 /testbed/sklearn/mixture/base.py(456):         return self._estimate_log_prob(X) + self._estimate_log_weights()
1.14 /testbed/sklearn/mixture/base.py(504):         log_prob_norm = logsumexp(weighted_log_prob, axis=1)
1.14 /testbed/sklearn/mixture/base.py(505):         with np.errstate(under='ignore'):
1.14 /testbed/sklearn/mixture/base.py(507):             log_resp = weighted_log_prob - log_prob_norm[:, np.newaxis]
1.14 /testbed/sklearn/mixture/base.py(508):         return log_prob_norm, log_resp
1.14 /testbed/sklearn/mixture/base.py(295):         return np.mean(log_prob_norm), log_resp
1.14 /testbed/sklearn/mixture/base.py(242):                 self._m_step(X, log_resp)
1.14 /testbed/sklearn/mixture/base.py(243):                 lower_bound = self._compute_lower_bound(
1.14 /testbed/sklearn/mixture/base.py(244):                     log_resp, log_prob_norm)
1.14 /testbed/sklearn/mixture/base.py(246):                 change = lower_bound - prev_lower_bound
1.14 /testbed/sklearn/mixture/base.py(247):                 self._print_verbose_msg_iter_end(n_iter, change)
1.14 /testbed/sklearn/mixture/base.py(521):         if n_iter % self.verbose_interval == 0:
1.14 /testbed/sklearn/mixture/base.py(249):                 if abs(change) < self.tol:
1.14 /testbed/sklearn/mixture/base.py(238):             for n_iter in range(1, self.max_iter + 1):
1.14 /testbed/sklearn/mixture/base.py(239):                 prev_lower_bound = lower_bound
1.14 /testbed/sklearn/mixture/base.py(241):                 log_prob_norm, log_resp = self._e_step(X)
1.14 /testbed/sklearn/mixture/base.py(294):         log_prob_norm, log_resp = self._estimate_log_prob_resp(X)
1.14 /testbed/sklearn/mixture/base.py(503):         weighted_log_prob = self._estimate_weighted_log_prob(X)
1.14 /testbed/sklearn/mixture/base.py(456):         return self._estimate_log_prob(X) + self._estimate_log_weights()
1.14 /testbed/sklearn/mixture/base.py(504):         log_prob_norm = logsumexp(weighted_log_prob, axis=1)
1.14 /testbed/sklearn/mixture/base.py(505):         with np.errstate(under='ignore'):
1.14 /testbed/sklearn/mixture/base.py(507):             log_resp = weighted_log_prob - log_prob_norm[:, np.newaxis]
1.14 /testbed/sklearn/mixture/base.py(508):         return log_prob_norm, log_resp
1.14 /testbed/sklearn/mixture/base.py(295):         return np.mean(log_prob_norm), log_resp
1.14 /testbed/sklearn/mixture/base.py(242):                 self._m_step(X, log_resp)
1.14 /testbed/sklearn/mixture/base.py(243):                 lower_bound = self._compute_lower_bound(
1.14 /testbed/sklearn/mixture/base.py(244):                     log_resp, log_prob_norm)
1.14 /testbed/sklearn/mixture/base.py(246):                 change = lower_bound - prev_lower_bound
1.14 /testbed/sklearn/mixture/base.py(247):                 self._print_verbose_msg_iter_end(n_iter, change)
1.14 /testbed/sklearn/mixture/base.py(521):         if n_iter % self.verbose_interval == 0:
1.14 /testbed/sklearn/mixture/base.py(249):                 if abs(change) < self.tol:
1.14 /testbed/sklearn/mixture/base.py(238):             for n_iter in range(1, self.max_iter + 1):
1.14 /testbed/sklearn/mixture/base.py(239):                 prev_lower_bound = lower_bound
1.14 /testbed/sklearn/mixture/base.py(241):                 log_prob_norm, log_resp = self._e_step(X)
1.14 /testbed/sklearn/mixture/base.py(294):         log_prob_norm, log_resp = self._estimate_log_prob_resp(X)
1.14 /testbed/sklearn/mixture/base.py(503):         weighted_log_prob = self._estimate_weighted_log_prob(X)
1.14 /testbed/sklearn/mixture/base.py(456):         return self._estimate_log_prob(X) + self._estimate_log_weights()
1.14 /testbed/sklearn/mixture/base.py(504):         log_prob_norm = logsumexp(weighted_log_prob, axis=1)
1.14 /testbed/sklearn/mixture/base.py(505):         with np.errstate(under='ignore'):
1.14 /testbed/sklearn/mixture/base.py(507):             log_resp = weighted_log_prob - log_prob_norm[:, np.newaxis]
1.14 /testbed/sklearn/mixture/base.py(508):         return log_prob_norm, log_resp
1.14 /testbed/sklearn/mixture/base.py(295):         return np.mean(log_prob_norm), log_resp
1.14 /testbed/sklearn/mixture/base.py(242):                 self._m_step(X, log_resp)
1.14 /testbed/sklearn/mixture/base.py(243):                 lower_bound = self._compute_lower_bound(
1.14 /testbed/sklearn/mixture/base.py(244):                     log_resp, log_prob_norm)
1.14 /testbed/sklearn/mixture/base.py(246):                 change = lower_bound - prev_lower_bound
1.14 /testbed/sklearn/mixture/base.py(247):                 self._print_verbose_msg_iter_end(n_iter, change)
1.14 /testbed/sklearn/mixture/base.py(521):         if n_iter % self.verbose_interval == 0:
1.14 /testbed/sklearn/mixture/base.py(249):                 if abs(change) < self.tol:
1.14 /testbed/sklearn/mixture/base.py(238):             for n_iter in range(1, self.max_iter + 1):
1.14 /testbed/sklearn/mixture/base.py(239):                 prev_lower_bound = lower_bound
1.14 /testbed/sklearn/mixture/base.py(241):                 log_prob_norm, log_resp = self._e_step(X)
1.14 /testbed/sklearn/mixture/base.py(294):         log_prob_norm, log_resp = self._estimate_log_prob_resp(X)
1.14 /testbed/sklearn/mixture/base.py(503):         weighted_log_prob = self._estimate_weighted_log_prob(X)
1.14 /testbed/sklearn/mixture/base.py(456):         return self._estimate_log_prob(X) + self._estimate_log_weights()
1.14 /testbed/sklearn/mixture/base.py(504):         log_prob_norm = logsumexp(weighted_log_prob, axis=1)
1.14 /testbed/sklearn/mixture/base.py(505):         with np.errstate(under='ignore'):
1.14 /testbed/sklearn/mixture/base.py(507):             log_resp = weighted_log_prob - log_prob_norm[:, np.newaxis]
1.14 /testbed/sklearn/mixture/base.py(508):         return log_prob_norm, log_resp
1.14 /testbed/sklearn/mixture/base.py(295):         return np.mean(log_prob_norm), log_resp
1.14 /testbed/sklearn/mixture/base.py(242):                 self._m_step(X, log_resp)
1.15 /testbed/sklearn/mixture/base.py(243):                 lower_bound = self._compute_lower_bound(
1.15 /testbed/sklearn/mixture/base.py(244):                     log_resp, log_prob_norm)
1.15 /testbed/sklearn/mixture/base.py(246):                 change = lower_bound - prev_lower_bound
1.15 /testbed/sklearn/mixture/base.py(247):                 self._print_verbose_msg_iter_end(n_iter, change)
1.15 /testbed/sklearn/mixture/base.py(521):         if n_iter % self.verbose_interval == 0:
1.15 /testbed/sklearn/mixture/base.py(249):                 if abs(change) < self.tol:
1.15 /testbed/sklearn/mixture/base.py(238):             for n_iter in range(1, self.max_iter + 1):
1.15 /testbed/sklearn/mixture/base.py(239):                 prev_lower_bound = lower_bound
1.15 /testbed/sklearn/mixture/base.py(241):                 log_prob_norm, log_resp = self._e_step(X)
1.15 /testbed/sklearn/mixture/base.py(294):         log_prob_norm, log_resp = self._estimate_log_prob_resp(X)
1.15 /testbed/sklearn/mixture/base.py(503):         weighted_log_prob = self._estimate_weighted_log_prob(X)
1.15 /testbed/sklearn/mixture/base.py(456):         return self._estimate_log_prob(X) + self._estimate_log_weights()
1.15 /testbed/sklearn/mixture/base.py(504):         log_prob_norm = logsumexp(weighted_log_prob, axis=1)
1.15 /testbed/sklearn/mixture/base.py(505):         with np.errstate(under='ignore'):
1.15 /testbed/sklearn/mixture/base.py(507):             log_resp = weighted_log_prob - log_prob_norm[:, np.newaxis]
1.15 /testbed/sklearn/mixture/base.py(508):         return log_prob_norm, log_resp
1.15 /testbed/sklearn/mixture/base.py(295):         return np.mean(log_prob_norm), log_resp
1.15 /testbed/sklearn/mixture/base.py(242):                 self._m_step(X, log_resp)
1.15 /testbed/sklearn/mixture/base.py(243):                 lower_bound = self._compute_lower_bound(
1.15 /testbed/sklearn/mixture/base.py(244):                     log_resp, log_prob_norm)
1.15 /testbed/sklearn/mixture/base.py(246):                 change = lower_bound - prev_lower_bound
1.15 /testbed/sklearn/mixture/base.py(247):                 self._print_verbose_msg_iter_end(n_iter, change)
1.15 /testbed/sklearn/mixture/base.py(521):         if n_iter % self.verbose_interval == 0:
1.15 /testbed/sklearn/mixture/base.py(249):                 if abs(change) < self.tol:
1.15 /testbed/sklearn/mixture/base.py(238):             for n_iter in range(1, self.max_iter + 1):
1.15 /testbed/sklearn/mixture/base.py(239):                 prev_lower_bound = lower_bound
1.15 /testbed/sklearn/mixture/base.py(241):                 log_prob_norm, log_resp = self._e_step(X)
1.15 /testbed/sklearn/mixture/base.py(294):         log_prob_norm, log_resp = self._estimate_log_prob_resp(X)
1.15 /testbed/sklearn/mixture/base.py(503):         weighted_log_prob = self._estimate_weighted_log_prob(X)
1.15 /testbed/sklearn/mixture/base.py(456):         return self._estimate_log_prob(X) + self._estimate_log_weights()
1.15 /testbed/sklearn/mixture/base.py(504):         log_prob_norm = logsumexp(weighted_log_prob, axis=1)
1.15 /testbed/sklearn/mixture/base.py(505):         with np.errstate(under='ignore'):
1.15 /testbed/sklearn/mixture/base.py(507):             log_resp = weighted_log_prob - log_prob_norm[:, np.newaxis]
1.15 /testbed/sklearn/mixture/base.py(508):         return log_prob_norm, log_resp
1.15 /testbed/sklearn/mixture/base.py(295):         return np.mean(log_prob_norm), log_resp
1.15 /testbed/sklearn/mixture/base.py(242):                 self._m_step(X, log_resp)
1.15 /testbed/sklearn/mixture/base.py(243):                 lower_bound = self._compute_lower_bound(
1.15 /testbed/sklearn/mixture/base.py(244):                     log_resp, log_prob_norm)
1.15 /testbed/sklearn/mixture/base.py(246):                 change = lower_bound - prev_lower_bound
1.15 /testbed/sklearn/mixture/base.py(247):                 self._print_verbose_msg_iter_end(n_iter, change)
1.15 /testbed/sklearn/mixture/base.py(521):         if n_iter % self.verbose_interval == 0:
1.15 /testbed/sklearn/mixture/base.py(249):                 if abs(change) < self.tol:
1.15 /testbed/sklearn/mixture/base.py(238):             for n_iter in range(1, self.max_iter + 1):
1.15 /testbed/sklearn/mixture/base.py(239):                 prev_lower_bound = lower_bound
1.15 /testbed/sklearn/mixture/base.py(241):                 log_prob_norm, log_resp = self._e_step(X)
1.15 /testbed/sklearn/mixture/base.py(294):         log_prob_norm, log_resp = self._estimate_log_prob_resp(X)
1.15 /testbed/sklearn/mixture/base.py(503):         weighted_log_prob = self._estimate_weighted_log_prob(X)
1.15 /testbed/sklearn/mixture/base.py(456):         return self._estimate_log_prob(X) + self._estimate_log_weights()
1.15 /testbed/sklearn/mixture/base.py(504):         log_prob_norm = logsumexp(weighted_log_prob, axis=1)
1.15 /testbed/sklearn/mixture/base.py(505):         with np.errstate(under='ignore'):
1.15 /testbed/sklearn/mixture/base.py(507):             log_resp = weighted_log_prob - log_prob_norm[:, np.newaxis]
1.15 /testbed/sklearn/mixture/base.py(508):         return log_prob_norm, log_resp
1.15 /testbed/sklearn/mixture/base.py(295):         return np.mean(log_prob_norm), log_resp
1.15 /testbed/sklearn/mixture/base.py(242):                 self._m_step(X, log_resp)
1.15 /testbed/sklearn/mixture/base.py(243):                 lower_bound = self._compute_lower_bound(
1.15 /testbed/sklearn/mixture/base.py(244):                     log_resp, log_prob_norm)
1.15 /testbed/sklearn/mixture/base.py(246):                 change = lower_bound - prev_lower_bound
1.15 /testbed/sklearn/mixture/base.py(247):                 self._print_verbose_msg_iter_end(n_iter, change)
1.15 /testbed/sklearn/mixture/base.py(521):         if n_iter % self.verbose_interval == 0:
1.15 /testbed/sklearn/mixture/base.py(249):                 if abs(change) < self.tol:
1.15 /testbed/sklearn/mixture/base.py(238):             for n_iter in range(1, self.max_iter + 1):
1.15 /testbed/sklearn/mixture/base.py(239):                 prev_lower_bound = lower_bound
1.15 /testbed/sklearn/mixture/base.py(241):                 log_prob_norm, log_resp = self._e_step(X)
1.15 /testbed/sklearn/mixture/base.py(294):         log_prob_norm, log_resp = self._estimate_log_prob_resp(X)
1.15 /testbed/sklearn/mixture/base.py(503):         weighted_log_prob = self._estimate_weighted_log_prob(X)
1.15 /testbed/sklearn/mixture/base.py(456):         return self._estimate_log_prob(X) + self._estimate_log_weights()
1.15 /testbed/sklearn/mixture/base.py(504):         log_prob_norm = logsumexp(weighted_log_prob, axis=1)
1.15 /testbed/sklearn/mixture/base.py(505):         with np.errstate(under='ignore'):
1.15 /testbed/sklearn/mixture/base.py(507):             log_resp = weighted_log_prob - log_prob_norm[:, np.newaxis]
1.15 /testbed/sklearn/mixture/base.py(508):         return log_prob_norm, log_resp
1.15 /testbed/sklearn/mixture/base.py(295):         return np.mean(log_prob_norm), log_resp
1.15 /testbed/sklearn/mixture/base.py(242):                 self._m_step(X, log_resp)
1.15 /testbed/sklearn/mixture/base.py(243):                 lower_bound = self._compute_lower_bound(
1.15 /testbed/sklearn/mixture/base.py(244):                     log_resp, log_prob_norm)
1.15 /testbed/sklearn/mixture/base.py(246):                 change = lower_bound - prev_lower_bound
1.15 /testbed/sklearn/mixture/base.py(247):                 self._print_verbose_msg_iter_end(n_iter, change)
1.15 /testbed/sklearn/mixture/base.py(521):         if n_iter % self.verbose_interval == 0:
1.15 /testbed/sklearn/mixture/base.py(522):             if self.verbose == 1:
1.15 /testbed/sklearn/mixture/base.py(524):             elif self.verbose >= 2:
1.15 /testbed/sklearn/mixture/base.py(249):                 if abs(change) < self.tol:
1.15 /testbed/sklearn/mixture/base.py(238):             for n_iter in range(1, self.max_iter + 1):
1.15 /testbed/sklearn/mixture/base.py(239):                 prev_lower_bound = lower_bound
1.15 /testbed/sklearn/mixture/base.py(241):                 log_prob_norm, log_resp = self._e_step(X)
1.15 /testbed/sklearn/mixture/base.py(294):         log_prob_norm, log_resp = self._estimate_log_prob_resp(X)
1.15 /testbed/sklearn/mixture/base.py(503):         weighted_log_prob = self._estimate_weighted_log_prob(X)
1.15 /testbed/sklearn/mixture/base.py(456):         return self._estimate_log_prob(X) + self._estimate_log_weights()
1.15 /testbed/sklearn/mixture/base.py(504):         log_prob_norm = logsumexp(weighted_log_prob, axis=1)
1.15 /testbed/sklearn/mixture/base.py(505):         with np.errstate(under='ignore'):
1.15 /testbed/sklearn/mixture/base.py(507):             log_resp = weighted_log_prob - log_prob_norm[:, np.newaxis]
1.15 /testbed/sklearn/mixture/base.py(508):         return log_prob_norm, log_resp
1.15 /testbed/sklearn/mixture/base.py(295):         return np.mean(log_prob_norm), log_resp
1.15 /testbed/sklearn/mixture/base.py(242):                 self._m_step(X, log_resp)
1.15 /testbed/sklearn/mixture/base.py(243):                 lower_bound = self._compute_lower_bound(
1.15 /testbed/sklearn/mixture/base.py(244):                     log_resp, log_prob_norm)
1.15 /testbed/sklearn/mixture/base.py(246):                 change = lower_bound - prev_lower_bound
1.15 /testbed/sklearn/mixture/base.py(247):                 self._print_verbose_msg_iter_end(n_iter, change)
1.15 /testbed/sklearn/mixture/base.py(521):         if n_iter % self.verbose_interval == 0:
1.15 /testbed/sklearn/mixture/base.py(249):                 if abs(change) < self.tol:
1.15 /testbed/sklearn/mixture/base.py(238):             for n_iter in range(1, self.max_iter + 1):
1.15 /testbed/sklearn/mixture/base.py(239):                 prev_lower_bound = lower_bound
1.15 /testbed/sklearn/mixture/base.py(241):                 log_prob_norm, log_resp = self._e_step(X)
1.15 /testbed/sklearn/mixture/base.py(294):         log_prob_norm, log_resp = self._estimate_log_prob_resp(X)
1.15 /testbed/sklearn/mixture/base.py(503):         weighted_log_prob = self._estimate_weighted_log_prob(X)
1.15 /testbed/sklearn/mixture/base.py(456):         return self._estimate_log_prob(X) + self._estimate_log_weights()
1.15 /testbed/sklearn/mixture/base.py(504):         log_prob_norm = logsumexp(weighted_log_prob, axis=1)
1.15 /testbed/sklearn/mixture/base.py(505):         with np.errstate(under='ignore'):
1.15 /testbed/sklearn/mixture/base.py(507):             log_resp = weighted_log_prob - log_prob_norm[:, np.newaxis]
1.15 /testbed/sklearn/mixture/base.py(508):         return log_prob_norm, log_resp
1.15 /testbed/sklearn/mixture/base.py(295):         return np.mean(log_prob_norm), log_resp
1.15 /testbed/sklearn/mixture/base.py(242):                 self._m_step(X, log_resp)
1.15 /testbed/sklearn/mixture/base.py(243):                 lower_bound = self._compute_lower_bound(
1.15 /testbed/sklearn/mixture/base.py(244):                     log_resp, log_prob_norm)
1.15 /testbed/sklearn/mixture/base.py(246):                 change = lower_bound - prev_lower_bound
1.15 /testbed/sklearn/mixture/base.py(247):                 self._print_verbose_msg_iter_end(n_iter, change)
1.15 /testbed/sklearn/mixture/base.py(521):         if n_iter % self.verbose_interval == 0:
1.15 /testbed/sklearn/mixture/base.py(249):                 if abs(change) < self.tol:
1.15 /testbed/sklearn/mixture/base.py(238):             for n_iter in range(1, self.max_iter + 1):
1.15 /testbed/sklearn/mixture/base.py(239):                 prev_lower_bound = lower_bound
1.15 /testbed/sklearn/mixture/base.py(241):                 log_prob_norm, log_resp = self._e_step(X)
1.15 /testbed/sklearn/mixture/base.py(294):         log_prob_norm, log_resp = self._estimate_log_prob_resp(X)
1.15 /testbed/sklearn/mixture/base.py(503):         weighted_log_prob = self._estimate_weighted_log_prob(X)
1.15 /testbed/sklearn/mixture/base.py(456):         return self._estimate_log_prob(X) + self._estimate_log_weights()
1.15 /testbed/sklearn/mixture/base.py(504):         log_prob_norm = logsumexp(weighted_log_prob, axis=1)
1.15 /testbed/sklearn/mixture/base.py(505):         with np.errstate(under='ignore'):
1.15 /testbed/sklearn/mixture/base.py(507):             log_resp = weighted_log_prob - log_prob_norm[:, np.newaxis]
1.15 /testbed/sklearn/mixture/base.py(508):         return log_prob_norm, log_resp
1.15 /testbed/sklearn/mixture/base.py(295):         return np.mean(log_prob_norm), log_resp
1.15 /testbed/sklearn/mixture/base.py(242):                 self._m_step(X, log_resp)
1.15 /testbed/sklearn/mixture/base.py(243):                 lower_bound = self._compute_lower_bound(
1.15 /testbed/sklearn/mixture/base.py(244):                     log_resp, log_prob_norm)
1.15 /testbed/sklearn/mixture/base.py(246):                 change = lower_bound - prev_lower_bound
1.15 /testbed/sklearn/mixture/base.py(247):                 self._print_verbose_msg_iter_end(n_iter, change)
1.15 /testbed/sklearn/mixture/base.py(521):         if n_iter % self.verbose_interval == 0:
1.15 /testbed/sklearn/mixture/base.py(249):                 if abs(change) < self.tol:
1.15 /testbed/sklearn/mixture/base.py(238):             for n_iter in range(1, self.max_iter + 1):
1.15 /testbed/sklearn/mixture/base.py(239):                 prev_lower_bound = lower_bound
1.15 /testbed/sklearn/mixture/base.py(241):                 log_prob_norm, log_resp = self._e_step(X)
1.15 /testbed/sklearn/mixture/base.py(294):         log_prob_norm, log_resp = self._estimate_log_prob_resp(X)
1.15 /testbed/sklearn/mixture/base.py(503):         weighted_log_prob = self._estimate_weighted_log_prob(X)
1.15 /testbed/sklearn/mixture/base.py(456):         return self._estimate_log_prob(X) + self._estimate_log_weights()
1.15 /testbed/sklearn/mixture/base.py(504):         log_prob_norm = logsumexp(weighted_log_prob, axis=1)
1.15 /testbed/sklearn/mixture/base.py(505):         with np.errstate(under='ignore'):
1.15 /testbed/sklearn/mixture/base.py(507):             log_resp = weighted_log_prob - log_prob_norm[:, np.newaxis]
1.15 /testbed/sklearn/mixture/base.py(508):         return log_prob_norm, log_resp
1.15 /testbed/sklearn/mixture/base.py(295):         return np.mean(log_prob_norm), log_resp
1.15 /testbed/sklearn/mixture/base.py(242):                 self._m_step(X, log_resp)
1.15 /testbed/sklearn/mixture/base.py(243):                 lower_bound = self._compute_lower_bound(
1.15 /testbed/sklearn/mixture/base.py(244):                     log_resp, log_prob_norm)
1.15 /testbed/sklearn/mixture/base.py(246):                 change = lower_bound - prev_lower_bound
1.15 /testbed/sklearn/mixture/base.py(247):                 self._print_verbose_msg_iter_end(n_iter, change)
1.15 /testbed/sklearn/mixture/base.py(521):         if n_iter % self.verbose_interval == 0:
1.15 /testbed/sklearn/mixture/base.py(249):                 if abs(change) < self.tol:
1.15 /testbed/sklearn/mixture/base.py(238):             for n_iter in range(1, self.max_iter + 1):
1.15 /testbed/sklearn/mixture/base.py(239):                 prev_lower_bound = lower_bound
1.15 /testbed/sklearn/mixture/base.py(241):                 log_prob_norm, log_resp = self._e_step(X)
1.15 /testbed/sklearn/mixture/base.py(294):         log_prob_norm, log_resp = self._estimate_log_prob_resp(X)
1.15 /testbed/sklearn/mixture/base.py(503):         weighted_log_prob = self._estimate_weighted_log_prob(X)
1.15 /testbed/sklearn/mixture/base.py(456):         return self._estimate_log_prob(X) + self._estimate_log_weights()
1.15 /testbed/sklearn/mixture/base.py(504):         log_prob_norm = logsumexp(weighted_log_prob, axis=1)
1.15 /testbed/sklearn/mixture/base.py(505):         with np.errstate(under='ignore'):
1.15 /testbed/sklearn/mixture/base.py(507):             log_resp = weighted_log_prob - log_prob_norm[:, np.newaxis]
1.15 /testbed/sklearn/mixture/base.py(508):         return log_prob_norm, log_resp
1.15 /testbed/sklearn/mixture/base.py(295):         return np.mean(log_prob_norm), log_resp
1.15 /testbed/sklearn/mixture/base.py(242):                 self._m_step(X, log_resp)
1.16 /testbed/sklearn/mixture/base.py(243):                 lower_bound = self._compute_lower_bound(
1.16 /testbed/sklearn/mixture/base.py(244):                     log_resp, log_prob_norm)
1.16 /testbed/sklearn/mixture/base.py(246):                 change = lower_bound - prev_lower_bound
1.16 /testbed/sklearn/mixture/base.py(247):                 self._print_verbose_msg_iter_end(n_iter, change)
1.16 /testbed/sklearn/mixture/base.py(521):         if n_iter % self.verbose_interval == 0:
1.16 /testbed/sklearn/mixture/base.py(249):                 if abs(change) < self.tol:
1.16 /testbed/sklearn/mixture/base.py(238):             for n_iter in range(1, self.max_iter + 1):
1.16 /testbed/sklearn/mixture/base.py(239):                 prev_lower_bound = lower_bound
1.16 /testbed/sklearn/mixture/base.py(241):                 log_prob_norm, log_resp = self._e_step(X)
1.16 /testbed/sklearn/mixture/base.py(294):         log_prob_norm, log_resp = self._estimate_log_prob_resp(X)
1.16 /testbed/sklearn/mixture/base.py(503):         weighted_log_prob = self._estimate_weighted_log_prob(X)
1.16 /testbed/sklearn/mixture/base.py(456):         return self._estimate_log_prob(X) + self._estimate_log_weights()
1.16 /testbed/sklearn/mixture/base.py(504):         log_prob_norm = logsumexp(weighted_log_prob, axis=1)
1.16 /testbed/sklearn/mixture/base.py(505):         with np.errstate(under='ignore'):
1.16 /testbed/sklearn/mixture/base.py(507):             log_resp = weighted_log_prob - log_prob_norm[:, np.newaxis]
1.16 /testbed/sklearn/mixture/base.py(508):         return log_prob_norm, log_resp
1.16 /testbed/sklearn/mixture/base.py(295):         return np.mean(log_prob_norm), log_resp
1.16 /testbed/sklearn/mixture/base.py(242):                 self._m_step(X, log_resp)
1.16 /testbed/sklearn/mixture/base.py(243):                 lower_bound = self._compute_lower_bound(
1.16 /testbed/sklearn/mixture/base.py(244):                     log_resp, log_prob_norm)
1.16 /testbed/sklearn/mixture/base.py(246):                 change = lower_bound - prev_lower_bound
1.16 /testbed/sklearn/mixture/base.py(247):                 self._print_verbose_msg_iter_end(n_iter, change)
1.16 /testbed/sklearn/mixture/base.py(521):         if n_iter % self.verbose_interval == 0:
1.16 /testbed/sklearn/mixture/base.py(249):                 if abs(change) < self.tol:
1.16 /testbed/sklearn/mixture/base.py(250):                     self.converged_ = True
1.16 /testbed/sklearn/mixture/base.py(251):                     break
1.16 /testbed/sklearn/mixture/base.py(253):             self._print_verbose_msg_init_end(lower_bound)
1.16 /testbed/sklearn/mixture/base.py(532):         if self.verbose == 1:
1.16 /testbed/sklearn/mixture/base.py(534):         elif self.verbose >= 2:
1.16 /testbed/sklearn/mixture/base.py(255):             if lower_bound > max_lower_bound:
1.16 /testbed/sklearn/mixture/base.py(256):                 max_lower_bound = lower_bound
1.16 /testbed/sklearn/mixture/base.py(257):                 best_params = self._get_parameters()
1.16 /testbed/sklearn/mixture/base.py(258):                 best_n_iter = n_iter
1.16 /testbed/sklearn/mixture/base.py(230):         for init in range(n_init):
1.16 /testbed/sklearn/mixture/base.py(231):             self._print_verbose_msg_init_beg(init)
1.16 /testbed/sklearn/mixture/base.py(512):         if self.verbose == 1:
1.16 /testbed/sklearn/mixture/base.py(514):         elif self.verbose >= 2:
1.16 /testbed/sklearn/mixture/base.py(233):             if do_init:
1.16 /testbed/sklearn/mixture/base.py(234):                 self._initialize_parameters(X, random_state)
1.16 /testbed/sklearn/mixture/base.py(141):         n_samples, _ = X.shape
1.16 /testbed/sklearn/mixture/base.py(143):         if self.init_params == 'kmeans':
1.16 /testbed/sklearn/mixture/base.py(144):             resp = np.zeros((n_samples, self.n_components))
1.16 /testbed/sklearn/mixture/base.py(145):             label = cluster.KMeans(n_clusters=self.n_components, n_init=1,
1.16 /testbed/sklearn/mixture/base.py(146):                                    random_state=random_state).fit(X).labels_
1.16 /testbed/sklearn/mixture/base.py(147):             resp[np.arange(n_samples), label] = 1
1.16 /testbed/sklearn/mixture/base.py(155):         self._initialize(X, resp)
1.16 /testbed/sklearn/mixture/base.py(236):             lower_bound = (-np.infty if do_init else self.lower_bound_)
1.16 /testbed/sklearn/mixture/base.py(238):             for n_iter in range(1, self.max_iter + 1):
1.16 /testbed/sklearn/mixture/base.py(239):                 prev_lower_bound = lower_bound
1.16 /testbed/sklearn/mixture/base.py(241):                 log_prob_norm, log_resp = self._e_step(X)
1.16 /testbed/sklearn/mixture/base.py(294):         log_prob_norm, log_resp = self._estimate_log_prob_resp(X)
1.16 /testbed/sklearn/mixture/base.py(503):         weighted_log_prob = self._estimate_weighted_log_prob(X)
1.16 /testbed/sklearn/mixture/base.py(456):         return self._estimate_log_prob(X) + self._estimate_log_weights()
1.16 /testbed/sklearn/mixture/base.py(504):         log_prob_norm = logsumexp(weighted_log_prob, axis=1)
1.16 /testbed/sklearn/mixture/base.py(505):         with np.errstate(under='ignore'):
1.16 /testbed/sklearn/mixture/base.py(507):             log_resp = weighted_log_prob - log_prob_norm[:, np.newaxis]
1.16 /testbed/sklearn/mixture/base.py(508):         return log_prob_norm, log_resp
1.16 /testbed/sklearn/mixture/base.py(295):         return np.mean(log_prob_norm), log_resp
1.16 /testbed/sklearn/mixture/base.py(242):                 self._m_step(X, log_resp)
1.17 /testbed/sklearn/mixture/base.py(243):                 lower_bound = self._compute_lower_bound(
1.17 /testbed/sklearn/mixture/base.py(244):                     log_resp, log_prob_norm)
1.17 /testbed/sklearn/mixture/base.py(246):                 change = lower_bound - prev_lower_bound
1.17 /testbed/sklearn/mixture/base.py(247):                 self._print_verbose_msg_iter_end(n_iter, change)
1.17 /testbed/sklearn/mixture/base.py(521):         if n_iter % self.verbose_interval == 0:
1.17 /testbed/sklearn/mixture/base.py(249):                 if abs(change) < self.tol:
1.17 /testbed/sklearn/mixture/base.py(238):             for n_iter in range(1, self.max_iter + 1):
1.17 /testbed/sklearn/mixture/base.py(239):                 prev_lower_bound = lower_bound
1.17 /testbed/sklearn/mixture/base.py(241):                 log_prob_norm, log_resp = self._e_step(X)
1.17 /testbed/sklearn/mixture/base.py(294):         log_prob_norm, log_resp = self._estimate_log_prob_resp(X)
1.17 /testbed/sklearn/mixture/base.py(503):         weighted_log_prob = self._estimate_weighted_log_prob(X)
1.17 /testbed/sklearn/mixture/base.py(456):         return self._estimate_log_prob(X) + self._estimate_log_weights()
1.17 /testbed/sklearn/mixture/base.py(504):         log_prob_norm = logsumexp(weighted_log_prob, axis=1)
1.17 /testbed/sklearn/mixture/base.py(505):         with np.errstate(under='ignore'):
1.17 /testbed/sklearn/mixture/base.py(507):             log_resp = weighted_log_prob - log_prob_norm[:, np.newaxis]
1.17 /testbed/sklearn/mixture/base.py(508):         return log_prob_norm, log_resp
1.17 /testbed/sklearn/mixture/base.py(295):         return np.mean(log_prob_norm), log_resp
1.17 /testbed/sklearn/mixture/base.py(242):                 self._m_step(X, log_resp)
1.17 /testbed/sklearn/mixture/base.py(243):                 lower_bound = self._compute_lower_bound(
1.17 /testbed/sklearn/mixture/base.py(244):                     log_resp, log_prob_norm)
1.17 /testbed/sklearn/mixture/base.py(246):                 change = lower_bound - prev_lower_bound
1.17 /testbed/sklearn/mixture/base.py(247):                 self._print_verbose_msg_iter_end(n_iter, change)
1.17 /testbed/sklearn/mixture/base.py(521):         if n_iter % self.verbose_interval == 0:
1.17 /testbed/sklearn/mixture/base.py(249):                 if abs(change) < self.tol:
1.17 /testbed/sklearn/mixture/base.py(238):             for n_iter in range(1, self.max_iter + 1):
1.17 /testbed/sklearn/mixture/base.py(239):                 prev_lower_bound = lower_bound
1.17 /testbed/sklearn/mixture/base.py(241):                 log_prob_norm, log_resp = self._e_step(X)
1.17 /testbed/sklearn/mixture/base.py(294):         log_prob_norm, log_resp = self._estimate_log_prob_resp(X)
1.17 /testbed/sklearn/mixture/base.py(503):         weighted_log_prob = self._estimate_weighted_log_prob(X)
1.17 /testbed/sklearn/mixture/base.py(456):         return self._estimate_log_prob(X) + self._estimate_log_weights()
1.17 /testbed/sklearn/mixture/base.py(504):         log_prob_norm = logsumexp(weighted_log_prob, axis=1)
1.17 /testbed/sklearn/mixture/base.py(505):         with np.errstate(under='ignore'):
1.17 /testbed/sklearn/mixture/base.py(507):             log_resp = weighted_log_prob - log_prob_norm[:, np.newaxis]
1.17 /testbed/sklearn/mixture/base.py(508):         return log_prob_norm, log_resp
1.17 /testbed/sklearn/mixture/base.py(295):         return np.mean(log_prob_norm), log_resp
1.17 /testbed/sklearn/mixture/base.py(242):                 self._m_step(X, log_resp)
1.17 /testbed/sklearn/mixture/base.py(243):                 lower_bound = self._compute_lower_bound(
1.17 /testbed/sklearn/mixture/base.py(244):                     log_resp, log_prob_norm)
1.17 /testbed/sklearn/mixture/base.py(246):                 change = lower_bound - prev_lower_bound
1.17 /testbed/sklearn/mixture/base.py(247):                 self._print_verbose_msg_iter_end(n_iter, change)
1.17 /testbed/sklearn/mixture/base.py(521):         if n_iter % self.verbose_interval == 0:
1.17 /testbed/sklearn/mixture/base.py(249):                 if abs(change) < self.tol:
1.17 /testbed/sklearn/mixture/base.py(238):             for n_iter in range(1, self.max_iter + 1):
1.17 /testbed/sklearn/mixture/base.py(239):                 prev_lower_bound = lower_bound
1.17 /testbed/sklearn/mixture/base.py(241):                 log_prob_norm, log_resp = self._e_step(X)
1.17 /testbed/sklearn/mixture/base.py(294):         log_prob_norm, log_resp = self._estimate_log_prob_resp(X)
1.17 /testbed/sklearn/mixture/base.py(503):         weighted_log_prob = self._estimate_weighted_log_prob(X)
1.17 /testbed/sklearn/mixture/base.py(456):         return self._estimate_log_prob(X) + self._estimate_log_weights()
1.17 /testbed/sklearn/mixture/base.py(504):         log_prob_norm = logsumexp(weighted_log_prob, axis=1)
1.17 /testbed/sklearn/mixture/base.py(505):         with np.errstate(under='ignore'):
1.17 /testbed/sklearn/mixture/base.py(507):             log_resp = weighted_log_prob - log_prob_norm[:, np.newaxis]
1.17 /testbed/sklearn/mixture/base.py(508):         return log_prob_norm, log_resp
1.17 /testbed/sklearn/mixture/base.py(295):         return np.mean(log_prob_norm), log_resp
1.17 /testbed/sklearn/mixture/base.py(242):                 self._m_step(X, log_resp)
1.17 /testbed/sklearn/mixture/base.py(243):                 lower_bound = self._compute_lower_bound(
1.17 /testbed/sklearn/mixture/base.py(244):                     log_resp, log_prob_norm)
1.17 /testbed/sklearn/mixture/base.py(246):                 change = lower_bound - prev_lower_bound
1.17 /testbed/sklearn/mixture/base.py(247):                 self._print_verbose_msg_iter_end(n_iter, change)
1.17 /testbed/sklearn/mixture/base.py(521):         if n_iter % self.verbose_interval == 0:
1.17 /testbed/sklearn/mixture/base.py(249):                 if abs(change) < self.tol:
1.17 /testbed/sklearn/mixture/base.py(238):             for n_iter in range(1, self.max_iter + 1):
1.17 /testbed/sklearn/mixture/base.py(239):                 prev_lower_bound = lower_bound
1.17 /testbed/sklearn/mixture/base.py(241):                 log_prob_norm, log_resp = self._e_step(X)
1.17 /testbed/sklearn/mixture/base.py(294):         log_prob_norm, log_resp = self._estimate_log_prob_resp(X)
1.17 /testbed/sklearn/mixture/base.py(503):         weighted_log_prob = self._estimate_weighted_log_prob(X)
1.17 /testbed/sklearn/mixture/base.py(456):         return self._estimate_log_prob(X) + self._estimate_log_weights()
1.17 /testbed/sklearn/mixture/base.py(504):         log_prob_norm = logsumexp(weighted_log_prob, axis=1)
1.17 /testbed/sklearn/mixture/base.py(505):         with np.errstate(under='ignore'):
1.17 /testbed/sklearn/mixture/base.py(507):             log_resp = weighted_log_prob - log_prob_norm[:, np.newaxis]
1.17 /testbed/sklearn/mixture/base.py(508):         return log_prob_norm, log_resp
1.17 /testbed/sklearn/mixture/base.py(295):         return np.mean(log_prob_norm), log_resp
1.17 /testbed/sklearn/mixture/base.py(242):                 self._m_step(X, log_resp)
1.17 /testbed/sklearn/mixture/base.py(243):                 lower_bound = self._compute_lower_bound(
1.17 /testbed/sklearn/mixture/base.py(244):                     log_resp, log_prob_norm)
1.17 /testbed/sklearn/mixture/base.py(246):                 change = lower_bound - prev_lower_bound
1.17 /testbed/sklearn/mixture/base.py(247):                 self._print_verbose_msg_iter_end(n_iter, change)
1.17 /testbed/sklearn/mixture/base.py(521):         if n_iter % self.verbose_interval == 0:
1.17 /testbed/sklearn/mixture/base.py(249):                 if abs(change) < self.tol:
1.17 /testbed/sklearn/mixture/base.py(238):             for n_iter in range(1, self.max_iter + 1):
1.17 /testbed/sklearn/mixture/base.py(239):                 prev_lower_bound = lower_bound
1.17 /testbed/sklearn/mixture/base.py(241):                 log_prob_norm, log_resp = self._e_step(X)
1.17 /testbed/sklearn/mixture/base.py(294):         log_prob_norm, log_resp = self._estimate_log_prob_resp(X)
1.17 /testbed/sklearn/mixture/base.py(503):         weighted_log_prob = self._estimate_weighted_log_prob(X)
1.17 /testbed/sklearn/mixture/base.py(456):         return self._estimate_log_prob(X) + self._estimate_log_weights()
1.17 /testbed/sklearn/mixture/base.py(504):         log_prob_norm = logsumexp(weighted_log_prob, axis=1)
1.17 /testbed/sklearn/mixture/base.py(505):         with np.errstate(under='ignore'):
1.17 /testbed/sklearn/mixture/base.py(507):             log_resp = weighted_log_prob - log_prob_norm[:, np.newaxis]
1.17 /testbed/sklearn/mixture/base.py(508):         return log_prob_norm, log_resp
1.17 /testbed/sklearn/mixture/base.py(295):         return np.mean(log_prob_norm), log_resp
1.17 /testbed/sklearn/mixture/base.py(242):                 self._m_step(X, log_resp)
1.17 /testbed/sklearn/mixture/base.py(243):                 lower_bound = self._compute_lower_bound(
1.17 /testbed/sklearn/mixture/base.py(244):                     log_resp, log_prob_norm)
1.17 /testbed/sklearn/mixture/base.py(246):                 change = lower_bound - prev_lower_bound
1.17 /testbed/sklearn/mixture/base.py(247):                 self._print_verbose_msg_iter_end(n_iter, change)
1.17 /testbed/sklearn/mixture/base.py(521):         if n_iter % self.verbose_interval == 0:
1.17 /testbed/sklearn/mixture/base.py(249):                 if abs(change) < self.tol:
1.17 /testbed/sklearn/mixture/base.py(238):             for n_iter in range(1, self.max_iter + 1):
1.17 /testbed/sklearn/mixture/base.py(239):                 prev_lower_bound = lower_bound
1.17 /testbed/sklearn/mixture/base.py(241):                 log_prob_norm, log_resp = self._e_step(X)
1.17 /testbed/sklearn/mixture/base.py(294):         log_prob_norm, log_resp = self._estimate_log_prob_resp(X)
1.17 /testbed/sklearn/mixture/base.py(503):         weighted_log_prob = self._estimate_weighted_log_prob(X)
1.17 /testbed/sklearn/mixture/base.py(456):         return self._estimate_log_prob(X) + self._estimate_log_weights()
1.17 /testbed/sklearn/mixture/base.py(504):         log_prob_norm = logsumexp(weighted_log_prob, axis=1)
1.17 /testbed/sklearn/mixture/base.py(505):         with np.errstate(under='ignore'):
1.17 /testbed/sklearn/mixture/base.py(507):             log_resp = weighted_log_prob - log_prob_norm[:, np.newaxis]
1.17 /testbed/sklearn/mixture/base.py(508):         return log_prob_norm, log_resp
1.17 /testbed/sklearn/mixture/base.py(295):         return np.mean(log_prob_norm), log_resp
1.17 /testbed/sklearn/mixture/base.py(242):                 self._m_step(X, log_resp)
1.17 /testbed/sklearn/mixture/base.py(243):                 lower_bound = self._compute_lower_bound(
1.17 /testbed/sklearn/mixture/base.py(244):                     log_resp, log_prob_norm)
1.17 /testbed/sklearn/mixture/base.py(246):                 change = lower_bound - prev_lower_bound
1.17 /testbed/sklearn/mixture/base.py(247):                 self._print_verbose_msg_iter_end(n_iter, change)
1.17 /testbed/sklearn/mixture/base.py(521):         if n_iter % self.verbose_interval == 0:
1.17 /testbed/sklearn/mixture/base.py(249):                 if abs(change) < self.tol:
1.17 /testbed/sklearn/mixture/base.py(238):             for n_iter in range(1, self.max_iter + 1):
1.17 /testbed/sklearn/mixture/base.py(239):                 prev_lower_bound = lower_bound
1.17 /testbed/sklearn/mixture/base.py(241):                 log_prob_norm, log_resp = self._e_step(X)
1.17 /testbed/sklearn/mixture/base.py(294):         log_prob_norm, log_resp = self._estimate_log_prob_resp(X)
1.17 /testbed/sklearn/mixture/base.py(503):         weighted_log_prob = self._estimate_weighted_log_prob(X)
1.17 /testbed/sklearn/mixture/base.py(456):         return self._estimate_log_prob(X) + self._estimate_log_weights()
1.17 /testbed/sklearn/mixture/base.py(504):         log_prob_norm = logsumexp(weighted_log_prob, axis=1)
1.17 /testbed/sklearn/mixture/base.py(505):         with np.errstate(under='ignore'):
1.17 /testbed/sklearn/mixture/base.py(507):             log_resp = weighted_log_prob - log_prob_norm[:, np.newaxis]
1.17 /testbed/sklearn/mixture/base.py(508):         return log_prob_norm, log_resp
1.17 /testbed/sklearn/mixture/base.py(295):         return np.mean(log_prob_norm), log_resp
1.17 /testbed/sklearn/mixture/base.py(242):                 self._m_step(X, log_resp)
1.17 /testbed/sklearn/mixture/base.py(243):                 lower_bound = self._compute_lower_bound(
1.17 /testbed/sklearn/mixture/base.py(244):                     log_resp, log_prob_norm)
1.17 /testbed/sklearn/mixture/base.py(246):                 change = lower_bound - prev_lower_bound
1.17 /testbed/sklearn/mixture/base.py(247):                 self._print_verbose_msg_iter_end(n_iter, change)
1.17 /testbed/sklearn/mixture/base.py(521):         if n_iter % self.verbose_interval == 0:
1.17 /testbed/sklearn/mixture/base.py(249):                 if abs(change) < self.tol:
1.17 /testbed/sklearn/mixture/base.py(238):             for n_iter in range(1, self.max_iter + 1):
1.17 /testbed/sklearn/mixture/base.py(239):                 prev_lower_bound = lower_bound
1.17 /testbed/sklearn/mixture/base.py(241):                 log_prob_norm, log_resp = self._e_step(X)
1.17 /testbed/sklearn/mixture/base.py(294):         log_prob_norm, log_resp = self._estimate_log_prob_resp(X)
1.17 /testbed/sklearn/mixture/base.py(503):         weighted_log_prob = self._estimate_weighted_log_prob(X)
1.17 /testbed/sklearn/mixture/base.py(456):         return self._estimate_log_prob(X) + self._estimate_log_weights()
1.17 /testbed/sklearn/mixture/base.py(504):         log_prob_norm = logsumexp(weighted_log_prob, axis=1)
1.18 /testbed/sklearn/mixture/base.py(505):         with np.errstate(under='ignore'):
1.18 /testbed/sklearn/mixture/base.py(507):             log_resp = weighted_log_prob - log_prob_norm[:, np.newaxis]
1.18 /testbed/sklearn/mixture/base.py(508):         return log_prob_norm, log_resp
1.18 /testbed/sklearn/mixture/base.py(295):         return np.mean(log_prob_norm), log_resp
1.18 /testbed/sklearn/mixture/base.py(242):                 self._m_step(X, log_resp)
1.18 /testbed/sklearn/mixture/base.py(243):                 lower_bound = self._compute_lower_bound(
1.18 /testbed/sklearn/mixture/base.py(244):                     log_resp, log_prob_norm)
1.18 /testbed/sklearn/mixture/base.py(246):                 change = lower_bound - prev_lower_bound
1.18 /testbed/sklearn/mixture/base.py(247):                 self._print_verbose_msg_iter_end(n_iter, change)
1.18 /testbed/sklearn/mixture/base.py(521):         if n_iter % self.verbose_interval == 0:
1.18 /testbed/sklearn/mixture/base.py(249):                 if abs(change) < self.tol:
1.18 /testbed/sklearn/mixture/base.py(238):             for n_iter in range(1, self.max_iter + 1):
1.18 /testbed/sklearn/mixture/base.py(239):                 prev_lower_bound = lower_bound
1.18 /testbed/sklearn/mixture/base.py(241):                 log_prob_norm, log_resp = self._e_step(X)
1.18 /testbed/sklearn/mixture/base.py(294):         log_prob_norm, log_resp = self._estimate_log_prob_resp(X)
1.18 /testbed/sklearn/mixture/base.py(503):         weighted_log_prob = self._estimate_weighted_log_prob(X)
1.18 /testbed/sklearn/mixture/base.py(456):         return self._estimate_log_prob(X) + self._estimate_log_weights()
1.18 /testbed/sklearn/mixture/base.py(504):         log_prob_norm = logsumexp(weighted_log_prob, axis=1)
1.18 /testbed/sklearn/mixture/base.py(505):         with np.errstate(under='ignore'):
1.18 /testbed/sklearn/mixture/base.py(507):             log_resp = weighted_log_prob - log_prob_norm[:, np.newaxis]
1.18 /testbed/sklearn/mixture/base.py(508):         return log_prob_norm, log_resp
1.18 /testbed/sklearn/mixture/base.py(295):         return np.mean(log_prob_norm), log_resp
1.18 /testbed/sklearn/mixture/base.py(242):                 self._m_step(X, log_resp)
1.18 /testbed/sklearn/mixture/base.py(243):                 lower_bound = self._compute_lower_bound(
1.18 /testbed/sklearn/mixture/base.py(244):                     log_resp, log_prob_norm)
1.18 /testbed/sklearn/mixture/base.py(246):                 change = lower_bound - prev_lower_bound
1.18 /testbed/sklearn/mixture/base.py(247):                 self._print_verbose_msg_iter_end(n_iter, change)
1.18 /testbed/sklearn/mixture/base.py(521):         if n_iter % self.verbose_interval == 0:
1.18 /testbed/sklearn/mixture/base.py(522):             if self.verbose == 1:
1.18 /testbed/sklearn/mixture/base.py(524):             elif self.verbose >= 2:
1.18 /testbed/sklearn/mixture/base.py(249):                 if abs(change) < self.tol:
1.18 /testbed/sklearn/mixture/base.py(238):             for n_iter in range(1, self.max_iter + 1):
1.18 /testbed/sklearn/mixture/base.py(239):                 prev_lower_bound = lower_bound
1.18 /testbed/sklearn/mixture/base.py(241):                 log_prob_norm, log_resp = self._e_step(X)
1.18 /testbed/sklearn/mixture/base.py(294):         log_prob_norm, log_resp = self._estimate_log_prob_resp(X)
1.18 /testbed/sklearn/mixture/base.py(503):         weighted_log_prob = self._estimate_weighted_log_prob(X)
1.18 /testbed/sklearn/mixture/base.py(456):         return self._estimate_log_prob(X) + self._estimate_log_weights()
1.18 /testbed/sklearn/mixture/base.py(504):         log_prob_norm = logsumexp(weighted_log_prob, axis=1)
1.18 /testbed/sklearn/mixture/base.py(505):         with np.errstate(under='ignore'):
1.18 /testbed/sklearn/mixture/base.py(507):             log_resp = weighted_log_prob - log_prob_norm[:, np.newaxis]
1.18 /testbed/sklearn/mixture/base.py(508):         return log_prob_norm, log_resp
1.18 /testbed/sklearn/mixture/base.py(295):         return np.mean(log_prob_norm), log_resp
1.18 /testbed/sklearn/mixture/base.py(242):                 self._m_step(X, log_resp)
1.18 /testbed/sklearn/mixture/base.py(243):                 lower_bound = self._compute_lower_bound(
1.18 /testbed/sklearn/mixture/base.py(244):                     log_resp, log_prob_norm)
1.18 /testbed/sklearn/mixture/base.py(246):                 change = lower_bound - prev_lower_bound
1.18 /testbed/sklearn/mixture/base.py(247):                 self._print_verbose_msg_iter_end(n_iter, change)
1.18 /testbed/sklearn/mixture/base.py(521):         if n_iter % self.verbose_interval == 0:
1.18 /testbed/sklearn/mixture/base.py(249):                 if abs(change) < self.tol:
1.18 /testbed/sklearn/mixture/base.py(238):             for n_iter in range(1, self.max_iter + 1):
1.18 /testbed/sklearn/mixture/base.py(239):                 prev_lower_bound = lower_bound
1.18 /testbed/sklearn/mixture/base.py(241):                 log_prob_norm, log_resp = self._e_step(X)
1.18 /testbed/sklearn/mixture/base.py(294):         log_prob_norm, log_resp = self._estimate_log_prob_resp(X)
1.18 /testbed/sklearn/mixture/base.py(503):         weighted_log_prob = self._estimate_weighted_log_prob(X)
1.18 /testbed/sklearn/mixture/base.py(456):         return self._estimate_log_prob(X) + self._estimate_log_weights()
1.18 /testbed/sklearn/mixture/base.py(504):         log_prob_norm = logsumexp(weighted_log_prob, axis=1)
1.18 /testbed/sklearn/mixture/base.py(505):         with np.errstate(under='ignore'):
1.18 /testbed/sklearn/mixture/base.py(507):             log_resp = weighted_log_prob - log_prob_norm[:, np.newaxis]
1.18 /testbed/sklearn/mixture/base.py(508):         return log_prob_norm, log_resp
1.18 /testbed/sklearn/mixture/base.py(295):         return np.mean(log_prob_norm), log_resp
1.18 /testbed/sklearn/mixture/base.py(242):                 self._m_step(X, log_resp)
1.18 /testbed/sklearn/mixture/base.py(243):                 lower_bound = self._compute_lower_bound(
1.18 /testbed/sklearn/mixture/base.py(244):                     log_resp, log_prob_norm)
1.18 /testbed/sklearn/mixture/base.py(246):                 change = lower_bound - prev_lower_bound
1.18 /testbed/sklearn/mixture/base.py(247):                 self._print_verbose_msg_iter_end(n_iter, change)
1.18 /testbed/sklearn/mixture/base.py(521):         if n_iter % self.verbose_interval == 0:
1.18 /testbed/sklearn/mixture/base.py(249):                 if abs(change) < self.tol:
1.18 /testbed/sklearn/mixture/base.py(238):             for n_iter in range(1, self.max_iter + 1):
1.18 /testbed/sklearn/mixture/base.py(239):                 prev_lower_bound = lower_bound
1.18 /testbed/sklearn/mixture/base.py(241):                 log_prob_norm, log_resp = self._e_step(X)
1.18 /testbed/sklearn/mixture/base.py(294):         log_prob_norm, log_resp = self._estimate_log_prob_resp(X)
1.18 /testbed/sklearn/mixture/base.py(503):         weighted_log_prob = self._estimate_weighted_log_prob(X)
1.18 /testbed/sklearn/mixture/base.py(456):         return self._estimate_log_prob(X) + self._estimate_log_weights()
1.18 /testbed/sklearn/mixture/base.py(504):         log_prob_norm = logsumexp(weighted_log_prob, axis=1)
1.18 /testbed/sklearn/mixture/base.py(505):         with np.errstate(under='ignore'):
1.18 /testbed/sklearn/mixture/base.py(507):             log_resp = weighted_log_prob - log_prob_norm[:, np.newaxis]
1.18 /testbed/sklearn/mixture/base.py(508):         return log_prob_norm, log_resp
1.18 /testbed/sklearn/mixture/base.py(295):         return np.mean(log_prob_norm), log_resp
1.18 /testbed/sklearn/mixture/base.py(242):                 self._m_step(X, log_resp)
1.18 /testbed/sklearn/mixture/base.py(243):                 lower_bound = self._compute_lower_bound(
1.18 /testbed/sklearn/mixture/base.py(244):                     log_resp, log_prob_norm)
1.18 /testbed/sklearn/mixture/base.py(246):                 change = lower_bound - prev_lower_bound
1.18 /testbed/sklearn/mixture/base.py(247):                 self._print_verbose_msg_iter_end(n_iter, change)
1.18 /testbed/sklearn/mixture/base.py(521):         if n_iter % self.verbose_interval == 0:
1.18 /testbed/sklearn/mixture/base.py(249):                 if abs(change) < self.tol:
1.18 /testbed/sklearn/mixture/base.py(238):             for n_iter in range(1, self.max_iter + 1):
1.18 /testbed/sklearn/mixture/base.py(239):                 prev_lower_bound = lower_bound
1.18 /testbed/sklearn/mixture/base.py(241):                 log_prob_norm, log_resp = self._e_step(X)
1.18 /testbed/sklearn/mixture/base.py(294):         log_prob_norm, log_resp = self._estimate_log_prob_resp(X)
1.18 /testbed/sklearn/mixture/base.py(503):         weighted_log_prob = self._estimate_weighted_log_prob(X)
1.18 /testbed/sklearn/mixture/base.py(456):         return self._estimate_log_prob(X) + self._estimate_log_weights()
1.18 /testbed/sklearn/mixture/base.py(504):         log_prob_norm = logsumexp(weighted_log_prob, axis=1)
1.18 /testbed/sklearn/mixture/base.py(505):         with np.errstate(under='ignore'):
1.18 /testbed/sklearn/mixture/base.py(507):             log_resp = weighted_log_prob - log_prob_norm[:, np.newaxis]
1.18 /testbed/sklearn/mixture/base.py(508):         return log_prob_norm, log_resp
1.18 /testbed/sklearn/mixture/base.py(295):         return np.mean(log_prob_norm), log_resp
1.18 /testbed/sklearn/mixture/base.py(242):                 self._m_step(X, log_resp)
1.18 /testbed/sklearn/mixture/base.py(243):                 lower_bound = self._compute_lower_bound(
1.18 /testbed/sklearn/mixture/base.py(244):                     log_resp, log_prob_norm)
1.18 /testbed/sklearn/mixture/base.py(246):                 change = lower_bound - prev_lower_bound
1.18 /testbed/sklearn/mixture/base.py(247):                 self._print_verbose_msg_iter_end(n_iter, change)
1.18 /testbed/sklearn/mixture/base.py(521):         if n_iter % self.verbose_interval == 0:
1.18 /testbed/sklearn/mixture/base.py(249):                 if abs(change) < self.tol:
1.18 /testbed/sklearn/mixture/base.py(238):             for n_iter in range(1, self.max_iter + 1):
1.18 /testbed/sklearn/mixture/base.py(239):                 prev_lower_bound = lower_bound
1.18 /testbed/sklearn/mixture/base.py(241):                 log_prob_norm, log_resp = self._e_step(X)
1.18 /testbed/sklearn/mixture/base.py(294):         log_prob_norm, log_resp = self._estimate_log_prob_resp(X)
1.18 /testbed/sklearn/mixture/base.py(503):         weighted_log_prob = self._estimate_weighted_log_prob(X)
1.18 /testbed/sklearn/mixture/base.py(456):         return self._estimate_log_prob(X) + self._estimate_log_weights()
1.18 /testbed/sklearn/mixture/base.py(504):         log_prob_norm = logsumexp(weighted_log_prob, axis=1)
1.18 /testbed/sklearn/mixture/base.py(505):         with np.errstate(under='ignore'):
1.18 /testbed/sklearn/mixture/base.py(507):             log_resp = weighted_log_prob - log_prob_norm[:, np.newaxis]
1.18 /testbed/sklearn/mixture/base.py(508):         return log_prob_norm, log_resp
1.18 /testbed/sklearn/mixture/base.py(295):         return np.mean(log_prob_norm), log_resp
1.18 /testbed/sklearn/mixture/base.py(242):                 self._m_step(X, log_resp)
1.18 /testbed/sklearn/mixture/base.py(243):                 lower_bound = self._compute_lower_bound(
1.18 /testbed/sklearn/mixture/base.py(244):                     log_resp, log_prob_norm)
1.18 /testbed/sklearn/mixture/base.py(246):                 change = lower_bound - prev_lower_bound
1.18 /testbed/sklearn/mixture/base.py(247):                 self._print_verbose_msg_iter_end(n_iter, change)
1.18 /testbed/sklearn/mixture/base.py(521):         if n_iter % self.verbose_interval == 0:
1.18 /testbed/sklearn/mixture/base.py(249):                 if abs(change) < self.tol:
1.18 /testbed/sklearn/mixture/base.py(238):             for n_iter in range(1, self.max_iter + 1):
1.18 /testbed/sklearn/mixture/base.py(239):                 prev_lower_bound = lower_bound
1.18 /testbed/sklearn/mixture/base.py(241):                 log_prob_norm, log_resp = self._e_step(X)
1.18 /testbed/sklearn/mixture/base.py(294):         log_prob_norm, log_resp = self._estimate_log_prob_resp(X)
1.18 /testbed/sklearn/mixture/base.py(503):         weighted_log_prob = self._estimate_weighted_log_prob(X)
1.18 /testbed/sklearn/mixture/base.py(456):         return self._estimate_log_prob(X) + self._estimate_log_weights()
1.18 /testbed/sklearn/mixture/base.py(504):         log_prob_norm = logsumexp(weighted_log_prob, axis=1)
1.18 /testbed/sklearn/mixture/base.py(505):         with np.errstate(under='ignore'):
1.18 /testbed/sklearn/mixture/base.py(507):             log_resp = weighted_log_prob - log_prob_norm[:, np.newaxis]
1.18 /testbed/sklearn/mixture/base.py(508):         return log_prob_norm, log_resp
1.18 /testbed/sklearn/mixture/base.py(295):         return np.mean(log_prob_norm), log_resp
1.18 /testbed/sklearn/mixture/base.py(242):                 self._m_step(X, log_resp)
1.18 /testbed/sklearn/mixture/base.py(243):                 lower_bound = self._compute_lower_bound(
1.18 /testbed/sklearn/mixture/base.py(244):                     log_resp, log_prob_norm)
1.18 /testbed/sklearn/mixture/base.py(246):                 change = lower_bound - prev_lower_bound
1.18 /testbed/sklearn/mixture/base.py(247):                 self._print_verbose_msg_iter_end(n_iter, change)
1.18 /testbed/sklearn/mixture/base.py(521):         if n_iter % self.verbose_interval == 0:
1.18 /testbed/sklearn/mixture/base.py(249):                 if abs(change) < self.tol:
1.18 /testbed/sklearn/mixture/base.py(238):             for n_iter in range(1, self.max_iter + 1):
1.18 /testbed/sklearn/mixture/base.py(239):                 prev_lower_bound = lower_bound
1.18 /testbed/sklearn/mixture/base.py(241):                 log_prob_norm, log_resp = self._e_step(X)
1.18 /testbed/sklearn/mixture/base.py(294):         log_prob_norm, log_resp = self._estimate_log_prob_resp(X)
1.18 /testbed/sklearn/mixture/base.py(503):         weighted_log_prob = self._estimate_weighted_log_prob(X)
1.18 /testbed/sklearn/mixture/base.py(456):         return self._estimate_log_prob(X) + self._estimate_log_weights()
1.18 /testbed/sklearn/mixture/base.py(504):         log_prob_norm = logsumexp(weighted_log_prob, axis=1)
1.18 /testbed/sklearn/mixture/base.py(505):         with np.errstate(under='ignore'):
1.18 /testbed/sklearn/mixture/base.py(507):             log_resp = weighted_log_prob - log_prob_norm[:, np.newaxis]
1.18 /testbed/sklearn/mixture/base.py(508):         return log_prob_norm, log_resp
1.18 /testbed/sklearn/mixture/base.py(295):         return np.mean(log_prob_norm), log_resp
1.18 /testbed/sklearn/mixture/base.py(242):                 self._m_step(X, log_resp)
1.19 /testbed/sklearn/mixture/base.py(243):                 lower_bound = self._compute_lower_bound(
1.19 /testbed/sklearn/mixture/base.py(244):                     log_resp, log_prob_norm)
1.19 /testbed/sklearn/mixture/base.py(246):                 change = lower_bound - prev_lower_bound
1.19 /testbed/sklearn/mixture/base.py(247):                 self._print_verbose_msg_iter_end(n_iter, change)
1.19 /testbed/sklearn/mixture/base.py(521):         if n_iter % self.verbose_interval == 0:
1.19 /testbed/sklearn/mixture/base.py(249):                 if abs(change) < self.tol:
1.19 /testbed/sklearn/mixture/base.py(238):             for n_iter in range(1, self.max_iter + 1):
1.19 /testbed/sklearn/mixture/base.py(239):                 prev_lower_bound = lower_bound
1.19 /testbed/sklearn/mixture/base.py(241):                 log_prob_norm, log_resp = self._e_step(X)
1.19 /testbed/sklearn/mixture/base.py(294):         log_prob_norm, log_resp = self._estimate_log_prob_resp(X)
1.19 /testbed/sklearn/mixture/base.py(503):         weighted_log_prob = self._estimate_weighted_log_prob(X)
1.19 /testbed/sklearn/mixture/base.py(456):         return self._estimate_log_prob(X) + self._estimate_log_weights()
1.19 /testbed/sklearn/mixture/base.py(504):         log_prob_norm = logsumexp(weighted_log_prob, axis=1)
1.19 /testbed/sklearn/mixture/base.py(505):         with np.errstate(under='ignore'):
1.19 /testbed/sklearn/mixture/base.py(507):             log_resp = weighted_log_prob - log_prob_norm[:, np.newaxis]
1.19 /testbed/sklearn/mixture/base.py(508):         return log_prob_norm, log_resp
1.19 /testbed/sklearn/mixture/base.py(295):         return np.mean(log_prob_norm), log_resp
1.19 /testbed/sklearn/mixture/base.py(242):                 self._m_step(X, log_resp)
1.19 /testbed/sklearn/mixture/base.py(243):                 lower_bound = self._compute_lower_bound(
1.19 /testbed/sklearn/mixture/base.py(244):                     log_resp, log_prob_norm)
1.19 /testbed/sklearn/mixture/base.py(246):                 change = lower_bound - prev_lower_bound
1.19 /testbed/sklearn/mixture/base.py(247):                 self._print_verbose_msg_iter_end(n_iter, change)
1.19 /testbed/sklearn/mixture/base.py(521):         if n_iter % self.verbose_interval == 0:
1.19 /testbed/sklearn/mixture/base.py(249):                 if abs(change) < self.tol:
1.19 /testbed/sklearn/mixture/base.py(250):                     self.converged_ = True
1.19 /testbed/sklearn/mixture/base.py(251):                     break
1.19 /testbed/sklearn/mixture/base.py(253):             self._print_verbose_msg_init_end(lower_bound)
1.19 /testbed/sklearn/mixture/base.py(532):         if self.verbose == 1:
1.19 /testbed/sklearn/mixture/base.py(534):         elif self.verbose >= 2:
1.19 /testbed/sklearn/mixture/base.py(255):             if lower_bound > max_lower_bound:
1.19 /testbed/sklearn/mixture/base.py(256):                 max_lower_bound = lower_bound
1.19 /testbed/sklearn/mixture/base.py(257):                 best_params = self._get_parameters()
1.19 /testbed/sklearn/mixture/base.py(258):                 best_n_iter = n_iter
1.19 /testbed/sklearn/mixture/base.py(230):         for init in range(n_init):
1.19 /testbed/sklearn/mixture/base.py(231):             self._print_verbose_msg_init_beg(init)
1.19 /testbed/sklearn/mixture/base.py(512):         if self.verbose == 1:
1.19 /testbed/sklearn/mixture/base.py(514):         elif self.verbose >= 2:
1.19 /testbed/sklearn/mixture/base.py(233):             if do_init:
1.19 /testbed/sklearn/mixture/base.py(234):                 self._initialize_parameters(X, random_state)
1.19 /testbed/sklearn/mixture/base.py(141):         n_samples, _ = X.shape
1.19 /testbed/sklearn/mixture/base.py(143):         if self.init_params == 'kmeans':
1.19 /testbed/sklearn/mixture/base.py(144):             resp = np.zeros((n_samples, self.n_components))
1.19 /testbed/sklearn/mixture/base.py(145):             label = cluster.KMeans(n_clusters=self.n_components, n_init=1,
1.19 /testbed/sklearn/mixture/base.py(146):                                    random_state=random_state).fit(X).labels_
1.19 /testbed/sklearn/mixture/base.py(147):             resp[np.arange(n_samples), label] = 1
1.19 /testbed/sklearn/mixture/base.py(155):         self._initialize(X, resp)
1.19 /testbed/sklearn/mixture/base.py(236):             lower_bound = (-np.infty if do_init else self.lower_bound_)
1.19 /testbed/sklearn/mixture/base.py(238):             for n_iter in range(1, self.max_iter + 1):
1.19 /testbed/sklearn/mixture/base.py(239):                 prev_lower_bound = lower_bound
1.19 /testbed/sklearn/mixture/base.py(241):                 log_prob_norm, log_resp = self._e_step(X)
1.19 /testbed/sklearn/mixture/base.py(294):         log_prob_norm, log_resp = self._estimate_log_prob_resp(X)
1.19 /testbed/sklearn/mixture/base.py(503):         weighted_log_prob = self._estimate_weighted_log_prob(X)
1.19 /testbed/sklearn/mixture/base.py(456):         return self._estimate_log_prob(X) + self._estimate_log_weights()
1.19 /testbed/sklearn/mixture/base.py(504):         log_prob_norm = logsumexp(weighted_log_prob, axis=1)
1.19 /testbed/sklearn/mixture/base.py(505):         with np.errstate(under='ignore'):
1.19 /testbed/sklearn/mixture/base.py(507):             log_resp = weighted_log_prob - log_prob_norm[:, np.newaxis]
1.19 /testbed/sklearn/mixture/base.py(508):         return log_prob_norm, log_resp
1.19 /testbed/sklearn/mixture/base.py(295):         return np.mean(log_prob_norm), log_resp
1.19 /testbed/sklearn/mixture/base.py(242):                 self._m_step(X, log_resp)
1.19 /testbed/sklearn/mixture/base.py(243):                 lower_bound = self._compute_lower_bound(
1.19 /testbed/sklearn/mixture/base.py(244):                     log_resp, log_prob_norm)
1.19 /testbed/sklearn/mixture/base.py(246):                 change = lower_bound - prev_lower_bound
1.19 /testbed/sklearn/mixture/base.py(247):                 self._print_verbose_msg_iter_end(n_iter, change)
1.19 /testbed/sklearn/mixture/base.py(521):         if n_iter % self.verbose_interval == 0:
1.19 /testbed/sklearn/mixture/base.py(249):                 if abs(change) < self.tol:
1.19 /testbed/sklearn/mixture/base.py(238):             for n_iter in range(1, self.max_iter + 1):
1.19 /testbed/sklearn/mixture/base.py(239):                 prev_lower_bound = lower_bound
1.19 /testbed/sklearn/mixture/base.py(241):                 log_prob_norm, log_resp = self._e_step(X)
1.19 /testbed/sklearn/mixture/base.py(294):         log_prob_norm, log_resp = self._estimate_log_prob_resp(X)
1.19 /testbed/sklearn/mixture/base.py(503):         weighted_log_prob = self._estimate_weighted_log_prob(X)
1.19 /testbed/sklearn/mixture/base.py(456):         return self._estimate_log_prob(X) + self._estimate_log_weights()
1.19 /testbed/sklearn/mixture/base.py(504):         log_prob_norm = logsumexp(weighted_log_prob, axis=1)
1.19 /testbed/sklearn/mixture/base.py(505):         with np.errstate(under='ignore'):
1.19 /testbed/sklearn/mixture/base.py(507):             log_resp = weighted_log_prob - log_prob_norm[:, np.newaxis]
1.19 /testbed/sklearn/mixture/base.py(508):         return log_prob_norm, log_resp
1.19 /testbed/sklearn/mixture/base.py(295):         return np.mean(log_prob_norm), log_resp
1.19 /testbed/sklearn/mixture/base.py(242):                 self._m_step(X, log_resp)
1.20 /testbed/sklearn/mixture/base.py(243):                 lower_bound = self._compute_lower_bound(
1.20 /testbed/sklearn/mixture/base.py(244):                     log_resp, log_prob_norm)
1.20 /testbed/sklearn/mixture/base.py(246):                 change = lower_bound - prev_lower_bound
1.20 /testbed/sklearn/mixture/base.py(247):                 self._print_verbose_msg_iter_end(n_iter, change)
1.20 /testbed/sklearn/mixture/base.py(521):         if n_iter % self.verbose_interval == 0:
1.20 /testbed/sklearn/mixture/base.py(249):                 if abs(change) < self.tol:
1.20 /testbed/sklearn/mixture/base.py(238):             for n_iter in range(1, self.max_iter + 1):
1.20 /testbed/sklearn/mixture/base.py(239):                 prev_lower_bound = lower_bound
1.20 /testbed/sklearn/mixture/base.py(241):                 log_prob_norm, log_resp = self._e_step(X)
1.20 /testbed/sklearn/mixture/base.py(294):         log_prob_norm, log_resp = self._estimate_log_prob_resp(X)
1.20 /testbed/sklearn/mixture/base.py(503):         weighted_log_prob = self._estimate_weighted_log_prob(X)
1.20 /testbed/sklearn/mixture/base.py(456):         return self._estimate_log_prob(X) + self._estimate_log_weights()
1.20 /testbed/sklearn/mixture/base.py(504):         log_prob_norm = logsumexp(weighted_log_prob, axis=1)
1.20 /testbed/sklearn/mixture/base.py(505):         with np.errstate(under='ignore'):
1.20 /testbed/sklearn/mixture/base.py(507):             log_resp = weighted_log_prob - log_prob_norm[:, np.newaxis]
1.20 /testbed/sklearn/mixture/base.py(508):         return log_prob_norm, log_resp
1.20 /testbed/sklearn/mixture/base.py(295):         return np.mean(log_prob_norm), log_resp
1.20 /testbed/sklearn/mixture/base.py(242):                 self._m_step(X, log_resp)
1.20 /testbed/sklearn/mixture/base.py(243):                 lower_bound = self._compute_lower_bound(
1.20 /testbed/sklearn/mixture/base.py(244):                     log_resp, log_prob_norm)
1.20 /testbed/sklearn/mixture/base.py(246):                 change = lower_bound - prev_lower_bound
1.20 /testbed/sklearn/mixture/base.py(247):                 self._print_verbose_msg_iter_end(n_iter, change)
1.20 /testbed/sklearn/mixture/base.py(521):         if n_iter % self.verbose_interval == 0:
1.20 /testbed/sklearn/mixture/base.py(249):                 if abs(change) < self.tol:
1.20 /testbed/sklearn/mixture/base.py(238):             for n_iter in range(1, self.max_iter + 1):
1.20 /testbed/sklearn/mixture/base.py(239):                 prev_lower_bound = lower_bound
1.20 /testbed/sklearn/mixture/base.py(241):                 log_prob_norm, log_resp = self._e_step(X)
1.20 /testbed/sklearn/mixture/base.py(294):         log_prob_norm, log_resp = self._estimate_log_prob_resp(X)
1.20 /testbed/sklearn/mixture/base.py(503):         weighted_log_prob = self._estimate_weighted_log_prob(X)
1.20 /testbed/sklearn/mixture/base.py(456):         return self._estimate_log_prob(X) + self._estimate_log_weights()
1.20 /testbed/sklearn/mixture/base.py(504):         log_prob_norm = logsumexp(weighted_log_prob, axis=1)
1.20 /testbed/sklearn/mixture/base.py(505):         with np.errstate(under='ignore'):
1.20 /testbed/sklearn/mixture/base.py(507):             log_resp = weighted_log_prob - log_prob_norm[:, np.newaxis]
1.20 /testbed/sklearn/mixture/base.py(508):         return log_prob_norm, log_resp
1.20 /testbed/sklearn/mixture/base.py(295):         return np.mean(log_prob_norm), log_resp
1.20 /testbed/sklearn/mixture/base.py(242):                 self._m_step(X, log_resp)
1.20 /testbed/sklearn/mixture/base.py(243):                 lower_bound = self._compute_lower_bound(
1.20 /testbed/sklearn/mixture/base.py(244):                     log_resp, log_prob_norm)
1.20 /testbed/sklearn/mixture/base.py(246):                 change = lower_bound - prev_lower_bound
1.20 /testbed/sklearn/mixture/base.py(247):                 self._print_verbose_msg_iter_end(n_iter, change)
1.20 /testbed/sklearn/mixture/base.py(521):         if n_iter % self.verbose_interval == 0:
1.20 /testbed/sklearn/mixture/base.py(249):                 if abs(change) < self.tol:
1.20 /testbed/sklearn/mixture/base.py(238):             for n_iter in range(1, self.max_iter + 1):
1.20 /testbed/sklearn/mixture/base.py(239):                 prev_lower_bound = lower_bound
1.20 /testbed/sklearn/mixture/base.py(241):                 log_prob_norm, log_resp = self._e_step(X)
1.20 /testbed/sklearn/mixture/base.py(294):         log_prob_norm, log_resp = self._estimate_log_prob_resp(X)
1.20 /testbed/sklearn/mixture/base.py(503):         weighted_log_prob = self._estimate_weighted_log_prob(X)
1.20 /testbed/sklearn/mixture/base.py(456):         return self._estimate_log_prob(X) + self._estimate_log_weights()
1.20 /testbed/sklearn/mixture/base.py(504):         log_prob_norm = logsumexp(weighted_log_prob, axis=1)
1.20 /testbed/sklearn/mixture/base.py(505):         with np.errstate(under='ignore'):
1.20 /testbed/sklearn/mixture/base.py(507):             log_resp = weighted_log_prob - log_prob_norm[:, np.newaxis]
1.20 /testbed/sklearn/mixture/base.py(508):         return log_prob_norm, log_resp
1.20 /testbed/sklearn/mixture/base.py(295):         return np.mean(log_prob_norm), log_resp
1.20 /testbed/sklearn/mixture/base.py(242):                 self._m_step(X, log_resp)
1.20 /testbed/sklearn/mixture/base.py(243):                 lower_bound = self._compute_lower_bound(
1.20 /testbed/sklearn/mixture/base.py(244):                     log_resp, log_prob_norm)
1.20 /testbed/sklearn/mixture/base.py(246):                 change = lower_bound - prev_lower_bound
1.20 /testbed/sklearn/mixture/base.py(247):                 self._print_verbose_msg_iter_end(n_iter, change)
1.20 /testbed/sklearn/mixture/base.py(521):         if n_iter % self.verbose_interval == 0:
1.20 /testbed/sklearn/mixture/base.py(249):                 if abs(change) < self.tol:
1.20 /testbed/sklearn/mixture/base.py(238):             for n_iter in range(1, self.max_iter + 1):
1.20 /testbed/sklearn/mixture/base.py(239):                 prev_lower_bound = lower_bound
1.20 /testbed/sklearn/mixture/base.py(241):                 log_prob_norm, log_resp = self._e_step(X)
1.20 /testbed/sklearn/mixture/base.py(294):         log_prob_norm, log_resp = self._estimate_log_prob_resp(X)
1.20 /testbed/sklearn/mixture/base.py(503):         weighted_log_prob = self._estimate_weighted_log_prob(X)
1.20 /testbed/sklearn/mixture/base.py(456):         return self._estimate_log_prob(X) + self._estimate_log_weights()
1.20 /testbed/sklearn/mixture/base.py(504):         log_prob_norm = logsumexp(weighted_log_prob, axis=1)
1.20 /testbed/sklearn/mixture/base.py(505):         with np.errstate(under='ignore'):
1.20 /testbed/sklearn/mixture/base.py(507):             log_resp = weighted_log_prob - log_prob_norm[:, np.newaxis]
1.20 /testbed/sklearn/mixture/base.py(508):         return log_prob_norm, log_resp
1.20 /testbed/sklearn/mixture/base.py(295):         return np.mean(log_prob_norm), log_resp
1.20 /testbed/sklearn/mixture/base.py(242):                 self._m_step(X, log_resp)
1.20 /testbed/sklearn/mixture/base.py(243):                 lower_bound = self._compute_lower_bound(
1.20 /testbed/sklearn/mixture/base.py(244):                     log_resp, log_prob_norm)
1.20 /testbed/sklearn/mixture/base.py(246):                 change = lower_bound - prev_lower_bound
1.20 /testbed/sklearn/mixture/base.py(247):                 self._print_verbose_msg_iter_end(n_iter, change)
1.20 /testbed/sklearn/mixture/base.py(521):         if n_iter % self.verbose_interval == 0:
1.20 /testbed/sklearn/mixture/base.py(249):                 if abs(change) < self.tol:
1.20 /testbed/sklearn/mixture/base.py(238):             for n_iter in range(1, self.max_iter + 1):
1.20 /testbed/sklearn/mixture/base.py(239):                 prev_lower_bound = lower_bound
1.20 /testbed/sklearn/mixture/base.py(241):                 log_prob_norm, log_resp = self._e_step(X)
1.20 /testbed/sklearn/mixture/base.py(294):         log_prob_norm, log_resp = self._estimate_log_prob_resp(X)
1.20 /testbed/sklearn/mixture/base.py(503):         weighted_log_prob = self._estimate_weighted_log_prob(X)
1.20 /testbed/sklearn/mixture/base.py(456):         return self._estimate_log_prob(X) + self._estimate_log_weights()
1.20 /testbed/sklearn/mixture/base.py(504):         log_prob_norm = logsumexp(weighted_log_prob, axis=1)
1.20 /testbed/sklearn/mixture/base.py(505):         with np.errstate(under='ignore'):
1.20 /testbed/sklearn/mixture/base.py(507):             log_resp = weighted_log_prob - log_prob_norm[:, np.newaxis]
1.20 /testbed/sklearn/mixture/base.py(508):         return log_prob_norm, log_resp
1.20 /testbed/sklearn/mixture/base.py(295):         return np.mean(log_prob_norm), log_resp
1.20 /testbed/sklearn/mixture/base.py(242):                 self._m_step(X, log_resp)
1.20 /testbed/sklearn/mixture/base.py(243):                 lower_bound = self._compute_lower_bound(
1.20 /testbed/sklearn/mixture/base.py(244):                     log_resp, log_prob_norm)
1.20 /testbed/sklearn/mixture/base.py(246):                 change = lower_bound - prev_lower_bound
1.20 /testbed/sklearn/mixture/base.py(247):                 self._print_verbose_msg_iter_end(n_iter, change)
1.20 /testbed/sklearn/mixture/base.py(521):         if n_iter % self.verbose_interval == 0:
1.20 /testbed/sklearn/mixture/base.py(249):                 if abs(change) < self.tol:
1.20 /testbed/sklearn/mixture/base.py(238):             for n_iter in range(1, self.max_iter + 1):
1.20 /testbed/sklearn/mixture/base.py(239):                 prev_lower_bound = lower_bound
1.20 /testbed/sklearn/mixture/base.py(241):                 log_prob_norm, log_resp = self._e_step(X)
1.20 /testbed/sklearn/mixture/base.py(294):         log_prob_norm, log_resp = self._estimate_log_prob_resp(X)
1.20 /testbed/sklearn/mixture/base.py(503):         weighted_log_prob = self._estimate_weighted_log_prob(X)
1.20 /testbed/sklearn/mixture/base.py(456):         return self._estimate_log_prob(X) + self._estimate_log_weights()
1.20 /testbed/sklearn/mixture/base.py(504):         log_prob_norm = logsumexp(weighted_log_prob, axis=1)
1.20 /testbed/sklearn/mixture/base.py(505):         with np.errstate(under='ignore'):
1.20 /testbed/sklearn/mixture/base.py(507):             log_resp = weighted_log_prob - log_prob_norm[:, np.newaxis]
1.20 /testbed/sklearn/mixture/base.py(508):         return log_prob_norm, log_resp
1.20 /testbed/sklearn/mixture/base.py(295):         return np.mean(log_prob_norm), log_resp
1.20 /testbed/sklearn/mixture/base.py(242):                 self._m_step(X, log_resp)
1.20 /testbed/sklearn/mixture/base.py(243):                 lower_bound = self._compute_lower_bound(
1.20 /testbed/sklearn/mixture/base.py(244):                     log_resp, log_prob_norm)
1.20 /testbed/sklearn/mixture/base.py(246):                 change = lower_bound - prev_lower_bound
1.20 /testbed/sklearn/mixture/base.py(247):                 self._print_verbose_msg_iter_end(n_iter, change)
1.20 /testbed/sklearn/mixture/base.py(521):         if n_iter % self.verbose_interval == 0:
1.20 /testbed/sklearn/mixture/base.py(249):                 if abs(change) < self.tol:
1.20 /testbed/sklearn/mixture/base.py(238):             for n_iter in range(1, self.max_iter + 1):
1.20 /testbed/sklearn/mixture/base.py(239):                 prev_lower_bound = lower_bound
1.20 /testbed/sklearn/mixture/base.py(241):                 log_prob_norm, log_resp = self._e_step(X)
1.20 /testbed/sklearn/mixture/base.py(294):         log_prob_norm, log_resp = self._estimate_log_prob_resp(X)
1.20 /testbed/sklearn/mixture/base.py(503):         weighted_log_prob = self._estimate_weighted_log_prob(X)
1.20 /testbed/sklearn/mixture/base.py(456):         return self._estimate_log_prob(X) + self._estimate_log_weights()
1.20 /testbed/sklearn/mixture/base.py(504):         log_prob_norm = logsumexp(weighted_log_prob, axis=1)
1.20 /testbed/sklearn/mixture/base.py(505):         with np.errstate(under='ignore'):
1.20 /testbed/sklearn/mixture/base.py(507):             log_resp = weighted_log_prob - log_prob_norm[:, np.newaxis]
1.20 /testbed/sklearn/mixture/base.py(508):         return log_prob_norm, log_resp
1.20 /testbed/sklearn/mixture/base.py(295):         return np.mean(log_prob_norm), log_resp
1.20 /testbed/sklearn/mixture/base.py(242):                 self._m_step(X, log_resp)
1.20 /testbed/sklearn/mixture/base.py(243):                 lower_bound = self._compute_lower_bound(
1.20 /testbed/sklearn/mixture/base.py(244):                     log_resp, log_prob_norm)
1.20 /testbed/sklearn/mixture/base.py(246):                 change = lower_bound - prev_lower_bound
1.20 /testbed/sklearn/mixture/base.py(247):                 self._print_verbose_msg_iter_end(n_iter, change)
1.20 /testbed/sklearn/mixture/base.py(521):         if n_iter % self.verbose_interval == 0:
1.20 /testbed/sklearn/mixture/base.py(249):                 if abs(change) < self.tol:
1.20 /testbed/sklearn/mixture/base.py(238):             for n_iter in range(1, self.max_iter + 1):
1.20 /testbed/sklearn/mixture/base.py(239):                 prev_lower_bound = lower_bound
1.20 /testbed/sklearn/mixture/base.py(241):                 log_prob_norm, log_resp = self._e_step(X)
1.20 /testbed/sklearn/mixture/base.py(294):         log_prob_norm, log_resp = self._estimate_log_prob_resp(X)
1.20 /testbed/sklearn/mixture/base.py(503):         weighted_log_prob = self._estimate_weighted_log_prob(X)
1.20 /testbed/sklearn/mixture/base.py(456):         return self._estimate_log_prob(X) + self._estimate_log_weights()
1.20 /testbed/sklearn/mixture/base.py(504):         log_prob_norm = logsumexp(weighted_log_prob, axis=1)
1.20 /testbed/sklearn/mixture/base.py(505):         with np.errstate(under='ignore'):
1.20 /testbed/sklearn/mixture/base.py(507):             log_resp = weighted_log_prob - log_prob_norm[:, np.newaxis]
1.20 /testbed/sklearn/mixture/base.py(508):         return log_prob_norm, log_resp
1.20 /testbed/sklearn/mixture/base.py(295):         return np.mean(log_prob_norm), log_resp
1.20 /testbed/sklearn/mixture/base.py(242):                 self._m_step(X, log_resp)
1.21 /testbed/sklearn/mixture/base.py(243):                 lower_bound = self._compute_lower_bound(
1.21 /testbed/sklearn/mixture/base.py(244):                     log_resp, log_prob_norm)
1.21 /testbed/sklearn/mixture/base.py(246):                 change = lower_bound - prev_lower_bound
1.21 /testbed/sklearn/mixture/base.py(247):                 self._print_verbose_msg_iter_end(n_iter, change)
1.21 /testbed/sklearn/mixture/base.py(521):         if n_iter % self.verbose_interval == 0:
1.21 /testbed/sklearn/mixture/base.py(522):             if self.verbose == 1:
1.21 /testbed/sklearn/mixture/base.py(524):             elif self.verbose >= 2:
1.21 /testbed/sklearn/mixture/base.py(249):                 if abs(change) < self.tol:
1.21 /testbed/sklearn/mixture/base.py(250):                     self.converged_ = True
1.21 /testbed/sklearn/mixture/base.py(251):                     break
1.21 /testbed/sklearn/mixture/base.py(253):             self._print_verbose_msg_init_end(lower_bound)
1.21 /testbed/sklearn/mixture/base.py(532):         if self.verbose == 1:
1.21 /testbed/sklearn/mixture/base.py(534):         elif self.verbose >= 2:
1.21 /testbed/sklearn/mixture/base.py(255):             if lower_bound > max_lower_bound:
1.21 /testbed/sklearn/mixture/base.py(230):         for init in range(n_init):
1.21 /testbed/sklearn/mixture/base.py(263):         _, log_resp = self._e_step(X)
1.21 /testbed/sklearn/mixture/base.py(294):         log_prob_norm, log_resp = self._estimate_log_prob_resp(X)
1.21 /testbed/sklearn/mixture/base.py(503):         weighted_log_prob = self._estimate_weighted_log_prob(X)
1.21 /testbed/sklearn/mixture/base.py(456):         return self._estimate_log_prob(X) + self._estimate_log_weights()
1.21 /testbed/sklearn/mixture/base.py(504):         log_prob_norm = logsumexp(weighted_log_prob, axis=1)
1.21 /testbed/sklearn/mixture/base.py(505):         with np.errstate(under='ignore'):
1.21 /testbed/sklearn/mixture/base.py(507):             log_resp = weighted_log_prob - log_prob_norm[:, np.newaxis]
1.21 /testbed/sklearn/mixture/base.py(508):         return log_prob_norm, log_resp
1.21 /testbed/sklearn/mixture/base.py(295):         return np.mean(log_prob_norm), log_resp
1.21 /testbed/sklearn/mixture/base.py(265):         if not self.converged_:
1.21 /testbed/sklearn/mixture/base.py(272):         self._set_parameters(best_params)
1.21 /testbed/sklearn/mixture/base.py(273):         self.n_iter_ = best_n_iter
1.21 /testbed/sklearn/mixture/base.py(274):         self.lower_bound_ = max_lower_bound
1.21 /testbed/sklearn/mixture/base.py(276):         return log_resp.argmax(axis=1)
1.21 /testbed/sklearn/mixture/base.py(372):         self._check_is_fitted()
1.21 /testbed/sklearn/mixture/base.py(373):         X = _check_X(X, None, self.means_.shape[1])
1.21 /testbed/sklearn/mixture/base.py(51):     X = check_array(X, dtype=[np.float64, np.float32],
1.21 /testbed/sklearn/mixture/base.py(52):                     ensure_min_samples=ensure_min_samples)
1.21 /testbed/sklearn/mixture/base.py(53):     if n_components is not None and X.shape[0] < n_components:
1.21 /testbed/sklearn/mixture/base.py(57):     if n_features is not None and X.shape[1] != n_features:
1.21 /testbed/sklearn/mixture/base.py(61):     return X
1.21 /testbed/sklearn/mixture/base.py(374):         return self._estimate_weighted_log_prob(X).argmax(axis=1)
1.21 /testbed/sklearn/mixture/base.py(456):         return self._estimate_log_prob(X) + self._estimate_log_weights()
=========================== short test summary info ============================
FAILED sklearn/tests/test_coverup_scikit-learn__scikit-learn-13142.py::test_gaussian_mixture_fit_predict_discrepancy
============================== 1 failed in 1.16s ===============================
+ cat coverage.cover
{"/testbed/sklearn/mixture/base.py": {"7": 1, "8": 1, "9": 1, "11": 1, "13": 1, "14": 1, "15": 1, "16": 1, "17": 1, "18": 1, "21": 1, "38": 1, "64": 2, "32": 0, "33": 0, "34": 0, "35": 0, "51": 2, "52": 2, "53": 2, "54": 0, "56": 0, "57": 2, "58": 0, "60": 0, "61": 2, "71": 1, "85": 1, "121": 1, "131": 1, "157": 1, "169": 1, "194": 1, "278": 1, "297": 1, "311": 1, "315": 1, "319": 1, "323": 1, "342": 1, "358": 1, "376": 1, "396": 1, "445": 1, "458": 1, "468": 1, "484": 1, "510": 1, "519": 1, "530": 1, "74": 1, "75": 1, "76": 1, "77": 1, "78": 1, "79": 1, "80": 1, "81": 1, "82": 1, "83": 1, "92": 1, "93": 0, "95": 0, "97": 1, "98": 0, "100": 0, "102": 1, "103": 0, "105": 0, "107": 1, "108": 0, "110": 0, "112": 1, "113": 0, "116": 0, "119": 1, "129": 0, "141": 5, "143": 5, "144": 5, "145": 5, "146": 5, "147": 5, "148": 0, "149": 0, "150": 0, "152": 0, "153": 0, "155": 5, "167": 0, "191": 0, "192": 0, "217": 1, "218": 1, "221": 1, "222": 1, "224": 1, "225": 1, "227": 1, "229": 1, "230": 6, "231": 5, "233": 5, "234": 5, "236": 5, "238": 82, "239": 82, "241": 82, "242": 82, "243": 82, "244": 82, "246": 82, "247": 82, "249": 82, "250": 5, "251": 5, "253": 5, "255": 5, "256": 3, "257": 3, "258": 3, "263": 1, "265": 1, "266": 0, "270": 0, "272": 1, "273": 1, "274": 1, "276": 1, "294": 83, "295": 83, "309": 0, "313": 0, "317": 0, "321": 0, "337": 0, "338": 0, "340": 0, "356": 0, "372": 1, "373": 1, "374": 1, "391": 0, "392": 0, "393": 0, "394": 0, "413": 0, "415": 0, "416": 0, "417": 0, "418": 0, "420": 0, "421": 0, "422": 0, "424": 0, "425": 0, "426": 0, "427": 0, "428": 0, "429": 0, "430": 0, "431": 0, "432": 0, "433": 0, "435": 0, "436": 0, "437": 0, "438": 0, "440": 0, "441": 0, "443": 0, "456": 84, "466": 0, "482": 0, "503": 83, "504": 83, "505": 83, "507": 83, "508": 83, "512": 5, "513": 0, "514": 5, "515": 0, "516": 0, "517": 0, "521": 82, "522": 6, "523": 0, "524": 6, "525": 0, "526": 0, "527": 0, "528": 0, "532": 5, "533": 0, "534": 5, "535": 0, "536": 0}}
+ git checkout 1c8668b0a021832386470ddf740d834e02c66f69
Note: switching to '1c8668b0a021832386470ddf740d834e02c66f69'.

You are in 'detached HEAD' state. You can look around, make experimental
changes and commit them, and you can discard any commits you make in this
state without impacting any branches by switching back to a branch.

If you want to create a new branch to retain commits you create, you may
do so (now or later) by using -c with the switch command. Example:

  git switch -c <new-branch-name>

Or undo this operation with:

  git switch -

Turn off this advice by setting config variable advice.detachedHead to false

HEAD is now at 1c8668b0a0 DOC what's new: Fix class name
+ git apply /root/pre_state.patch
error: unrecognized input
